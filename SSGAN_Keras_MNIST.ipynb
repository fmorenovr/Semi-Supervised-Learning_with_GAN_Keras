{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a09e486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5077c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "722a62c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e479c473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "846f9e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q -U tensorflow-addons==0.11.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1847d7",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c569491",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import verifyDir\n",
    "from utils.networks import normalize, unnormalize, plot_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01ef1bc",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9387fc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.MNIST import load_real_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcc42eb",
   "metadata": {},
   "source": [
    "### Discriminator & Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a66d0498",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.MNIST import define_discriminator\n",
    "from utils.MNIST import define_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7417a6ca",
   "metadata": {},
   "source": [
    "### Semi-Supervised GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a00d8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.networks import define_gan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b1de47",
   "metadata": {},
   "source": [
    "### Selecting sub-set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6eb80e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.networks import select_supervised_samples, generate_real_samples\n",
    "from utils.networks import generate_fake_samples, generate_latent_points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3069f9b9",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e26395d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the generator and discriminator\n",
    "def train(generator_model, unsupervised_model, supervised_model, gan_model, dataset, dataset_test, \n",
    "          latent_dim=100, n_epochs=20, n_batch=100, percent_samples=1.0):\n",
    "    \n",
    "    # select supervised dataset\n",
    "    X_sup, y_sup = select_supervised_samples(dataset, percent_samples=percent_samples)\n",
    "    print(\"Sup samples:\", X_sup.shape, y_sup.shape)\n",
    "    \n",
    "    # calculate the number of batches per training epoch\n",
    "    bat_per_epo = int(dataset[0].shape[0] / n_batch)\n",
    "    \n",
    "    # calculate the number of training iterations\n",
    "    n_steps = bat_per_epo * n_epochs\n",
    "    print('n_epochs=%d, n_batch=%d, batch/epoch=%d, steps=%d' % (n_epochs, n_batch, bat_per_epo, n_steps))\n",
    "    \n",
    "    # manually enumerate epochs\n",
    "    f_history = open(f\"{LOG_PATH}SSL_GAN.csv\", \"w\")\n",
    "    f_history.write(\"step,generator_loss,unsupervised_real_loss,unsupervised_fake_loss,supervised_loss,supervised_acc,train_loss,test_loss,train_acc,test_acc\\n\")\n",
    "    for step in range(1,n_steps+1):\n",
    "#         t_start = time.time()\n",
    "        # update supervised discriminator (c)\n",
    "        [Xsup_real, ysup_real], _ = generate_real_samples([X_sup, y_sup], n_batch)\n",
    "        c_loss, c_acc = supervised_model.train_on_batch(Xsup_real, ysup_real)\n",
    "        \n",
    "        # update unsupervised discriminator (d)\n",
    "        [X_real, _], y_real = generate_real_samples(dataset, n_batch)\n",
    "        d_loss1 = unsupervised_model.train_on_batch(X_real, y_real)\n",
    "        \n",
    "        X_fake, y_fake = generate_fake_samples(generator_model, latent_dim, n_batch)\n",
    "        d_loss2 = unsupervised_model.train_on_batch(X_fake, y_fake)\n",
    "        \n",
    "        # update generator (g)\n",
    "        X_gan, y_gan = generate_latent_points(latent_dim, n_batch), np.ones((n_batch, 1))\n",
    "        g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "#         t_total = (time.time() - t_start)\n",
    "        # summarize loss on this batch\n",
    "    \n",
    "        # Train - Test\n",
    "        X_train, y_train = dataset\n",
    "        loss_train, acc_train = supervised_model.evaluate(X_train, y_train, verbose=0)\n",
    "\n",
    "        # evaluate the test classifier model\n",
    "        X_test, y_test = dataset_test\n",
    "        loss_test, acc_test = supervised_model.evaluate(X_test, y_test, verbose=0)\n",
    "        \n",
    "        # Log\n",
    "        print('step: %d | Train: G_Loss: %.3f, ' \\\n",
    "              'D_unsup_loss_real: %.3f, D_unsup_loss_fake: %.3f, ' \\\n",
    "              'D_sup_loss: %.3f, D_sup_acc: %.2f ' \\\n",
    "              'Train acc: %.3f Test acc: %.3f ' %(step, g_loss,\n",
    "                                                d_loss1, d_loss2,\n",
    "                                                c_loss, c_acc*100,\n",
    "                                                 acc_train*100, acc_test*100))#, end = '\\r')\n",
    "        f_history.write(f\"{step},{g_loss},{d_loss1},{d_loss2},{c_loss},{c_acc*100},{loss_train},{loss_test},{acc_train*100},{acc_test*100}\\n\")\n",
    "        \n",
    "        if step==1:\n",
    "            plot_data(X_test, 0, \"test\", grid_size = [10, 10], OUT_PATH=LOG_PATH, gray=True)\n",
    "        # evaluate the model performance every so often\n",
    "        if (step) % (100) == 0 or step == 1:\n",
    "            #summarize_performance(step, generator_model, supervised_model, latent_dim, dataset, dataset_test)\n",
    "            # prepare fake examples\n",
    "            X_generated, _ = generate_fake_samples(generator_model, latent_dim, n_samples=100)\n",
    "            # scale from [-1,1] to [0,1]\n",
    "            plot_data(X_generated, step, \"generated\", grid_size = [10, 10], OUT_PATH=LOG_PATH, gray=True)\n",
    "            \n",
    "            X_train, y_train = dataset\n",
    "            _, acc = supervised_model.evaluate(X_train, y_train, verbose=0)\n",
    "            print('Train Classifier Accuracy: %.3f%%\\n' % (acc * 100))\n",
    "            \n",
    "            # evaluate the test classifier model\n",
    "            X_test, y_test = dataset_test\n",
    "            _, acc = supervised_model.evaluate(X_test, y_test, verbose=0)\n",
    "            print('Test Classifier Accuracy: %.3f%%\\n' % (acc * 100))\n",
    "            # save the generator model\n",
    "            filename2 = f'{LOG_PATH}generator_model_{step}.h5'\n",
    "            generator_model.save(filename2)\n",
    "            # save the classifier model\n",
    "            filename3 = f'{LOG_PATH}supervised_model_{step}.h5'\n",
    "            supervised_model.save(filename3)\n",
    "            print('>Saving models Generator: %s and Supervised: %s' % (filename2, filename3))\n",
    "    \n",
    "    f_history.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2dd749",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "675d0275",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (28, 28, 1)\n",
    "num_classes = 10\n",
    "\n",
    "# learning rate\n",
    "learning_rate = 2e-4\n",
    "\n",
    "labeled_rate = 1/600\n",
    "labeled_samples = 60000*labeled_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50879c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_PATH = f\"Logs/SSGAN_MNIST/Classifier_{int(labeled_samples)}/\"\n",
    "verifyDir(LOG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad872b8",
   "metadata": {},
   "source": [
    "### Creating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad0b4edd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create the discriminator models\n",
    "unsupervised_model, supervised_model = define_discriminator(in_shape=input_shape, n_classes=num_classes, learning_rate = learning_rate)\n",
    "# create the generator\n",
    "generator_model = define_generator(latent_dim=100)\n",
    "# create the gan\n",
    "gan_model = define_gan(generator_model, unsupervised_model, learning_rate = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15bff699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 14, 14, 128)       1280      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 7, 7, 128)         147584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 128)         147584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                20490     \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 316,938\n",
      "Trainable params: 0\n",
      "Non-trainable params: 316,938\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "supervised_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d77be472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 14, 14, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 28, 28, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 1)         6273      \n",
      "_________________________________________________________________\n",
      "model_1 (Model)              (None, 1)                 316938    \n",
      "=================================================================\n",
      "Total params: 1,481,227\n",
      "Trainable params: 1,164,289\n",
      "Non-trainable params: 316,938\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2003354",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e690741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1) (60000,) (10000, 28, 28, 1) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# load image data\n",
    "dataset, dataset_test = load_real_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991f6198",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d0e06c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sup samples: (94, 28, 28, 1) (94,)\n",
      "n_epochs=20, n_batch=128, batch/epoch=468, steps=9360\n",
      "step: 1 | Train: G_Loss: 0.095, D_unsup_loss_real: 0.096, D_unsup_loss_fake: 2.399, D_sup_loss: 2.300, D_sup_acc: 8.59 Train acc: 8.728 Test acc: 8.250 \n",
      "Train Classifier Accuracy: 8.728%\n",
      "\n",
      "Test Classifier Accuracy: 8.250%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_1.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_1.h5\n",
      "step: 2 | Train: G_Loss: 0.096, D_unsup_loss_real: 0.094, D_unsup_loss_fake: 2.397, D_sup_loss: 2.293, D_sup_acc: 8.28 Train acc: 10.947 Test acc: 10.590 \n",
      "step: 3 | Train: G_Loss: 0.096, D_unsup_loss_real: 0.092, D_unsup_loss_fake: 2.396, D_sup_loss: 2.285, D_sup_acc: 10.64 Train acc: 14.052 Test acc: 13.840 \n",
      "step: 4 | Train: G_Loss: 0.096, D_unsup_loss_real: 0.091, D_unsup_loss_fake: 2.396, D_sup_loss: 2.271, D_sup_acc: 13.82 Train acc: 19.248 Test acc: 19.900 \n",
      "step: 5 | Train: G_Loss: 0.096, D_unsup_loss_real: 0.089, D_unsup_loss_fake: 2.396, D_sup_loss: 2.259, D_sup_acc: 19.94 Train acc: 11.240 Test acc: 11.480 \n",
      "step: 6 | Train: G_Loss: 0.096, D_unsup_loss_real: 0.089, D_unsup_loss_fake: 2.396, D_sup_loss: 2.244, D_sup_acc: 11.51 Train acc: 30.323 Test acc: 30.920 \n",
      "step: 7 | Train: G_Loss: 0.096, D_unsup_loss_real: 0.089, D_unsup_loss_fake: 2.396, D_sup_loss: 2.225, D_sup_acc: 30.95 Train acc: 42.382 Test acc: 43.270 \n",
      "step: 8 | Train: G_Loss: 0.097, D_unsup_loss_real: 0.088, D_unsup_loss_fake: 2.394, D_sup_loss: 2.198, D_sup_acc: 43.39 Train acc: 31.827 Test acc: 32.870 \n",
      "step: 9 | Train: G_Loss: 0.098, D_unsup_loss_real: 0.089, D_unsup_loss_fake: 2.390, D_sup_loss: 2.170, D_sup_acc: 33.06 Train acc: 33.005 Test acc: 34.210 \n",
      "step: 10 | Train: G_Loss: 0.099, D_unsup_loss_real: 0.091, D_unsup_loss_fake: 2.379, D_sup_loss: 2.135, D_sup_acc: 34.36 Train acc: 34.565 Test acc: 35.860 \n",
      "step: 11 | Train: G_Loss: 0.099, D_unsup_loss_real: 0.092, D_unsup_loss_fake: 2.372, D_sup_loss: 2.092, D_sup_acc: 35.85 Train acc: 50.887 Test acc: 52.300 \n",
      "step: 12 | Train: G_Loss: 0.100, D_unsup_loss_real: 0.094, D_unsup_loss_fake: 2.365, D_sup_loss: 2.034, D_sup_acc: 52.36 Train acc: 52.637 Test acc: 53.720 \n",
      "step: 13 | Train: G_Loss: 0.100, D_unsup_loss_real: 0.089, D_unsup_loss_fake: 2.363, D_sup_loss: 1.972, D_sup_acc: 53.73 Train acc: 56.357 Test acc: 57.520 \n",
      "step: 14 | Train: G_Loss: 0.101, D_unsup_loss_real: 0.085, D_unsup_loss_fake: 2.358, D_sup_loss: 1.904, D_sup_acc: 57.65 Train acc: 57.662 Test acc: 57.980 \n",
      "step: 15 | Train: G_Loss: 0.104, D_unsup_loss_real: 0.074, D_unsup_loss_fake: 2.344, D_sup_loss: 1.798, D_sup_acc: 58.16 Train acc: 40.543 Test acc: 43.520 \n",
      "step: 16 | Train: G_Loss: 0.108, D_unsup_loss_real: 0.069, D_unsup_loss_fake: 2.317, D_sup_loss: 1.748, D_sup_acc: 43.62 Train acc: 64.792 Test acc: 65.860 \n",
      "step: 17 | Train: G_Loss: 0.111, D_unsup_loss_real: 0.062, D_unsup_loss_fake: 2.287, D_sup_loss: 1.635, D_sup_acc: 65.83 Train acc: 64.838 Test acc: 64.960 \n",
      "step: 18 | Train: G_Loss: 0.116, D_unsup_loss_real: 0.053, D_unsup_loss_fake: 2.261, D_sup_loss: 1.537, D_sup_acc: 65.11 Train acc: 58.127 Test acc: 58.680 \n",
      "step: 19 | Train: G_Loss: 0.122, D_unsup_loss_real: 0.049, D_unsup_loss_fake: 2.249, D_sup_loss: 1.460, D_sup_acc: 58.85 Train acc: 60.130 Test acc: 60.880 \n",
      "step: 20 | Train: G_Loss: 0.136, D_unsup_loss_real: 0.050, D_unsup_loss_fake: 2.225, D_sup_loss: 1.342, D_sup_acc: 61.04 Train acc: 54.368 Test acc: 55.350 \n",
      "step: 21 | Train: G_Loss: 0.154, D_unsup_loss_real: 0.063, D_unsup_loss_fake: 2.159, D_sup_loss: 1.337, D_sup_acc: 55.41 Train acc: 54.545 Test acc: 55.580 \n",
      "step: 22 | Train: G_Loss: 0.165, D_unsup_loss_real: 0.090, D_unsup_loss_fake: 2.047, D_sup_loss: 1.300, D_sup_acc: 55.71 Train acc: 60.465 Test acc: 61.770 \n",
      "step: 23 | Train: G_Loss: 0.180, D_unsup_loss_real: 0.090, D_unsup_loss_fake: 1.944, D_sup_loss: 1.233, D_sup_acc: 61.94 Train acc: 60.338 Test acc: 61.290 \n",
      "step: 24 | Train: G_Loss: 0.200, D_unsup_loss_real: 0.069, D_unsup_loss_fake: 1.859, D_sup_loss: 1.194, D_sup_acc: 61.42 Train acc: 60.320 Test acc: 61.350 \n",
      "step: 25 | Train: G_Loss: 0.241, D_unsup_loss_real: 0.081, D_unsup_loss_fake: 1.733, D_sup_loss: 1.179, D_sup_acc: 61.49 Train acc: 70.293 Test acc: 70.690 \n",
      "step: 26 | Train: G_Loss: 0.298, D_unsup_loss_real: 0.078, D_unsup_loss_fake: 1.624, D_sup_loss: 0.988, D_sup_acc: 70.80 Train acc: 69.753 Test acc: 70.460 \n",
      "step: 27 | Train: G_Loss: 0.391, D_unsup_loss_real: 0.100, D_unsup_loss_fake: 1.531, D_sup_loss: 0.949, D_sup_acc: 70.54 Train acc: 72.047 Test acc: 73.000 \n",
      "step: 28 | Train: G_Loss: 0.473, D_unsup_loss_real: 0.150, D_unsup_loss_fake: 1.408, D_sup_loss: 0.925, D_sup_acc: 73.15 Train acc: 65.382 Test acc: 66.640 \n",
      "step: 29 | Train: G_Loss: 0.592, D_unsup_loss_real: 0.140, D_unsup_loss_fake: 1.256, D_sup_loss: 0.946, D_sup_acc: 66.87 Train acc: 65.240 Test acc: 66.190 \n",
      "step: 30 | Train: G_Loss: 0.644, D_unsup_loss_real: 0.178, D_unsup_loss_fake: 1.161, D_sup_loss: 1.016, D_sup_acc: 66.32 Train acc: 67.507 Test acc: 68.140 \n",
      "step: 31 | Train: G_Loss: 0.852, D_unsup_loss_real: 0.167, D_unsup_loss_fake: 1.152, D_sup_loss: 0.896, D_sup_acc: 68.28 Train acc: 73.753 Test acc: 74.910 \n",
      "step: 32 | Train: G_Loss: 0.763, D_unsup_loss_real: 0.262, D_unsup_loss_fake: 1.040, D_sup_loss: 0.825, D_sup_acc: 74.98 Train acc: 68.460 Test acc: 68.770 \n",
      "step: 33 | Train: G_Loss: 1.608, D_unsup_loss_real: 0.108, D_unsup_loss_fake: 1.064, D_sup_loss: 0.913, D_sup_acc: 68.93 Train acc: 70.965 Test acc: 72.390 \n",
      "step: 34 | Train: G_Loss: 0.624, D_unsup_loss_real: 0.716, D_unsup_loss_fake: 1.130, D_sup_loss: 0.858, D_sup_acc: 72.51 Train acc: 63.808 Test acc: 64.660 \n",
      "step: 35 | Train: G_Loss: 1.006, D_unsup_loss_real: 0.043, D_unsup_loss_fake: 1.062, D_sup_loss: 1.148, D_sup_acc: 64.80 Train acc: 73.653 Test acc: 74.160 \n",
      "step: 36 | Train: G_Loss: 1.818, D_unsup_loss_real: 0.195, D_unsup_loss_fake: 1.035, D_sup_loss: 0.786, D_sup_acc: 74.26 Train acc: 75.212 Test acc: 76.330 \n",
      "step: 37 | Train: G_Loss: 0.747, D_unsup_loss_real: 1.012, D_unsup_loss_fake: 1.548, D_sup_loss: 0.756, D_sup_acc: 76.43 Train acc: 73.865 Test acc: 74.680 \n",
      "step: 38 | Train: G_Loss: 1.651, D_unsup_loss_real: 0.143, D_unsup_loss_fake: 0.938, D_sup_loss: 0.754, D_sup_acc: 74.92 Train acc: 73.678 Test acc: 74.350 \n",
      "step: 39 | Train: G_Loss: 0.999, D_unsup_loss_real: 0.685, D_unsup_loss_fake: 1.119, D_sup_loss: 0.753, D_sup_acc: 74.51 Train acc: 74.452 Test acc: 75.570 \n",
      "step: 40 | Train: G_Loss: 1.143, D_unsup_loss_real: 0.349, D_unsup_loss_fake: 1.063, D_sup_loss: 0.782, D_sup_acc: 75.77 Train acc: 73.253 Test acc: 73.950 \n",
      "step: 41 | Train: G_Loss: 1.242, D_unsup_loss_real: 0.368, D_unsup_loss_fake: 0.968, D_sup_loss: 0.809, D_sup_acc: 74.04 Train acc: 74.438 Test acc: 75.470 \n",
      "step: 42 | Train: G_Loss: 1.259, D_unsup_loss_real: 0.462, D_unsup_loss_fake: 1.250, D_sup_loss: 0.799, D_sup_acc: 75.60 Train acc: 75.843 Test acc: 76.820 \n",
      "step: 43 | Train: G_Loss: 1.228, D_unsup_loss_real: 0.667, D_unsup_loss_fake: 1.505, D_sup_loss: 0.753, D_sup_acc: 77.01 Train acc: 75.865 Test acc: 76.680 \n",
      "step: 44 | Train: G_Loss: 1.210, D_unsup_loss_real: 0.714, D_unsup_loss_fake: 1.438, D_sup_loss: 0.725, D_sup_acc: 76.89 Train acc: 77.025 Test acc: 77.840 \n",
      "step: 45 | Train: G_Loss: 0.938, D_unsup_loss_real: 0.918, D_unsup_loss_fake: 1.498, D_sup_loss: 0.706, D_sup_acc: 78.02 Train acc: 74.717 Test acc: 75.770 \n",
      "step: 46 | Train: G_Loss: 1.153, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 1.385, D_sup_loss: 0.782, D_sup_acc: 75.99 Train acc: 74.063 Test acc: 74.900 \n",
      "step: 47 | Train: G_Loss: 1.139, D_unsup_loss_real: 0.774, D_unsup_loss_fake: 1.639, D_sup_loss: 0.791, D_sup_acc: 75.13 Train acc: 74.612 Test acc: 75.690 \n",
      "step: 48 | Train: G_Loss: 1.091, D_unsup_loss_real: 0.887, D_unsup_loss_fake: 1.312, D_sup_loss: 0.774, D_sup_acc: 75.92 Train acc: 75.798 Test acc: 76.410 \n",
      "step: 49 | Train: G_Loss: 0.989, D_unsup_loss_real: 0.884, D_unsup_loss_fake: 1.258, D_sup_loss: 0.740, D_sup_acc: 76.61 Train acc: 76.287 Test acc: 76.850 \n",
      "step: 50 | Train: G_Loss: 0.982, D_unsup_loss_real: 0.904, D_unsup_loss_fake: 1.436, D_sup_loss: 0.723, D_sup_acc: 77.06 Train acc: 75.862 Test acc: 76.760 \n",
      "step: 51 | Train: G_Loss: 0.900, D_unsup_loss_real: 0.952, D_unsup_loss_fake: 1.334, D_sup_loss: 0.741, D_sup_acc: 76.96 Train acc: 76.318 Test acc: 77.340 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 52 | Train: G_Loss: 0.846, D_unsup_loss_real: 0.893, D_unsup_loss_fake: 1.240, D_sup_loss: 0.735, D_sup_acc: 77.57 Train acc: 75.312 Test acc: 76.620 \n",
      "step: 53 | Train: G_Loss: 0.844, D_unsup_loss_real: 0.850, D_unsup_loss_fake: 1.453, D_sup_loss: 0.766, D_sup_acc: 76.83 Train acc: 75.478 Test acc: 76.090 \n",
      "step: 54 | Train: G_Loss: 0.943, D_unsup_loss_real: 0.823, D_unsup_loss_fake: 1.181, D_sup_loss: 0.744, D_sup_acc: 76.32 Train acc: 76.342 Test acc: 76.950 \n",
      "step: 55 | Train: G_Loss: 0.902, D_unsup_loss_real: 0.921, D_unsup_loss_fake: 1.224, D_sup_loss: 0.723, D_sup_acc: 77.17 Train acc: 76.602 Test acc: 77.160 \n",
      "step: 56 | Train: G_Loss: 0.777, D_unsup_loss_real: 1.014, D_unsup_loss_fake: 1.187, D_sup_loss: 0.720, D_sup_acc: 77.40 Train acc: 76.740 Test acc: 77.420 \n",
      "step: 57 | Train: G_Loss: 0.921, D_unsup_loss_real: 0.787, D_unsup_loss_fake: 1.240, D_sup_loss: 0.709, D_sup_acc: 77.68 Train acc: 76.893 Test acc: 77.810 \n",
      "step: 58 | Train: G_Loss: 0.815, D_unsup_loss_real: 0.905, D_unsup_loss_fake: 1.200, D_sup_loss: 0.715, D_sup_acc: 78.00 Train acc: 76.875 Test acc: 77.890 \n",
      "step: 59 | Train: G_Loss: 0.744, D_unsup_loss_real: 0.931, D_unsup_loss_fake: 1.106, D_sup_loss: 0.720, D_sup_acc: 78.12 Train acc: 76.343 Test acc: 77.260 \n",
      "step: 60 | Train: G_Loss: 0.783, D_unsup_loss_real: 0.816, D_unsup_loss_fake: 1.114, D_sup_loss: 0.731, D_sup_acc: 77.51 Train acc: 76.257 Test acc: 77.110 \n",
      "step: 61 | Train: G_Loss: 0.819, D_unsup_loss_real: 0.803, D_unsup_loss_fake: 1.088, D_sup_loss: 0.736, D_sup_acc: 77.33 Train acc: 76.558 Test acc: 77.230 \n",
      "step: 62 | Train: G_Loss: 0.806, D_unsup_loss_real: 0.845, D_unsup_loss_fake: 1.111, D_sup_loss: 0.731, D_sup_acc: 77.44 Train acc: 76.363 Test acc: 77.260 \n",
      "step: 63 | Train: G_Loss: 0.746, D_unsup_loss_real: 0.889, D_unsup_loss_fake: 1.150, D_sup_loss: 0.736, D_sup_acc: 77.43 Train acc: 76.490 Test acc: 77.180 \n",
      "step: 64 | Train: G_Loss: 0.800, D_unsup_loss_real: 0.777, D_unsup_loss_fake: 1.058, D_sup_loss: 0.737, D_sup_acc: 77.40 Train acc: 76.615 Test acc: 77.560 \n",
      "step: 65 | Train: G_Loss: 0.794, D_unsup_loss_real: 0.895, D_unsup_loss_fake: 1.127, D_sup_loss: 0.734, D_sup_acc: 77.83 Train acc: 76.625 Test acc: 77.570 \n",
      "step: 66 | Train: G_Loss: 0.766, D_unsup_loss_real: 0.806, D_unsup_loss_fake: 1.029, D_sup_loss: 0.742, D_sup_acc: 77.76 Train acc: 77.278 Test acc: 78.280 \n",
      "step: 67 | Train: G_Loss: 0.749, D_unsup_loss_real: 0.783, D_unsup_loss_fake: 1.053, D_sup_loss: 0.737, D_sup_acc: 78.51 Train acc: 77.323 Test acc: 78.450 \n",
      "step: 68 | Train: G_Loss: 0.783, D_unsup_loss_real: 0.822, D_unsup_loss_fake: 1.063, D_sup_loss: 0.733, D_sup_acc: 78.63 Train acc: 76.938 Test acc: 77.930 \n",
      "step: 69 | Train: G_Loss: 0.762, D_unsup_loss_real: 0.889, D_unsup_loss_fake: 1.145, D_sup_loss: 0.738, D_sup_acc: 78.10 Train acc: 76.975 Test acc: 77.980 \n",
      "step: 70 | Train: G_Loss: 0.702, D_unsup_loss_real: 0.876, D_unsup_loss_fake: 1.109, D_sup_loss: 0.738, D_sup_acc: 78.19 Train acc: 77.128 Test acc: 78.170 \n",
      "step: 71 | Train: G_Loss: 0.767, D_unsup_loss_real: 0.818, D_unsup_loss_fake: 1.048, D_sup_loss: 0.739, D_sup_acc: 78.39 Train acc: 76.603 Test acc: 77.550 \n",
      "step: 72 | Train: G_Loss: 0.782, D_unsup_loss_real: 0.846, D_unsup_loss_fake: 1.135, D_sup_loss: 0.747, D_sup_acc: 77.75 Train acc: 77.045 Test acc: 77.930 \n",
      "step: 73 | Train: G_Loss: 0.762, D_unsup_loss_real: 0.778, D_unsup_loss_fake: 1.056, D_sup_loss: 0.744, D_sup_acc: 78.14 Train acc: 76.697 Test acc: 77.620 \n",
      "step: 74 | Train: G_Loss: 0.803, D_unsup_loss_real: 0.766, D_unsup_loss_fake: 1.010, D_sup_loss: 0.745, D_sup_acc: 77.84 Train acc: 76.463 Test acc: 77.330 \n",
      "step: 75 | Train: G_Loss: 0.754, D_unsup_loss_real: 0.847, D_unsup_loss_fake: 1.061, D_sup_loss: 0.750, D_sup_acc: 77.57 Train acc: 76.140 Test acc: 77.240 \n",
      "step: 76 | Train: G_Loss: 0.831, D_unsup_loss_real: 0.757, D_unsup_loss_fake: 1.026, D_sup_loss: 0.756, D_sup_acc: 77.45 Train acc: 76.337 Test acc: 77.500 \n",
      "step: 77 | Train: G_Loss: 0.741, D_unsup_loss_real: 0.880, D_unsup_loss_fake: 1.034, D_sup_loss: 0.755, D_sup_acc: 77.73 Train acc: 76.903 Test acc: 77.890 \n",
      "step: 78 | Train: G_Loss: 0.765, D_unsup_loss_real: 0.805, D_unsup_loss_fake: 0.991, D_sup_loss: 0.747, D_sup_acc: 78.08 Train acc: 77.227 Test acc: 78.050 \n",
      "step: 79 | Train: G_Loss: 0.749, D_unsup_loss_real: 0.789, D_unsup_loss_fake: 1.008, D_sup_loss: 0.744, D_sup_acc: 78.29 Train acc: 76.852 Test acc: 77.840 \n",
      "step: 80 | Train: G_Loss: 0.755, D_unsup_loss_real: 0.815, D_unsup_loss_fake: 1.014, D_sup_loss: 0.746, D_sup_acc: 78.07 Train acc: 76.882 Test acc: 78.190 \n",
      "step: 81 | Train: G_Loss: 0.731, D_unsup_loss_real: 0.798, D_unsup_loss_fake: 1.027, D_sup_loss: 0.747, D_sup_acc: 78.45 Train acc: 77.050 Test acc: 78.010 \n",
      "step: 82 | Train: G_Loss: 0.760, D_unsup_loss_real: 0.789, D_unsup_loss_fake: 1.053, D_sup_loss: 0.750, D_sup_acc: 78.28 Train acc: 76.755 Test acc: 77.810 \n",
      "step: 83 | Train: G_Loss: 0.700, D_unsup_loss_real: 0.895, D_unsup_loss_fake: 1.014, D_sup_loss: 0.758, D_sup_acc: 78.02 Train acc: 76.420 Test acc: 77.500 \n",
      "step: 84 | Train: G_Loss: 0.791, D_unsup_loss_real: 0.821, D_unsup_loss_fake: 1.032, D_sup_loss: 0.763, D_sup_acc: 77.74 Train acc: 76.720 Test acc: 77.830 \n",
      "step: 85 | Train: G_Loss: 0.795, D_unsup_loss_real: 0.706, D_unsup_loss_fake: 1.010, D_sup_loss: 0.757, D_sup_acc: 78.07 Train acc: 76.425 Test acc: 77.410 \n",
      "step: 86 | Train: G_Loss: 0.847, D_unsup_loss_real: 0.762, D_unsup_loss_fake: 0.971, D_sup_loss: 0.757, D_sup_acc: 77.64 Train acc: 76.268 Test acc: 77.280 \n",
      "step: 87 | Train: G_Loss: 0.752, D_unsup_loss_real: 0.808, D_unsup_loss_fake: 0.940, D_sup_loss: 0.758, D_sup_acc: 77.52 Train acc: 76.997 Test acc: 77.960 \n",
      "step: 88 | Train: G_Loss: 0.762, D_unsup_loss_real: 0.764, D_unsup_loss_fake: 0.988, D_sup_loss: 0.748, D_sup_acc: 78.20 Train acc: 77.345 Test acc: 78.290 \n",
      "step: 89 | Train: G_Loss: 0.726, D_unsup_loss_real: 0.821, D_unsup_loss_fake: 0.940, D_sup_loss: 0.747, D_sup_acc: 78.52 Train acc: 77.200 Test acc: 78.340 \n",
      "step: 90 | Train: G_Loss: 0.772, D_unsup_loss_real: 0.775, D_unsup_loss_fake: 0.950, D_sup_loss: 0.744, D_sup_acc: 78.56 Train acc: 77.093 Test acc: 77.970 \n",
      "step: 91 | Train: G_Loss: 0.755, D_unsup_loss_real: 0.727, D_unsup_loss_fake: 1.045, D_sup_loss: 0.750, D_sup_acc: 78.17 Train acc: 76.837 Test acc: 78.000 \n",
      "step: 92 | Train: G_Loss: 0.777, D_unsup_loss_real: 0.833, D_unsup_loss_fake: 0.937, D_sup_loss: 0.750, D_sup_acc: 78.24 Train acc: 76.715 Test acc: 77.780 \n",
      "step: 93 | Train: G_Loss: 0.783, D_unsup_loss_real: 0.834, D_unsup_loss_fake: 0.988, D_sup_loss: 0.754, D_sup_acc: 77.99 Train acc: 76.867 Test acc: 78.180 \n",
      "step: 94 | Train: G_Loss: 0.784, D_unsup_loss_real: 0.708, D_unsup_loss_fake: 0.943, D_sup_loss: 0.747, D_sup_acc: 78.42 Train acc: 76.835 Test acc: 77.910 \n",
      "step: 95 | Train: G_Loss: 0.801, D_unsup_loss_real: 0.795, D_unsup_loss_fake: 0.973, D_sup_loss: 0.742, D_sup_acc: 78.15 Train acc: 76.878 Test acc: 77.890 \n",
      "step: 96 | Train: G_Loss: 0.774, D_unsup_loss_real: 0.867, D_unsup_loss_fake: 0.898, D_sup_loss: 0.747, D_sup_acc: 78.09 Train acc: 76.923 Test acc: 78.200 \n",
      "step: 97 | Train: G_Loss: 0.739, D_unsup_loss_real: 0.765, D_unsup_loss_fake: 0.962, D_sup_loss: 0.747, D_sup_acc: 78.41 Train acc: 76.953 Test acc: 77.990 \n",
      "step: 98 | Train: G_Loss: 0.768, D_unsup_loss_real: 0.799, D_unsup_loss_fake: 0.977, D_sup_loss: 0.747, D_sup_acc: 78.23 Train acc: 76.795 Test acc: 78.110 \n",
      "step: 99 | Train: G_Loss: 0.803, D_unsup_loss_real: 0.757, D_unsup_loss_fake: 0.970, D_sup_loss: 0.753, D_sup_acc: 78.32 Train acc: 77.340 Test acc: 78.410 \n",
      "step: 100 | Train: G_Loss: 0.788, D_unsup_loss_real: 0.806, D_unsup_loss_fake: 0.967, D_sup_loss: 0.749, D_sup_acc: 78.65 Train acc: 77.418 Test acc: 78.490 \n",
      "Train Classifier Accuracy: 77.418%\n",
      "\n",
      "Test Classifier Accuracy: 78.490%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_100.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_100.h5\n",
      "step: 101 | Train: G_Loss: 0.813, D_unsup_loss_real: 0.764, D_unsup_loss_fake: 0.853, D_sup_loss: 0.743, D_sup_acc: 78.73 Train acc: 77.522 Test acc: 78.480 \n",
      "step: 102 | Train: G_Loss: 0.798, D_unsup_loss_real: 0.769, D_unsup_loss_fake: 0.955, D_sup_loss: 0.738, D_sup_acc: 78.70 Train acc: 77.388 Test acc: 78.290 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 103 | Train: G_Loss: 0.788, D_unsup_loss_real: 0.834, D_unsup_loss_fake: 0.943, D_sup_loss: 0.743, D_sup_acc: 78.52 Train acc: 77.152 Test acc: 78.310 \n",
      "step: 104 | Train: G_Loss: 0.779, D_unsup_loss_real: 0.794, D_unsup_loss_fake: 1.001, D_sup_loss: 0.747, D_sup_acc: 78.52 Train acc: 77.398 Test acc: 78.380 \n",
      "step: 105 | Train: G_Loss: 0.792, D_unsup_loss_real: 0.820, D_unsup_loss_fake: 0.945, D_sup_loss: 0.740, D_sup_acc: 78.60 Train acc: 77.458 Test acc: 78.570 \n",
      "step: 106 | Train: G_Loss: 0.839, D_unsup_loss_real: 0.853, D_unsup_loss_fake: 0.910, D_sup_loss: 0.736, D_sup_acc: 78.78 Train acc: 77.398 Test acc: 78.550 \n",
      "step: 107 | Train: G_Loss: 0.807, D_unsup_loss_real: 0.790, D_unsup_loss_fake: 0.957, D_sup_loss: 0.736, D_sup_acc: 78.75 Train acc: 77.452 Test acc: 78.650 \n",
      "step: 108 | Train: G_Loss: 0.747, D_unsup_loss_real: 0.815, D_unsup_loss_fake: 0.987, D_sup_loss: 0.737, D_sup_acc: 78.87 Train acc: 77.197 Test acc: 78.190 \n",
      "step: 109 | Train: G_Loss: 0.770, D_unsup_loss_real: 0.754, D_unsup_loss_fake: 0.959, D_sup_loss: 0.739, D_sup_acc: 78.44 Train acc: 77.058 Test acc: 78.080 \n",
      "step: 110 | Train: G_Loss: 0.752, D_unsup_loss_real: 0.839, D_unsup_loss_fake: 0.937, D_sup_loss: 0.739, D_sup_acc: 78.32 Train acc: 77.068 Test acc: 77.970 \n",
      "step: 111 | Train: G_Loss: 0.800, D_unsup_loss_real: 0.776, D_unsup_loss_fake: 0.997, D_sup_loss: 0.739, D_sup_acc: 78.19 Train acc: 77.195 Test acc: 78.190 \n",
      "step: 112 | Train: G_Loss: 0.809, D_unsup_loss_real: 0.787, D_unsup_loss_fake: 0.972, D_sup_loss: 0.738, D_sup_acc: 78.43 Train acc: 77.117 Test acc: 78.050 \n",
      "step: 113 | Train: G_Loss: 0.813, D_unsup_loss_real: 0.807, D_unsup_loss_fake: 0.943, D_sup_loss: 0.740, D_sup_acc: 78.28 Train acc: 77.278 Test acc: 78.360 \n",
      "step: 114 | Train: G_Loss: 0.829, D_unsup_loss_real: 0.770, D_unsup_loss_fake: 0.946, D_sup_loss: 0.731, D_sup_acc: 78.57 Train acc: 77.132 Test acc: 78.280 \n",
      "step: 115 | Train: G_Loss: 0.805, D_unsup_loss_real: 0.775, D_unsup_loss_fake: 0.831, D_sup_loss: 0.733, D_sup_acc: 78.50 Train acc: 76.933 Test acc: 77.860 \n",
      "step: 116 | Train: G_Loss: 0.848, D_unsup_loss_real: 0.700, D_unsup_loss_fake: 0.945, D_sup_loss: 0.735, D_sup_acc: 78.09 Train acc: 77.047 Test acc: 78.090 \n",
      "step: 117 | Train: G_Loss: 0.805, D_unsup_loss_real: 0.815, D_unsup_loss_fake: 0.929, D_sup_loss: 0.732, D_sup_acc: 78.27 Train acc: 77.092 Test acc: 78.170 \n",
      "step: 118 | Train: G_Loss: 0.791, D_unsup_loss_real: 0.779, D_unsup_loss_fake: 0.880, D_sup_loss: 0.733, D_sup_acc: 78.43 Train acc: 77.198 Test acc: 78.250 \n",
      "step: 119 | Train: G_Loss: 0.824, D_unsup_loss_real: 0.840, D_unsup_loss_fake: 0.937, D_sup_loss: 0.739, D_sup_acc: 78.50 Train acc: 77.373 Test acc: 78.490 \n",
      "step: 120 | Train: G_Loss: 0.798, D_unsup_loss_real: 0.886, D_unsup_loss_fake: 0.871, D_sup_loss: 0.739, D_sup_acc: 78.72 Train acc: 77.222 Test acc: 78.020 \n",
      "step: 121 | Train: G_Loss: 0.795, D_unsup_loss_real: 0.766, D_unsup_loss_fake: 0.909, D_sup_loss: 0.738, D_sup_acc: 78.27 Train acc: 77.472 Test acc: 78.600 \n",
      "step: 122 | Train: G_Loss: 0.804, D_unsup_loss_real: 0.754, D_unsup_loss_fake: 0.922, D_sup_loss: 0.732, D_sup_acc: 78.86 Train acc: 77.610 Test acc: 78.600 \n",
      "step: 123 | Train: G_Loss: 0.828, D_unsup_loss_real: 0.799, D_unsup_loss_fake: 0.923, D_sup_loss: 0.727, D_sup_acc: 78.86 Train acc: 77.655 Test acc: 78.470 \n",
      "step: 124 | Train: G_Loss: 0.776, D_unsup_loss_real: 0.788, D_unsup_loss_fake: 0.874, D_sup_loss: 0.725, D_sup_acc: 78.70 Train acc: 77.600 Test acc: 78.620 \n",
      "step: 125 | Train: G_Loss: 0.780, D_unsup_loss_real: 0.766, D_unsup_loss_fake: 0.921, D_sup_loss: 0.721, D_sup_acc: 78.87 Train acc: 77.660 Test acc: 78.690 \n",
      "step: 126 | Train: G_Loss: 0.789, D_unsup_loss_real: 0.737, D_unsup_loss_fake: 0.926, D_sup_loss: 0.720, D_sup_acc: 78.91 Train acc: 77.302 Test acc: 78.350 \n",
      "step: 127 | Train: G_Loss: 0.764, D_unsup_loss_real: 0.833, D_unsup_loss_fake: 0.920, D_sup_loss: 0.720, D_sup_acc: 78.61 Train acc: 77.615 Test acc: 78.790 \n",
      "step: 128 | Train: G_Loss: 0.839, D_unsup_loss_real: 0.743, D_unsup_loss_fake: 0.967, D_sup_loss: 0.718, D_sup_acc: 79.02 Train acc: 77.765 Test acc: 78.920 \n",
      "step: 129 | Train: G_Loss: 0.797, D_unsup_loss_real: 0.777, D_unsup_loss_fake: 0.869, D_sup_loss: 0.715, D_sup_acc: 79.19 Train acc: 77.775 Test acc: 79.040 \n",
      "step: 130 | Train: G_Loss: 0.830, D_unsup_loss_real: 0.823, D_unsup_loss_fake: 0.854, D_sup_loss: 0.714, D_sup_acc: 79.27 Train acc: 77.918 Test acc: 78.950 \n",
      "step: 131 | Train: G_Loss: 0.823, D_unsup_loss_real: 0.810, D_unsup_loss_fake: 0.956, D_sup_loss: 0.710, D_sup_acc: 79.17 Train acc: 77.907 Test acc: 79.120 \n",
      "step: 132 | Train: G_Loss: 0.812, D_unsup_loss_real: 0.756, D_unsup_loss_fake: 0.838, D_sup_loss: 0.705, D_sup_acc: 79.35 Train acc: 77.765 Test acc: 79.030 \n",
      "step: 133 | Train: G_Loss: 0.765, D_unsup_loss_real: 0.795, D_unsup_loss_fake: 0.964, D_sup_loss: 0.707, D_sup_acc: 79.27 Train acc: 77.880 Test acc: 79.290 \n",
      "step: 134 | Train: G_Loss: 0.849, D_unsup_loss_real: 0.767, D_unsup_loss_fake: 0.924, D_sup_loss: 0.706, D_sup_acc: 79.51 Train acc: 78.033 Test acc: 79.440 \n",
      "step: 135 | Train: G_Loss: 0.755, D_unsup_loss_real: 0.786, D_unsup_loss_fake: 0.923, D_sup_loss: 0.703, D_sup_acc: 79.67 Train acc: 78.105 Test acc: 79.400 \n",
      "step: 136 | Train: G_Loss: 0.817, D_unsup_loss_real: 0.808, D_unsup_loss_fake: 0.864, D_sup_loss: 0.702, D_sup_acc: 79.64 Train acc: 78.145 Test acc: 79.360 \n",
      "step: 137 | Train: G_Loss: 0.797, D_unsup_loss_real: 0.779, D_unsup_loss_fake: 0.880, D_sup_loss: 0.699, D_sup_acc: 79.60 Train acc: 78.260 Test acc: 79.600 \n",
      "step: 138 | Train: G_Loss: 0.836, D_unsup_loss_real: 0.727, D_unsup_loss_fake: 0.915, D_sup_loss: 0.694, D_sup_acc: 79.84 Train acc: 77.853 Test acc: 78.930 \n",
      "step: 139 | Train: G_Loss: 0.854, D_unsup_loss_real: 0.785, D_unsup_loss_fake: 0.854, D_sup_loss: 0.707, D_sup_acc: 79.18 Train acc: 78.262 Test acc: 79.560 \n",
      "step: 140 | Train: G_Loss: 0.814, D_unsup_loss_real: 0.758, D_unsup_loss_fake: 0.829, D_sup_loss: 0.695, D_sup_acc: 79.80 Train acc: 78.370 Test acc: 79.540 \n",
      "step: 141 | Train: G_Loss: 0.826, D_unsup_loss_real: 0.778, D_unsup_loss_fake: 0.821, D_sup_loss: 0.691, D_sup_acc: 79.78 Train acc: 78.532 Test acc: 79.600 \n",
      "step: 142 | Train: G_Loss: 0.756, D_unsup_loss_real: 0.738, D_unsup_loss_fake: 0.878, D_sup_loss: 0.687, D_sup_acc: 79.85 Train acc: 78.433 Test acc: 79.510 \n",
      "step: 143 | Train: G_Loss: 0.850, D_unsup_loss_real: 0.839, D_unsup_loss_fake: 0.892, D_sup_loss: 0.684, D_sup_acc: 79.75 Train acc: 78.150 Test acc: 79.430 \n",
      "step: 144 | Train: G_Loss: 0.780, D_unsup_loss_real: 0.813, D_unsup_loss_fake: 0.883, D_sup_loss: 0.692, D_sup_acc: 79.67 Train acc: 78.533 Test acc: 79.880 \n",
      "step: 145 | Train: G_Loss: 0.816, D_unsup_loss_real: 0.752, D_unsup_loss_fake: 0.922, D_sup_loss: 0.685, D_sup_acc: 80.10 Train acc: 78.513 Test acc: 79.770 \n",
      "step: 146 | Train: G_Loss: 0.793, D_unsup_loss_real: 0.715, D_unsup_loss_fake: 0.904, D_sup_loss: 0.686, D_sup_acc: 80.00 Train acc: 78.577 Test acc: 79.910 \n",
      "step: 147 | Train: G_Loss: 0.776, D_unsup_loss_real: 0.803, D_unsup_loss_fake: 0.835, D_sup_loss: 0.681, D_sup_acc: 80.14 Train acc: 78.217 Test acc: 79.660 \n",
      "step: 148 | Train: G_Loss: 0.850, D_unsup_loss_real: 0.761, D_unsup_loss_fake: 0.856, D_sup_loss: 0.688, D_sup_acc: 79.87 Train acc: 78.058 Test acc: 79.460 \n",
      "step: 149 | Train: G_Loss: 0.836, D_unsup_loss_real: 0.783, D_unsup_loss_fake: 0.781, D_sup_loss: 0.687, D_sup_acc: 79.70 Train acc: 78.088 Test acc: 79.150 \n",
      "step: 150 | Train: G_Loss: 0.832, D_unsup_loss_real: 0.753, D_unsup_loss_fake: 0.883, D_sup_loss: 0.690, D_sup_acc: 79.41 Train acc: 78.322 Test acc: 79.650 \n",
      "step: 151 | Train: G_Loss: 0.818, D_unsup_loss_real: 0.762, D_unsup_loss_fake: 0.859, D_sup_loss: 0.686, D_sup_acc: 79.88 Train acc: 78.273 Test acc: 79.520 \n",
      "step: 152 | Train: G_Loss: 0.815, D_unsup_loss_real: 0.754, D_unsup_loss_fake: 0.899, D_sup_loss: 0.685, D_sup_acc: 79.77 Train acc: 77.978 Test acc: 79.020 \n",
      "step: 153 | Train: G_Loss: 0.809, D_unsup_loss_real: 0.743, D_unsup_loss_fake: 0.875, D_sup_loss: 0.690, D_sup_acc: 79.27 Train acc: 78.082 Test acc: 79.300 \n",
      "step: 154 | Train: G_Loss: 0.806, D_unsup_loss_real: 0.761, D_unsup_loss_fake: 0.894, D_sup_loss: 0.689, D_sup_acc: 79.54 Train acc: 78.315 Test acc: 79.460 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 155 | Train: G_Loss: 0.850, D_unsup_loss_real: 0.791, D_unsup_loss_fake: 0.882, D_sup_loss: 0.687, D_sup_acc: 79.70 Train acc: 78.352 Test acc: 79.560 \n",
      "step: 156 | Train: G_Loss: 0.802, D_unsup_loss_real: 0.819, D_unsup_loss_fake: 0.827, D_sup_loss: 0.682, D_sup_acc: 79.79 Train acc: 78.383 Test acc: 79.580 \n",
      "step: 157 | Train: G_Loss: 0.806, D_unsup_loss_real: 0.713, D_unsup_loss_fake: 0.840, D_sup_loss: 0.682, D_sup_acc: 79.83 Train acc: 78.558 Test acc: 80.010 \n",
      "step: 158 | Train: G_Loss: 0.772, D_unsup_loss_real: 0.751, D_unsup_loss_fake: 0.818, D_sup_loss: 0.675, D_sup_acc: 80.24 Train acc: 78.433 Test acc: 79.950 \n",
      "step: 159 | Train: G_Loss: 0.859, D_unsup_loss_real: 0.746, D_unsup_loss_fake: 0.858, D_sup_loss: 0.677, D_sup_acc: 80.19 Train acc: 78.483 Test acc: 79.940 \n",
      "step: 160 | Train: G_Loss: 0.828, D_unsup_loss_real: 0.770, D_unsup_loss_fake: 0.912, D_sup_loss: 0.675, D_sup_acc: 80.18 Train acc: 78.475 Test acc: 79.850 \n",
      "step: 161 | Train: G_Loss: 0.798, D_unsup_loss_real: 0.788, D_unsup_loss_fake: 0.810, D_sup_loss: 0.675, D_sup_acc: 80.08 Train acc: 78.302 Test acc: 79.750 \n",
      "step: 162 | Train: G_Loss: 0.803, D_unsup_loss_real: 0.733, D_unsup_loss_fake: 0.824, D_sup_loss: 0.679, D_sup_acc: 80.00 Train acc: 78.540 Test acc: 79.810 \n",
      "step: 163 | Train: G_Loss: 0.825, D_unsup_loss_real: 0.722, D_unsup_loss_fake: 0.917, D_sup_loss: 0.673, D_sup_acc: 80.04 Train acc: 78.208 Test acc: 79.540 \n",
      "step: 164 | Train: G_Loss: 0.862, D_unsup_loss_real: 0.766, D_unsup_loss_fake: 0.850, D_sup_loss: 0.678, D_sup_acc: 79.80 Train acc: 78.030 Test acc: 79.450 \n",
      "step: 165 | Train: G_Loss: 0.828, D_unsup_loss_real: 0.777, D_unsup_loss_fake: 0.835, D_sup_loss: 0.681, D_sup_acc: 79.70 Train acc: 78.092 Test acc: 79.490 \n",
      "step: 166 | Train: G_Loss: 0.857, D_unsup_loss_real: 0.683, D_unsup_loss_fake: 0.840, D_sup_loss: 0.684, D_sup_acc: 79.73 Train acc: 78.142 Test acc: 79.550 \n",
      "step: 167 | Train: G_Loss: 0.811, D_unsup_loss_real: 0.742, D_unsup_loss_fake: 0.834, D_sup_loss: 0.681, D_sup_acc: 79.78 Train acc: 78.027 Test acc: 79.520 \n",
      "step: 168 | Train: G_Loss: 0.810, D_unsup_loss_real: 0.778, D_unsup_loss_fake: 0.821, D_sup_loss: 0.680, D_sup_acc: 79.72 Train acc: 78.575 Test acc: 79.860 \n",
      "step: 169 | Train: G_Loss: 0.819, D_unsup_loss_real: 0.753, D_unsup_loss_fake: 0.858, D_sup_loss: 0.668, D_sup_acc: 80.09 Train acc: 78.585 Test acc: 79.850 \n",
      "step: 170 | Train: G_Loss: 0.850, D_unsup_loss_real: 0.748, D_unsup_loss_fake: 0.878, D_sup_loss: 0.669, D_sup_acc: 80.08 Train acc: 78.280 Test acc: 79.520 \n",
      "step: 171 | Train: G_Loss: 0.860, D_unsup_loss_real: 0.773, D_unsup_loss_fake: 0.796, D_sup_loss: 0.674, D_sup_acc: 79.75 Train acc: 78.515 Test acc: 79.770 \n",
      "step: 172 | Train: G_Loss: 0.794, D_unsup_loss_real: 0.729, D_unsup_loss_fake: 0.902, D_sup_loss: 0.667, D_sup_acc: 80.01 Train acc: 78.525 Test acc: 79.750 \n",
      "step: 173 | Train: G_Loss: 0.798, D_unsup_loss_real: 0.733, D_unsup_loss_fake: 0.838, D_sup_loss: 0.669, D_sup_acc: 79.99 Train acc: 78.618 Test acc: 79.880 \n",
      "step: 174 | Train: G_Loss: 0.889, D_unsup_loss_real: 0.731, D_unsup_loss_fake: 0.904, D_sup_loss: 0.668, D_sup_acc: 80.12 Train acc: 78.677 Test acc: 79.940 \n",
      "step: 175 | Train: G_Loss: 0.872, D_unsup_loss_real: 0.720, D_unsup_loss_fake: 0.802, D_sup_loss: 0.665, D_sup_acc: 80.19 Train acc: 78.632 Test acc: 80.000 \n",
      "step: 176 | Train: G_Loss: 0.824, D_unsup_loss_real: 0.743, D_unsup_loss_fake: 0.789, D_sup_loss: 0.665, D_sup_acc: 80.25 Train acc: 78.520 Test acc: 79.840 \n",
      "step: 177 | Train: G_Loss: 0.900, D_unsup_loss_real: 0.747, D_unsup_loss_fake: 0.845, D_sup_loss: 0.667, D_sup_acc: 80.08 Train acc: 78.725 Test acc: 79.870 \n",
      "step: 178 | Train: G_Loss: 0.875, D_unsup_loss_real: 0.707, D_unsup_loss_fake: 0.854, D_sup_loss: 0.661, D_sup_acc: 80.12 Train acc: 78.703 Test acc: 79.740 \n",
      "step: 179 | Train: G_Loss: 0.914, D_unsup_loss_real: 0.781, D_unsup_loss_fake: 0.913, D_sup_loss: 0.660, D_sup_acc: 79.99 Train acc: 78.850 Test acc: 80.200 \n",
      "step: 180 | Train: G_Loss: 0.853, D_unsup_loss_real: 0.806, D_unsup_loss_fake: 0.778, D_sup_loss: 0.660, D_sup_acc: 80.45 Train acc: 78.882 Test acc: 80.350 \n",
      "step: 181 | Train: G_Loss: 0.868, D_unsup_loss_real: 0.650, D_unsup_loss_fake: 0.746, D_sup_loss: 0.657, D_sup_acc: 80.58 Train acc: 78.597 Test acc: 80.000 \n",
      "step: 182 | Train: G_Loss: 0.858, D_unsup_loss_real: 0.753, D_unsup_loss_fake: 0.833, D_sup_loss: 0.662, D_sup_acc: 80.22 Train acc: 78.872 Test acc: 80.140 \n",
      "step: 183 | Train: G_Loss: 0.834, D_unsup_loss_real: 0.708, D_unsup_loss_fake: 0.843, D_sup_loss: 0.654, D_sup_acc: 80.36 Train acc: 78.662 Test acc: 79.790 \n",
      "step: 184 | Train: G_Loss: 0.828, D_unsup_loss_real: 0.751, D_unsup_loss_fake: 0.846, D_sup_loss: 0.661, D_sup_acc: 80.02 Train acc: 78.743 Test acc: 80.040 \n",
      "step: 185 | Train: G_Loss: 0.875, D_unsup_loss_real: 0.784, D_unsup_loss_fake: 0.866, D_sup_loss: 0.657, D_sup_acc: 80.28 Train acc: 78.792 Test acc: 80.060 \n",
      "step: 186 | Train: G_Loss: 0.819, D_unsup_loss_real: 0.714, D_unsup_loss_fake: 0.809, D_sup_loss: 0.660, D_sup_acc: 80.28 Train acc: 78.940 Test acc: 79.930 \n",
      "step: 187 | Train: G_Loss: 0.851, D_unsup_loss_real: 0.721, D_unsup_loss_fake: 0.860, D_sup_loss: 0.656, D_sup_acc: 80.16 Train acc: 79.003 Test acc: 80.320 \n",
      "step: 188 | Train: G_Loss: 0.886, D_unsup_loss_real: 0.787, D_unsup_loss_fake: 0.841, D_sup_loss: 0.652, D_sup_acc: 80.57 Train acc: 78.785 Test acc: 80.010 \n",
      "step: 189 | Train: G_Loss: 0.817, D_unsup_loss_real: 0.742, D_unsup_loss_fake: 0.780, D_sup_loss: 0.657, D_sup_acc: 80.25 Train acc: 78.725 Test acc: 80.000 \n",
      "step: 190 | Train: G_Loss: 0.857, D_unsup_loss_real: 0.752, D_unsup_loss_fake: 0.825, D_sup_loss: 0.658, D_sup_acc: 80.25 Train acc: 78.682 Test acc: 79.720 \n",
      "step: 191 | Train: G_Loss: 0.854, D_unsup_loss_real: 0.755, D_unsup_loss_fake: 0.876, D_sup_loss: 0.659, D_sup_acc: 79.97 Train acc: 78.528 Test acc: 79.600 \n",
      "step: 192 | Train: G_Loss: 0.826, D_unsup_loss_real: 0.750, D_unsup_loss_fake: 0.793, D_sup_loss: 0.660, D_sup_acc: 79.83 Train acc: 78.895 Test acc: 80.040 \n",
      "step: 193 | Train: G_Loss: 0.880, D_unsup_loss_real: 0.739, D_unsup_loss_fake: 0.850, D_sup_loss: 0.652, D_sup_acc: 80.24 Train acc: 79.047 Test acc: 80.220 \n",
      "step: 194 | Train: G_Loss: 0.853, D_unsup_loss_real: 0.697, D_unsup_loss_fake: 0.809, D_sup_loss: 0.649, D_sup_acc: 80.46 Train acc: 78.800 Test acc: 80.010 \n",
      "step: 195 | Train: G_Loss: 0.827, D_unsup_loss_real: 0.691, D_unsup_loss_fake: 0.860, D_sup_loss: 0.649, D_sup_acc: 80.25 Train acc: 78.883 Test acc: 79.890 \n",
      "step: 196 | Train: G_Loss: 0.849, D_unsup_loss_real: 0.777, D_unsup_loss_fake: 0.826, D_sup_loss: 0.651, D_sup_acc: 80.13 Train acc: 78.882 Test acc: 80.060 \n",
      "step: 197 | Train: G_Loss: 0.882, D_unsup_loss_real: 0.723, D_unsup_loss_fake: 0.836, D_sup_loss: 0.649, D_sup_acc: 80.29 Train acc: 79.020 Test acc: 80.140 \n",
      "step: 198 | Train: G_Loss: 0.903, D_unsup_loss_real: 0.708, D_unsup_loss_fake: 0.805, D_sup_loss: 0.647, D_sup_acc: 80.38 Train acc: 79.190 Test acc: 80.460 \n",
      "step: 199 | Train: G_Loss: 0.819, D_unsup_loss_real: 0.734, D_unsup_loss_fake: 0.806, D_sup_loss: 0.642, D_sup_acc: 80.69 Train acc: 79.453 Test acc: 80.630 \n",
      "step: 200 | Train: G_Loss: 0.826, D_unsup_loss_real: 0.771, D_unsup_loss_fake: 0.870, D_sup_loss: 0.636, D_sup_acc: 80.86 Train acc: 79.512 Test acc: 80.820 \n",
      "Train Classifier Accuracy: 79.512%\n",
      "\n",
      "Test Classifier Accuracy: 80.820%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_200.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_200.h5\n",
      "step: 201 | Train: G_Loss: 0.847, D_unsup_loss_real: 0.716, D_unsup_loss_fake: 0.856, D_sup_loss: 0.633, D_sup_acc: 81.05 Train acc: 79.610 Test acc: 80.930 \n",
      "step: 202 | Train: G_Loss: 0.893, D_unsup_loss_real: 0.705, D_unsup_loss_fake: 0.793, D_sup_loss: 0.632, D_sup_acc: 81.14 Train acc: 79.503 Test acc: 80.750 \n",
      "step: 203 | Train: G_Loss: 0.886, D_unsup_loss_real: 0.800, D_unsup_loss_fake: 0.803, D_sup_loss: 0.636, D_sup_acc: 80.98 Train acc: 79.322 Test acc: 80.560 \n",
      "step: 204 | Train: G_Loss: 0.790, D_unsup_loss_real: 0.756, D_unsup_loss_fake: 0.828, D_sup_loss: 0.640, D_sup_acc: 80.79 Train acc: 79.663 Test acc: 81.060 \n",
      "step: 205 | Train: G_Loss: 0.860, D_unsup_loss_real: 0.724, D_unsup_loss_fake: 0.803, D_sup_loss: 0.631, D_sup_acc: 81.29 Train acc: 79.405 Test acc: 80.720 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 206 | Train: G_Loss: 0.835, D_unsup_loss_real: 0.741, D_unsup_loss_fake: 0.880, D_sup_loss: 0.635, D_sup_acc: 80.96 Train acc: 79.812 Test acc: 81.110 \n",
      "step: 207 | Train: G_Loss: 0.925, D_unsup_loss_real: 0.720, D_unsup_loss_fake: 0.791, D_sup_loss: 0.626, D_sup_acc: 81.34 Train acc: 79.763 Test acc: 81.250 \n",
      "step: 208 | Train: G_Loss: 0.855, D_unsup_loss_real: 0.711, D_unsup_loss_fake: 0.799, D_sup_loss: 0.624, D_sup_acc: 81.48 Train acc: 79.843 Test acc: 81.160 \n",
      "step: 209 | Train: G_Loss: 0.805, D_unsup_loss_real: 0.740, D_unsup_loss_fake: 0.836, D_sup_loss: 0.622, D_sup_acc: 81.39 Train acc: 79.960 Test acc: 81.240 \n",
      "step: 210 | Train: G_Loss: 0.865, D_unsup_loss_real: 0.714, D_unsup_loss_fake: 0.785, D_sup_loss: 0.621, D_sup_acc: 81.48 Train acc: 79.852 Test acc: 81.070 \n",
      "step: 211 | Train: G_Loss: 0.813, D_unsup_loss_real: 0.746, D_unsup_loss_fake: 0.815, D_sup_loss: 0.621, D_sup_acc: 81.30 Train acc: 79.788 Test acc: 81.130 \n",
      "step: 212 | Train: G_Loss: 0.860, D_unsup_loss_real: 0.714, D_unsup_loss_fake: 0.812, D_sup_loss: 0.622, D_sup_acc: 81.37 Train acc: 79.727 Test acc: 80.970 \n",
      "step: 213 | Train: G_Loss: 0.862, D_unsup_loss_real: 0.677, D_unsup_loss_fake: 0.751, D_sup_loss: 0.621, D_sup_acc: 81.21 Train acc: 79.610 Test acc: 80.770 \n",
      "step: 214 | Train: G_Loss: 0.854, D_unsup_loss_real: 0.733, D_unsup_loss_fake: 0.863, D_sup_loss: 0.626, D_sup_acc: 81.01 Train acc: 79.772 Test acc: 80.990 \n",
      "step: 215 | Train: G_Loss: 0.885, D_unsup_loss_real: 0.669, D_unsup_loss_fake: 0.807, D_sup_loss: 0.622, D_sup_acc: 81.21 Train acc: 79.885 Test acc: 81.120 \n",
      "step: 216 | Train: G_Loss: 0.835, D_unsup_loss_real: 0.718, D_unsup_loss_fake: 0.736, D_sup_loss: 0.619, D_sup_acc: 81.34 Train acc: 80.013 Test acc: 81.250 \n",
      "step: 217 | Train: G_Loss: 0.853, D_unsup_loss_real: 0.742, D_unsup_loss_fake: 0.837, D_sup_loss: 0.615, D_sup_acc: 81.48 Train acc: 80.080 Test acc: 81.390 \n",
      "step: 218 | Train: G_Loss: 0.900, D_unsup_loss_real: 0.732, D_unsup_loss_fake: 0.760, D_sup_loss: 0.610, D_sup_acc: 81.61 Train acc: 80.262 Test acc: 81.320 \n",
      "step: 219 | Train: G_Loss: 0.864, D_unsup_loss_real: 0.729, D_unsup_loss_fake: 0.834, D_sup_loss: 0.609, D_sup_acc: 81.54 Train acc: 80.222 Test acc: 81.290 \n",
      "step: 220 | Train: G_Loss: 0.854, D_unsup_loss_real: 0.740, D_unsup_loss_fake: 0.831, D_sup_loss: 0.611, D_sup_acc: 81.52 Train acc: 80.132 Test acc: 81.260 \n",
      "step: 221 | Train: G_Loss: 0.922, D_unsup_loss_real: 0.719, D_unsup_loss_fake: 0.844, D_sup_loss: 0.611, D_sup_acc: 81.50 Train acc: 79.827 Test acc: 81.040 \n",
      "step: 222 | Train: G_Loss: 0.895, D_unsup_loss_real: 0.728, D_unsup_loss_fake: 0.820, D_sup_loss: 0.616, D_sup_acc: 81.27 Train acc: 79.742 Test acc: 80.890 \n",
      "step: 223 | Train: G_Loss: 0.864, D_unsup_loss_real: 0.722, D_unsup_loss_fake: 0.799, D_sup_loss: 0.618, D_sup_acc: 81.13 Train acc: 80.038 Test acc: 81.380 \n",
      "step: 224 | Train: G_Loss: 0.876, D_unsup_loss_real: 0.696, D_unsup_loss_fake: 0.730, D_sup_loss: 0.607, D_sup_acc: 81.62 Train acc: 79.897 Test acc: 81.130 \n",
      "step: 225 | Train: G_Loss: 0.873, D_unsup_loss_real: 0.820, D_unsup_loss_fake: 0.823, D_sup_loss: 0.609, D_sup_acc: 81.35 Train acc: 80.220 Test acc: 81.560 \n",
      "step: 226 | Train: G_Loss: 0.810, D_unsup_loss_real: 0.694, D_unsup_loss_fake: 0.825, D_sup_loss: 0.605, D_sup_acc: 81.79 Train acc: 80.295 Test acc: 81.690 \n",
      "step: 227 | Train: G_Loss: 0.892, D_unsup_loss_real: 0.655, D_unsup_loss_fake: 0.778, D_sup_loss: 0.601, D_sup_acc: 81.92 Train acc: 80.325 Test acc: 81.700 \n",
      "step: 228 | Train: G_Loss: 0.873, D_unsup_loss_real: 0.753, D_unsup_loss_fake: 0.878, D_sup_loss: 0.600, D_sup_acc: 81.92 Train acc: 80.375 Test acc: 81.540 \n",
      "step: 229 | Train: G_Loss: 0.862, D_unsup_loss_real: 0.744, D_unsup_loss_fake: 0.806, D_sup_loss: 0.603, D_sup_acc: 81.77 Train acc: 80.547 Test acc: 81.730 \n",
      "step: 230 | Train: G_Loss: 0.874, D_unsup_loss_real: 0.727, D_unsup_loss_fake: 0.804, D_sup_loss: 0.597, D_sup_acc: 81.96 Train acc: 80.527 Test acc: 81.640 \n",
      "step: 231 | Train: G_Loss: 0.843, D_unsup_loss_real: 0.667, D_unsup_loss_fake: 0.903, D_sup_loss: 0.600, D_sup_acc: 81.86 Train acc: 80.472 Test acc: 81.700 \n",
      "step: 232 | Train: G_Loss: 0.899, D_unsup_loss_real: 0.698, D_unsup_loss_fake: 0.775, D_sup_loss: 0.603, D_sup_acc: 81.91 Train acc: 80.443 Test acc: 81.690 \n",
      "step: 233 | Train: G_Loss: 0.936, D_unsup_loss_real: 0.639, D_unsup_loss_fake: 0.790, D_sup_loss: 0.601, D_sup_acc: 81.92 Train acc: 80.552 Test acc: 81.770 \n",
      "step: 234 | Train: G_Loss: 0.913, D_unsup_loss_real: 0.684, D_unsup_loss_fake: 0.781, D_sup_loss: 0.599, D_sup_acc: 82.00 Train acc: 80.368 Test acc: 81.520 \n",
      "step: 235 | Train: G_Loss: 0.890, D_unsup_loss_real: 0.760, D_unsup_loss_fake: 0.794, D_sup_loss: 0.602, D_sup_acc: 81.73 Train acc: 80.185 Test acc: 81.150 \n",
      "step: 236 | Train: G_Loss: 0.919, D_unsup_loss_real: 0.725, D_unsup_loss_fake: 0.847, D_sup_loss: 0.607, D_sup_acc: 81.38 Train acc: 80.163 Test acc: 81.150 \n",
      "step: 237 | Train: G_Loss: 0.877, D_unsup_loss_real: 0.755, D_unsup_loss_fake: 0.841, D_sup_loss: 0.606, D_sup_acc: 81.39 Train acc: 80.170 Test acc: 81.210 \n",
      "step: 238 | Train: G_Loss: 0.821, D_unsup_loss_real: 0.719, D_unsup_loss_fake: 0.790, D_sup_loss: 0.604, D_sup_acc: 81.45 Train acc: 80.660 Test acc: 81.760 \n",
      "step: 239 | Train: G_Loss: 0.852, D_unsup_loss_real: 0.784, D_unsup_loss_fake: 0.810, D_sup_loss: 0.596, D_sup_acc: 81.98 Train acc: 80.378 Test acc: 81.530 \n",
      "step: 240 | Train: G_Loss: 0.824, D_unsup_loss_real: 0.696, D_unsup_loss_fake: 0.750, D_sup_loss: 0.600, D_sup_acc: 81.76 Train acc: 80.668 Test acc: 81.790 \n",
      "step: 241 | Train: G_Loss: 0.864, D_unsup_loss_real: 0.693, D_unsup_loss_fake: 0.778, D_sup_loss: 0.595, D_sup_acc: 82.01 Train acc: 80.342 Test acc: 81.510 \n",
      "step: 242 | Train: G_Loss: 0.885, D_unsup_loss_real: 0.666, D_unsup_loss_fake: 0.812, D_sup_loss: 0.597, D_sup_acc: 81.73 Train acc: 80.845 Test acc: 82.120 \n",
      "step: 243 | Train: G_Loss: 0.841, D_unsup_loss_real: 0.712, D_unsup_loss_fake: 0.861, D_sup_loss: 0.585, D_sup_acc: 82.34 Train acc: 81.027 Test acc: 82.300 \n",
      "step: 244 | Train: G_Loss: 0.857, D_unsup_loss_real: 0.724, D_unsup_loss_fake: 0.810, D_sup_loss: 0.581, D_sup_acc: 82.52 Train acc: 80.738 Test acc: 81.990 \n",
      "step: 245 | Train: G_Loss: 0.893, D_unsup_loss_real: 0.659, D_unsup_loss_fake: 0.811, D_sup_loss: 0.587, D_sup_acc: 82.21 Train acc: 80.643 Test acc: 82.000 \n",
      "step: 246 | Train: G_Loss: 0.873, D_unsup_loss_real: 0.706, D_unsup_loss_fake: 0.782, D_sup_loss: 0.587, D_sup_acc: 82.23 Train acc: 80.522 Test acc: 81.740 \n",
      "step: 247 | Train: G_Loss: 0.905, D_unsup_loss_real: 0.673, D_unsup_loss_fake: 0.806, D_sup_loss: 0.592, D_sup_acc: 81.97 Train acc: 80.083 Test acc: 81.430 \n",
      "step: 248 | Train: G_Loss: 0.855, D_unsup_loss_real: 0.651, D_unsup_loss_fake: 0.724, D_sup_loss: 0.601, D_sup_acc: 81.65 Train acc: 79.892 Test acc: 81.230 \n",
      "step: 249 | Train: G_Loss: 0.915, D_unsup_loss_real: 0.761, D_unsup_loss_fake: 0.806, D_sup_loss: 0.597, D_sup_acc: 81.47 Train acc: 80.435 Test acc: 81.520 \n",
      "step: 250 | Train: G_Loss: 0.951, D_unsup_loss_real: 0.650, D_unsup_loss_fake: 0.727, D_sup_loss: 0.587, D_sup_acc: 81.75 Train acc: 80.558 Test acc: 81.720 \n",
      "step: 251 | Train: G_Loss: 0.900, D_unsup_loss_real: 0.653, D_unsup_loss_fake: 0.770, D_sup_loss: 0.581, D_sup_acc: 81.95 Train acc: 80.390 Test acc: 81.490 \n",
      "step: 252 | Train: G_Loss: 0.825, D_unsup_loss_real: 0.805, D_unsup_loss_fake: 0.826, D_sup_loss: 0.587, D_sup_acc: 81.71 Train acc: 80.470 Test acc: 81.550 \n",
      "step: 253 | Train: G_Loss: 0.919, D_unsup_loss_real: 0.656, D_unsup_loss_fake: 0.809, D_sup_loss: 0.586, D_sup_acc: 81.77 Train acc: 80.740 Test acc: 81.980 \n",
      "step: 254 | Train: G_Loss: 0.882, D_unsup_loss_real: 0.700, D_unsup_loss_fake: 0.757, D_sup_loss: 0.578, D_sup_acc: 82.19 Train acc: 80.717 Test acc: 82.050 \n",
      "step: 255 | Train: G_Loss: 0.907, D_unsup_loss_real: 0.652, D_unsup_loss_fake: 0.778, D_sup_loss: 0.577, D_sup_acc: 82.28 Train acc: 80.847 Test acc: 82.080 \n",
      "step: 256 | Train: G_Loss: 0.896, D_unsup_loss_real: 0.741, D_unsup_loss_fake: 0.815, D_sup_loss: 0.576, D_sup_acc: 82.31 Train acc: 80.613 Test acc: 81.990 \n",
      "step: 257 | Train: G_Loss: 0.914, D_unsup_loss_real: 0.768, D_unsup_loss_fake: 0.785, D_sup_loss: 0.583, D_sup_acc: 82.21 Train acc: 80.990 Test acc: 82.200 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 258 | Train: G_Loss: 0.883, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.766, D_sup_loss: 0.575, D_sup_acc: 82.42 Train acc: 81.052 Test acc: 82.320 \n",
      "step: 259 | Train: G_Loss: 0.936, D_unsup_loss_real: 0.695, D_unsup_loss_fake: 0.820, D_sup_loss: 0.574, D_sup_acc: 82.54 Train acc: 80.947 Test acc: 82.360 \n",
      "step: 260 | Train: G_Loss: 0.900, D_unsup_loss_real: 0.709, D_unsup_loss_fake: 0.727, D_sup_loss: 0.575, D_sup_acc: 82.58 Train acc: 80.988 Test acc: 82.310 \n",
      "step: 261 | Train: G_Loss: 0.891, D_unsup_loss_real: 0.734, D_unsup_loss_fake: 0.822, D_sup_loss: 0.574, D_sup_acc: 82.53 Train acc: 81.213 Test acc: 82.440 \n",
      "step: 262 | Train: G_Loss: 0.945, D_unsup_loss_real: 0.659, D_unsup_loss_fake: 0.786, D_sup_loss: 0.571, D_sup_acc: 82.65 Train acc: 80.865 Test acc: 82.200 \n",
      "step: 263 | Train: G_Loss: 0.901, D_unsup_loss_real: 0.752, D_unsup_loss_fake: 0.764, D_sup_loss: 0.577, D_sup_acc: 82.42 Train acc: 80.998 Test acc: 82.510 \n",
      "step: 264 | Train: G_Loss: 0.918, D_unsup_loss_real: 0.703, D_unsup_loss_fake: 0.790, D_sup_loss: 0.574, D_sup_acc: 82.73 Train acc: 80.987 Test acc: 82.370 \n",
      "step: 265 | Train: G_Loss: 0.875, D_unsup_loss_real: 0.687, D_unsup_loss_fake: 0.685, D_sup_loss: 0.574, D_sup_acc: 82.58 Train acc: 81.082 Test acc: 82.380 \n",
      "step: 266 | Train: G_Loss: 0.877, D_unsup_loss_real: 0.688, D_unsup_loss_fake: 0.827, D_sup_loss: 0.569, D_sup_acc: 82.60 Train acc: 81.315 Test acc: 82.650 \n",
      "step: 267 | Train: G_Loss: 0.896, D_unsup_loss_real: 0.698, D_unsup_loss_fake: 0.842, D_sup_loss: 0.565, D_sup_acc: 82.86 Train acc: 81.218 Test acc: 82.630 \n",
      "step: 268 | Train: G_Loss: 0.858, D_unsup_loss_real: 0.678, D_unsup_loss_fake: 0.769, D_sup_loss: 0.564, D_sup_acc: 82.85 Train acc: 81.380 Test acc: 82.660 \n",
      "step: 269 | Train: G_Loss: 0.877, D_unsup_loss_real: 0.649, D_unsup_loss_fake: 0.758, D_sup_loss: 0.564, D_sup_acc: 82.87 Train acc: 81.757 Test acc: 82.940 \n",
      "step: 270 | Train: G_Loss: 0.934, D_unsup_loss_real: 0.660, D_unsup_loss_fake: 0.792, D_sup_loss: 0.556, D_sup_acc: 83.16 Train acc: 81.522 Test acc: 82.880 \n",
      "step: 271 | Train: G_Loss: 0.970, D_unsup_loss_real: 0.730, D_unsup_loss_fake: 0.733, D_sup_loss: 0.557, D_sup_acc: 83.10 Train acc: 81.663 Test acc: 82.890 \n",
      "step: 272 | Train: G_Loss: 0.898, D_unsup_loss_real: 0.685, D_unsup_loss_fake: 0.721, D_sup_loss: 0.555, D_sup_acc: 83.11 Train acc: 81.723 Test acc: 83.010 \n",
      "step: 273 | Train: G_Loss: 0.917, D_unsup_loss_real: 0.725, D_unsup_loss_fake: 0.765, D_sup_loss: 0.553, D_sup_acc: 83.21 Train acc: 81.845 Test acc: 83.130 \n",
      "step: 274 | Train: G_Loss: 0.875, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.839, D_sup_loss: 0.551, D_sup_acc: 83.34 Train acc: 81.840 Test acc: 83.130 \n",
      "step: 275 | Train: G_Loss: 0.891, D_unsup_loss_real: 0.748, D_unsup_loss_fake: 0.810, D_sup_loss: 0.549, D_sup_acc: 83.34 Train acc: 81.447 Test acc: 82.900 \n",
      "step: 276 | Train: G_Loss: 0.912, D_unsup_loss_real: 0.647, D_unsup_loss_fake: 0.804, D_sup_loss: 0.553, D_sup_acc: 83.12 Train acc: 81.567 Test acc: 82.880 \n",
      "step: 277 | Train: G_Loss: 0.966, D_unsup_loss_real: 0.732, D_unsup_loss_fake: 0.739, D_sup_loss: 0.552, D_sup_acc: 83.10 Train acc: 81.470 Test acc: 82.940 \n",
      "step: 278 | Train: G_Loss: 0.914, D_unsup_loss_real: 0.706, D_unsup_loss_fake: 0.738, D_sup_loss: 0.554, D_sup_acc: 83.16 Train acc: 81.528 Test acc: 82.920 \n",
      "step: 279 | Train: G_Loss: 0.919, D_unsup_loss_real: 0.690, D_unsup_loss_fake: 0.750, D_sup_loss: 0.553, D_sup_acc: 83.14 Train acc: 81.763 Test acc: 82.960 \n",
      "step: 280 | Train: G_Loss: 0.897, D_unsup_loss_real: 0.685, D_unsup_loss_fake: 0.829, D_sup_loss: 0.550, D_sup_acc: 83.17 Train acc: 81.740 Test acc: 82.970 \n",
      "step: 281 | Train: G_Loss: 0.904, D_unsup_loss_real: 0.746, D_unsup_loss_fake: 0.758, D_sup_loss: 0.549, D_sup_acc: 83.19 Train acc: 81.613 Test acc: 83.040 \n",
      "step: 282 | Train: G_Loss: 0.908, D_unsup_loss_real: 0.721, D_unsup_loss_fake: 0.865, D_sup_loss: 0.553, D_sup_acc: 83.24 Train acc: 81.905 Test acc: 83.300 \n",
      "step: 283 | Train: G_Loss: 0.934, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.712, D_sup_loss: 0.551, D_sup_acc: 83.51 Train acc: 81.775 Test acc: 83.000 \n",
      "step: 284 | Train: G_Loss: 0.899, D_unsup_loss_real: 0.677, D_unsup_loss_fake: 0.750, D_sup_loss: 0.552, D_sup_acc: 83.21 Train acc: 81.993 Test acc: 83.130 \n",
      "step: 285 | Train: G_Loss: 0.940, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.706, D_sup_loss: 0.547, D_sup_acc: 83.34 Train acc: 82.185 Test acc: 83.350 \n",
      "step: 286 | Train: G_Loss: 0.919, D_unsup_loss_real: 0.746, D_unsup_loss_fake: 0.802, D_sup_loss: 0.540, D_sup_acc: 83.56 Train acc: 82.603 Test acc: 83.800 \n",
      "step: 287 | Train: G_Loss: 0.877, D_unsup_loss_real: 0.712, D_unsup_loss_fake: 0.740, D_sup_loss: 0.533, D_sup_acc: 84.00 Train acc: 82.650 Test acc: 83.910 \n",
      "step: 288 | Train: G_Loss: 0.892, D_unsup_loss_real: 0.707, D_unsup_loss_fake: 0.738, D_sup_loss: 0.530, D_sup_acc: 84.11 Train acc: 82.372 Test acc: 83.730 \n",
      "step: 289 | Train: G_Loss: 0.861, D_unsup_loss_real: 0.694, D_unsup_loss_fake: 0.823, D_sup_loss: 0.534, D_sup_acc: 83.94 Train acc: 81.950 Test acc: 83.030 \n",
      "step: 290 | Train: G_Loss: 0.877, D_unsup_loss_real: 0.759, D_unsup_loss_fake: 0.844, D_sup_loss: 0.540, D_sup_acc: 83.24 Train acc: 82.293 Test acc: 83.500 \n",
      "step: 291 | Train: G_Loss: 0.933, D_unsup_loss_real: 0.702, D_unsup_loss_fake: 0.780, D_sup_loss: 0.538, D_sup_acc: 83.71 Train acc: 82.090 Test acc: 83.130 \n",
      "step: 292 | Train: G_Loss: 0.878, D_unsup_loss_real: 0.668, D_unsup_loss_fake: 0.827, D_sup_loss: 0.540, D_sup_acc: 83.33 Train acc: 81.795 Test acc: 82.710 \n",
      "step: 293 | Train: G_Loss: 0.943, D_unsup_loss_real: 0.696, D_unsup_loss_fake: 0.707, D_sup_loss: 0.548, D_sup_acc: 82.93 Train acc: 81.593 Test acc: 82.630 \n",
      "step: 294 | Train: G_Loss: 0.896, D_unsup_loss_real: 0.716, D_unsup_loss_fake: 0.747, D_sup_loss: 0.554, D_sup_acc: 82.85 Train acc: 81.600 Test acc: 82.810 \n",
      "step: 295 | Train: G_Loss: 0.877, D_unsup_loss_real: 0.689, D_unsup_loss_fake: 0.737, D_sup_loss: 0.551, D_sup_acc: 83.03 Train acc: 81.775 Test acc: 82.830 \n",
      "step: 296 | Train: G_Loss: 0.984, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.787, D_sup_loss: 0.545, D_sup_acc: 83.05 Train acc: 81.785 Test acc: 82.870 \n",
      "step: 297 | Train: G_Loss: 0.900, D_unsup_loss_real: 0.743, D_unsup_loss_fake: 0.763, D_sup_loss: 0.547, D_sup_acc: 83.09 Train acc: 81.595 Test acc: 82.820 \n",
      "step: 298 | Train: G_Loss: 0.889, D_unsup_loss_real: 0.663, D_unsup_loss_fake: 0.777, D_sup_loss: 0.549, D_sup_acc: 83.04 Train acc: 81.552 Test acc: 82.750 \n",
      "step: 299 | Train: G_Loss: 0.878, D_unsup_loss_real: 0.701, D_unsup_loss_fake: 0.788, D_sup_loss: 0.546, D_sup_acc: 82.97 Train acc: 81.650 Test acc: 82.740 \n",
      "step: 300 | Train: G_Loss: 0.905, D_unsup_loss_real: 0.668, D_unsup_loss_fake: 0.730, D_sup_loss: 0.544, D_sup_acc: 82.96 Train acc: 81.987 Test acc: 83.130 \n",
      "Train Classifier Accuracy: 81.987%\n",
      "\n",
      "Test Classifier Accuracy: 83.130%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_300.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_300.h5\n",
      "step: 301 | Train: G_Loss: 0.927, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.722, D_sup_loss: 0.540, D_sup_acc: 83.34 Train acc: 81.677 Test acc: 83.020 \n",
      "step: 302 | Train: G_Loss: 0.952, D_unsup_loss_real: 0.683, D_unsup_loss_fake: 0.836, D_sup_loss: 0.540, D_sup_acc: 83.23 Train acc: 81.307 Test acc: 82.840 \n",
      "step: 303 | Train: G_Loss: 0.931, D_unsup_loss_real: 0.806, D_unsup_loss_fake: 0.806, D_sup_loss: 0.546, D_sup_acc: 83.06 Train acc: 81.358 Test acc: 82.850 \n",
      "step: 304 | Train: G_Loss: 0.968, D_unsup_loss_real: 0.650, D_unsup_loss_fake: 0.820, D_sup_loss: 0.548, D_sup_acc: 83.07 Train acc: 81.337 Test acc: 82.830 \n",
      "step: 305 | Train: G_Loss: 0.920, D_unsup_loss_real: 0.658, D_unsup_loss_fake: 0.724, D_sup_loss: 0.545, D_sup_acc: 83.04 Train acc: 81.382 Test acc: 82.850 \n",
      "step: 306 | Train: G_Loss: 0.927, D_unsup_loss_real: 0.746, D_unsup_loss_fake: 0.718, D_sup_loss: 0.540, D_sup_acc: 83.07 Train acc: 81.720 Test acc: 83.170 \n",
      "step: 307 | Train: G_Loss: 0.951, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.720, D_sup_loss: 0.535, D_sup_acc: 83.37 Train acc: 81.718 Test acc: 83.280 \n",
      "step: 308 | Train: G_Loss: 0.947, D_unsup_loss_real: 0.669, D_unsup_loss_fake: 0.778, D_sup_loss: 0.533, D_sup_acc: 83.49 Train acc: 81.958 Test acc: 83.440 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 309 | Train: G_Loss: 0.925, D_unsup_loss_real: 0.675, D_unsup_loss_fake: 0.810, D_sup_loss: 0.531, D_sup_acc: 83.65 Train acc: 82.530 Test acc: 83.840 \n",
      "step: 310 | Train: G_Loss: 0.991, D_unsup_loss_real: 0.707, D_unsup_loss_fake: 0.739, D_sup_loss: 0.526, D_sup_acc: 84.04 Train acc: 82.578 Test acc: 83.770 \n",
      "step: 311 | Train: G_Loss: 0.921, D_unsup_loss_real: 0.719, D_unsup_loss_fake: 0.783, D_sup_loss: 0.529, D_sup_acc: 83.98 Train acc: 82.637 Test acc: 83.710 \n",
      "step: 312 | Train: G_Loss: 0.904, D_unsup_loss_real: 0.735, D_unsup_loss_fake: 0.737, D_sup_loss: 0.525, D_sup_acc: 83.92 Train acc: 82.472 Test acc: 83.650 \n",
      "step: 313 | Train: G_Loss: 0.983, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.739, D_sup_loss: 0.526, D_sup_acc: 83.86 Train acc: 82.103 Test acc: 83.290 \n",
      "step: 314 | Train: G_Loss: 0.879, D_unsup_loss_real: 0.721, D_unsup_loss_fake: 0.754, D_sup_loss: 0.528, D_sup_acc: 83.50 Train acc: 82.213 Test acc: 83.460 \n",
      "step: 315 | Train: G_Loss: 0.956, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.733, D_sup_loss: 0.523, D_sup_acc: 83.67 Train acc: 81.787 Test acc: 82.970 \n",
      "step: 316 | Train: G_Loss: 0.938, D_unsup_loss_real: 0.695, D_unsup_loss_fake: 0.786, D_sup_loss: 0.529, D_sup_acc: 83.19 Train acc: 82.045 Test acc: 83.280 \n",
      "step: 317 | Train: G_Loss: 0.961, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.708, D_sup_loss: 0.524, D_sup_acc: 83.49 Train acc: 82.003 Test acc: 83.330 \n",
      "step: 318 | Train: G_Loss: 0.921, D_unsup_loss_real: 0.705, D_unsup_loss_fake: 0.849, D_sup_loss: 0.527, D_sup_acc: 83.54 Train acc: 82.198 Test acc: 83.280 \n",
      "step: 319 | Train: G_Loss: 0.929, D_unsup_loss_real: 0.715, D_unsup_loss_fake: 0.706, D_sup_loss: 0.525, D_sup_acc: 83.48 Train acc: 82.515 Test acc: 83.760 \n",
      "step: 320 | Train: G_Loss: 0.907, D_unsup_loss_real: 0.707, D_unsup_loss_fake: 0.751, D_sup_loss: 0.519, D_sup_acc: 83.97 Train acc: 82.448 Test acc: 83.800 \n",
      "step: 321 | Train: G_Loss: 0.974, D_unsup_loss_real: 0.648, D_unsup_loss_fake: 0.802, D_sup_loss: 0.517, D_sup_acc: 84.00 Train acc: 82.375 Test acc: 83.710 \n",
      "step: 322 | Train: G_Loss: 0.885, D_unsup_loss_real: 0.752, D_unsup_loss_fake: 0.809, D_sup_loss: 0.519, D_sup_acc: 83.92 Train acc: 83.057 Test acc: 84.440 \n",
      "step: 323 | Train: G_Loss: 0.954, D_unsup_loss_real: 0.685, D_unsup_loss_fake: 0.754, D_sup_loss: 0.509, D_sup_acc: 84.64 Train acc: 82.973 Test acc: 84.380 \n",
      "step: 324 | Train: G_Loss: 0.903, D_unsup_loss_real: 0.667, D_unsup_loss_fake: 0.772, D_sup_loss: 0.510, D_sup_acc: 84.58 Train acc: 83.058 Test acc: 84.370 \n",
      "step: 325 | Train: G_Loss: 0.909, D_unsup_loss_real: 0.740, D_unsup_loss_fake: 0.803, D_sup_loss: 0.509, D_sup_acc: 84.57 Train acc: 82.983 Test acc: 84.310 \n",
      "step: 326 | Train: G_Loss: 0.912, D_unsup_loss_real: 0.680, D_unsup_loss_fake: 0.779, D_sup_loss: 0.508, D_sup_acc: 84.51 Train acc: 82.402 Test acc: 83.760 \n",
      "step: 327 | Train: G_Loss: 0.853, D_unsup_loss_real: 0.725, D_unsup_loss_fake: 0.874, D_sup_loss: 0.517, D_sup_acc: 83.96 Train acc: 82.657 Test acc: 83.720 \n",
      "step: 328 | Train: G_Loss: 0.973, D_unsup_loss_real: 0.729, D_unsup_loss_fake: 0.742, D_sup_loss: 0.518, D_sup_acc: 83.93 Train acc: 82.515 Test acc: 83.520 \n",
      "step: 329 | Train: G_Loss: 0.929, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.763, D_sup_loss: 0.521, D_sup_acc: 83.73 Train acc: 82.695 Test acc: 83.960 \n",
      "step: 330 | Train: G_Loss: 0.873, D_unsup_loss_real: 0.718, D_unsup_loss_fake: 0.745, D_sup_loss: 0.516, D_sup_acc: 84.16 Train acc: 82.782 Test acc: 83.890 \n",
      "step: 331 | Train: G_Loss: 0.924, D_unsup_loss_real: 0.691, D_unsup_loss_fake: 0.763, D_sup_loss: 0.514, D_sup_acc: 84.09 Train acc: 83.005 Test acc: 84.130 \n",
      "step: 332 | Train: G_Loss: 0.959, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.765, D_sup_loss: 0.512, D_sup_acc: 84.33 Train acc: 83.035 Test acc: 84.150 \n",
      "step: 333 | Train: G_Loss: 0.980, D_unsup_loss_real: 0.668, D_unsup_loss_fake: 0.741, D_sup_loss: 0.513, D_sup_acc: 84.35 Train acc: 82.975 Test acc: 83.980 \n",
      "step: 334 | Train: G_Loss: 0.967, D_unsup_loss_real: 0.670, D_unsup_loss_fake: 0.713, D_sup_loss: 0.518, D_sup_acc: 84.18 Train acc: 82.667 Test acc: 83.730 \n",
      "step: 335 | Train: G_Loss: 0.931, D_unsup_loss_real: 0.715, D_unsup_loss_fake: 0.613, D_sup_loss: 0.520, D_sup_acc: 83.94 Train acc: 82.955 Test acc: 84.190 \n",
      "step: 336 | Train: G_Loss: 0.949, D_unsup_loss_real: 0.702, D_unsup_loss_fake: 0.752, D_sup_loss: 0.512, D_sup_acc: 84.39 Train acc: 83.057 Test acc: 84.190 \n",
      "step: 337 | Train: G_Loss: 0.898, D_unsup_loss_real: 0.688, D_unsup_loss_fake: 0.750, D_sup_loss: 0.508, D_sup_acc: 84.39 Train acc: 82.728 Test acc: 84.060 \n",
      "step: 338 | Train: G_Loss: 0.931, D_unsup_loss_real: 0.657, D_unsup_loss_fake: 0.775, D_sup_loss: 0.507, D_sup_acc: 84.26 Train acc: 83.165 Test acc: 84.460 \n",
      "step: 339 | Train: G_Loss: 0.953, D_unsup_loss_real: 0.660, D_unsup_loss_fake: 0.763, D_sup_loss: 0.503, D_sup_acc: 84.66 Train acc: 83.557 Test acc: 84.890 \n",
      "step: 340 | Train: G_Loss: 0.970, D_unsup_loss_real: 0.735, D_unsup_loss_fake: 0.758, D_sup_loss: 0.499, D_sup_acc: 85.08 Train acc: 83.720 Test acc: 84.650 \n",
      "step: 341 | Train: G_Loss: 0.918, D_unsup_loss_real: 0.653, D_unsup_loss_fake: 0.770, D_sup_loss: 0.505, D_sup_acc: 84.84 Train acc: 83.470 Test acc: 84.600 \n",
      "step: 342 | Train: G_Loss: 0.905, D_unsup_loss_real: 0.726, D_unsup_loss_fake: 0.769, D_sup_loss: 0.506, D_sup_acc: 84.79 Train acc: 83.617 Test acc: 84.690 \n",
      "step: 343 | Train: G_Loss: 0.918, D_unsup_loss_real: 0.678, D_unsup_loss_fake: 0.809, D_sup_loss: 0.500, D_sup_acc: 84.88 Train acc: 83.785 Test acc: 85.130 \n",
      "step: 344 | Train: G_Loss: 0.966, D_unsup_loss_real: 0.671, D_unsup_loss_fake: 0.739, D_sup_loss: 0.494, D_sup_acc: 85.32 Train acc: 83.675 Test acc: 84.850 \n",
      "step: 345 | Train: G_Loss: 0.927, D_unsup_loss_real: 0.654, D_unsup_loss_fake: 0.706, D_sup_loss: 0.496, D_sup_acc: 85.04 Train acc: 83.780 Test acc: 85.060 \n",
      "step: 346 | Train: G_Loss: 0.911, D_unsup_loss_real: 0.673, D_unsup_loss_fake: 0.726, D_sup_loss: 0.492, D_sup_acc: 85.25 Train acc: 83.725 Test acc: 84.880 \n",
      "step: 347 | Train: G_Loss: 0.942, D_unsup_loss_real: 0.749, D_unsup_loss_fake: 0.873, D_sup_loss: 0.491, D_sup_acc: 85.07 Train acc: 83.858 Test acc: 85.050 \n",
      "step: 348 | Train: G_Loss: 0.921, D_unsup_loss_real: 0.726, D_unsup_loss_fake: 0.800, D_sup_loss: 0.491, D_sup_acc: 85.24 Train acc: 83.858 Test acc: 85.100 \n",
      "step: 349 | Train: G_Loss: 0.950, D_unsup_loss_real: 0.647, D_unsup_loss_fake: 0.717, D_sup_loss: 0.493, D_sup_acc: 85.29 Train acc: 83.740 Test acc: 85.030 \n",
      "step: 350 | Train: G_Loss: 0.931, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.710, D_sup_loss: 0.493, D_sup_acc: 85.22 Train acc: 83.748 Test acc: 84.920 \n",
      "step: 351 | Train: G_Loss: 0.983, D_unsup_loss_real: 0.671, D_unsup_loss_fake: 0.754, D_sup_loss: 0.495, D_sup_acc: 85.11 Train acc: 83.882 Test acc: 85.250 \n",
      "step: 352 | Train: G_Loss: 0.861, D_unsup_loss_real: 0.702, D_unsup_loss_fake: 0.755, D_sup_loss: 0.489, D_sup_acc: 85.44 Train acc: 84.058 Test acc: 85.400 \n",
      "step: 353 | Train: G_Loss: 0.925, D_unsup_loss_real: 0.642, D_unsup_loss_fake: 0.807, D_sup_loss: 0.486, D_sup_acc: 85.57 Train acc: 83.848 Test acc: 85.250 \n",
      "step: 354 | Train: G_Loss: 0.936, D_unsup_loss_real: 0.648, D_unsup_loss_fake: 0.751, D_sup_loss: 0.491, D_sup_acc: 85.44 Train acc: 83.955 Test acc: 85.220 \n",
      "step: 355 | Train: G_Loss: 0.937, D_unsup_loss_real: 0.664, D_unsup_loss_fake: 0.796, D_sup_loss: 0.490, D_sup_acc: 85.41 Train acc: 83.787 Test acc: 85.040 \n",
      "step: 356 | Train: G_Loss: 0.933, D_unsup_loss_real: 0.664, D_unsup_loss_fake: 0.697, D_sup_loss: 0.488, D_sup_acc: 85.23 Train acc: 83.498 Test acc: 84.840 \n",
      "step: 357 | Train: G_Loss: 0.983, D_unsup_loss_real: 0.663, D_unsup_loss_fake: 0.750, D_sup_loss: 0.494, D_sup_acc: 85.03 Train acc: 83.703 Test acc: 84.810 \n",
      "step: 358 | Train: G_Loss: 0.926, D_unsup_loss_real: 0.754, D_unsup_loss_fake: 0.769, D_sup_loss: 0.491, D_sup_acc: 85.00 Train acc: 83.785 Test acc: 84.730 \n",
      "step: 359 | Train: G_Loss: 0.980, D_unsup_loss_real: 0.698, D_unsup_loss_fake: 0.852, D_sup_loss: 0.494, D_sup_acc: 84.91 Train acc: 83.958 Test acc: 84.940 \n",
      "step: 360 | Train: G_Loss: 0.906, D_unsup_loss_real: 0.660, D_unsup_loss_fake: 0.771, D_sup_loss: 0.491, D_sup_acc: 85.13 Train acc: 83.822 Test acc: 84.890 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 361 | Train: G_Loss: 0.891, D_unsup_loss_real: 0.698, D_unsup_loss_fake: 0.759, D_sup_loss: 0.493, D_sup_acc: 85.08 Train acc: 83.708 Test acc: 84.640 \n",
      "step: 362 | Train: G_Loss: 0.982, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.767, D_sup_loss: 0.498, D_sup_acc: 84.83 Train acc: 83.430 Test acc: 84.620 \n",
      "step: 363 | Train: G_Loss: 0.916, D_unsup_loss_real: 0.674, D_unsup_loss_fake: 0.702, D_sup_loss: 0.499, D_sup_acc: 84.80 Train acc: 83.728 Test acc: 84.880 \n",
      "step: 364 | Train: G_Loss: 0.955, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.690, D_sup_loss: 0.492, D_sup_acc: 85.06 Train acc: 83.692 Test acc: 84.660 \n",
      "step: 365 | Train: G_Loss: 0.932, D_unsup_loss_real: 0.715, D_unsup_loss_fake: 0.733, D_sup_loss: 0.493, D_sup_acc: 84.85 Train acc: 84.145 Test acc: 85.490 \n",
      "step: 366 | Train: G_Loss: 0.957, D_unsup_loss_real: 0.692, D_unsup_loss_fake: 0.832, D_sup_loss: 0.482, D_sup_acc: 85.67 Train acc: 84.525 Test acc: 85.770 \n",
      "step: 367 | Train: G_Loss: 0.932, D_unsup_loss_real: 0.713, D_unsup_loss_fake: 0.808, D_sup_loss: 0.474, D_sup_acc: 85.95 Train acc: 84.488 Test acc: 85.780 \n",
      "step: 368 | Train: G_Loss: 0.915, D_unsup_loss_real: 0.747, D_unsup_loss_fake: 0.754, D_sup_loss: 0.476, D_sup_acc: 85.96 Train acc: 84.138 Test acc: 85.510 \n",
      "step: 369 | Train: G_Loss: 0.902, D_unsup_loss_real: 0.687, D_unsup_loss_fake: 0.790, D_sup_loss: 0.479, D_sup_acc: 85.69 Train acc: 84.298 Test acc: 85.550 \n",
      "step: 370 | Train: G_Loss: 0.995, D_unsup_loss_real: 0.676, D_unsup_loss_fake: 0.836, D_sup_loss: 0.477, D_sup_acc: 85.73 Train acc: 84.012 Test acc: 85.320 \n",
      "step: 371 | Train: G_Loss: 0.910, D_unsup_loss_real: 0.772, D_unsup_loss_fake: 0.751, D_sup_loss: 0.482, D_sup_acc: 85.51 Train acc: 84.025 Test acc: 85.250 \n",
      "step: 372 | Train: G_Loss: 0.928, D_unsup_loss_real: 0.655, D_unsup_loss_fake: 0.782, D_sup_loss: 0.482, D_sup_acc: 85.44 Train acc: 83.965 Test acc: 85.170 \n",
      "step: 373 | Train: G_Loss: 0.913, D_unsup_loss_real: 0.743, D_unsup_loss_fake: 0.786, D_sup_loss: 0.481, D_sup_acc: 85.36 Train acc: 84.427 Test acc: 85.720 \n",
      "step: 374 | Train: G_Loss: 0.855, D_unsup_loss_real: 0.685, D_unsup_loss_fake: 0.782, D_sup_loss: 0.476, D_sup_acc: 85.90 Train acc: 84.580 Test acc: 85.870 \n",
      "step: 375 | Train: G_Loss: 0.926, D_unsup_loss_real: 0.665, D_unsup_loss_fake: 0.824, D_sup_loss: 0.475, D_sup_acc: 86.05 Train acc: 84.712 Test acc: 85.850 \n",
      "step: 376 | Train: G_Loss: 0.968, D_unsup_loss_real: 0.738, D_unsup_loss_fake: 0.863, D_sup_loss: 0.473, D_sup_acc: 86.02 Train acc: 84.595 Test acc: 85.870 \n",
      "step: 377 | Train: G_Loss: 0.977, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.731, D_sup_loss: 0.476, D_sup_acc: 86.05 Train acc: 84.452 Test acc: 85.650 \n",
      "step: 378 | Train: G_Loss: 0.924, D_unsup_loss_real: 0.774, D_unsup_loss_fake: 0.822, D_sup_loss: 0.477, D_sup_acc: 85.83 Train acc: 84.677 Test acc: 85.870 \n",
      "step: 379 | Train: G_Loss: 0.911, D_unsup_loss_real: 0.699, D_unsup_loss_fake: 0.703, D_sup_loss: 0.476, D_sup_acc: 86.05 Train acc: 84.297 Test acc: 85.340 \n",
      "step: 380 | Train: G_Loss: 0.948, D_unsup_loss_real: 0.631, D_unsup_loss_fake: 0.834, D_sup_loss: 0.475, D_sup_acc: 85.53 Train acc: 84.698 Test acc: 85.780 \n",
      "step: 381 | Train: G_Loss: 0.939, D_unsup_loss_real: 0.685, D_unsup_loss_fake: 0.747, D_sup_loss: 0.474, D_sup_acc: 85.96 Train acc: 84.582 Test acc: 85.650 \n",
      "step: 382 | Train: G_Loss: 0.881, D_unsup_loss_real: 0.725, D_unsup_loss_fake: 0.795, D_sup_loss: 0.475, D_sup_acc: 85.83 Train acc: 84.658 Test acc: 85.860 \n",
      "step: 383 | Train: G_Loss: 0.875, D_unsup_loss_real: 0.671, D_unsup_loss_fake: 0.739, D_sup_loss: 0.476, D_sup_acc: 86.03 Train acc: 84.587 Test acc: 85.720 \n",
      "step: 384 | Train: G_Loss: 0.943, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.831, D_sup_loss: 0.468, D_sup_acc: 85.90 Train acc: 84.965 Test acc: 85.920 \n",
      "step: 385 | Train: G_Loss: 0.955, D_unsup_loss_real: 0.699, D_unsup_loss_fake: 0.708, D_sup_loss: 0.466, D_sup_acc: 86.10 Train acc: 84.638 Test acc: 85.630 \n",
      "step: 386 | Train: G_Loss: 0.882, D_unsup_loss_real: 0.734, D_unsup_loss_fake: 0.724, D_sup_loss: 0.469, D_sup_acc: 85.80 Train acc: 84.203 Test acc: 85.170 \n",
      "step: 387 | Train: G_Loss: 0.863, D_unsup_loss_real: 0.707, D_unsup_loss_fake: 0.855, D_sup_loss: 0.470, D_sup_acc: 85.34 Train acc: 83.655 Test acc: 84.860 \n",
      "step: 388 | Train: G_Loss: 0.917, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.806, D_sup_loss: 0.476, D_sup_acc: 85.05 Train acc: 84.297 Test acc: 85.510 \n",
      "step: 389 | Train: G_Loss: 0.921, D_unsup_loss_real: 0.686, D_unsup_loss_fake: 0.769, D_sup_loss: 0.470, D_sup_acc: 85.69 Train acc: 84.647 Test acc: 85.790 \n",
      "step: 390 | Train: G_Loss: 1.001, D_unsup_loss_real: 0.642, D_unsup_loss_fake: 0.814, D_sup_loss: 0.465, D_sup_acc: 85.97 Train acc: 84.897 Test acc: 86.120 \n",
      "step: 391 | Train: G_Loss: 0.905, D_unsup_loss_real: 0.695, D_unsup_loss_fake: 0.654, D_sup_loss: 0.462, D_sup_acc: 86.30 Train acc: 85.095 Test acc: 86.150 \n",
      "step: 392 | Train: G_Loss: 0.895, D_unsup_loss_real: 0.692, D_unsup_loss_fake: 0.717, D_sup_loss: 0.457, D_sup_acc: 86.33 Train acc: 84.812 Test acc: 85.890 \n",
      "step: 393 | Train: G_Loss: 0.927, D_unsup_loss_real: 0.652, D_unsup_loss_fake: 0.757, D_sup_loss: 0.461, D_sup_acc: 86.07 Train acc: 85.170 Test acc: 86.280 \n",
      "step: 394 | Train: G_Loss: 0.922, D_unsup_loss_real: 0.656, D_unsup_loss_fake: 0.804, D_sup_loss: 0.457, D_sup_acc: 86.45 Train acc: 85.078 Test acc: 86.240 \n",
      "step: 395 | Train: G_Loss: 0.873, D_unsup_loss_real: 0.804, D_unsup_loss_fake: 0.773, D_sup_loss: 0.461, D_sup_acc: 86.41 Train acc: 85.120 Test acc: 86.200 \n",
      "step: 396 | Train: G_Loss: 0.948, D_unsup_loss_real: 0.673, D_unsup_loss_fake: 0.829, D_sup_loss: 0.462, D_sup_acc: 86.37 Train acc: 85.363 Test acc: 86.510 \n",
      "step: 397 | Train: G_Loss: 0.969, D_unsup_loss_real: 0.713, D_unsup_loss_fake: 0.814, D_sup_loss: 0.456, D_sup_acc: 86.68 Train acc: 85.808 Test acc: 87.010 \n",
      "step: 398 | Train: G_Loss: 0.964, D_unsup_loss_real: 0.679, D_unsup_loss_fake: 0.774, D_sup_loss: 0.454, D_sup_acc: 87.16 Train acc: 85.930 Test acc: 87.110 \n",
      "step: 399 | Train: G_Loss: 0.945, D_unsup_loss_real: 0.698, D_unsup_loss_fake: 0.757, D_sup_loss: 0.452, D_sup_acc: 87.27 Train acc: 85.998 Test acc: 87.170 \n",
      "step: 400 | Train: G_Loss: 0.933, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.765, D_sup_loss: 0.448, D_sup_acc: 87.33 Train acc: 86.257 Test acc: 87.440 \n",
      "Train Classifier Accuracy: 86.257%\n",
      "\n",
      "Test Classifier Accuracy: 87.440%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_400.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_400.h5\n",
      "step: 401 | Train: G_Loss: 0.919, D_unsup_loss_real: 0.682, D_unsup_loss_fake: 0.795, D_sup_loss: 0.447, D_sup_acc: 87.60 Train acc: 86.228 Test acc: 87.400 \n",
      "step: 402 | Train: G_Loss: 0.957, D_unsup_loss_real: 0.691, D_unsup_loss_fake: 0.772, D_sup_loss: 0.447, D_sup_acc: 87.56 Train acc: 86.252 Test acc: 87.480 \n",
      "step: 403 | Train: G_Loss: 0.961, D_unsup_loss_real: 0.675, D_unsup_loss_fake: 0.768, D_sup_loss: 0.445, D_sup_acc: 87.64 Train acc: 86.155 Test acc: 87.350 \n",
      "step: 404 | Train: G_Loss: 0.910, D_unsup_loss_real: 0.697, D_unsup_loss_fake: 0.755, D_sup_loss: 0.445, D_sup_acc: 87.50 Train acc: 86.228 Test acc: 87.460 \n",
      "step: 405 | Train: G_Loss: 0.943, D_unsup_loss_real: 0.675, D_unsup_loss_fake: 0.787, D_sup_loss: 0.443, D_sup_acc: 87.62 Train acc: 86.303 Test acc: 87.590 \n",
      "step: 406 | Train: G_Loss: 0.857, D_unsup_loss_real: 0.718, D_unsup_loss_fake: 0.819, D_sup_loss: 0.442, D_sup_acc: 87.75 Train acc: 86.293 Test acc: 87.550 \n",
      "step: 407 | Train: G_Loss: 0.955, D_unsup_loss_real: 0.668, D_unsup_loss_fake: 0.736, D_sup_loss: 0.447, D_sup_acc: 87.70 Train acc: 86.295 Test acc: 87.490 \n",
      "step: 408 | Train: G_Loss: 0.960, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.800, D_sup_loss: 0.448, D_sup_acc: 87.65 Train acc: 85.748 Test acc: 86.720 \n",
      "step: 409 | Train: G_Loss: 0.925, D_unsup_loss_real: 0.668, D_unsup_loss_fake: 0.749, D_sup_loss: 0.453, D_sup_acc: 86.89 Train acc: 85.617 Test acc: 86.390 \n",
      "step: 410 | Train: G_Loss: 0.940, D_unsup_loss_real: 0.704, D_unsup_loss_fake: 0.772, D_sup_loss: 0.456, D_sup_acc: 86.56 Train acc: 85.218 Test acc: 86.190 \n",
      "step: 411 | Train: G_Loss: 0.982, D_unsup_loss_real: 0.687, D_unsup_loss_fake: 0.817, D_sup_loss: 0.461, D_sup_acc: 86.36 Train acc: 85.428 Test acc: 86.300 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 412 | Train: G_Loss: 0.970, D_unsup_loss_real: 0.655, D_unsup_loss_fake: 0.698, D_sup_loss: 0.460, D_sup_acc: 86.47 Train acc: 85.628 Test acc: 86.420 \n",
      "step: 413 | Train: G_Loss: 0.877, D_unsup_loss_real: 0.724, D_unsup_loss_fake: 0.797, D_sup_loss: 0.458, D_sup_acc: 86.59 Train acc: 85.957 Test acc: 86.690 \n",
      "step: 414 | Train: G_Loss: 1.015, D_unsup_loss_real: 0.669, D_unsup_loss_fake: 0.770, D_sup_loss: 0.452, D_sup_acc: 86.86 Train acc: 85.752 Test acc: 86.730 \n",
      "step: 415 | Train: G_Loss: 0.929, D_unsup_loss_real: 0.786, D_unsup_loss_fake: 0.723, D_sup_loss: 0.451, D_sup_acc: 86.90 Train acc: 85.932 Test acc: 86.850 \n",
      "step: 416 | Train: G_Loss: 0.913, D_unsup_loss_real: 0.646, D_unsup_loss_fake: 0.820, D_sup_loss: 0.449, D_sup_acc: 87.02 Train acc: 86.008 Test acc: 86.960 \n",
      "step: 417 | Train: G_Loss: 0.914, D_unsup_loss_real: 0.713, D_unsup_loss_fake: 0.751, D_sup_loss: 0.444, D_sup_acc: 87.11 Train acc: 86.015 Test acc: 87.060 \n",
      "step: 418 | Train: G_Loss: 0.992, D_unsup_loss_real: 0.678, D_unsup_loss_fake: 0.771, D_sup_loss: 0.442, D_sup_acc: 87.22 Train acc: 86.058 Test acc: 87.190 \n",
      "step: 419 | Train: G_Loss: 0.884, D_unsup_loss_real: 0.728, D_unsup_loss_fake: 0.778, D_sup_loss: 0.441, D_sup_acc: 87.35 Train acc: 86.113 Test acc: 87.200 \n",
      "step: 420 | Train: G_Loss: 0.927, D_unsup_loss_real: 0.657, D_unsup_loss_fake: 0.750, D_sup_loss: 0.440, D_sup_acc: 87.36 Train acc: 86.180 Test acc: 87.260 \n",
      "step: 421 | Train: G_Loss: 0.918, D_unsup_loss_real: 0.725, D_unsup_loss_fake: 0.752, D_sup_loss: 0.440, D_sup_acc: 87.42 Train acc: 86.270 Test acc: 87.380 \n",
      "step: 422 | Train: G_Loss: 0.940, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.731, D_sup_loss: 0.439, D_sup_acc: 87.54 Train acc: 85.883 Test acc: 86.920 \n",
      "step: 423 | Train: G_Loss: 0.920, D_unsup_loss_real: 0.694, D_unsup_loss_fake: 0.788, D_sup_loss: 0.446, D_sup_acc: 87.09 Train acc: 85.772 Test acc: 86.690 \n",
      "step: 424 | Train: G_Loss: 0.886, D_unsup_loss_real: 0.652, D_unsup_loss_fake: 0.822, D_sup_loss: 0.450, D_sup_acc: 86.86 Train acc: 85.632 Test acc: 86.530 \n",
      "step: 425 | Train: G_Loss: 0.891, D_unsup_loss_real: 0.721, D_unsup_loss_fake: 0.825, D_sup_loss: 0.453, D_sup_acc: 86.70 Train acc: 86.062 Test acc: 87.060 \n",
      "step: 426 | Train: G_Loss: 0.936, D_unsup_loss_real: 0.653, D_unsup_loss_fake: 0.738, D_sup_loss: 0.447, D_sup_acc: 87.22 Train acc: 86.173 Test acc: 87.330 \n",
      "step: 427 | Train: G_Loss: 0.873, D_unsup_loss_real: 0.727, D_unsup_loss_fake: 0.762, D_sup_loss: 0.444, D_sup_acc: 87.49 Train acc: 86.435 Test acc: 87.590 \n",
      "step: 428 | Train: G_Loss: 0.942, D_unsup_loss_real: 0.658, D_unsup_loss_fake: 0.819, D_sup_loss: 0.442, D_sup_acc: 87.75 Train acc: 86.375 Test acc: 87.490 \n",
      "step: 429 | Train: G_Loss: 0.951, D_unsup_loss_real: 0.663, D_unsup_loss_fake: 0.827, D_sup_loss: 0.439, D_sup_acc: 87.65 Train acc: 86.488 Test acc: 87.700 \n",
      "step: 430 | Train: G_Loss: 0.901, D_unsup_loss_real: 0.745, D_unsup_loss_fake: 0.819, D_sup_loss: 0.433, D_sup_acc: 87.86 Train acc: 86.487 Test acc: 87.590 \n",
      "step: 431 | Train: G_Loss: 0.911, D_unsup_loss_real: 0.669, D_unsup_loss_fake: 0.718, D_sup_loss: 0.438, D_sup_acc: 87.75 Train acc: 86.563 Test acc: 87.740 \n",
      "step: 432 | Train: G_Loss: 0.901, D_unsup_loss_real: 0.666, D_unsup_loss_fake: 0.712, D_sup_loss: 0.436, D_sup_acc: 87.89 Train acc: 86.510 Test acc: 87.500 \n",
      "step: 433 | Train: G_Loss: 0.921, D_unsup_loss_real: 0.671, D_unsup_loss_fake: 0.751, D_sup_loss: 0.436, D_sup_acc: 87.66 Train acc: 86.442 Test acc: 87.850 \n",
      "step: 434 | Train: G_Loss: 0.924, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.784, D_sup_loss: 0.435, D_sup_acc: 88.00 Train acc: 86.222 Test acc: 87.360 \n",
      "step: 435 | Train: G_Loss: 0.947, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.795, D_sup_loss: 0.442, D_sup_acc: 87.52 Train acc: 86.427 Test acc: 87.530 \n",
      "step: 436 | Train: G_Loss: 0.947, D_unsup_loss_real: 0.653, D_unsup_loss_fake: 0.784, D_sup_loss: 0.440, D_sup_acc: 87.69 Train acc: 86.312 Test acc: 87.480 \n",
      "step: 437 | Train: G_Loss: 0.930, D_unsup_loss_real: 0.709, D_unsup_loss_fake: 0.720, D_sup_loss: 0.438, D_sup_acc: 87.64 Train acc: 86.343 Test acc: 87.590 \n",
      "step: 438 | Train: G_Loss: 0.890, D_unsup_loss_real: 0.732, D_unsup_loss_fake: 0.829, D_sup_loss: 0.435, D_sup_acc: 87.74 Train acc: 86.513 Test acc: 87.870 \n",
      "step: 439 | Train: G_Loss: 0.934, D_unsup_loss_real: 0.704, D_unsup_loss_fake: 0.832, D_sup_loss: 0.435, D_sup_acc: 88.02 Train acc: 86.677 Test acc: 87.680 \n",
      "step: 440 | Train: G_Loss: 0.901, D_unsup_loss_real: 0.679, D_unsup_loss_fake: 0.714, D_sup_loss: 0.436, D_sup_acc: 87.84 Train acc: 86.437 Test acc: 87.470 \n",
      "step: 441 | Train: G_Loss: 0.913, D_unsup_loss_real: 0.695, D_unsup_loss_fake: 0.768, D_sup_loss: 0.435, D_sup_acc: 87.63 Train acc: 86.283 Test acc: 87.390 \n",
      "step: 442 | Train: G_Loss: 1.002, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.782, D_sup_loss: 0.436, D_sup_acc: 87.55 Train acc: 86.512 Test acc: 87.470 \n",
      "step: 443 | Train: G_Loss: 0.935, D_unsup_loss_real: 0.683, D_unsup_loss_fake: 0.726, D_sup_loss: 0.434, D_sup_acc: 87.63 Train acc: 86.662 Test acc: 87.860 \n",
      "step: 444 | Train: G_Loss: 0.899, D_unsup_loss_real: 0.654, D_unsup_loss_fake: 0.782, D_sup_loss: 0.428, D_sup_acc: 88.00 Train acc: 86.412 Test acc: 87.520 \n",
      "step: 445 | Train: G_Loss: 0.924, D_unsup_loss_real: 0.687, D_unsup_loss_fake: 0.798, D_sup_loss: 0.435, D_sup_acc: 87.68 Train acc: 86.503 Test acc: 87.510 \n",
      "step: 446 | Train: G_Loss: 0.894, D_unsup_loss_real: 0.770, D_unsup_loss_fake: 0.733, D_sup_loss: 0.435, D_sup_acc: 87.67 Train acc: 86.517 Test acc: 87.550 \n",
      "step: 447 | Train: G_Loss: 0.902, D_unsup_loss_real: 0.669, D_unsup_loss_fake: 0.803, D_sup_loss: 0.432, D_sup_acc: 87.71 Train acc: 86.300 Test acc: 87.200 \n",
      "step: 448 | Train: G_Loss: 0.921, D_unsup_loss_real: 0.719, D_unsup_loss_fake: 0.816, D_sup_loss: 0.430, D_sup_acc: 87.36 Train acc: 86.380 Test acc: 87.420 \n",
      "step: 449 | Train: G_Loss: 0.935, D_unsup_loss_real: 0.715, D_unsup_loss_fake: 0.823, D_sup_loss: 0.431, D_sup_acc: 87.58 Train acc: 86.432 Test acc: 87.510 \n",
      "step: 450 | Train: G_Loss: 0.942, D_unsup_loss_real: 0.657, D_unsup_loss_fake: 0.841, D_sup_loss: 0.428, D_sup_acc: 87.67 Train acc: 86.777 Test acc: 87.870 \n",
      "step: 451 | Train: G_Loss: 0.946, D_unsup_loss_real: 0.657, D_unsup_loss_fake: 0.791, D_sup_loss: 0.423, D_sup_acc: 88.02 Train acc: 86.520 Test acc: 87.530 \n",
      "step: 452 | Train: G_Loss: 0.904, D_unsup_loss_real: 0.745, D_unsup_loss_fake: 0.762, D_sup_loss: 0.428, D_sup_acc: 87.69 Train acc: 86.538 Test acc: 87.710 \n",
      "step: 453 | Train: G_Loss: 0.968, D_unsup_loss_real: 0.662, D_unsup_loss_fake: 0.776, D_sup_loss: 0.426, D_sup_acc: 87.87 Train acc: 86.697 Test acc: 87.710 \n",
      "step: 454 | Train: G_Loss: 0.915, D_unsup_loss_real: 0.659, D_unsup_loss_fake: 0.788, D_sup_loss: 0.424, D_sup_acc: 87.87 Train acc: 86.992 Test acc: 88.080 \n",
      "step: 455 | Train: G_Loss: 0.972, D_unsup_loss_real: 0.650, D_unsup_loss_fake: 0.746, D_sup_loss: 0.421, D_sup_acc: 88.23 Train acc: 87.082 Test acc: 88.170 \n",
      "step: 456 | Train: G_Loss: 0.920, D_unsup_loss_real: 0.733, D_unsup_loss_fake: 0.741, D_sup_loss: 0.419, D_sup_acc: 88.32 Train acc: 87.008 Test acc: 88.110 \n",
      "step: 457 | Train: G_Loss: 0.870, D_unsup_loss_real: 0.726, D_unsup_loss_fake: 0.759, D_sup_loss: 0.420, D_sup_acc: 88.26 Train acc: 86.853 Test acc: 88.090 \n",
      "step: 458 | Train: G_Loss: 0.922, D_unsup_loss_real: 0.679, D_unsup_loss_fake: 0.761, D_sup_loss: 0.424, D_sup_acc: 88.24 Train acc: 86.680 Test acc: 87.760 \n",
      "step: 459 | Train: G_Loss: 0.983, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.760, D_sup_loss: 0.426, D_sup_acc: 87.91 Train acc: 86.847 Test acc: 88.040 \n",
      "step: 460 | Train: G_Loss: 0.899, D_unsup_loss_real: 0.715, D_unsup_loss_fake: 0.757, D_sup_loss: 0.421, D_sup_acc: 88.19 Train acc: 86.345 Test acc: 87.400 \n",
      "step: 461 | Train: G_Loss: 0.947, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.695, D_sup_loss: 0.434, D_sup_acc: 87.56 Train acc: 86.530 Test acc: 87.770 \n",
      "step: 462 | Train: G_Loss: 0.986, D_unsup_loss_real: 0.683, D_unsup_loss_fake: 0.814, D_sup_loss: 0.429, D_sup_acc: 87.92 Train acc: 86.840 Test acc: 87.910 \n",
      "step: 463 | Train: G_Loss: 0.943, D_unsup_loss_real: 0.690, D_unsup_loss_fake: 0.736, D_sup_loss: 0.421, D_sup_acc: 88.06 Train acc: 86.952 Test acc: 87.990 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 464 | Train: G_Loss: 1.004, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.739, D_sup_loss: 0.421, D_sup_acc: 88.14 Train acc: 86.932 Test acc: 87.860 \n",
      "step: 465 | Train: G_Loss: 0.944, D_unsup_loss_real: 0.698, D_unsup_loss_fake: 0.753, D_sup_loss: 0.415, D_sup_acc: 88.01 Train acc: 86.493 Test acc: 87.280 \n",
      "step: 466 | Train: G_Loss: 0.921, D_unsup_loss_real: 0.660, D_unsup_loss_fake: 0.746, D_sup_loss: 0.425, D_sup_acc: 87.44 Train acc: 86.607 Test acc: 87.400 \n",
      "step: 467 | Train: G_Loss: 0.914, D_unsup_loss_real: 0.679, D_unsup_loss_fake: 0.776, D_sup_loss: 0.423, D_sup_acc: 87.56 Train acc: 86.952 Test acc: 87.690 \n",
      "step: 468 | Train: G_Loss: 0.956, D_unsup_loss_real: 0.701, D_unsup_loss_fake: 0.705, D_sup_loss: 0.416, D_sup_acc: 87.85 Train acc: 87.200 Test acc: 87.990 \n",
      "step: 469 | Train: G_Loss: 0.921, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.737, D_sup_loss: 0.414, D_sup_acc: 88.14 Train acc: 87.357 Test acc: 88.290 \n",
      "step: 470 | Train: G_Loss: 0.924, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.717, D_sup_loss: 0.408, D_sup_acc: 88.44 Train acc: 87.285 Test acc: 88.520 \n",
      "step: 471 | Train: G_Loss: 0.861, D_unsup_loss_real: 0.681, D_unsup_loss_fake: 0.795, D_sup_loss: 0.402, D_sup_acc: 88.67 Train acc: 87.165 Test acc: 88.410 \n",
      "step: 472 | Train: G_Loss: 1.011, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.833, D_sup_loss: 0.403, D_sup_acc: 88.56 Train acc: 87.063 Test acc: 88.100 \n",
      "step: 473 | Train: G_Loss: 1.031, D_unsup_loss_real: 0.667, D_unsup_loss_fake: 0.706, D_sup_loss: 0.412, D_sup_acc: 88.25 Train acc: 87.357 Test acc: 88.380 \n",
      "step: 474 | Train: G_Loss: 0.923, D_unsup_loss_real: 0.791, D_unsup_loss_fake: 0.777, D_sup_loss: 0.407, D_sup_acc: 88.53 Train acc: 86.962 Test acc: 87.860 \n",
      "step: 475 | Train: G_Loss: 0.958, D_unsup_loss_real: 0.659, D_unsup_loss_fake: 0.700, D_sup_loss: 0.412, D_sup_acc: 88.00 Train acc: 87.267 Test acc: 88.350 \n",
      "step: 476 | Train: G_Loss: 0.963, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.850, D_sup_loss: 0.406, D_sup_acc: 88.50 Train acc: 87.120 Test acc: 88.080 \n",
      "step: 477 | Train: G_Loss: 0.992, D_unsup_loss_real: 0.763, D_unsup_loss_fake: 0.875, D_sup_loss: 0.409, D_sup_acc: 88.23 Train acc: 86.918 Test acc: 87.970 \n",
      "step: 478 | Train: G_Loss: 1.010, D_unsup_loss_real: 0.655, D_unsup_loss_fake: 0.757, D_sup_loss: 0.415, D_sup_acc: 88.12 Train acc: 86.738 Test acc: 87.740 \n",
      "step: 479 | Train: G_Loss: 0.979, D_unsup_loss_real: 0.693, D_unsup_loss_fake: 0.724, D_sup_loss: 0.419, D_sup_acc: 87.89 Train acc: 86.918 Test acc: 87.830 \n",
      "step: 480 | Train: G_Loss: 0.921, D_unsup_loss_real: 0.732, D_unsup_loss_fake: 0.780, D_sup_loss: 0.416, D_sup_acc: 87.98 Train acc: 87.085 Test acc: 88.220 \n",
      "step: 481 | Train: G_Loss: 0.937, D_unsup_loss_real: 0.647, D_unsup_loss_fake: 0.773, D_sup_loss: 0.413, D_sup_acc: 88.37 Train acc: 87.008 Test acc: 88.180 \n",
      "step: 482 | Train: G_Loss: 0.944, D_unsup_loss_real: 0.755, D_unsup_loss_fake: 0.800, D_sup_loss: 0.412, D_sup_acc: 88.33 Train acc: 87.223 Test acc: 88.180 \n",
      "step: 483 | Train: G_Loss: 0.969, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.764, D_sup_loss: 0.411, D_sup_acc: 88.33 Train acc: 87.375 Test acc: 88.270 \n",
      "step: 484 | Train: G_Loss: 0.966, D_unsup_loss_real: 0.699, D_unsup_loss_fake: 0.798, D_sup_loss: 0.408, D_sup_acc: 88.42 Train acc: 87.558 Test acc: 88.600 \n",
      "step: 485 | Train: G_Loss: 0.970, D_unsup_loss_real: 0.729, D_unsup_loss_fake: 0.838, D_sup_loss: 0.406, D_sup_acc: 88.74 Train acc: 87.068 Test acc: 88.010 \n",
      "step: 486 | Train: G_Loss: 0.967, D_unsup_loss_real: 0.718, D_unsup_loss_fake: 0.738, D_sup_loss: 0.419, D_sup_acc: 88.15 Train acc: 87.235 Test acc: 88.210 \n",
      "step: 487 | Train: G_Loss: 0.984, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.652, D_sup_loss: 0.414, D_sup_acc: 88.36 Train acc: 87.368 Test acc: 88.290 \n",
      "step: 488 | Train: G_Loss: 0.948, D_unsup_loss_real: 0.646, D_unsup_loss_fake: 0.750, D_sup_loss: 0.409, D_sup_acc: 88.44 Train acc: 87.548 Test acc: 88.530 \n",
      "step: 489 | Train: G_Loss: 0.944, D_unsup_loss_real: 0.714, D_unsup_loss_fake: 0.758, D_sup_loss: 0.407, D_sup_acc: 88.67 Train acc: 87.882 Test acc: 88.770 \n",
      "step: 490 | Train: G_Loss: 0.926, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.725, D_sup_loss: 0.400, D_sup_acc: 88.91 Train acc: 88.048 Test acc: 88.920 \n",
      "step: 491 | Train: G_Loss: 0.911, D_unsup_loss_real: 0.682, D_unsup_loss_fake: 0.727, D_sup_loss: 0.398, D_sup_acc: 89.06 Train acc: 87.813 Test acc: 88.820 \n",
      "step: 492 | Train: G_Loss: 0.983, D_unsup_loss_real: 0.742, D_unsup_loss_fake: 0.791, D_sup_loss: 0.403, D_sup_acc: 88.96 Train acc: 87.793 Test acc: 88.760 \n",
      "step: 493 | Train: G_Loss: 0.933, D_unsup_loss_real: 0.663, D_unsup_loss_fake: 0.730, D_sup_loss: 0.402, D_sup_acc: 88.90 Train acc: 87.723 Test acc: 88.710 \n",
      "step: 494 | Train: G_Loss: 0.957, D_unsup_loss_real: 0.657, D_unsup_loss_fake: 0.767, D_sup_loss: 0.404, D_sup_acc: 88.85 Train acc: 87.538 Test acc: 88.590 \n",
      "step: 495 | Train: G_Loss: 0.972, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.761, D_sup_loss: 0.404, D_sup_acc: 88.73 Train acc: 87.518 Test acc: 88.470 \n",
      "step: 496 | Train: G_Loss: 0.953, D_unsup_loss_real: 0.663, D_unsup_loss_fake: 0.724, D_sup_loss: 0.404, D_sup_acc: 88.62 Train acc: 87.322 Test acc: 88.210 \n",
      "step: 497 | Train: G_Loss: 0.945, D_unsup_loss_real: 0.727, D_unsup_loss_fake: 0.775, D_sup_loss: 0.410, D_sup_acc: 88.36 Train acc: 87.567 Test acc: 88.410 \n",
      "step: 498 | Train: G_Loss: 1.017, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.773, D_sup_loss: 0.408, D_sup_acc: 88.56 Train acc: 87.668 Test acc: 88.600 \n",
      "step: 499 | Train: G_Loss: 0.908, D_unsup_loss_real: 0.717, D_unsup_loss_fake: 0.739, D_sup_loss: 0.402, D_sup_acc: 88.74 Train acc: 87.640 Test acc: 88.670 \n",
      "step: 500 | Train: G_Loss: 0.957, D_unsup_loss_real: 0.681, D_unsup_loss_fake: 0.745, D_sup_loss: 0.402, D_sup_acc: 88.81 Train acc: 87.823 Test acc: 88.690 \n",
      "Train Classifier Accuracy: 87.823%\n",
      "\n",
      "Test Classifier Accuracy: 88.690%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_500.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_500.h5\n",
      "step: 501 | Train: G_Loss: 0.917, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.669, D_sup_loss: 0.396, D_sup_acc: 88.83 Train acc: 87.312 Test acc: 88.310 \n",
      "step: 502 | Train: G_Loss: 0.895, D_unsup_loss_real: 0.711, D_unsup_loss_fake: 0.771, D_sup_loss: 0.403, D_sup_acc: 88.45 Train acc: 87.472 Test acc: 88.390 \n",
      "step: 503 | Train: G_Loss: 0.930, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.815, D_sup_loss: 0.400, D_sup_acc: 88.54 Train acc: 87.673 Test acc: 88.750 \n",
      "step: 504 | Train: G_Loss: 0.997, D_unsup_loss_real: 0.658, D_unsup_loss_fake: 0.812, D_sup_loss: 0.396, D_sup_acc: 88.89 Train acc: 87.660 Test acc: 88.710 \n",
      "step: 505 | Train: G_Loss: 0.934, D_unsup_loss_real: 0.660, D_unsup_loss_fake: 0.728, D_sup_loss: 0.399, D_sup_acc: 88.84 Train acc: 87.947 Test acc: 88.970 \n",
      "step: 506 | Train: G_Loss: 0.992, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.702, D_sup_loss: 0.393, D_sup_acc: 89.11 Train acc: 87.857 Test acc: 88.790 \n",
      "step: 507 | Train: G_Loss: 1.012, D_unsup_loss_real: 0.720, D_unsup_loss_fake: 0.721, D_sup_loss: 0.395, D_sup_acc: 88.93 Train acc: 87.585 Test acc: 88.490 \n",
      "step: 508 | Train: G_Loss: 0.906, D_unsup_loss_real: 0.687, D_unsup_loss_fake: 0.789, D_sup_loss: 0.397, D_sup_acc: 88.64 Train acc: 87.403 Test acc: 88.220 \n",
      "step: 509 | Train: G_Loss: 0.960, D_unsup_loss_real: 0.688, D_unsup_loss_fake: 0.769, D_sup_loss: 0.401, D_sup_acc: 88.37 Train acc: 87.760 Test acc: 88.490 \n",
      "step: 510 | Train: G_Loss: 0.930, D_unsup_loss_real: 0.649, D_unsup_loss_fake: 0.698, D_sup_loss: 0.394, D_sup_acc: 88.64 Train acc: 87.923 Test acc: 88.740 \n",
      "step: 511 | Train: G_Loss: 0.945, D_unsup_loss_real: 0.686, D_unsup_loss_fake: 0.730, D_sup_loss: 0.389, D_sup_acc: 88.88 Train acc: 87.757 Test acc: 88.590 \n",
      "step: 512 | Train: G_Loss: 0.946, D_unsup_loss_real: 0.718, D_unsup_loss_fake: 0.706, D_sup_loss: 0.390, D_sup_acc: 88.73 Train acc: 88.000 Test acc: 88.820 \n",
      "step: 513 | Train: G_Loss: 0.863, D_unsup_loss_real: 0.666, D_unsup_loss_fake: 0.778, D_sup_loss: 0.386, D_sup_acc: 88.96 Train acc: 88.178 Test acc: 89.100 \n",
      "step: 514 | Train: G_Loss: 1.041, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.821, D_sup_loss: 0.384, D_sup_acc: 89.24 Train acc: 88.147 Test acc: 89.220 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 515 | Train: G_Loss: 0.986, D_unsup_loss_real: 0.729, D_unsup_loss_fake: 0.814, D_sup_loss: 0.386, D_sup_acc: 89.36 Train acc: 88.115 Test acc: 89.090 \n",
      "step: 516 | Train: G_Loss: 1.011, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.701, D_sup_loss: 0.388, D_sup_acc: 89.23 Train acc: 87.758 Test acc: 88.700 \n",
      "step: 517 | Train: G_Loss: 0.967, D_unsup_loss_real: 0.648, D_unsup_loss_fake: 0.647, D_sup_loss: 0.391, D_sup_acc: 88.84 Train acc: 87.762 Test acc: 88.660 \n",
      "step: 518 | Train: G_Loss: 0.963, D_unsup_loss_real: 0.669, D_unsup_loss_fake: 0.682, D_sup_loss: 0.391, D_sup_acc: 88.80 Train acc: 87.782 Test acc: 88.620 \n",
      "step: 519 | Train: G_Loss: 0.931, D_unsup_loss_real: 0.658, D_unsup_loss_fake: 0.731, D_sup_loss: 0.389, D_sup_acc: 88.76 Train acc: 87.955 Test acc: 88.820 \n",
      "step: 520 | Train: G_Loss: 0.940, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.800, D_sup_loss: 0.384, D_sup_acc: 88.96 Train acc: 88.105 Test acc: 88.800 \n",
      "step: 521 | Train: G_Loss: 0.993, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.835, D_sup_loss: 0.385, D_sup_acc: 88.94 Train acc: 88.048 Test acc: 88.860 \n",
      "step: 522 | Train: G_Loss: 0.917, D_unsup_loss_real: 0.762, D_unsup_loss_fake: 0.728, D_sup_loss: 0.389, D_sup_acc: 89.00 Train acc: 88.082 Test acc: 88.850 \n",
      "step: 523 | Train: G_Loss: 0.971, D_unsup_loss_real: 0.652, D_unsup_loss_fake: 0.660, D_sup_loss: 0.389, D_sup_acc: 88.99 Train acc: 88.198 Test acc: 88.990 \n",
      "step: 524 | Train: G_Loss: 0.959, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.720, D_sup_loss: 0.381, D_sup_acc: 89.13 Train acc: 88.217 Test acc: 89.220 \n",
      "step: 525 | Train: G_Loss: 0.901, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.791, D_sup_loss: 0.377, D_sup_acc: 89.36 Train acc: 88.185 Test acc: 89.150 \n",
      "step: 526 | Train: G_Loss: 1.073, D_unsup_loss_real: 0.658, D_unsup_loss_fake: 0.762, D_sup_loss: 0.377, D_sup_acc: 89.28 Train acc: 88.315 Test acc: 89.120 \n",
      "step: 527 | Train: G_Loss: 0.946, D_unsup_loss_real: 0.708, D_unsup_loss_fake: 0.770, D_sup_loss: 0.374, D_sup_acc: 89.26 Train acc: 88.392 Test acc: 89.180 \n",
      "step: 528 | Train: G_Loss: 0.916, D_unsup_loss_real: 0.703, D_unsup_loss_fake: 0.692, D_sup_loss: 0.381, D_sup_acc: 89.31 Train acc: 88.293 Test acc: 89.110 \n",
      "step: 529 | Train: G_Loss: 0.952, D_unsup_loss_real: 0.671, D_unsup_loss_fake: 0.722, D_sup_loss: 0.377, D_sup_acc: 89.25 Train acc: 88.330 Test acc: 89.160 \n",
      "step: 530 | Train: G_Loss: 0.946, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.781, D_sup_loss: 0.376, D_sup_acc: 89.30 Train acc: 88.510 Test acc: 89.270 \n",
      "step: 531 | Train: G_Loss: 0.951, D_unsup_loss_real: 0.711, D_unsup_loss_fake: 0.812, D_sup_loss: 0.374, D_sup_acc: 89.41 Train acc: 88.152 Test acc: 88.920 \n",
      "step: 532 | Train: G_Loss: 1.053, D_unsup_loss_real: 0.676, D_unsup_loss_fake: 0.780, D_sup_loss: 0.388, D_sup_acc: 89.06 Train acc: 88.458 Test acc: 89.300 \n",
      "step: 533 | Train: G_Loss: 0.967, D_unsup_loss_real: 0.712, D_unsup_loss_fake: 0.685, D_sup_loss: 0.383, D_sup_acc: 89.44 Train acc: 88.455 Test acc: 89.330 \n",
      "step: 534 | Train: G_Loss: 0.924, D_unsup_loss_real: 0.684, D_unsup_loss_fake: 0.689, D_sup_loss: 0.378, D_sup_acc: 89.46 Train acc: 88.325 Test acc: 89.200 \n",
      "step: 535 | Train: G_Loss: 0.976, D_unsup_loss_real: 0.631, D_unsup_loss_fake: 0.747, D_sup_loss: 0.377, D_sup_acc: 89.34 Train acc: 88.152 Test acc: 89.210 \n",
      "step: 536 | Train: G_Loss: 0.965, D_unsup_loss_real: 0.700, D_unsup_loss_fake: 0.772, D_sup_loss: 0.378, D_sup_acc: 89.34 Train acc: 88.402 Test acc: 89.380 \n",
      "step: 537 | Train: G_Loss: 1.006, D_unsup_loss_real: 0.681, D_unsup_loss_fake: 0.815, D_sup_loss: 0.375, D_sup_acc: 89.51 Train acc: 88.148 Test acc: 89.050 \n",
      "step: 538 | Train: G_Loss: 1.041, D_unsup_loss_real: 0.680, D_unsup_loss_fake: 0.761, D_sup_loss: 0.381, D_sup_acc: 89.19 Train acc: 88.138 Test acc: 89.130 \n",
      "step: 539 | Train: G_Loss: 0.997, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.793, D_sup_loss: 0.377, D_sup_acc: 89.27 Train acc: 88.145 Test acc: 89.020 \n",
      "step: 540 | Train: G_Loss: 1.060, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.736, D_sup_loss: 0.378, D_sup_acc: 89.16 Train acc: 87.602 Test acc: 88.420 \n",
      "step: 541 | Train: G_Loss: 1.020, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.707, D_sup_loss: 0.391, D_sup_acc: 88.57 Train acc: 87.797 Test acc: 88.530 \n",
      "step: 542 | Train: G_Loss: 0.963, D_unsup_loss_real: 0.730, D_unsup_loss_fake: 0.649, D_sup_loss: 0.390, D_sup_acc: 88.67 Train acc: 87.622 Test acc: 88.470 \n",
      "step: 543 | Train: G_Loss: 0.939, D_unsup_loss_real: 0.677, D_unsup_loss_fake: 0.764, D_sup_loss: 0.385, D_sup_acc: 88.62 Train acc: 88.008 Test acc: 88.990 \n",
      "step: 544 | Train: G_Loss: 1.031, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.712, D_sup_loss: 0.379, D_sup_acc: 89.13 Train acc: 88.233 Test acc: 89.210 \n",
      "step: 545 | Train: G_Loss: 0.929, D_unsup_loss_real: 0.682, D_unsup_loss_fake: 0.734, D_sup_loss: 0.376, D_sup_acc: 89.35 Train acc: 87.892 Test acc: 88.570 \n",
      "step: 546 | Train: G_Loss: 1.005, D_unsup_loss_real: 0.734, D_unsup_loss_fake: 0.724, D_sup_loss: 0.385, D_sup_acc: 88.71 Train acc: 87.745 Test acc: 88.380 \n",
      "step: 547 | Train: G_Loss: 0.931, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.774, D_sup_loss: 0.387, D_sup_acc: 88.53 Train acc: 87.858 Test acc: 88.820 \n",
      "step: 548 | Train: G_Loss: 0.926, D_unsup_loss_real: 0.683, D_unsup_loss_fake: 0.752, D_sup_loss: 0.389, D_sup_acc: 88.96 Train acc: 88.280 Test acc: 89.150 \n",
      "step: 549 | Train: G_Loss: 0.970, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.755, D_sup_loss: 0.382, D_sup_acc: 89.29 Train acc: 88.430 Test acc: 89.270 \n",
      "step: 550 | Train: G_Loss: 1.051, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.741, D_sup_loss: 0.374, D_sup_acc: 89.41 Train acc: 88.617 Test acc: 89.500 \n",
      "step: 551 | Train: G_Loss: 0.960, D_unsup_loss_real: 0.711, D_unsup_loss_fake: 0.767, D_sup_loss: 0.370, D_sup_acc: 89.63 Train acc: 88.440 Test acc: 89.270 \n",
      "step: 552 | Train: G_Loss: 0.989, D_unsup_loss_real: 0.675, D_unsup_loss_fake: 0.737, D_sup_loss: 0.375, D_sup_acc: 89.41 Train acc: 88.542 Test acc: 89.470 \n",
      "step: 553 | Train: G_Loss: 1.016, D_unsup_loss_real: 0.657, D_unsup_loss_fake: 0.663, D_sup_loss: 0.374, D_sup_acc: 89.60 Train acc: 88.545 Test acc: 89.470 \n",
      "step: 554 | Train: G_Loss: 0.982, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.688, D_sup_loss: 0.368, D_sup_acc: 89.60 Train acc: 88.572 Test acc: 89.470 \n",
      "step: 555 | Train: G_Loss: 1.000, D_unsup_loss_real: 0.663, D_unsup_loss_fake: 0.661, D_sup_loss: 0.369, D_sup_acc: 89.60 Train acc: 88.605 Test acc: 89.680 \n",
      "step: 556 | Train: G_Loss: 1.038, D_unsup_loss_real: 0.669, D_unsup_loss_fake: 0.760, D_sup_loss: 0.365, D_sup_acc: 89.81 Train acc: 88.645 Test acc: 89.620 \n",
      "step: 557 | Train: G_Loss: 0.882, D_unsup_loss_real: 0.740, D_unsup_loss_fake: 0.749, D_sup_loss: 0.365, D_sup_acc: 89.75 Train acc: 88.367 Test acc: 89.210 \n",
      "step: 558 | Train: G_Loss: 1.000, D_unsup_loss_real: 0.707, D_unsup_loss_fake: 0.789, D_sup_loss: 0.371, D_sup_acc: 89.35 Train acc: 88.448 Test acc: 89.330 \n",
      "step: 559 | Train: G_Loss: 0.937, D_unsup_loss_real: 0.731, D_unsup_loss_fake: 0.749, D_sup_loss: 0.371, D_sup_acc: 89.46 Train acc: 88.360 Test acc: 89.350 \n",
      "step: 560 | Train: G_Loss: 0.922, D_unsup_loss_real: 0.663, D_unsup_loss_fake: 0.673, D_sup_loss: 0.375, D_sup_acc: 89.48 Train acc: 88.658 Test acc: 89.680 \n",
      "step: 561 | Train: G_Loss: 0.907, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.703, D_sup_loss: 0.367, D_sup_acc: 89.81 Train acc: 88.407 Test acc: 89.430 \n",
      "step: 562 | Train: G_Loss: 0.984, D_unsup_loss_real: 0.695, D_unsup_loss_fake: 0.711, D_sup_loss: 0.371, D_sup_acc: 89.56 Train acc: 88.427 Test acc: 89.260 \n",
      "step: 563 | Train: G_Loss: 0.975, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.730, D_sup_loss: 0.364, D_sup_acc: 89.40 Train acc: 88.510 Test acc: 89.490 \n",
      "step: 564 | Train: G_Loss: 0.994, D_unsup_loss_real: 0.705, D_unsup_loss_fake: 0.788, D_sup_loss: 0.361, D_sup_acc: 89.62 Train acc: 88.638 Test acc: 89.570 \n",
      "step: 565 | Train: G_Loss: 0.974, D_unsup_loss_real: 0.673, D_unsup_loss_fake: 0.729, D_sup_loss: 0.357, D_sup_acc: 89.70 Train acc: 88.407 Test acc: 89.340 \n",
      "step: 566 | Train: G_Loss: 0.960, D_unsup_loss_real: 0.778, D_unsup_loss_fake: 0.768, D_sup_loss: 0.365, D_sup_acc: 89.46 Train acc: 88.778 Test acc: 89.710 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 567 | Train: G_Loss: 0.922, D_unsup_loss_real: 0.687, D_unsup_loss_fake: 0.693, D_sup_loss: 0.364, D_sup_acc: 89.84 Train acc: 88.827 Test acc: 89.830 \n",
      "step: 568 | Train: G_Loss: 0.917, D_unsup_loss_real: 0.646, D_unsup_loss_fake: 0.816, D_sup_loss: 0.357, D_sup_acc: 89.96 Train acc: 88.765 Test acc: 89.670 \n",
      "step: 569 | Train: G_Loss: 0.929, D_unsup_loss_real: 0.671, D_unsup_loss_fake: 0.800, D_sup_loss: 0.357, D_sup_acc: 89.80 Train acc: 88.375 Test acc: 89.150 \n",
      "step: 570 | Train: G_Loss: 0.897, D_unsup_loss_real: 0.654, D_unsup_loss_fake: 0.704, D_sup_loss: 0.371, D_sup_acc: 89.29 Train acc: 88.603 Test acc: 89.370 \n",
      "step: 571 | Train: G_Loss: 0.992, D_unsup_loss_real: 0.751, D_unsup_loss_fake: 0.747, D_sup_loss: 0.362, D_sup_acc: 89.50 Train acc: 88.922 Test acc: 89.930 \n",
      "step: 572 | Train: G_Loss: 1.005, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.711, D_sup_loss: 0.352, D_sup_acc: 90.06 Train acc: 88.818 Test acc: 89.600 \n",
      "step: 573 | Train: G_Loss: 0.938, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.684, D_sup_loss: 0.352, D_sup_acc: 89.73 Train acc: 89.195 Test acc: 90.210 \n",
      "step: 574 | Train: G_Loss: 0.952, D_unsup_loss_real: 0.681, D_unsup_loss_fake: 0.743, D_sup_loss: 0.346, D_sup_acc: 90.33 Train acc: 89.155 Test acc: 89.780 \n",
      "step: 575 | Train: G_Loss: 0.966, D_unsup_loss_real: 0.684, D_unsup_loss_fake: 0.728, D_sup_loss: 0.353, D_sup_acc: 89.91 Train acc: 89.068 Test acc: 89.730 \n",
      "step: 576 | Train: G_Loss: 0.940, D_unsup_loss_real: 0.672, D_unsup_loss_fake: 0.729, D_sup_loss: 0.356, D_sup_acc: 89.85 Train acc: 88.937 Test acc: 89.730 \n",
      "step: 577 | Train: G_Loss: 0.929, D_unsup_loss_real: 0.681, D_unsup_loss_fake: 0.664, D_sup_loss: 0.354, D_sup_acc: 89.86 Train acc: 89.133 Test acc: 89.960 \n",
      "step: 578 | Train: G_Loss: 0.931, D_unsup_loss_real: 0.694, D_unsup_loss_fake: 0.705, D_sup_loss: 0.351, D_sup_acc: 90.09 Train acc: 89.040 Test acc: 89.970 \n",
      "step: 579 | Train: G_Loss: 0.965, D_unsup_loss_real: 0.654, D_unsup_loss_fake: 0.713, D_sup_loss: 0.349, D_sup_acc: 90.10 Train acc: 88.968 Test acc: 89.900 \n",
      "step: 580 | Train: G_Loss: 0.986, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.747, D_sup_loss: 0.351, D_sup_acc: 90.03 Train acc: 89.057 Test acc: 90.050 \n",
      "step: 581 | Train: G_Loss: 0.987, D_unsup_loss_real: 0.669, D_unsup_loss_fake: 0.727, D_sup_loss: 0.345, D_sup_acc: 90.18 Train acc: 88.882 Test acc: 90.120 \n",
      "step: 582 | Train: G_Loss: 0.975, D_unsup_loss_real: 0.673, D_unsup_loss_fake: 0.796, D_sup_loss: 0.346, D_sup_acc: 90.24 Train acc: 89.318 Test acc: 90.290 \n",
      "step: 583 | Train: G_Loss: 1.003, D_unsup_loss_real: 0.679, D_unsup_loss_fake: 0.777, D_sup_loss: 0.343, D_sup_acc: 90.41 Train acc: 89.185 Test acc: 90.180 \n",
      "step: 584 | Train: G_Loss: 1.020, D_unsup_loss_real: 0.743, D_unsup_loss_fake: 0.813, D_sup_loss: 0.347, D_sup_acc: 90.30 Train acc: 88.827 Test acc: 89.680 \n",
      "step: 585 | Train: G_Loss: 1.032, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.726, D_sup_loss: 0.357, D_sup_acc: 89.81 Train acc: 89.040 Test acc: 89.910 \n",
      "step: 586 | Train: G_Loss: 0.958, D_unsup_loss_real: 0.713, D_unsup_loss_fake: 0.639, D_sup_loss: 0.352, D_sup_acc: 90.04 Train acc: 89.202 Test acc: 90.280 \n",
      "step: 587 | Train: G_Loss: 1.010, D_unsup_loss_real: 0.658, D_unsup_loss_fake: 0.706, D_sup_loss: 0.344, D_sup_acc: 90.40 Train acc: 89.200 Test acc: 90.160 \n",
      "step: 588 | Train: G_Loss: 1.015, D_unsup_loss_real: 0.674, D_unsup_loss_fake: 0.735, D_sup_loss: 0.346, D_sup_acc: 90.28 Train acc: 89.142 Test acc: 90.060 \n",
      "step: 589 | Train: G_Loss: 0.940, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.863, D_sup_loss: 0.348, D_sup_acc: 90.19 Train acc: 89.108 Test acc: 90.070 \n",
      "step: 590 | Train: G_Loss: 1.002, D_unsup_loss_real: 0.691, D_unsup_loss_fake: 0.714, D_sup_loss: 0.351, D_sup_acc: 90.20 Train acc: 89.098 Test acc: 90.150 \n",
      "step: 591 | Train: G_Loss: 0.984, D_unsup_loss_real: 0.692, D_unsup_loss_fake: 0.780, D_sup_loss: 0.348, D_sup_acc: 90.27 Train acc: 88.730 Test acc: 89.730 \n",
      "step: 592 | Train: G_Loss: 0.988, D_unsup_loss_real: 0.696, D_unsup_loss_fake: 0.693, D_sup_loss: 0.358, D_sup_acc: 89.86 Train acc: 88.768 Test acc: 89.730 \n",
      "step: 593 | Train: G_Loss: 1.081, D_unsup_loss_real: 0.644, D_unsup_loss_fake: 0.625, D_sup_loss: 0.356, D_sup_acc: 89.86 Train acc: 89.110 Test acc: 90.280 \n",
      "step: 594 | Train: G_Loss: 1.045, D_unsup_loss_real: 0.672, D_unsup_loss_fake: 0.684, D_sup_loss: 0.346, D_sup_acc: 90.40 Train acc: 89.117 Test acc: 90.150 \n",
      "step: 595 | Train: G_Loss: 1.009, D_unsup_loss_real: 0.659, D_unsup_loss_fake: 0.669, D_sup_loss: 0.345, D_sup_acc: 90.27 Train acc: 89.247 Test acc: 90.290 \n",
      "step: 596 | Train: G_Loss: 1.008, D_unsup_loss_real: 0.653, D_unsup_loss_fake: 0.757, D_sup_loss: 0.345, D_sup_acc: 90.41 Train acc: 89.282 Test acc: 90.190 \n",
      "step: 597 | Train: G_Loss: 0.931, D_unsup_loss_real: 0.697, D_unsup_loss_fake: 0.721, D_sup_loss: 0.346, D_sup_acc: 90.31 Train acc: 89.047 Test acc: 89.780 \n",
      "step: 598 | Train: G_Loss: 0.936, D_unsup_loss_real: 0.677, D_unsup_loss_fake: 0.710, D_sup_loss: 0.353, D_sup_acc: 89.91 Train acc: 89.030 Test acc: 89.810 \n",
      "step: 599 | Train: G_Loss: 0.975, D_unsup_loss_real: 0.692, D_unsup_loss_fake: 0.691, D_sup_loss: 0.351, D_sup_acc: 89.94 Train acc: 89.373 Test acc: 90.400 \n",
      "step: 600 | Train: G_Loss: 0.877, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.676, D_sup_loss: 0.345, D_sup_acc: 90.52 Train acc: 89.372 Test acc: 90.370 \n",
      "Train Classifier Accuracy: 89.372%\n",
      "\n",
      "Test Classifier Accuracy: 90.370%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_600.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_600.h5\n",
      "step: 601 | Train: G_Loss: 1.018, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.835, D_sup_loss: 0.338, D_sup_acc: 90.49 Train acc: 89.085 Test acc: 90.000 \n",
      "step: 602 | Train: G_Loss: 0.961, D_unsup_loss_real: 0.665, D_unsup_loss_fake: 0.776, D_sup_loss: 0.346, D_sup_acc: 90.13 Train acc: 89.257 Test acc: 90.290 \n",
      "step: 603 | Train: G_Loss: 1.012, D_unsup_loss_real: 0.695, D_unsup_loss_fake: 0.708, D_sup_loss: 0.342, D_sup_acc: 90.41 Train acc: 89.298 Test acc: 90.130 \n",
      "step: 604 | Train: G_Loss: 1.020, D_unsup_loss_real: 0.665, D_unsup_loss_fake: 0.732, D_sup_loss: 0.342, D_sup_acc: 90.25 Train acc: 89.387 Test acc: 90.290 \n",
      "step: 605 | Train: G_Loss: 0.930, D_unsup_loss_real: 0.669, D_unsup_loss_fake: 0.731, D_sup_loss: 0.339, D_sup_acc: 90.41 Train acc: 89.190 Test acc: 90.100 \n",
      "step: 606 | Train: G_Loss: 0.947, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.712, D_sup_loss: 0.342, D_sup_acc: 90.23 Train acc: 89.627 Test acc: 90.720 \n",
      "step: 607 | Train: G_Loss: 1.021, D_unsup_loss_real: 0.684, D_unsup_loss_fake: 0.720, D_sup_loss: 0.332, D_sup_acc: 90.84 Train acc: 89.550 Test acc: 90.800 \n",
      "step: 608 | Train: G_Loss: 0.993, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.742, D_sup_loss: 0.332, D_sup_acc: 90.92 Train acc: 89.453 Test acc: 90.570 \n",
      "step: 609 | Train: G_Loss: 0.945, D_unsup_loss_real: 0.648, D_unsup_loss_fake: 0.712, D_sup_loss: 0.335, D_sup_acc: 90.69 Train acc: 89.550 Test acc: 90.530 \n",
      "step: 610 | Train: G_Loss: 1.006, D_unsup_loss_real: 0.707, D_unsup_loss_fake: 0.719, D_sup_loss: 0.335, D_sup_acc: 90.65 Train acc: 89.428 Test acc: 90.310 \n",
      "step: 611 | Train: G_Loss: 1.001, D_unsup_loss_real: 0.659, D_unsup_loss_fake: 0.733, D_sup_loss: 0.343, D_sup_acc: 90.43 Train acc: 89.467 Test acc: 90.370 \n",
      "step: 612 | Train: G_Loss: 0.935, D_unsup_loss_real: 0.666, D_unsup_loss_fake: 0.794, D_sup_loss: 0.339, D_sup_acc: 90.49 Train acc: 89.518 Test acc: 90.260 \n",
      "step: 613 | Train: G_Loss: 0.988, D_unsup_loss_real: 0.680, D_unsup_loss_fake: 0.754, D_sup_loss: 0.338, D_sup_acc: 90.38 Train acc: 89.462 Test acc: 90.410 \n",
      "step: 614 | Train: G_Loss: 1.009, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.752, D_sup_loss: 0.342, D_sup_acc: 90.53 Train acc: 89.220 Test acc: 90.100 \n",
      "step: 615 | Train: G_Loss: 0.904, D_unsup_loss_real: 0.771, D_unsup_loss_fake: 0.742, D_sup_loss: 0.345, D_sup_acc: 90.23 Train acc: 89.263 Test acc: 90.080 \n",
      "step: 616 | Train: G_Loss: 0.971, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.665, D_sup_loss: 0.345, D_sup_acc: 90.21 Train acc: 89.327 Test acc: 90.070 \n",
      "step: 617 | Train: G_Loss: 0.937, D_unsup_loss_real: 0.696, D_unsup_loss_fake: 0.702, D_sup_loss: 0.341, D_sup_acc: 90.20 Train acc: 89.288 Test acc: 90.010 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 618 | Train: G_Loss: 0.871, D_unsup_loss_real: 0.655, D_unsup_loss_fake: 0.726, D_sup_loss: 0.343, D_sup_acc: 90.14 Train acc: 89.377 Test acc: 90.090 \n",
      "step: 619 | Train: G_Loss: 0.934, D_unsup_loss_real: 0.675, D_unsup_loss_fake: 0.787, D_sup_loss: 0.339, D_sup_acc: 90.22 Train acc: 89.102 Test acc: 89.790 \n",
      "step: 620 | Train: G_Loss: 0.908, D_unsup_loss_real: 0.694, D_unsup_loss_fake: 0.846, D_sup_loss: 0.347, D_sup_acc: 89.92 Train acc: 88.838 Test acc: 89.550 \n",
      "step: 621 | Train: G_Loss: 0.951, D_unsup_loss_real: 0.646, D_unsup_loss_fake: 0.715, D_sup_loss: 0.350, D_sup_acc: 89.68 Train acc: 89.028 Test acc: 89.850 \n",
      "step: 622 | Train: G_Loss: 0.960, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.681, D_sup_loss: 0.345, D_sup_acc: 89.98 Train acc: 88.855 Test acc: 89.590 \n",
      "step: 623 | Train: G_Loss: 1.038, D_unsup_loss_real: 0.693, D_unsup_loss_fake: 0.757, D_sup_loss: 0.347, D_sup_acc: 89.72 Train acc: 88.972 Test acc: 89.780 \n",
      "step: 624 | Train: G_Loss: 0.971, D_unsup_loss_real: 0.727, D_unsup_loss_fake: 0.730, D_sup_loss: 0.345, D_sup_acc: 89.91 Train acc: 89.265 Test acc: 90.060 \n",
      "step: 625 | Train: G_Loss: 0.993, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.753, D_sup_loss: 0.339, D_sup_acc: 90.19 Train acc: 89.395 Test acc: 90.260 \n",
      "step: 626 | Train: G_Loss: 0.950, D_unsup_loss_real: 0.653, D_unsup_loss_fake: 0.675, D_sup_loss: 0.337, D_sup_acc: 90.38 Train acc: 89.433 Test acc: 90.320 \n",
      "step: 627 | Train: G_Loss: 1.008, D_unsup_loss_real: 0.749, D_unsup_loss_fake: 0.763, D_sup_loss: 0.335, D_sup_acc: 90.44 Train acc: 89.555 Test acc: 90.590 \n",
      "step: 628 | Train: G_Loss: 0.979, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.675, D_sup_loss: 0.336, D_sup_acc: 90.71 Train acc: 89.680 Test acc: 90.730 \n",
      "step: 629 | Train: G_Loss: 0.899, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.738, D_sup_loss: 0.332, D_sup_acc: 90.85 Train acc: 89.655 Test acc: 90.720 \n",
      "step: 630 | Train: G_Loss: 0.884, D_unsup_loss_real: 0.708, D_unsup_loss_fake: 0.722, D_sup_loss: 0.334, D_sup_acc: 90.84 Train acc: 89.838 Test acc: 90.880 \n",
      "step: 631 | Train: G_Loss: 1.004, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.718, D_sup_loss: 0.330, D_sup_acc: 91.00 Train acc: 89.885 Test acc: 90.930 \n",
      "step: 632 | Train: G_Loss: 1.036, D_unsup_loss_real: 0.659, D_unsup_loss_fake: 0.716, D_sup_loss: 0.330, D_sup_acc: 91.04 Train acc: 89.900 Test acc: 90.830 \n",
      "step: 633 | Train: G_Loss: 0.961, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.742, D_sup_loss: 0.331, D_sup_acc: 90.95 Train acc: 89.735 Test acc: 90.650 \n",
      "step: 634 | Train: G_Loss: 0.997, D_unsup_loss_real: 0.667, D_unsup_loss_fake: 0.706, D_sup_loss: 0.327, D_sup_acc: 90.77 Train acc: 89.737 Test acc: 90.700 \n",
      "step: 635 | Train: G_Loss: 0.971, D_unsup_loss_real: 0.696, D_unsup_loss_fake: 0.684, D_sup_loss: 0.324, D_sup_acc: 90.82 Train acc: 90.045 Test acc: 90.980 \n",
      "step: 636 | Train: G_Loss: 0.985, D_unsup_loss_real: 0.690, D_unsup_loss_fake: 0.720, D_sup_loss: 0.319, D_sup_acc: 91.09 Train acc: 89.995 Test acc: 91.100 \n",
      "step: 637 | Train: G_Loss: 0.948, D_unsup_loss_real: 0.707, D_unsup_loss_fake: 0.655, D_sup_loss: 0.318, D_sup_acc: 91.21 Train acc: 89.860 Test acc: 90.870 \n",
      "step: 638 | Train: G_Loss: 0.957, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.712, D_sup_loss: 0.319, D_sup_acc: 90.99 Train acc: 89.987 Test acc: 91.010 \n",
      "step: 639 | Train: G_Loss: 0.967, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.740, D_sup_loss: 0.319, D_sup_acc: 91.12 Train acc: 89.900 Test acc: 90.860 \n",
      "step: 640 | Train: G_Loss: 0.947, D_unsup_loss_real: 0.651, D_unsup_loss_fake: 0.770, D_sup_loss: 0.324, D_sup_acc: 90.98 Train acc: 89.733 Test acc: 90.570 \n",
      "step: 641 | Train: G_Loss: 1.026, D_unsup_loss_real: 0.692, D_unsup_loss_fake: 0.723, D_sup_loss: 0.329, D_sup_acc: 90.69 Train acc: 89.847 Test acc: 90.650 \n",
      "step: 642 | Train: G_Loss: 0.948, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.745, D_sup_loss: 0.327, D_sup_acc: 90.77 Train acc: 89.925 Test acc: 90.700 \n",
      "step: 643 | Train: G_Loss: 1.045, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.687, D_sup_loss: 0.328, D_sup_acc: 90.82 Train acc: 89.860 Test acc: 90.700 \n",
      "step: 644 | Train: G_Loss: 1.061, D_unsup_loss_real: 0.688, D_unsup_loss_fake: 0.701, D_sup_loss: 0.330, D_sup_acc: 90.82 Train acc: 89.858 Test acc: 90.750 \n",
      "step: 645 | Train: G_Loss: 0.989, D_unsup_loss_real: 0.671, D_unsup_loss_fake: 0.771, D_sup_loss: 0.329, D_sup_acc: 90.87 Train acc: 89.963 Test acc: 90.940 \n",
      "step: 646 | Train: G_Loss: 1.017, D_unsup_loss_real: 0.659, D_unsup_loss_fake: 0.770, D_sup_loss: 0.326, D_sup_acc: 91.05 Train acc: 89.893 Test acc: 90.860 \n",
      "step: 647 | Train: G_Loss: 1.053, D_unsup_loss_real: 0.746, D_unsup_loss_fake: 0.692, D_sup_loss: 0.324, D_sup_acc: 90.98 Train acc: 89.948 Test acc: 90.780 \n",
      "step: 648 | Train: G_Loss: 1.027, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.729, D_sup_loss: 0.324, D_sup_acc: 90.90 Train acc: 89.897 Test acc: 90.600 \n",
      "step: 649 | Train: G_Loss: 0.993, D_unsup_loss_real: 0.654, D_unsup_loss_fake: 0.658, D_sup_loss: 0.329, D_sup_acc: 90.72 Train acc: 90.048 Test acc: 90.700 \n",
      "step: 650 | Train: G_Loss: 1.037, D_unsup_loss_real: 0.723, D_unsup_loss_fake: 0.715, D_sup_loss: 0.328, D_sup_acc: 90.82 Train acc: 89.880 Test acc: 90.580 \n",
      "step: 651 | Train: G_Loss: 1.008, D_unsup_loss_real: 0.703, D_unsup_loss_fake: 0.745, D_sup_loss: 0.326, D_sup_acc: 90.70 Train acc: 90.255 Test acc: 90.960 \n",
      "step: 652 | Train: G_Loss: 0.930, D_unsup_loss_real: 0.695, D_unsup_loss_fake: 0.740, D_sup_loss: 0.319, D_sup_acc: 91.07 Train acc: 90.378 Test acc: 91.170 \n",
      "step: 653 | Train: G_Loss: 1.001, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.717, D_sup_loss: 0.317, D_sup_acc: 91.28 Train acc: 90.223 Test acc: 90.960 \n",
      "step: 654 | Train: G_Loss: 0.998, D_unsup_loss_real: 0.708, D_unsup_loss_fake: 0.714, D_sup_loss: 0.320, D_sup_acc: 91.07 Train acc: 90.447 Test acc: 91.360 \n",
      "step: 655 | Train: G_Loss: 1.004, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.688, D_sup_loss: 0.315, D_sup_acc: 91.47 Train acc: 90.432 Test acc: 91.280 \n",
      "step: 656 | Train: G_Loss: 0.931, D_unsup_loss_real: 0.669, D_unsup_loss_fake: 0.730, D_sup_loss: 0.315, D_sup_acc: 91.39 Train acc: 90.487 Test acc: 91.220 \n",
      "step: 657 | Train: G_Loss: 1.018, D_unsup_loss_real: 0.728, D_unsup_loss_fake: 0.848, D_sup_loss: 0.314, D_sup_acc: 91.33 Train acc: 90.493 Test acc: 91.310 \n",
      "step: 658 | Train: G_Loss: 0.997, D_unsup_loss_real: 0.682, D_unsup_loss_fake: 0.727, D_sup_loss: 0.316, D_sup_acc: 91.42 Train acc: 90.430 Test acc: 91.190 \n",
      "step: 659 | Train: G_Loss: 0.936, D_unsup_loss_real: 0.656, D_unsup_loss_fake: 0.710, D_sup_loss: 0.316, D_sup_acc: 91.30 Train acc: 90.338 Test acc: 91.150 \n",
      "step: 660 | Train: G_Loss: 0.956, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.715, D_sup_loss: 0.314, D_sup_acc: 91.26 Train acc: 90.135 Test acc: 90.970 \n",
      "step: 661 | Train: G_Loss: 0.946, D_unsup_loss_real: 0.673, D_unsup_loss_fake: 0.706, D_sup_loss: 0.317, D_sup_acc: 91.08 Train acc: 89.782 Test acc: 90.540 \n",
      "step: 662 | Train: G_Loss: 0.997, D_unsup_loss_real: 0.652, D_unsup_loss_fake: 0.754, D_sup_loss: 0.324, D_sup_acc: 90.66 Train acc: 89.880 Test acc: 90.530 \n",
      "step: 663 | Train: G_Loss: 1.014, D_unsup_loss_real: 0.649, D_unsup_loss_fake: 0.762, D_sup_loss: 0.325, D_sup_acc: 90.65 Train acc: 89.975 Test acc: 90.540 \n",
      "step: 664 | Train: G_Loss: 1.059, D_unsup_loss_real: 0.669, D_unsup_loss_fake: 0.703, D_sup_loss: 0.326, D_sup_acc: 90.66 Train acc: 89.887 Test acc: 90.660 \n",
      "step: 665 | Train: G_Loss: 1.078, D_unsup_loss_real: 0.651, D_unsup_loss_fake: 0.683, D_sup_loss: 0.329, D_sup_acc: 90.78 Train acc: 90.048 Test acc: 90.750 \n",
      "step: 666 | Train: G_Loss: 1.014, D_unsup_loss_real: 0.699, D_unsup_loss_fake: 0.648, D_sup_loss: 0.322, D_sup_acc: 90.87 Train acc: 90.362 Test acc: 91.050 \n",
      "step: 667 | Train: G_Loss: 0.932, D_unsup_loss_real: 0.672, D_unsup_loss_fake: 0.745, D_sup_loss: 0.314, D_sup_acc: 91.16 Train acc: 90.648 Test acc: 91.290 \n",
      "step: 668 | Train: G_Loss: 1.010, D_unsup_loss_real: 0.631, D_unsup_loss_fake: 0.780, D_sup_loss: 0.306, D_sup_acc: 91.40 Train acc: 90.685 Test acc: 91.550 \n",
      "step: 669 | Train: G_Loss: 0.990, D_unsup_loss_real: 0.628, D_unsup_loss_fake: 0.687, D_sup_loss: 0.307, D_sup_acc: 91.66 Train acc: 90.432 Test acc: 91.260 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 670 | Train: G_Loss: 1.063, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.723, D_sup_loss: 0.311, D_sup_acc: 91.37 Train acc: 90.673 Test acc: 91.470 \n",
      "step: 671 | Train: G_Loss: 0.987, D_unsup_loss_real: 0.664, D_unsup_loss_fake: 0.703, D_sup_loss: 0.309, D_sup_acc: 91.58 Train acc: 90.555 Test acc: 91.350 \n",
      "step: 672 | Train: G_Loss: 1.021, D_unsup_loss_real: 0.720, D_unsup_loss_fake: 0.677, D_sup_loss: 0.310, D_sup_acc: 91.46 Train acc: 90.305 Test acc: 91.200 \n",
      "step: 673 | Train: G_Loss: 1.010, D_unsup_loss_real: 0.731, D_unsup_loss_fake: 0.739, D_sup_loss: 0.312, D_sup_acc: 91.31 Train acc: 90.262 Test acc: 90.910 \n",
      "step: 674 | Train: G_Loss: 1.048, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.685, D_sup_loss: 0.314, D_sup_acc: 91.02 Train acc: 90.525 Test acc: 91.190 \n",
      "step: 675 | Train: G_Loss: 1.025, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.691, D_sup_loss: 0.310, D_sup_acc: 91.30 Train acc: 90.413 Test acc: 91.200 \n",
      "step: 676 | Train: G_Loss: 0.970, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.696, D_sup_loss: 0.308, D_sup_acc: 91.31 Train acc: 90.255 Test acc: 91.210 \n",
      "step: 677 | Train: G_Loss: 1.031, D_unsup_loss_real: 0.674, D_unsup_loss_fake: 0.721, D_sup_loss: 0.311, D_sup_acc: 91.32 Train acc: 90.270 Test acc: 91.070 \n",
      "step: 678 | Train: G_Loss: 0.919, D_unsup_loss_real: 0.706, D_unsup_loss_fake: 0.675, D_sup_loss: 0.312, D_sup_acc: 91.18 Train acc: 90.257 Test acc: 91.130 \n",
      "step: 679 | Train: G_Loss: 0.946, D_unsup_loss_real: 0.666, D_unsup_loss_fake: 0.746, D_sup_loss: 0.311, D_sup_acc: 91.24 Train acc: 90.112 Test acc: 91.060 \n",
      "step: 680 | Train: G_Loss: 0.969, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.728, D_sup_loss: 0.314, D_sup_acc: 91.17 Train acc: 89.970 Test acc: 90.730 \n",
      "step: 681 | Train: G_Loss: 1.077, D_unsup_loss_real: 0.708, D_unsup_loss_fake: 0.663, D_sup_loss: 0.314, D_sup_acc: 90.85 Train acc: 90.250 Test acc: 91.010 \n",
      "step: 682 | Train: G_Loss: 0.990, D_unsup_loss_real: 0.534, D_unsup_loss_fake: 0.739, D_sup_loss: 0.310, D_sup_acc: 91.12 Train acc: 90.567 Test acc: 91.160 \n",
      "step: 683 | Train: G_Loss: 1.013, D_unsup_loss_real: 0.670, D_unsup_loss_fake: 0.679, D_sup_loss: 0.305, D_sup_acc: 91.27 Train acc: 90.447 Test acc: 91.160 \n",
      "step: 684 | Train: G_Loss: 1.028, D_unsup_loss_real: 0.703, D_unsup_loss_fake: 0.731, D_sup_loss: 0.307, D_sup_acc: 91.27 Train acc: 90.607 Test acc: 91.290 \n",
      "step: 685 | Train: G_Loss: 1.032, D_unsup_loss_real: 0.675, D_unsup_loss_fake: 0.775, D_sup_loss: 0.303, D_sup_acc: 91.40 Train acc: 90.452 Test acc: 91.040 \n",
      "step: 686 | Train: G_Loss: 1.016, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.676, D_sup_loss: 0.309, D_sup_acc: 91.15 Train acc: 90.408 Test acc: 90.990 \n",
      "step: 687 | Train: G_Loss: 1.001, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.695, D_sup_loss: 0.306, D_sup_acc: 91.10 Train acc: 90.442 Test acc: 91.170 \n",
      "step: 688 | Train: G_Loss: 1.062, D_unsup_loss_real: 0.631, D_unsup_loss_fake: 0.712, D_sup_loss: 0.303, D_sup_acc: 91.28 Train acc: 90.473 Test acc: 91.130 \n",
      "step: 689 | Train: G_Loss: 0.996, D_unsup_loss_real: 0.733, D_unsup_loss_fake: 0.688, D_sup_loss: 0.305, D_sup_acc: 91.24 Train acc: 90.170 Test acc: 90.950 \n",
      "step: 690 | Train: G_Loss: 1.015, D_unsup_loss_real: 0.662, D_unsup_loss_fake: 0.735, D_sup_loss: 0.309, D_sup_acc: 91.06 Train acc: 90.550 Test acc: 91.380 \n",
      "step: 691 | Train: G_Loss: 0.922, D_unsup_loss_real: 0.754, D_unsup_loss_fake: 0.757, D_sup_loss: 0.304, D_sup_acc: 91.49 Train acc: 90.562 Test acc: 91.330 \n",
      "step: 692 | Train: G_Loss: 0.918, D_unsup_loss_real: 0.677, D_unsup_loss_fake: 0.695, D_sup_loss: 0.305, D_sup_acc: 91.44 Train acc: 90.512 Test acc: 91.360 \n",
      "step: 693 | Train: G_Loss: 1.104, D_unsup_loss_real: 0.635, D_unsup_loss_fake: 0.760, D_sup_loss: 0.309, D_sup_acc: 91.47 Train acc: 90.792 Test acc: 91.660 \n",
      "step: 694 | Train: G_Loss: 0.985, D_unsup_loss_real: 0.720, D_unsup_loss_fake: 0.767, D_sup_loss: 0.305, D_sup_acc: 91.77 Train acc: 90.817 Test acc: 91.680 \n",
      "step: 695 | Train: G_Loss: 1.031, D_unsup_loss_real: 0.661, D_unsup_loss_fake: 0.678, D_sup_loss: 0.303, D_sup_acc: 91.79 Train acc: 90.900 Test acc: 91.740 \n",
      "step: 696 | Train: G_Loss: 0.964, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.650, D_sup_loss: 0.303, D_sup_acc: 91.84 Train acc: 90.890 Test acc: 91.780 \n",
      "step: 697 | Train: G_Loss: 0.994, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.650, D_sup_loss: 0.301, D_sup_acc: 91.88 Train acc: 90.687 Test acc: 91.600 \n",
      "step: 698 | Train: G_Loss: 0.998, D_unsup_loss_real: 0.674, D_unsup_loss_fake: 0.736, D_sup_loss: 0.302, D_sup_acc: 91.71 Train acc: 90.607 Test acc: 91.490 \n",
      "step: 699 | Train: G_Loss: 0.999, D_unsup_loss_real: 0.682, D_unsup_loss_fake: 0.741, D_sup_loss: 0.305, D_sup_acc: 91.60 Train acc: 90.407 Test acc: 91.320 \n",
      "step: 700 | Train: G_Loss: 1.053, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.692, D_sup_loss: 0.311, D_sup_acc: 91.43 Train acc: 90.650 Test acc: 91.490 \n",
      "Train Classifier Accuracy: 90.650%\n",
      "\n",
      "Test Classifier Accuracy: 91.490%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_700.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_700.h5\n",
      "step: 701 | Train: G_Loss: 1.056, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.627, D_sup_loss: 0.304, D_sup_acc: 91.60 Train acc: 90.685 Test acc: 91.470 \n",
      "step: 702 | Train: G_Loss: 0.997, D_unsup_loss_real: 0.658, D_unsup_loss_fake: 0.716, D_sup_loss: 0.302, D_sup_acc: 91.58 Train acc: 90.847 Test acc: 91.590 \n",
      "step: 703 | Train: G_Loss: 0.998, D_unsup_loss_real: 0.648, D_unsup_loss_fake: 0.720, D_sup_loss: 0.300, D_sup_acc: 91.70 Train acc: 90.837 Test acc: 91.660 \n",
      "step: 704 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.647, D_unsup_loss_fake: 0.728, D_sup_loss: 0.299, D_sup_acc: 91.77 Train acc: 90.922 Test acc: 91.810 \n",
      "step: 705 | Train: G_Loss: 1.046, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.709, D_sup_loss: 0.295, D_sup_acc: 91.91 Train acc: 90.878 Test acc: 91.740 \n",
      "step: 706 | Train: G_Loss: 1.017, D_unsup_loss_real: 0.696, D_unsup_loss_fake: 0.657, D_sup_loss: 0.295, D_sup_acc: 91.84 Train acc: 90.957 Test acc: 91.640 \n",
      "step: 707 | Train: G_Loss: 0.952, D_unsup_loss_real: 0.700, D_unsup_loss_fake: 0.705, D_sup_loss: 0.294, D_sup_acc: 91.75 Train acc: 90.892 Test acc: 91.710 \n",
      "step: 708 | Train: G_Loss: 1.014, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.802, D_sup_loss: 0.298, D_sup_acc: 91.81 Train acc: 90.713 Test acc: 91.460 \n",
      "step: 709 | Train: G_Loss: 1.042, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.661, D_sup_loss: 0.301, D_sup_acc: 91.57 Train acc: 90.618 Test acc: 91.430 \n",
      "step: 710 | Train: G_Loss: 0.978, D_unsup_loss_real: 0.667, D_unsup_loss_fake: 0.694, D_sup_loss: 0.300, D_sup_acc: 91.54 Train acc: 90.855 Test acc: 91.670 \n",
      "step: 711 | Train: G_Loss: 1.030, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.777, D_sup_loss: 0.300, D_sup_acc: 91.78 Train acc: 90.750 Test acc: 91.740 \n",
      "step: 712 | Train: G_Loss: 1.036, D_unsup_loss_real: 0.734, D_unsup_loss_fake: 0.721, D_sup_loss: 0.301, D_sup_acc: 91.84 Train acc: 90.617 Test acc: 91.480 \n",
      "step: 713 | Train: G_Loss: 1.069, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.692, D_sup_loss: 0.307, D_sup_acc: 91.59 Train acc: 90.863 Test acc: 91.610 \n",
      "step: 714 | Train: G_Loss: 0.941, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.638, D_sup_loss: 0.300, D_sup_acc: 91.72 Train acc: 90.743 Test acc: 91.540 \n",
      "step: 715 | Train: G_Loss: 0.985, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.782, D_sup_loss: 0.300, D_sup_acc: 91.65 Train acc: 90.992 Test acc: 91.810 \n",
      "step: 716 | Train: G_Loss: 1.025, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.709, D_sup_loss: 0.295, D_sup_acc: 91.91 Train acc: 90.885 Test acc: 91.590 \n",
      "step: 717 | Train: G_Loss: 1.084, D_unsup_loss_real: 0.653, D_unsup_loss_fake: 0.662, D_sup_loss: 0.295, D_sup_acc: 91.70 Train acc: 90.933 Test acc: 91.610 \n",
      "step: 718 | Train: G_Loss: 1.027, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.644, D_sup_loss: 0.295, D_sup_acc: 91.72 Train acc: 90.960 Test acc: 91.660 \n",
      "step: 719 | Train: G_Loss: 1.105, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.707, D_sup_loss: 0.294, D_sup_acc: 91.77 Train acc: 90.950 Test acc: 91.690 \n",
      "step: 720 | Train: G_Loss: 0.972, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.621, D_sup_loss: 0.296, D_sup_acc: 91.80 Train acc: 90.753 Test acc: 91.480 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 721 | Train: G_Loss: 1.030, D_unsup_loss_real: 0.703, D_unsup_loss_fake: 0.674, D_sup_loss: 0.297, D_sup_acc: 91.59 Train acc: 90.827 Test acc: 91.600 \n",
      "step: 722 | Train: G_Loss: 1.047, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.718, D_sup_loss: 0.299, D_sup_acc: 91.71 Train acc: 90.457 Test acc: 91.260 \n",
      "step: 723 | Train: G_Loss: 1.082, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.697, D_sup_loss: 0.305, D_sup_acc: 91.37 Train acc: 90.815 Test acc: 91.590 \n",
      "step: 724 | Train: G_Loss: 1.052, D_unsup_loss_real: 0.681, D_unsup_loss_fake: 0.638, D_sup_loss: 0.298, D_sup_acc: 91.70 Train acc: 90.587 Test acc: 91.370 \n",
      "step: 725 | Train: G_Loss: 0.940, D_unsup_loss_real: 0.710, D_unsup_loss_fake: 0.679, D_sup_loss: 0.299, D_sup_acc: 91.48 Train acc: 90.568 Test acc: 91.310 \n",
      "step: 726 | Train: G_Loss: 0.994, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.636, D_sup_loss: 0.298, D_sup_acc: 91.42 Train acc: 90.553 Test acc: 91.290 \n",
      "step: 727 | Train: G_Loss: 0.971, D_unsup_loss_real: 0.639, D_unsup_loss_fake: 0.662, D_sup_loss: 0.294, D_sup_acc: 91.40 Train acc: 90.638 Test acc: 91.400 \n",
      "step: 728 | Train: G_Loss: 0.964, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.819, D_sup_loss: 0.291, D_sup_acc: 91.51 Train acc: 90.817 Test acc: 91.660 \n",
      "step: 729 | Train: G_Loss: 1.125, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.815, D_sup_loss: 0.289, D_sup_acc: 91.77 Train acc: 90.610 Test acc: 91.380 \n",
      "step: 730 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.654, D_unsup_loss_fake: 0.674, D_sup_loss: 0.297, D_sup_acc: 91.49 Train acc: 90.655 Test acc: 91.380 \n",
      "step: 731 | Train: G_Loss: 1.055, D_unsup_loss_real: 0.703, D_unsup_loss_fake: 0.697, D_sup_loss: 0.299, D_sup_acc: 91.49 Train acc: 90.738 Test acc: 91.560 \n",
      "step: 732 | Train: G_Loss: 1.086, D_unsup_loss_real: 0.702, D_unsup_loss_fake: 0.640, D_sup_loss: 0.299, D_sup_acc: 91.67 Train acc: 90.797 Test acc: 91.510 \n",
      "step: 733 | Train: G_Loss: 1.002, D_unsup_loss_real: 0.657, D_unsup_loss_fake: 0.720, D_sup_loss: 0.298, D_sup_acc: 91.62 Train acc: 90.700 Test acc: 91.590 \n",
      "step: 734 | Train: G_Loss: 1.044, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.698, D_sup_loss: 0.300, D_sup_acc: 91.70 Train acc: 90.623 Test acc: 91.420 \n",
      "step: 735 | Train: G_Loss: 1.018, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.627, D_sup_loss: 0.300, D_sup_acc: 91.53 Train acc: 90.857 Test acc: 91.600 \n",
      "step: 736 | Train: G_Loss: 1.030, D_unsup_loss_real: 0.707, D_unsup_loss_fake: 0.736, D_sup_loss: 0.294, D_sup_acc: 91.71 Train acc: 91.030 Test acc: 91.740 \n",
      "step: 737 | Train: G_Loss: 1.075, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.680, D_sup_loss: 0.295, D_sup_acc: 91.84 Train acc: 90.980 Test acc: 91.610 \n",
      "step: 738 | Train: G_Loss: 1.047, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.714, D_sup_loss: 0.296, D_sup_acc: 91.72 Train acc: 90.670 Test acc: 91.400 \n",
      "step: 739 | Train: G_Loss: 1.078, D_unsup_loss_real: 0.659, D_unsup_loss_fake: 0.703, D_sup_loss: 0.297, D_sup_acc: 91.51 Train acc: 90.482 Test acc: 91.160 \n",
      "step: 740 | Train: G_Loss: 1.047, D_unsup_loss_real: 0.682, D_unsup_loss_fake: 0.661, D_sup_loss: 0.301, D_sup_acc: 91.27 Train acc: 90.528 Test acc: 91.170 \n",
      "step: 741 | Train: G_Loss: 0.988, D_unsup_loss_real: 0.670, D_unsup_loss_fake: 0.733, D_sup_loss: 0.298, D_sup_acc: 91.28 Train acc: 90.812 Test acc: 91.700 \n",
      "step: 742 | Train: G_Loss: 1.042, D_unsup_loss_real: 0.631, D_unsup_loss_fake: 0.729, D_sup_loss: 0.293, D_sup_acc: 91.80 Train acc: 90.907 Test acc: 91.720 \n",
      "step: 743 | Train: G_Loss: 1.036, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.697, D_sup_loss: 0.292, D_sup_acc: 91.82 Train acc: 90.927 Test acc: 91.880 \n",
      "step: 744 | Train: G_Loss: 0.961, D_unsup_loss_real: 0.659, D_unsup_loss_fake: 0.682, D_sup_loss: 0.293, D_sup_acc: 91.98 Train acc: 91.178 Test acc: 92.080 \n",
      "step: 745 | Train: G_Loss: 1.066, D_unsup_loss_real: 0.657, D_unsup_loss_fake: 0.741, D_sup_loss: 0.290, D_sup_acc: 92.18 Train acc: 91.163 Test acc: 92.010 \n",
      "step: 746 | Train: G_Loss: 1.046, D_unsup_loss_real: 0.677, D_unsup_loss_fake: 0.678, D_sup_loss: 0.291, D_sup_acc: 92.11 Train acc: 90.778 Test acc: 91.480 \n",
      "step: 747 | Train: G_Loss: 1.053, D_unsup_loss_real: 0.681, D_unsup_loss_fake: 0.664, D_sup_loss: 0.298, D_sup_acc: 91.59 Train acc: 90.680 Test acc: 91.420 \n",
      "step: 748 | Train: G_Loss: 1.009, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.666, D_sup_loss: 0.295, D_sup_acc: 91.53 Train acc: 90.715 Test acc: 91.330 \n",
      "step: 749 | Train: G_Loss: 1.047, D_unsup_loss_real: 0.660, D_unsup_loss_fake: 0.702, D_sup_loss: 0.296, D_sup_acc: 91.44 Train acc: 90.815 Test acc: 91.520 \n",
      "step: 750 | Train: G_Loss: 1.048, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.720, D_sup_loss: 0.294, D_sup_acc: 91.63 Train acc: 90.588 Test acc: 91.340 \n",
      "step: 751 | Train: G_Loss: 1.028, D_unsup_loss_real: 0.655, D_unsup_loss_fake: 0.756, D_sup_loss: 0.300, D_sup_acc: 91.45 Train acc: 90.503 Test acc: 91.140 \n",
      "step: 752 | Train: G_Loss: 1.001, D_unsup_loss_real: 0.663, D_unsup_loss_fake: 0.691, D_sup_loss: 0.301, D_sup_acc: 91.25 Train acc: 90.637 Test acc: 91.350 \n",
      "step: 753 | Train: G_Loss: 1.049, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.668, D_sup_loss: 0.296, D_sup_acc: 91.46 Train acc: 90.648 Test acc: 91.440 \n",
      "step: 754 | Train: G_Loss: 1.048, D_unsup_loss_real: 0.667, D_unsup_loss_fake: 0.681, D_sup_loss: 0.294, D_sup_acc: 91.55 Train acc: 90.903 Test acc: 91.710 \n",
      "step: 755 | Train: G_Loss: 1.045, D_unsup_loss_real: 0.685, D_unsup_loss_fake: 0.668, D_sup_loss: 0.290, D_sup_acc: 91.81 Train acc: 90.915 Test acc: 91.690 \n",
      "step: 756 | Train: G_Loss: 1.058, D_unsup_loss_real: 0.660, D_unsup_loss_fake: 0.644, D_sup_loss: 0.294, D_sup_acc: 91.80 Train acc: 90.942 Test acc: 91.810 \n",
      "step: 757 | Train: G_Loss: 1.054, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.666, D_sup_loss: 0.292, D_sup_acc: 91.91 Train acc: 90.845 Test acc: 91.680 \n",
      "step: 758 | Train: G_Loss: 0.966, D_unsup_loss_real: 0.655, D_unsup_loss_fake: 0.720, D_sup_loss: 0.291, D_sup_acc: 91.79 Train acc: 91.143 Test acc: 91.970 \n",
      "step: 759 | Train: G_Loss: 1.075, D_unsup_loss_real: 0.657, D_unsup_loss_fake: 0.621, D_sup_loss: 0.288, D_sup_acc: 92.06 Train acc: 91.198 Test acc: 92.040 \n",
      "step: 760 | Train: G_Loss: 1.153, D_unsup_loss_real: 0.692, D_unsup_loss_fake: 0.752, D_sup_loss: 0.286, D_sup_acc: 92.14 Train acc: 91.198 Test acc: 91.780 \n",
      "step: 761 | Train: G_Loss: 1.017, D_unsup_loss_real: 0.691, D_unsup_loss_fake: 0.702, D_sup_loss: 0.292, D_sup_acc: 91.88 Train acc: 91.097 Test acc: 91.720 \n",
      "step: 762 | Train: G_Loss: 1.075, D_unsup_loss_real: 0.653, D_unsup_loss_fake: 0.662, D_sup_loss: 0.294, D_sup_acc: 91.82 Train acc: 91.200 Test acc: 91.820 \n",
      "step: 763 | Train: G_Loss: 1.016, D_unsup_loss_real: 0.708, D_unsup_loss_fake: 0.677, D_sup_loss: 0.291, D_sup_acc: 91.92 Train acc: 91.120 Test acc: 91.870 \n",
      "step: 764 | Train: G_Loss: 1.025, D_unsup_loss_real: 0.707, D_unsup_loss_fake: 0.679, D_sup_loss: 0.291, D_sup_acc: 91.97 Train acc: 91.120 Test acc: 91.650 \n",
      "step: 765 | Train: G_Loss: 1.037, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.693, D_sup_loss: 0.291, D_sup_acc: 91.76 Train acc: 91.210 Test acc: 91.880 \n",
      "step: 766 | Train: G_Loss: 1.011, D_unsup_loss_real: 0.696, D_unsup_loss_fake: 0.667, D_sup_loss: 0.289, D_sup_acc: 91.98 Train acc: 91.200 Test acc: 91.770 \n",
      "step: 767 | Train: G_Loss: 1.117, D_unsup_loss_real: 0.662, D_unsup_loss_fake: 0.665, D_sup_loss: 0.290, D_sup_acc: 91.87 Train acc: 91.333 Test acc: 92.010 \n",
      "step: 768 | Train: G_Loss: 0.956, D_unsup_loss_real: 0.652, D_unsup_loss_fake: 0.609, D_sup_loss: 0.288, D_sup_acc: 92.11 Train acc: 91.358 Test acc: 92.060 \n",
      "step: 769 | Train: G_Loss: 1.044, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.644, D_sup_loss: 0.285, D_sup_acc: 92.16 Train acc: 91.068 Test acc: 91.840 \n",
      "step: 770 | Train: G_Loss: 1.030, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.697, D_sup_loss: 0.288, D_sup_acc: 91.94 Train acc: 91.065 Test acc: 91.850 \n",
      "step: 771 | Train: G_Loss: 0.998, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.783, D_sup_loss: 0.285, D_sup_acc: 91.95 Train acc: 91.122 Test acc: 91.850 \n",
      "step: 772 | Train: G_Loss: 1.011, D_unsup_loss_real: 0.665, D_unsup_loss_fake: 0.712, D_sup_loss: 0.283, D_sup_acc: 91.95 Train acc: 91.025 Test acc: 91.760 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 773 | Train: G_Loss: 1.062, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.720, D_sup_loss: 0.286, D_sup_acc: 91.86 Train acc: 91.063 Test acc: 91.750 \n",
      "step: 774 | Train: G_Loss: 1.060, D_unsup_loss_real: 0.707, D_unsup_loss_fake: 0.676, D_sup_loss: 0.289, D_sup_acc: 91.85 Train acc: 91.232 Test acc: 91.860 \n",
      "step: 775 | Train: G_Loss: 1.032, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.732, D_sup_loss: 0.287, D_sup_acc: 91.96 Train acc: 91.362 Test acc: 92.090 \n",
      "step: 776 | Train: G_Loss: 1.072, D_unsup_loss_real: 0.642, D_unsup_loss_fake: 0.674, D_sup_loss: 0.280, D_sup_acc: 92.19 Train acc: 91.238 Test acc: 91.900 \n",
      "step: 777 | Train: G_Loss: 1.078, D_unsup_loss_real: 0.679, D_unsup_loss_fake: 0.750, D_sup_loss: 0.282, D_sup_acc: 92.00 Train acc: 91.075 Test acc: 91.850 \n",
      "step: 778 | Train: G_Loss: 1.116, D_unsup_loss_real: 0.686, D_unsup_loss_fake: 0.640, D_sup_loss: 0.286, D_sup_acc: 91.95 Train acc: 91.177 Test acc: 92.020 \n",
      "step: 779 | Train: G_Loss: 1.054, D_unsup_loss_real: 0.692, D_unsup_loss_fake: 0.642, D_sup_loss: 0.284, D_sup_acc: 92.12 Train acc: 91.267 Test acc: 92.050 \n",
      "step: 780 | Train: G_Loss: 0.970, D_unsup_loss_real: 0.685, D_unsup_loss_fake: 0.630, D_sup_loss: 0.284, D_sup_acc: 92.15 Train acc: 91.373 Test acc: 92.150 \n",
      "step: 781 | Train: G_Loss: 1.016, D_unsup_loss_real: 0.653, D_unsup_loss_fake: 0.759, D_sup_loss: 0.285, D_sup_acc: 92.25 Train acc: 91.285 Test acc: 92.060 \n",
      "step: 782 | Train: G_Loss: 1.032, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.643, D_sup_loss: 0.282, D_sup_acc: 92.16 Train acc: 91.407 Test acc: 91.990 \n",
      "step: 783 | Train: G_Loss: 0.999, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.705, D_sup_loss: 0.279, D_sup_acc: 92.09 Train acc: 91.355 Test acc: 92.030 \n",
      "step: 784 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.654, D_unsup_loss_fake: 0.759, D_sup_loss: 0.283, D_sup_acc: 92.13 Train acc: 91.382 Test acc: 92.050 \n",
      "step: 785 | Train: G_Loss: 0.986, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.637, D_sup_loss: 0.286, D_sup_acc: 92.15 Train acc: 91.413 Test acc: 92.070 \n",
      "step: 786 | Train: G_Loss: 1.063, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.664, D_sup_loss: 0.283, D_sup_acc: 92.17 Train acc: 91.338 Test acc: 92.120 \n",
      "step: 787 | Train: G_Loss: 0.994, D_unsup_loss_real: 0.723, D_unsup_loss_fake: 0.694, D_sup_loss: 0.282, D_sup_acc: 92.22 Train acc: 91.417 Test acc: 92.050 \n",
      "step: 788 | Train: G_Loss: 1.069, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.767, D_sup_loss: 0.283, D_sup_acc: 92.15 Train acc: 91.255 Test acc: 91.910 \n",
      "step: 789 | Train: G_Loss: 1.052, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.724, D_sup_loss: 0.286, D_sup_acc: 92.01 Train acc: 91.458 Test acc: 92.140 \n",
      "step: 790 | Train: G_Loss: 1.032, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.653, D_sup_loss: 0.285, D_sup_acc: 92.24 Train acc: 91.222 Test acc: 91.840 \n",
      "step: 791 | Train: G_Loss: 0.959, D_unsup_loss_real: 0.681, D_unsup_loss_fake: 0.737, D_sup_loss: 0.288, D_sup_acc: 91.94 Train acc: 91.237 Test acc: 91.860 \n",
      "step: 792 | Train: G_Loss: 1.066, D_unsup_loss_real: 0.661, D_unsup_loss_fake: 0.710, D_sup_loss: 0.285, D_sup_acc: 91.96 Train acc: 91.283 Test acc: 92.000 \n",
      "step: 793 | Train: G_Loss: 0.984, D_unsup_loss_real: 0.664, D_unsup_loss_fake: 0.723, D_sup_loss: 0.284, D_sup_acc: 92.10 Train acc: 91.247 Test acc: 91.960 \n",
      "step: 794 | Train: G_Loss: 1.143, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.757, D_sup_loss: 0.285, D_sup_acc: 92.06 Train acc: 91.305 Test acc: 91.960 \n",
      "step: 795 | Train: G_Loss: 0.998, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.606, D_sup_loss: 0.284, D_sup_acc: 92.06 Train acc: 91.383 Test acc: 92.140 \n",
      "step: 796 | Train: G_Loss: 1.051, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.678, D_sup_loss: 0.279, D_sup_acc: 92.24 Train acc: 91.610 Test acc: 92.290 \n",
      "step: 797 | Train: G_Loss: 1.083, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.697, D_sup_loss: 0.274, D_sup_acc: 92.39 Train acc: 91.640 Test acc: 92.430 \n",
      "step: 798 | Train: G_Loss: 1.034, D_unsup_loss_real: 0.661, D_unsup_loss_fake: 0.611, D_sup_loss: 0.275, D_sup_acc: 92.53 Train acc: 91.573 Test acc: 92.440 \n",
      "step: 799 | Train: G_Loss: 0.983, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.774, D_sup_loss: 0.272, D_sup_acc: 92.54 Train acc: 91.487 Test acc: 92.160 \n",
      "step: 800 | Train: G_Loss: 1.057, D_unsup_loss_real: 0.727, D_unsup_loss_fake: 0.768, D_sup_loss: 0.274, D_sup_acc: 92.26 Train acc: 91.187 Test acc: 91.810 \n",
      "Train Classifier Accuracy: 91.187%\n",
      "\n",
      "Test Classifier Accuracy: 91.810%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_800.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_800.h5\n",
      "step: 801 | Train: G_Loss: 0.927, D_unsup_loss_real: 0.665, D_unsup_loss_fake: 0.720, D_sup_loss: 0.282, D_sup_acc: 91.91 Train acc: 91.220 Test acc: 91.940 \n",
      "step: 802 | Train: G_Loss: 0.970, D_unsup_loss_real: 0.647, D_unsup_loss_fake: 0.733, D_sup_loss: 0.281, D_sup_acc: 92.04 Train acc: 91.142 Test acc: 91.800 \n",
      "step: 803 | Train: G_Loss: 1.065, D_unsup_loss_real: 0.650, D_unsup_loss_fake: 0.716, D_sup_loss: 0.284, D_sup_acc: 91.90 Train acc: 91.025 Test acc: 91.670 \n",
      "step: 804 | Train: G_Loss: 1.082, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.658, D_sup_loss: 0.289, D_sup_acc: 91.78 Train acc: 91.177 Test acc: 91.820 \n",
      "step: 805 | Train: G_Loss: 1.050, D_unsup_loss_real: 0.721, D_unsup_loss_fake: 0.657, D_sup_loss: 0.286, D_sup_acc: 91.92 Train acc: 91.318 Test acc: 92.030 \n",
      "step: 806 | Train: G_Loss: 0.973, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.653, D_sup_loss: 0.283, D_sup_acc: 92.13 Train acc: 91.223 Test acc: 91.960 \n",
      "step: 807 | Train: G_Loss: 1.026, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.731, D_sup_loss: 0.282, D_sup_acc: 92.06 Train acc: 91.258 Test acc: 92.010 \n",
      "step: 808 | Train: G_Loss: 1.002, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.682, D_sup_loss: 0.281, D_sup_acc: 92.11 Train acc: 91.198 Test acc: 91.900 \n",
      "step: 809 | Train: G_Loss: 1.111, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.668, D_sup_loss: 0.281, D_sup_acc: 92.00 Train acc: 91.057 Test acc: 91.740 \n",
      "step: 810 | Train: G_Loss: 1.008, D_unsup_loss_real: 0.687, D_unsup_loss_fake: 0.631, D_sup_loss: 0.282, D_sup_acc: 91.84 Train acc: 90.942 Test acc: 91.710 \n",
      "step: 811 | Train: G_Loss: 0.993, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.681, D_sup_loss: 0.282, D_sup_acc: 91.81 Train acc: 90.827 Test acc: 91.620 \n",
      "step: 812 | Train: G_Loss: 1.082, D_unsup_loss_real: 0.727, D_unsup_loss_fake: 0.813, D_sup_loss: 0.286, D_sup_acc: 91.73 Train acc: 90.610 Test acc: 91.350 \n",
      "step: 813 | Train: G_Loss: 1.002, D_unsup_loss_real: 0.681, D_unsup_loss_fake: 0.756, D_sup_loss: 0.295, D_sup_acc: 91.46 Train acc: 90.820 Test acc: 91.520 \n",
      "step: 814 | Train: G_Loss: 1.024, D_unsup_loss_real: 0.660, D_unsup_loss_fake: 0.644, D_sup_loss: 0.294, D_sup_acc: 91.63 Train acc: 90.898 Test acc: 91.560 \n",
      "step: 815 | Train: G_Loss: 1.097, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.673, D_sup_loss: 0.290, D_sup_acc: 91.67 Train acc: 90.792 Test acc: 91.450 \n",
      "step: 816 | Train: G_Loss: 1.047, D_unsup_loss_real: 0.644, D_unsup_loss_fake: 0.704, D_sup_loss: 0.291, D_sup_acc: 91.56 Train acc: 90.772 Test acc: 91.420 \n",
      "step: 817 | Train: G_Loss: 1.097, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.640, D_sup_loss: 0.293, D_sup_acc: 91.53 Train acc: 91.033 Test acc: 91.770 \n",
      "step: 818 | Train: G_Loss: 1.029, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.680, D_sup_loss: 0.284, D_sup_acc: 91.87 Train acc: 91.080 Test acc: 91.950 \n",
      "step: 819 | Train: G_Loss: 1.047, D_unsup_loss_real: 0.703, D_unsup_loss_fake: 0.755, D_sup_loss: 0.282, D_sup_acc: 92.05 Train acc: 90.932 Test acc: 91.660 \n",
      "step: 820 | Train: G_Loss: 1.034, D_unsup_loss_real: 0.666, D_unsup_loss_fake: 0.711, D_sup_loss: 0.286, D_sup_acc: 91.77 Train acc: 90.715 Test acc: 91.450 \n",
      "step: 821 | Train: G_Loss: 1.063, D_unsup_loss_real: 0.653, D_unsup_loss_fake: 0.701, D_sup_loss: 0.287, D_sup_acc: 91.56 Train acc: 90.990 Test acc: 91.790 \n",
      "step: 822 | Train: G_Loss: 1.043, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.702, D_sup_loss: 0.285, D_sup_acc: 91.89 Train acc: 91.275 Test acc: 91.960 \n",
      "step: 823 | Train: G_Loss: 1.136, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.672, D_sup_loss: 0.282, D_sup_acc: 92.06 Train acc: 91.297 Test acc: 92.080 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 824 | Train: G_Loss: 1.040, D_unsup_loss_real: 0.639, D_unsup_loss_fake: 0.623, D_sup_loss: 0.279, D_sup_acc: 92.18 Train acc: 91.245 Test acc: 92.050 \n",
      "step: 825 | Train: G_Loss: 1.014, D_unsup_loss_real: 0.690, D_unsup_loss_fake: 0.640, D_sup_loss: 0.282, D_sup_acc: 92.15 Train acc: 91.263 Test acc: 91.970 \n",
      "step: 826 | Train: G_Loss: 1.008, D_unsup_loss_real: 0.678, D_unsup_loss_fake: 0.760, D_sup_loss: 0.283, D_sup_acc: 92.07 Train acc: 91.317 Test acc: 91.930 \n",
      "step: 827 | Train: G_Loss: 0.985, D_unsup_loss_real: 0.684, D_unsup_loss_fake: 0.737, D_sup_loss: 0.281, D_sup_acc: 92.02 Train acc: 91.365 Test acc: 92.060 \n",
      "step: 828 | Train: G_Loss: 1.057, D_unsup_loss_real: 0.687, D_unsup_loss_fake: 0.691, D_sup_loss: 0.281, D_sup_acc: 92.16 Train acc: 91.255 Test acc: 91.980 \n",
      "step: 829 | Train: G_Loss: 1.002, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.646, D_sup_loss: 0.282, D_sup_acc: 92.08 Train acc: 91.390 Test acc: 92.010 \n",
      "step: 830 | Train: G_Loss: 1.008, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.692, D_sup_loss: 0.278, D_sup_acc: 92.11 Train acc: 91.560 Test acc: 92.130 \n",
      "step: 831 | Train: G_Loss: 1.072, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.657, D_sup_loss: 0.274, D_sup_acc: 92.23 Train acc: 91.420 Test acc: 92.200 \n",
      "step: 832 | Train: G_Loss: 1.046, D_unsup_loss_real: 0.635, D_unsup_loss_fake: 0.728, D_sup_loss: 0.272, D_sup_acc: 92.30 Train acc: 91.437 Test acc: 92.220 \n",
      "step: 833 | Train: G_Loss: 1.047, D_unsup_loss_real: 0.688, D_unsup_loss_fake: 0.750, D_sup_loss: 0.273, D_sup_acc: 92.32 Train acc: 91.507 Test acc: 92.340 \n",
      "step: 834 | Train: G_Loss: 1.077, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.742, D_sup_loss: 0.277, D_sup_acc: 92.44 Train acc: 91.670 Test acc: 92.280 \n",
      "step: 835 | Train: G_Loss: 1.099, D_unsup_loss_real: 0.680, D_unsup_loss_fake: 0.693, D_sup_loss: 0.275, D_sup_acc: 92.38 Train acc: 91.673 Test acc: 92.360 \n",
      "step: 836 | Train: G_Loss: 1.054, D_unsup_loss_real: 0.706, D_unsup_loss_fake: 0.622, D_sup_loss: 0.278, D_sup_acc: 92.46 Train acc: 91.732 Test acc: 92.460 \n",
      "step: 837 | Train: G_Loss: 0.985, D_unsup_loss_real: 0.673, D_unsup_loss_fake: 0.735, D_sup_loss: 0.275, D_sup_acc: 92.56 Train acc: 91.727 Test acc: 92.420 \n",
      "step: 838 | Train: G_Loss: 1.072, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.685, D_sup_loss: 0.275, D_sup_acc: 92.52 Train acc: 91.698 Test acc: 92.480 \n",
      "step: 839 | Train: G_Loss: 1.086, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.715, D_sup_loss: 0.272, D_sup_acc: 92.58 Train acc: 91.620 Test acc: 92.350 \n",
      "step: 840 | Train: G_Loss: 1.072, D_unsup_loss_real: 0.701, D_unsup_loss_fake: 0.704, D_sup_loss: 0.273, D_sup_acc: 92.45 Train acc: 91.793 Test acc: 92.510 \n",
      "step: 841 | Train: G_Loss: 1.054, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.647, D_sup_loss: 0.274, D_sup_acc: 92.60 Train acc: 91.795 Test acc: 92.480 \n",
      "step: 842 | Train: G_Loss: 1.031, D_unsup_loss_real: 0.707, D_unsup_loss_fake: 0.704, D_sup_loss: 0.274, D_sup_acc: 92.58 Train acc: 91.700 Test acc: 92.440 \n",
      "step: 843 | Train: G_Loss: 1.042, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.670, D_sup_loss: 0.275, D_sup_acc: 92.54 Train acc: 91.803 Test acc: 92.500 \n",
      "step: 844 | Train: G_Loss: 0.984, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.686, D_sup_loss: 0.272, D_sup_acc: 92.59 Train acc: 91.813 Test acc: 92.580 \n",
      "step: 845 | Train: G_Loss: 0.992, D_unsup_loss_real: 0.666, D_unsup_loss_fake: 0.709, D_sup_loss: 0.273, D_sup_acc: 92.67 Train acc: 91.797 Test acc: 92.490 \n",
      "step: 846 | Train: G_Loss: 1.119, D_unsup_loss_real: 0.679, D_unsup_loss_fake: 0.700, D_sup_loss: 0.274, D_sup_acc: 92.58 Train acc: 91.660 Test acc: 92.350 \n",
      "step: 847 | Train: G_Loss: 1.084, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.701, D_sup_loss: 0.278, D_sup_acc: 92.45 Train acc: 91.612 Test acc: 92.350 \n",
      "step: 848 | Train: G_Loss: 1.078, D_unsup_loss_real: 0.734, D_unsup_loss_fake: 0.680, D_sup_loss: 0.278, D_sup_acc: 92.45 Train acc: 91.465 Test acc: 92.150 \n",
      "step: 849 | Train: G_Loss: 0.960, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.637, D_sup_loss: 0.279, D_sup_acc: 92.25 Train acc: 91.442 Test acc: 92.140 \n",
      "step: 850 | Train: G_Loss: 1.010, D_unsup_loss_real: 0.683, D_unsup_loss_fake: 0.764, D_sup_loss: 0.278, D_sup_acc: 92.24 Train acc: 91.657 Test acc: 92.320 \n",
      "step: 851 | Train: G_Loss: 1.061, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.767, D_sup_loss: 0.275, D_sup_acc: 92.42 Train acc: 91.623 Test acc: 92.240 \n",
      "step: 852 | Train: G_Loss: 1.022, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.683, D_sup_loss: 0.274, D_sup_acc: 92.34 Train acc: 91.610 Test acc: 92.310 \n",
      "step: 853 | Train: G_Loss: 1.033, D_unsup_loss_real: 0.679, D_unsup_loss_fake: 0.645, D_sup_loss: 0.273, D_sup_acc: 92.41 Train acc: 91.457 Test acc: 91.980 \n",
      "step: 854 | Train: G_Loss: 1.139, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.675, D_sup_loss: 0.275, D_sup_acc: 92.08 Train acc: 91.265 Test acc: 91.840 \n",
      "step: 855 | Train: G_Loss: 0.992, D_unsup_loss_real: 0.715, D_unsup_loss_fake: 0.630, D_sup_loss: 0.275, D_sup_acc: 91.94 Train acc: 91.390 Test acc: 92.090 \n",
      "step: 856 | Train: G_Loss: 0.943, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.719, D_sup_loss: 0.278, D_sup_acc: 92.19 Train acc: 91.517 Test acc: 92.130 \n",
      "step: 857 | Train: G_Loss: 1.105, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.741, D_sup_loss: 0.275, D_sup_acc: 92.23 Train acc: 91.477 Test acc: 92.130 \n",
      "step: 858 | Train: G_Loss: 1.037, D_unsup_loss_real: 0.652, D_unsup_loss_fake: 0.721, D_sup_loss: 0.274, D_sup_acc: 92.23 Train acc: 91.478 Test acc: 92.080 \n",
      "step: 859 | Train: G_Loss: 1.027, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.620, D_sup_loss: 0.274, D_sup_acc: 92.18 Train acc: 91.800 Test acc: 92.440 \n",
      "step: 860 | Train: G_Loss: 1.027, D_unsup_loss_real: 0.690, D_unsup_loss_fake: 0.671, D_sup_loss: 0.268, D_sup_acc: 92.54 Train acc: 91.887 Test acc: 92.480 \n",
      "step: 861 | Train: G_Loss: 1.094, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.685, D_sup_loss: 0.267, D_sup_acc: 92.58 Train acc: 91.928 Test acc: 92.630 \n",
      "step: 862 | Train: G_Loss: 1.087, D_unsup_loss_real: 0.658, D_unsup_loss_fake: 0.660, D_sup_loss: 0.265, D_sup_acc: 92.72 Train acc: 91.723 Test acc: 92.370 \n",
      "step: 863 | Train: G_Loss: 1.014, D_unsup_loss_real: 0.686, D_unsup_loss_fake: 0.658, D_sup_loss: 0.270, D_sup_acc: 92.47 Train acc: 91.790 Test acc: 92.410 \n",
      "step: 864 | Train: G_Loss: 1.030, D_unsup_loss_real: 0.698, D_unsup_loss_fake: 0.728, D_sup_loss: 0.268, D_sup_acc: 92.51 Train acc: 91.725 Test acc: 92.440 \n",
      "step: 865 | Train: G_Loss: 1.037, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.658, D_sup_loss: 0.273, D_sup_acc: 92.54 Train acc: 91.745 Test acc: 92.450 \n",
      "step: 866 | Train: G_Loss: 1.069, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.616, D_sup_loss: 0.270, D_sup_acc: 92.55 Train acc: 91.778 Test acc: 92.470 \n",
      "step: 867 | Train: G_Loss: 1.080, D_unsup_loss_real: 0.680, D_unsup_loss_fake: 0.722, D_sup_loss: 0.267, D_sup_acc: 92.57 Train acc: 91.750 Test acc: 92.470 \n",
      "step: 868 | Train: G_Loss: 1.062, D_unsup_loss_real: 0.669, D_unsup_loss_fake: 0.721, D_sup_loss: 0.265, D_sup_acc: 92.57 Train acc: 91.705 Test acc: 92.420 \n",
      "step: 869 | Train: G_Loss: 1.016, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.630, D_sup_loss: 0.268, D_sup_acc: 92.52 Train acc: 91.782 Test acc: 92.350 \n",
      "step: 870 | Train: G_Loss: 1.071, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.684, D_sup_loss: 0.267, D_sup_acc: 92.45 Train acc: 91.833 Test acc: 92.450 \n",
      "step: 871 | Train: G_Loss: 1.036, D_unsup_loss_real: 0.702, D_unsup_loss_fake: 0.759, D_sup_loss: 0.263, D_sup_acc: 92.55 Train acc: 91.620 Test acc: 92.480 \n",
      "step: 872 | Train: G_Loss: 1.019, D_unsup_loss_real: 0.694, D_unsup_loss_fake: 0.649, D_sup_loss: 0.271, D_sup_acc: 92.58 Train acc: 91.790 Test acc: 92.460 \n",
      "step: 873 | Train: G_Loss: 1.064, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.704, D_sup_loss: 0.267, D_sup_acc: 92.56 Train acc: 91.598 Test acc: 92.350 \n",
      "step: 874 | Train: G_Loss: 1.014, D_unsup_loss_real: 0.674, D_unsup_loss_fake: 0.672, D_sup_loss: 0.269, D_sup_acc: 92.45 Train acc: 91.787 Test acc: 92.520 \n",
      "step: 875 | Train: G_Loss: 0.994, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.712, D_sup_loss: 0.267, D_sup_acc: 92.61 Train acc: 91.913 Test acc: 92.560 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 876 | Train: G_Loss: 1.065, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.697, D_sup_loss: 0.266, D_sup_acc: 92.65 Train acc: 91.952 Test acc: 92.490 \n",
      "step: 877 | Train: G_Loss: 1.026, D_unsup_loss_real: 0.663, D_unsup_loss_fake: 0.650, D_sup_loss: 0.264, D_sup_acc: 92.58 Train acc: 91.908 Test acc: 92.540 \n",
      "step: 878 | Train: G_Loss: 1.082, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.645, D_sup_loss: 0.265, D_sup_acc: 92.63 Train acc: 91.950 Test acc: 92.520 \n",
      "step: 879 | Train: G_Loss: 1.003, D_unsup_loss_real: 0.661, D_unsup_loss_fake: 0.627, D_sup_loss: 0.265, D_sup_acc: 92.61 Train acc: 91.883 Test acc: 92.420 \n",
      "step: 880 | Train: G_Loss: 0.997, D_unsup_loss_real: 0.663, D_unsup_loss_fake: 0.709, D_sup_loss: 0.265, D_sup_acc: 92.52 Train acc: 91.850 Test acc: 92.420 \n",
      "step: 881 | Train: G_Loss: 1.080, D_unsup_loss_real: 0.652, D_unsup_loss_fake: 0.692, D_sup_loss: 0.265, D_sup_acc: 92.52 Train acc: 91.900 Test acc: 92.490 \n",
      "step: 882 | Train: G_Loss: 0.979, D_unsup_loss_real: 0.631, D_unsup_loss_fake: 0.666, D_sup_loss: 0.263, D_sup_acc: 92.58 Train acc: 91.937 Test acc: 92.560 \n",
      "step: 883 | Train: G_Loss: 0.990, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.682, D_sup_loss: 0.260, D_sup_acc: 92.65 Train acc: 91.932 Test acc: 92.480 \n",
      "step: 884 | Train: G_Loss: 1.045, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.611, D_sup_loss: 0.259, D_sup_acc: 92.58 Train acc: 92.015 Test acc: 92.630 \n",
      "step: 885 | Train: G_Loss: 1.001, D_unsup_loss_real: 0.682, D_unsup_loss_fake: 0.668, D_sup_loss: 0.259, D_sup_acc: 92.72 Train acc: 92.028 Test acc: 92.700 \n",
      "step: 886 | Train: G_Loss: 1.086, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.724, D_sup_loss: 0.257, D_sup_acc: 92.79 Train acc: 92.030 Test acc: 92.700 \n",
      "step: 887 | Train: G_Loss: 1.025, D_unsup_loss_real: 0.662, D_unsup_loss_fake: 0.701, D_sup_loss: 0.259, D_sup_acc: 92.79 Train acc: 91.765 Test acc: 92.460 \n",
      "step: 888 | Train: G_Loss: 1.014, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.624, D_sup_loss: 0.265, D_sup_acc: 92.56 Train acc: 91.867 Test acc: 92.650 \n",
      "step: 889 | Train: G_Loss: 1.070, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.623, D_sup_loss: 0.264, D_sup_acc: 92.74 Train acc: 91.705 Test acc: 92.430 \n",
      "step: 890 | Train: G_Loss: 1.073, D_unsup_loss_real: 0.668, D_unsup_loss_fake: 0.657, D_sup_loss: 0.265, D_sup_acc: 92.53 Train acc: 91.798 Test acc: 92.400 \n",
      "step: 891 | Train: G_Loss: 1.055, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.709, D_sup_loss: 0.263, D_sup_acc: 92.50 Train acc: 91.975 Test acc: 92.730 \n",
      "step: 892 | Train: G_Loss: 1.006, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.750, D_sup_loss: 0.260, D_sup_acc: 92.82 Train acc: 91.962 Test acc: 92.780 \n",
      "step: 893 | Train: G_Loss: 1.072, D_unsup_loss_real: 0.648, D_unsup_loss_fake: 0.717, D_sup_loss: 0.262, D_sup_acc: 92.87 Train acc: 92.078 Test acc: 92.730 \n",
      "step: 894 | Train: G_Loss: 1.118, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.655, D_sup_loss: 0.258, D_sup_acc: 92.82 Train acc: 92.053 Test acc: 92.680 \n",
      "step: 895 | Train: G_Loss: 1.074, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.580, D_sup_loss: 0.255, D_sup_acc: 92.77 Train acc: 92.157 Test acc: 92.840 \n",
      "step: 896 | Train: G_Loss: 1.074, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.658, D_sup_loss: 0.254, D_sup_acc: 92.93 Train acc: 92.223 Test acc: 93.030 \n",
      "step: 897 | Train: G_Loss: 1.076, D_unsup_loss_real: 0.712, D_unsup_loss_fake: 0.705, D_sup_loss: 0.254, D_sup_acc: 93.12 Train acc: 92.243 Test acc: 92.990 \n",
      "step: 898 | Train: G_Loss: 1.078, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.672, D_sup_loss: 0.254, D_sup_acc: 93.08 Train acc: 92.288 Test acc: 93.070 \n",
      "step: 899 | Train: G_Loss: 0.987, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.740, D_sup_loss: 0.253, D_sup_acc: 93.16 Train acc: 92.202 Test acc: 93.010 \n",
      "step: 900 | Train: G_Loss: 1.046, D_unsup_loss_real: 0.682, D_unsup_loss_fake: 0.721, D_sup_loss: 0.256, D_sup_acc: 93.10 Train acc: 91.987 Test acc: 92.680 \n",
      "Train Classifier Accuracy: 91.987%\n",
      "\n",
      "Test Classifier Accuracy: 92.680%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_900.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_900.h5\n",
      "step: 901 | Train: G_Loss: 1.085, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.709, D_sup_loss: 0.263, D_sup_acc: 92.77 Train acc: 92.043 Test acc: 92.770 \n",
      "step: 902 | Train: G_Loss: 1.061, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.677, D_sup_loss: 0.262, D_sup_acc: 92.86 Train acc: 92.140 Test acc: 92.920 \n",
      "step: 903 | Train: G_Loss: 0.978, D_unsup_loss_real: 0.733, D_unsup_loss_fake: 0.697, D_sup_loss: 0.257, D_sup_acc: 93.01 Train acc: 92.233 Test acc: 93.060 \n",
      "step: 904 | Train: G_Loss: 1.066, D_unsup_loss_real: 0.663, D_unsup_loss_fake: 0.691, D_sup_loss: 0.252, D_sup_acc: 93.15 Train acc: 92.062 Test acc: 92.780 \n",
      "step: 905 | Train: G_Loss: 0.978, D_unsup_loss_real: 0.665, D_unsup_loss_fake: 0.707, D_sup_loss: 0.256, D_sup_acc: 92.87 Train acc: 92.105 Test acc: 92.880 \n",
      "step: 906 | Train: G_Loss: 0.972, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.654, D_sup_loss: 0.255, D_sup_acc: 92.97 Train acc: 91.893 Test acc: 92.600 \n",
      "step: 907 | Train: G_Loss: 1.051, D_unsup_loss_real: 0.537, D_unsup_loss_fake: 0.685, D_sup_loss: 0.255, D_sup_acc: 92.69 Train acc: 92.053 Test acc: 92.680 \n",
      "step: 908 | Train: G_Loss: 1.000, D_unsup_loss_real: 0.706, D_unsup_loss_fake: 0.793, D_sup_loss: 0.255, D_sup_acc: 92.77 Train acc: 91.967 Test acc: 92.670 \n",
      "step: 909 | Train: G_Loss: 1.022, D_unsup_loss_real: 0.673, D_unsup_loss_fake: 0.725, D_sup_loss: 0.260, D_sup_acc: 92.76 Train acc: 91.917 Test acc: 92.620 \n",
      "step: 910 | Train: G_Loss: 1.037, D_unsup_loss_real: 0.655, D_unsup_loss_fake: 0.639, D_sup_loss: 0.263, D_sup_acc: 92.71 Train acc: 91.892 Test acc: 92.520 \n",
      "step: 911 | Train: G_Loss: 1.043, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.658, D_sup_loss: 0.262, D_sup_acc: 92.61 Train acc: 91.800 Test acc: 92.410 \n",
      "step: 912 | Train: G_Loss: 1.092, D_unsup_loss_real: 0.660, D_unsup_loss_fake: 0.655, D_sup_loss: 0.261, D_sup_acc: 92.51 Train acc: 91.898 Test acc: 92.620 \n",
      "step: 913 | Train: G_Loss: 1.052, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.755, D_sup_loss: 0.261, D_sup_acc: 92.71 Train acc: 91.785 Test acc: 92.570 \n",
      "step: 914 | Train: G_Loss: 1.105, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.754, D_sup_loss: 0.266, D_sup_acc: 92.66 Train acc: 91.897 Test acc: 92.620 \n",
      "step: 915 | Train: G_Loss: 1.108, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.690, D_sup_loss: 0.261, D_sup_acc: 92.71 Train acc: 92.107 Test acc: 92.830 \n",
      "step: 916 | Train: G_Loss: 1.063, D_unsup_loss_real: 0.635, D_unsup_loss_fake: 0.611, D_sup_loss: 0.257, D_sup_acc: 92.92 Train acc: 92.102 Test acc: 92.780 \n",
      "step: 917 | Train: G_Loss: 1.030, D_unsup_loss_real: 0.631, D_unsup_loss_fake: 0.704, D_sup_loss: 0.256, D_sup_acc: 92.87 Train acc: 92.342 Test acc: 92.900 \n",
      "step: 918 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.689, D_sup_loss: 0.252, D_sup_acc: 92.99 Train acc: 92.292 Test acc: 92.940 \n",
      "step: 919 | Train: G_Loss: 1.055, D_unsup_loss_real: 0.687, D_unsup_loss_fake: 0.638, D_sup_loss: 0.252, D_sup_acc: 93.03 Train acc: 92.330 Test acc: 92.950 \n",
      "step: 920 | Train: G_Loss: 0.904, D_unsup_loss_real: 0.652, D_unsup_loss_fake: 0.672, D_sup_loss: 0.250, D_sup_acc: 93.04 Train acc: 92.288 Test acc: 92.910 \n",
      "step: 921 | Train: G_Loss: 1.068, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.697, D_sup_loss: 0.253, D_sup_acc: 93.00 Train acc: 92.177 Test acc: 92.840 \n",
      "step: 922 | Train: G_Loss: 1.028, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.649, D_sup_loss: 0.256, D_sup_acc: 92.93 Train acc: 92.367 Test acc: 93.030 \n",
      "step: 923 | Train: G_Loss: 1.130, D_unsup_loss_real: 0.696, D_unsup_loss_fake: 0.793, D_sup_loss: 0.253, D_sup_acc: 93.12 Train acc: 92.365 Test acc: 93.020 \n",
      "step: 924 | Train: G_Loss: 1.078, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.690, D_sup_loss: 0.257, D_sup_acc: 93.11 Train acc: 92.283 Test acc: 92.950 \n",
      "step: 925 | Train: G_Loss: 1.045, D_unsup_loss_real: 0.644, D_unsup_loss_fake: 0.679, D_sup_loss: 0.257, D_sup_acc: 93.04 Train acc: 92.342 Test acc: 92.940 \n",
      "step: 926 | Train: G_Loss: 1.041, D_unsup_loss_real: 0.656, D_unsup_loss_fake: 0.646, D_sup_loss: 0.257, D_sup_acc: 93.03 Train acc: 92.365 Test acc: 92.860 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 927 | Train: G_Loss: 1.044, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.651, D_sup_loss: 0.256, D_sup_acc: 92.95 Train acc: 92.250 Test acc: 92.890 \n",
      "step: 928 | Train: G_Loss: 1.015, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.752, D_sup_loss: 0.255, D_sup_acc: 92.98 Train acc: 92.310 Test acc: 92.950 \n",
      "step: 929 | Train: G_Loss: 1.015, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.606, D_sup_loss: 0.257, D_sup_acc: 93.04 Train acc: 92.355 Test acc: 93.070 \n",
      "step: 930 | Train: G_Loss: 1.026, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.635, D_sup_loss: 0.254, D_sup_acc: 93.16 Train acc: 92.457 Test acc: 93.120 \n",
      "step: 931 | Train: G_Loss: 1.038, D_unsup_loss_real: 0.700, D_unsup_loss_fake: 0.715, D_sup_loss: 0.253, D_sup_acc: 93.21 Train acc: 92.390 Test acc: 93.050 \n",
      "step: 932 | Train: G_Loss: 1.056, D_unsup_loss_real: 0.689, D_unsup_loss_fake: 0.677, D_sup_loss: 0.256, D_sup_acc: 93.14 Train acc: 92.395 Test acc: 93.040 \n",
      "step: 933 | Train: G_Loss: 1.047, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.651, D_sup_loss: 0.256, D_sup_acc: 93.13 Train acc: 92.328 Test acc: 93.010 \n",
      "step: 934 | Train: G_Loss: 1.053, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.743, D_sup_loss: 0.255, D_sup_acc: 93.10 Train acc: 92.287 Test acc: 92.950 \n",
      "step: 935 | Train: G_Loss: 1.023, D_unsup_loss_real: 0.651, D_unsup_loss_fake: 0.650, D_sup_loss: 0.255, D_sup_acc: 93.04 Train acc: 92.528 Test acc: 93.260 \n",
      "step: 936 | Train: G_Loss: 1.080, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.691, D_sup_loss: 0.248, D_sup_acc: 93.35 Train acc: 92.602 Test acc: 93.270 \n",
      "step: 937 | Train: G_Loss: 1.089, D_unsup_loss_real: 0.677, D_unsup_loss_fake: 0.641, D_sup_loss: 0.248, D_sup_acc: 93.36 Train acc: 92.632 Test acc: 93.310 \n",
      "step: 938 | Train: G_Loss: 1.069, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.724, D_sup_loss: 0.250, D_sup_acc: 93.39 Train acc: 92.608 Test acc: 93.350 \n",
      "step: 939 | Train: G_Loss: 1.061, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.674, D_sup_loss: 0.250, D_sup_acc: 93.43 Train acc: 92.478 Test acc: 93.230 \n",
      "step: 940 | Train: G_Loss: 1.015, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.614, D_sup_loss: 0.251, D_sup_acc: 93.32 Train acc: 92.467 Test acc: 93.170 \n",
      "step: 941 | Train: G_Loss: 1.052, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.706, D_sup_loss: 0.252, D_sup_acc: 93.26 Train acc: 92.458 Test acc: 93.100 \n",
      "step: 942 | Train: G_Loss: 1.054, D_unsup_loss_real: 0.697, D_unsup_loss_fake: 0.721, D_sup_loss: 0.251, D_sup_acc: 93.19 Train acc: 92.468 Test acc: 93.070 \n",
      "step: 943 | Train: G_Loss: 1.089, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.630, D_sup_loss: 0.252, D_sup_acc: 93.16 Train acc: 92.450 Test acc: 93.030 \n",
      "step: 944 | Train: G_Loss: 1.198, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.737, D_sup_loss: 0.252, D_sup_acc: 93.12 Train acc: 92.403 Test acc: 93.060 \n",
      "step: 945 | Train: G_Loss: 1.055, D_unsup_loss_real: 0.684, D_unsup_loss_fake: 0.604, D_sup_loss: 0.254, D_sup_acc: 93.15 Train acc: 92.382 Test acc: 92.960 \n",
      "step: 946 | Train: G_Loss: 1.043, D_unsup_loss_real: 0.663, D_unsup_loss_fake: 0.647, D_sup_loss: 0.254, D_sup_acc: 93.05 Train acc: 92.535 Test acc: 93.070 \n",
      "step: 947 | Train: G_Loss: 1.063, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.660, D_sup_loss: 0.251, D_sup_acc: 93.16 Train acc: 92.578 Test acc: 93.260 \n",
      "step: 948 | Train: G_Loss: 1.066, D_unsup_loss_real: 0.651, D_unsup_loss_fake: 0.706, D_sup_loss: 0.248, D_sup_acc: 93.35 Train acc: 92.503 Test acc: 93.090 \n",
      "step: 949 | Train: G_Loss: 1.038, D_unsup_loss_real: 0.671, D_unsup_loss_fake: 0.721, D_sup_loss: 0.249, D_sup_acc: 93.18 Train acc: 92.330 Test acc: 92.990 \n",
      "step: 950 | Train: G_Loss: 1.016, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.657, D_sup_loss: 0.255, D_sup_acc: 93.08 Train acc: 92.403 Test acc: 92.990 \n",
      "step: 951 | Train: G_Loss: 1.077, D_unsup_loss_real: 0.649, D_unsup_loss_fake: 0.670, D_sup_loss: 0.252, D_sup_acc: 93.08 Train acc: 92.473 Test acc: 93.210 \n",
      "step: 952 | Train: G_Loss: 1.097, D_unsup_loss_real: 0.723, D_unsup_loss_fake: 0.678, D_sup_loss: 0.249, D_sup_acc: 93.30 Train acc: 92.470 Test acc: 93.170 \n",
      "step: 953 | Train: G_Loss: 1.096, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.661, D_sup_loss: 0.256, D_sup_acc: 93.26 Train acc: 92.313 Test acc: 92.920 \n",
      "step: 954 | Train: G_Loss: 1.071, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.653, D_sup_loss: 0.258, D_sup_acc: 93.01 Train acc: 92.080 Test acc: 92.730 \n",
      "step: 955 | Train: G_Loss: 1.036, D_unsup_loss_real: 0.670, D_unsup_loss_fake: 0.664, D_sup_loss: 0.263, D_sup_acc: 92.82 Train acc: 92.265 Test acc: 92.900 \n",
      "step: 956 | Train: G_Loss: 1.060, D_unsup_loss_real: 0.681, D_unsup_loss_fake: 0.688, D_sup_loss: 0.255, D_sup_acc: 92.99 Train acc: 92.410 Test acc: 93.060 \n",
      "step: 957 | Train: G_Loss: 1.051, D_unsup_loss_real: 0.628, D_unsup_loss_fake: 0.651, D_sup_loss: 0.250, D_sup_acc: 93.15 Train acc: 92.328 Test acc: 93.000 \n",
      "step: 958 | Train: G_Loss: 1.045, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.674, D_sup_loss: 0.252, D_sup_acc: 93.09 Train acc: 92.163 Test acc: 92.850 \n",
      "step: 959 | Train: G_Loss: 1.082, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.712, D_sup_loss: 0.253, D_sup_acc: 92.94 Train acc: 92.203 Test acc: 92.960 \n",
      "step: 960 | Train: G_Loss: 1.017, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.668, D_sup_loss: 0.252, D_sup_acc: 93.05 Train acc: 92.320 Test acc: 93.080 \n",
      "step: 961 | Train: G_Loss: 1.043, D_unsup_loss_real: 0.683, D_unsup_loss_fake: 0.716, D_sup_loss: 0.248, D_sup_acc: 93.17 Train acc: 92.300 Test acc: 92.880 \n",
      "step: 962 | Train: G_Loss: 1.015, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.599, D_sup_loss: 0.255, D_sup_acc: 92.97 Train acc: 92.217 Test acc: 92.920 \n",
      "step: 963 | Train: G_Loss: 1.046, D_unsup_loss_real: 0.631, D_unsup_loss_fake: 0.650, D_sup_loss: 0.253, D_sup_acc: 93.01 Train acc: 92.515 Test acc: 93.060 \n",
      "step: 964 | Train: G_Loss: 1.028, D_unsup_loss_real: 0.676, D_unsup_loss_fake: 0.715, D_sup_loss: 0.249, D_sup_acc: 93.15 Train acc: 92.590 Test acc: 93.230 \n",
      "step: 965 | Train: G_Loss: 1.067, D_unsup_loss_real: 0.657, D_unsup_loss_fake: 0.697, D_sup_loss: 0.248, D_sup_acc: 93.32 Train acc: 92.600 Test acc: 93.280 \n",
      "step: 966 | Train: G_Loss: 1.052, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.737, D_sup_loss: 0.246, D_sup_acc: 93.36 Train acc: 92.628 Test acc: 93.280 \n",
      "step: 967 | Train: G_Loss: 1.026, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.746, D_sup_loss: 0.248, D_sup_acc: 93.36 Train acc: 92.573 Test acc: 93.280 \n",
      "step: 968 | Train: G_Loss: 1.019, D_unsup_loss_real: 0.639, D_unsup_loss_fake: 0.672, D_sup_loss: 0.249, D_sup_acc: 93.36 Train acc: 92.643 Test acc: 93.430 \n",
      "step: 969 | Train: G_Loss: 1.052, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.645, D_sup_loss: 0.246, D_sup_acc: 93.51 Train acc: 92.730 Test acc: 93.350 \n",
      "step: 970 | Train: G_Loss: 1.114, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.644, D_sup_loss: 0.243, D_sup_acc: 93.43 Train acc: 92.802 Test acc: 93.410 \n",
      "step: 971 | Train: G_Loss: 1.104, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.626, D_sup_loss: 0.242, D_sup_acc: 93.49 Train acc: 92.852 Test acc: 93.370 \n",
      "step: 972 | Train: G_Loss: 1.084, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.655, D_sup_loss: 0.246, D_sup_acc: 93.45 Train acc: 92.955 Test acc: 93.510 \n",
      "step: 973 | Train: G_Loss: 1.090, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.604, D_sup_loss: 0.243, D_sup_acc: 93.59 Train acc: 92.995 Test acc: 93.540 \n",
      "step: 974 | Train: G_Loss: 1.052, D_unsup_loss_real: 0.674, D_unsup_loss_fake: 0.655, D_sup_loss: 0.237, D_sup_acc: 93.62 Train acc: 92.858 Test acc: 93.410 \n",
      "step: 975 | Train: G_Loss: 1.062, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.696, D_sup_loss: 0.238, D_sup_acc: 93.49 Train acc: 92.903 Test acc: 93.420 \n",
      "step: 976 | Train: G_Loss: 1.155, D_unsup_loss_real: 0.644, D_unsup_loss_fake: 0.709, D_sup_loss: 0.236, D_sup_acc: 93.50 Train acc: 92.840 Test acc: 93.400 \n",
      "step: 977 | Train: G_Loss: 0.998, D_unsup_loss_real: 0.664, D_unsup_loss_fake: 0.630, D_sup_loss: 0.241, D_sup_acc: 93.48 Train acc: 92.682 Test acc: 93.190 \n",
      "step: 978 | Train: G_Loss: 1.066, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.584, D_sup_loss: 0.246, D_sup_acc: 93.28 Train acc: 92.730 Test acc: 93.200 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 979 | Train: G_Loss: 1.065, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.598, D_sup_loss: 0.245, D_sup_acc: 93.29 Train acc: 92.810 Test acc: 93.230 \n",
      "step: 980 | Train: G_Loss: 1.085, D_unsup_loss_real: 0.697, D_unsup_loss_fake: 0.696, D_sup_loss: 0.243, D_sup_acc: 93.32 Train acc: 92.765 Test acc: 93.230 \n",
      "step: 981 | Train: G_Loss: 1.058, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.695, D_sup_loss: 0.242, D_sup_acc: 93.32 Train acc: 92.737 Test acc: 93.110 \n",
      "step: 982 | Train: G_Loss: 1.048, D_unsup_loss_real: 0.628, D_unsup_loss_fake: 0.717, D_sup_loss: 0.245, D_sup_acc: 93.20 Train acc: 92.837 Test acc: 93.440 \n",
      "step: 983 | Train: G_Loss: 1.008, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.618, D_sup_loss: 0.241, D_sup_acc: 93.52 Train acc: 92.797 Test acc: 93.420 \n",
      "step: 984 | Train: G_Loss: 1.043, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.634, D_sup_loss: 0.239, D_sup_acc: 93.50 Train acc: 92.683 Test acc: 93.290 \n",
      "step: 985 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.658, D_sup_loss: 0.240, D_sup_acc: 93.37 Train acc: 92.687 Test acc: 93.240 \n",
      "step: 986 | Train: G_Loss: 1.045, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.630, D_sup_loss: 0.240, D_sup_acc: 93.33 Train acc: 92.647 Test acc: 93.150 \n",
      "step: 987 | Train: G_Loss: 1.051, D_unsup_loss_real: 0.721, D_unsup_loss_fake: 0.751, D_sup_loss: 0.243, D_sup_acc: 93.24 Train acc: 92.663 Test acc: 93.190 \n",
      "step: 988 | Train: G_Loss: 0.971, D_unsup_loss_real: 0.667, D_unsup_loss_fake: 0.619, D_sup_loss: 0.246, D_sup_acc: 93.28 Train acc: 92.850 Test acc: 93.400 \n",
      "step: 989 | Train: G_Loss: 1.079, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.700, D_sup_loss: 0.240, D_sup_acc: 93.48 Train acc: 92.787 Test acc: 93.450 \n",
      "step: 990 | Train: G_Loss: 1.083, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.685, D_sup_loss: 0.239, D_sup_acc: 93.53 Train acc: 92.995 Test acc: 93.570 \n",
      "step: 991 | Train: G_Loss: 1.060, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.602, D_sup_loss: 0.236, D_sup_acc: 93.65 Train acc: 92.965 Test acc: 93.610 \n",
      "step: 992 | Train: G_Loss: 1.115, D_unsup_loss_real: 0.639, D_unsup_loss_fake: 0.693, D_sup_loss: 0.239, D_sup_acc: 93.69 Train acc: 92.943 Test acc: 93.500 \n",
      "step: 993 | Train: G_Loss: 1.114, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.673, D_sup_loss: 0.234, D_sup_acc: 93.58 Train acc: 92.910 Test acc: 93.590 \n",
      "step: 994 | Train: G_Loss: 1.097, D_unsup_loss_real: 0.639, D_unsup_loss_fake: 0.653, D_sup_loss: 0.234, D_sup_acc: 93.67 Train acc: 93.007 Test acc: 93.660 \n",
      "step: 995 | Train: G_Loss: 1.023, D_unsup_loss_real: 0.686, D_unsup_loss_fake: 0.708, D_sup_loss: 0.234, D_sup_acc: 93.74 Train acc: 92.990 Test acc: 93.590 \n",
      "step: 996 | Train: G_Loss: 1.048, D_unsup_loss_real: 0.644, D_unsup_loss_fake: 0.662, D_sup_loss: 0.237, D_sup_acc: 93.67 Train acc: 93.057 Test acc: 93.660 \n",
      "step: 997 | Train: G_Loss: 1.037, D_unsup_loss_real: 0.687, D_unsup_loss_fake: 0.659, D_sup_loss: 0.235, D_sup_acc: 93.74 Train acc: 93.152 Test acc: 93.720 \n",
      "step: 998 | Train: G_Loss: 1.082, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.596, D_sup_loss: 0.233, D_sup_acc: 93.80 Train acc: 93.133 Test acc: 93.800 \n",
      "step: 999 | Train: G_Loss: 1.112, D_unsup_loss_real: 0.653, D_unsup_loss_fake: 0.627, D_sup_loss: 0.234, D_sup_acc: 93.88 Train acc: 93.048 Test acc: 93.640 \n",
      "step: 1000 | Train: G_Loss: 1.017, D_unsup_loss_real: 0.686, D_unsup_loss_fake: 0.709, D_sup_loss: 0.237, D_sup_acc: 93.72 Train acc: 92.927 Test acc: 93.470 \n",
      "Train Classifier Accuracy: 92.927%\n",
      "\n",
      "Test Classifier Accuracy: 93.470%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_1000.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_1000.h5\n",
      "step: 1001 | Train: G_Loss: 1.045, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.732, D_sup_loss: 0.240, D_sup_acc: 93.55 Train acc: 93.002 Test acc: 93.500 \n",
      "step: 1002 | Train: G_Loss: 1.130, D_unsup_loss_real: 0.694, D_unsup_loss_fake: 0.626, D_sup_loss: 0.238, D_sup_acc: 93.58 Train acc: 92.905 Test acc: 93.420 \n",
      "step: 1003 | Train: G_Loss: 1.028, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.650, D_sup_loss: 0.240, D_sup_acc: 93.50 Train acc: 92.955 Test acc: 93.550 \n",
      "step: 1004 | Train: G_Loss: 1.058, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.596, D_sup_loss: 0.238, D_sup_acc: 93.63 Train acc: 93.050 Test acc: 93.680 \n",
      "step: 1005 | Train: G_Loss: 1.129, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.694, D_sup_loss: 0.232, D_sup_acc: 93.76 Train acc: 93.310 Test acc: 93.870 \n",
      "step: 1006 | Train: G_Loss: 1.157, D_unsup_loss_real: 0.694, D_unsup_loss_fake: 0.684, D_sup_loss: 0.227, D_sup_acc: 93.95 Train acc: 93.218 Test acc: 93.860 \n",
      "step: 1007 | Train: G_Loss: 1.063, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.705, D_sup_loss: 0.229, D_sup_acc: 93.94 Train acc: 93.160 Test acc: 93.640 \n",
      "step: 1008 | Train: G_Loss: 1.047, D_unsup_loss_real: 0.681, D_unsup_loss_fake: 0.683, D_sup_loss: 0.234, D_sup_acc: 93.72 Train acc: 93.235 Test acc: 93.790 \n",
      "step: 1009 | Train: G_Loss: 1.118, D_unsup_loss_real: 0.660, D_unsup_loss_fake: 0.641, D_sup_loss: 0.233, D_sup_acc: 93.87 Train acc: 93.263 Test acc: 93.820 \n",
      "step: 1010 | Train: G_Loss: 1.038, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.666, D_sup_loss: 0.232, D_sup_acc: 93.90 Train acc: 93.285 Test acc: 93.790 \n",
      "step: 1011 | Train: G_Loss: 1.057, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.658, D_sup_loss: 0.229, D_sup_acc: 93.87 Train acc: 93.063 Test acc: 93.490 \n",
      "step: 1012 | Train: G_Loss: 1.128, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.693, D_sup_loss: 0.234, D_sup_acc: 93.57 Train acc: 93.250 Test acc: 93.750 \n",
      "step: 1013 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.732, D_sup_loss: 0.230, D_sup_acc: 93.83 Train acc: 93.150 Test acc: 93.810 \n",
      "step: 1014 | Train: G_Loss: 1.069, D_unsup_loss_real: 0.707, D_unsup_loss_fake: 0.722, D_sup_loss: 0.230, D_sup_acc: 93.89 Train acc: 93.055 Test acc: 93.760 \n",
      "step: 1015 | Train: G_Loss: 1.123, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.633, D_sup_loss: 0.232, D_sup_acc: 93.84 Train acc: 93.062 Test acc: 93.650 \n",
      "step: 1016 | Train: G_Loss: 1.006, D_unsup_loss_real: 0.648, D_unsup_loss_fake: 0.587, D_sup_loss: 0.232, D_sup_acc: 93.73 Train acc: 92.940 Test acc: 93.560 \n",
      "step: 1017 | Train: G_Loss: 1.076, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.608, D_sup_loss: 0.232, D_sup_acc: 93.64 Train acc: 92.800 Test acc: 93.480 \n",
      "step: 1018 | Train: G_Loss: 1.034, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.721, D_sup_loss: 0.234, D_sup_acc: 93.56 Train acc: 92.663 Test acc: 93.330 \n",
      "step: 1019 | Train: G_Loss: 1.120, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.701, D_sup_loss: 0.235, D_sup_acc: 93.41 Train acc: 92.742 Test acc: 93.320 \n",
      "step: 1020 | Train: G_Loss: 1.112, D_unsup_loss_real: 0.666, D_unsup_loss_fake: 0.687, D_sup_loss: 0.236, D_sup_acc: 93.40 Train acc: 92.745 Test acc: 93.430 \n",
      "step: 1021 | Train: G_Loss: 1.149, D_unsup_loss_real: 0.644, D_unsup_loss_fake: 0.713, D_sup_loss: 0.235, D_sup_acc: 93.51 Train acc: 92.703 Test acc: 93.360 \n",
      "step: 1022 | Train: G_Loss: 0.984, D_unsup_loss_real: 0.666, D_unsup_loss_fake: 0.634, D_sup_loss: 0.238, D_sup_acc: 93.44 Train acc: 92.790 Test acc: 93.310 \n",
      "step: 1023 | Train: G_Loss: 1.011, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.630, D_sup_loss: 0.236, D_sup_acc: 93.39 Train acc: 92.833 Test acc: 93.480 \n",
      "step: 1024 | Train: G_Loss: 1.077, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.643, D_sup_loss: 0.235, D_sup_acc: 93.56 Train acc: 92.825 Test acc: 93.330 \n",
      "step: 1025 | Train: G_Loss: 1.058, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.697, D_sup_loss: 0.241, D_sup_acc: 93.41 Train acc: 92.662 Test acc: 93.170 \n",
      "step: 1026 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.682, D_unsup_loss_fake: 0.715, D_sup_loss: 0.247, D_sup_acc: 93.26 Train acc: 92.958 Test acc: 93.500 \n",
      "step: 1027 | Train: G_Loss: 1.077, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.675, D_sup_loss: 0.239, D_sup_acc: 93.58 Train acc: 92.985 Test acc: 93.510 \n",
      "step: 1028 | Train: G_Loss: 1.065, D_unsup_loss_real: 0.676, D_unsup_loss_fake: 0.655, D_sup_loss: 0.237, D_sup_acc: 93.59 Train acc: 93.062 Test acc: 93.760 \n",
      "step: 1029 | Train: G_Loss: 1.053, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.601, D_sup_loss: 0.235, D_sup_acc: 93.84 Train acc: 93.030 Test acc: 93.580 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1030 | Train: G_Loss: 1.104, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.707, D_sup_loss: 0.234, D_sup_acc: 93.66 Train acc: 93.068 Test acc: 93.740 \n",
      "step: 1031 | Train: G_Loss: 1.014, D_unsup_loss_real: 0.675, D_unsup_loss_fake: 0.621, D_sup_loss: 0.233, D_sup_acc: 93.82 Train acc: 93.025 Test acc: 93.600 \n",
      "step: 1032 | Train: G_Loss: 1.050, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.634, D_sup_loss: 0.238, D_sup_acc: 93.68 Train acc: 93.080 Test acc: 93.700 \n",
      "step: 1033 | Train: G_Loss: 1.056, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.684, D_sup_loss: 0.234, D_sup_acc: 93.78 Train acc: 93.065 Test acc: 93.620 \n",
      "step: 1034 | Train: G_Loss: 1.087, D_unsup_loss_real: 0.646, D_unsup_loss_fake: 0.739, D_sup_loss: 0.237, D_sup_acc: 93.70 Train acc: 93.115 Test acc: 93.630 \n",
      "step: 1035 | Train: G_Loss: 1.106, D_unsup_loss_real: 0.654, D_unsup_loss_fake: 0.650, D_sup_loss: 0.232, D_sup_acc: 93.71 Train acc: 93.217 Test acc: 93.780 \n",
      "step: 1036 | Train: G_Loss: 1.018, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.641, D_sup_loss: 0.231, D_sup_acc: 93.86 Train acc: 93.190 Test acc: 93.650 \n",
      "step: 1037 | Train: G_Loss: 1.053, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.643, D_sup_loss: 0.232, D_sup_acc: 93.73 Train acc: 93.313 Test acc: 93.840 \n",
      "step: 1038 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.725, D_sup_loss: 0.231, D_sup_acc: 93.92 Train acc: 93.277 Test acc: 93.790 \n",
      "step: 1039 | Train: G_Loss: 1.080, D_unsup_loss_real: 0.644, D_unsup_loss_fake: 0.619, D_sup_loss: 0.232, D_sup_acc: 93.87 Train acc: 93.128 Test acc: 93.730 \n",
      "step: 1040 | Train: G_Loss: 1.036, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.578, D_sup_loss: 0.233, D_sup_acc: 93.81 Train acc: 93.193 Test acc: 93.790 \n",
      "step: 1041 | Train: G_Loss: 1.110, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.695, D_sup_loss: 0.231, D_sup_acc: 93.87 Train acc: 93.148 Test acc: 93.690 \n",
      "step: 1042 | Train: G_Loss: 1.052, D_unsup_loss_real: 0.671, D_unsup_loss_fake: 0.592, D_sup_loss: 0.230, D_sup_acc: 93.77 Train acc: 93.253 Test acc: 93.780 \n",
      "step: 1043 | Train: G_Loss: 1.047, D_unsup_loss_real: 0.649, D_unsup_loss_fake: 0.648, D_sup_loss: 0.230, D_sup_acc: 93.86 Train acc: 93.148 Test acc: 93.730 \n",
      "step: 1044 | Train: G_Loss: 1.000, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.593, D_sup_loss: 0.230, D_sup_acc: 93.81 Train acc: 92.882 Test acc: 93.530 \n",
      "step: 1045 | Train: G_Loss: 1.162, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.671, D_sup_loss: 0.239, D_sup_acc: 93.61 Train acc: 92.988 Test acc: 93.640 \n",
      "step: 1046 | Train: G_Loss: 1.061, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.693, D_sup_loss: 0.237, D_sup_acc: 93.72 Train acc: 92.965 Test acc: 93.600 \n",
      "step: 1047 | Train: G_Loss: 1.060, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.716, D_sup_loss: 0.239, D_sup_acc: 93.68 Train acc: 92.947 Test acc: 93.580 \n",
      "step: 1048 | Train: G_Loss: 1.009, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.661, D_sup_loss: 0.239, D_sup_acc: 93.66 Train acc: 92.905 Test acc: 93.450 \n",
      "step: 1049 | Train: G_Loss: 0.987, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.711, D_sup_loss: 0.243, D_sup_acc: 93.53 Train acc: 92.988 Test acc: 93.480 \n",
      "step: 1050 | Train: G_Loss: 1.105, D_unsup_loss_real: 0.667, D_unsup_loss_fake: 0.598, D_sup_loss: 0.242, D_sup_acc: 93.56 Train acc: 92.937 Test acc: 93.540 \n",
      "step: 1051 | Train: G_Loss: 1.044, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.741, D_sup_loss: 0.241, D_sup_acc: 93.62 Train acc: 93.003 Test acc: 93.540 \n",
      "step: 1052 | Train: G_Loss: 1.107, D_unsup_loss_real: 0.659, D_unsup_loss_fake: 0.676, D_sup_loss: 0.238, D_sup_acc: 93.62 Train acc: 93.058 Test acc: 93.660 \n",
      "step: 1053 | Train: G_Loss: 1.129, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.632, D_sup_loss: 0.232, D_sup_acc: 93.74 Train acc: 93.065 Test acc: 93.680 \n",
      "step: 1054 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.705, D_unsup_loss_fake: 0.679, D_sup_loss: 0.233, D_sup_acc: 93.76 Train acc: 93.112 Test acc: 93.730 \n",
      "step: 1055 | Train: G_Loss: 1.109, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.669, D_sup_loss: 0.235, D_sup_acc: 93.81 Train acc: 93.193 Test acc: 93.730 \n",
      "step: 1056 | Train: G_Loss: 1.082, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.679, D_sup_loss: 0.233, D_sup_acc: 93.81 Train acc: 93.175 Test acc: 93.670 \n",
      "step: 1057 | Train: G_Loss: 1.063, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.714, D_sup_loss: 0.235, D_sup_acc: 93.75 Train acc: 93.013 Test acc: 93.520 \n",
      "step: 1058 | Train: G_Loss: 1.164, D_unsup_loss_real: 0.706, D_unsup_loss_fake: 0.700, D_sup_loss: 0.236, D_sup_acc: 93.60 Train acc: 93.053 Test acc: 93.560 \n",
      "step: 1059 | Train: G_Loss: 1.048, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.632, D_sup_loss: 0.237, D_sup_acc: 93.64 Train acc: 92.997 Test acc: 93.440 \n",
      "step: 1060 | Train: G_Loss: 1.101, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.757, D_sup_loss: 0.238, D_sup_acc: 93.52 Train acc: 93.110 Test acc: 93.630 \n",
      "step: 1061 | Train: G_Loss: 1.155, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.627, D_sup_loss: 0.236, D_sup_acc: 93.71 Train acc: 93.208 Test acc: 93.690 \n",
      "step: 1062 | Train: G_Loss: 1.079, D_unsup_loss_real: 0.644, D_unsup_loss_fake: 0.664, D_sup_loss: 0.232, D_sup_acc: 93.77 Train acc: 93.245 Test acc: 93.690 \n",
      "step: 1063 | Train: G_Loss: 1.071, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.663, D_sup_loss: 0.233, D_sup_acc: 93.77 Train acc: 93.232 Test acc: 93.720 \n",
      "step: 1064 | Train: G_Loss: 1.111, D_unsup_loss_real: 0.682, D_unsup_loss_fake: 0.741, D_sup_loss: 0.230, D_sup_acc: 93.80 Train acc: 93.218 Test acc: 93.590 \n",
      "step: 1065 | Train: G_Loss: 1.096, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.615, D_sup_loss: 0.234, D_sup_acc: 93.67 Train acc: 93.297 Test acc: 93.750 \n",
      "step: 1066 | Train: G_Loss: 1.067, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.576, D_sup_loss: 0.229, D_sup_acc: 93.83 Train acc: 93.312 Test acc: 93.700 \n",
      "step: 1067 | Train: G_Loss: 1.079, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.670, D_sup_loss: 0.227, D_sup_acc: 93.78 Train acc: 93.288 Test acc: 93.830 \n",
      "step: 1068 | Train: G_Loss: 1.044, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.656, D_sup_loss: 0.225, D_sup_acc: 93.91 Train acc: 93.220 Test acc: 93.670 \n",
      "step: 1069 | Train: G_Loss: 1.072, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.691, D_sup_loss: 0.225, D_sup_acc: 93.75 Train acc: 93.012 Test acc: 93.490 \n",
      "step: 1070 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.652, D_unsup_loss_fake: 0.707, D_sup_loss: 0.233, D_sup_acc: 93.57 Train acc: 93.300 Test acc: 93.780 \n",
      "step: 1071 | Train: G_Loss: 1.092, D_unsup_loss_real: 0.685, D_unsup_loss_fake: 0.740, D_sup_loss: 0.227, D_sup_acc: 93.86 Train acc: 93.295 Test acc: 93.740 \n",
      "step: 1072 | Train: G_Loss: 1.182, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.629, D_sup_loss: 0.229, D_sup_acc: 93.82 Train acc: 93.273 Test acc: 93.760 \n",
      "step: 1073 | Train: G_Loss: 1.028, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.619, D_sup_loss: 0.228, D_sup_acc: 93.84 Train acc: 93.228 Test acc: 93.750 \n",
      "step: 1074 | Train: G_Loss: 1.072, D_unsup_loss_real: 0.678, D_unsup_loss_fake: 0.676, D_sup_loss: 0.226, D_sup_acc: 93.83 Train acc: 93.338 Test acc: 93.890 \n",
      "step: 1075 | Train: G_Loss: 1.046, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.673, D_sup_loss: 0.224, D_sup_acc: 93.97 Train acc: 93.367 Test acc: 93.930 \n",
      "step: 1076 | Train: G_Loss: 1.103, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.693, D_sup_loss: 0.224, D_sup_acc: 94.01 Train acc: 93.320 Test acc: 93.910 \n",
      "step: 1077 | Train: G_Loss: 1.109, D_unsup_loss_real: 0.667, D_unsup_loss_fake: 0.706, D_sup_loss: 0.225, D_sup_acc: 93.99 Train acc: 93.157 Test acc: 93.900 \n",
      "step: 1078 | Train: G_Loss: 1.110, D_unsup_loss_real: 0.663, D_unsup_loss_fake: 0.634, D_sup_loss: 0.228, D_sup_acc: 93.98 Train acc: 93.108 Test acc: 93.700 \n",
      "step: 1079 | Train: G_Loss: 1.116, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.708, D_sup_loss: 0.229, D_sup_acc: 93.78 Train acc: 93.237 Test acc: 93.890 \n",
      "step: 1080 | Train: G_Loss: 1.062, D_unsup_loss_real: 0.681, D_unsup_loss_fake: 0.669, D_sup_loss: 0.226, D_sup_acc: 93.97 Train acc: 93.167 Test acc: 93.630 \n",
      "step: 1081 | Train: G_Loss: 1.088, D_unsup_loss_real: 0.534, D_unsup_loss_fake: 0.634, D_sup_loss: 0.228, D_sup_acc: 93.71 Train acc: 93.088 Test acc: 93.600 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1082 | Train: G_Loss: 1.090, D_unsup_loss_real: 0.692, D_unsup_loss_fake: 0.684, D_sup_loss: 0.227, D_sup_acc: 93.68 Train acc: 93.037 Test acc: 93.560 \n",
      "step: 1083 | Train: G_Loss: 1.208, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.747, D_sup_loss: 0.233, D_sup_acc: 93.64 Train acc: 93.153 Test acc: 93.670 \n",
      "step: 1084 | Train: G_Loss: 1.126, D_unsup_loss_real: 0.691, D_unsup_loss_fake: 0.663, D_sup_loss: 0.231, D_sup_acc: 93.75 Train acc: 93.140 Test acc: 93.590 \n",
      "step: 1085 | Train: G_Loss: 1.066, D_unsup_loss_real: 0.655, D_unsup_loss_fake: 0.602, D_sup_loss: 0.230, D_sup_acc: 93.67 Train acc: 93.062 Test acc: 93.560 \n",
      "step: 1086 | Train: G_Loss: 1.041, D_unsup_loss_real: 0.718, D_unsup_loss_fake: 0.646, D_sup_loss: 0.235, D_sup_acc: 93.64 Train acc: 93.060 Test acc: 93.590 \n",
      "step: 1087 | Train: G_Loss: 1.042, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.645, D_sup_loss: 0.232, D_sup_acc: 93.67 Train acc: 93.320 Test acc: 93.910 \n",
      "step: 1088 | Train: G_Loss: 0.949, D_unsup_loss_real: 0.698, D_unsup_loss_fake: 0.689, D_sup_loss: 0.227, D_sup_acc: 93.99 Train acc: 93.315 Test acc: 93.880 \n",
      "step: 1089 | Train: G_Loss: 0.992, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.751, D_sup_loss: 0.230, D_sup_acc: 93.96 Train acc: 93.273 Test acc: 93.670 \n",
      "step: 1090 | Train: G_Loss: 1.039, D_unsup_loss_real: 0.675, D_unsup_loss_fake: 0.635, D_sup_loss: 0.231, D_sup_acc: 93.75 Train acc: 93.240 Test acc: 93.590 \n",
      "step: 1091 | Train: G_Loss: 1.032, D_unsup_loss_real: 0.654, D_unsup_loss_fake: 0.687, D_sup_loss: 0.233, D_sup_acc: 93.67 Train acc: 93.118 Test acc: 93.620 \n",
      "step: 1092 | Train: G_Loss: 1.038, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.652, D_sup_loss: 0.236, D_sup_acc: 93.70 Train acc: 93.027 Test acc: 93.590 \n",
      "step: 1093 | Train: G_Loss: 1.018, D_unsup_loss_real: 0.669, D_unsup_loss_fake: 0.676, D_sup_loss: 0.239, D_sup_acc: 93.67 Train acc: 93.180 Test acc: 93.850 \n",
      "step: 1094 | Train: G_Loss: 1.044, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.614, D_sup_loss: 0.234, D_sup_acc: 93.93 Train acc: 93.230 Test acc: 93.830 \n",
      "step: 1095 | Train: G_Loss: 1.034, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.603, D_sup_loss: 0.232, D_sup_acc: 93.91 Train acc: 93.272 Test acc: 93.760 \n",
      "step: 1096 | Train: G_Loss: 0.979, D_unsup_loss_real: 0.550, D_unsup_loss_fake: 0.706, D_sup_loss: 0.230, D_sup_acc: 93.84 Train acc: 93.177 Test acc: 93.670 \n",
      "step: 1097 | Train: G_Loss: 0.965, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.759, D_sup_loss: 0.228, D_sup_acc: 93.75 Train acc: 93.200 Test acc: 93.810 \n",
      "step: 1098 | Train: G_Loss: 1.066, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.712, D_sup_loss: 0.232, D_sup_acc: 93.89 Train acc: 93.128 Test acc: 93.480 \n",
      "step: 1099 | Train: G_Loss: 1.104, D_unsup_loss_real: 0.657, D_unsup_loss_fake: 0.646, D_sup_loss: 0.234, D_sup_acc: 93.56 Train acc: 93.235 Test acc: 93.650 \n",
      "step: 1100 | Train: G_Loss: 1.011, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.706, D_sup_loss: 0.227, D_sup_acc: 93.73 Train acc: 93.187 Test acc: 93.550 \n",
      "Train Classifier Accuracy: 93.187%\n",
      "\n",
      "Test Classifier Accuracy: 93.550%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_1100.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_1100.h5\n",
      "step: 1101 | Train: G_Loss: 1.103, D_unsup_loss_real: 0.666, D_unsup_loss_fake: 0.659, D_sup_loss: 0.227, D_sup_acc: 93.63 Train acc: 93.117 Test acc: 93.410 \n",
      "step: 1102 | Train: G_Loss: 1.100, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.637, D_sup_loss: 0.231, D_sup_acc: 93.49 Train acc: 93.315 Test acc: 93.680 \n",
      "step: 1103 | Train: G_Loss: 0.975, D_unsup_loss_real: 0.647, D_unsup_loss_fake: 0.663, D_sup_loss: 0.225, D_sup_acc: 93.76 Train acc: 93.468 Test acc: 94.010 \n",
      "step: 1104 | Train: G_Loss: 1.065, D_unsup_loss_real: 0.653, D_unsup_loss_fake: 0.706, D_sup_loss: 0.223, D_sup_acc: 94.09 Train acc: 93.502 Test acc: 94.110 \n",
      "step: 1105 | Train: G_Loss: 1.008, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.639, D_sup_loss: 0.225, D_sup_acc: 94.18 Train acc: 93.507 Test acc: 94.120 \n",
      "step: 1106 | Train: G_Loss: 1.083, D_unsup_loss_real: 0.678, D_unsup_loss_fake: 0.718, D_sup_loss: 0.228, D_sup_acc: 94.19 Train acc: 93.628 Test acc: 94.260 \n",
      "step: 1107 | Train: G_Loss: 1.096, D_unsup_loss_real: 0.646, D_unsup_loss_fake: 0.662, D_sup_loss: 0.221, D_sup_acc: 94.33 Train acc: 93.567 Test acc: 94.170 \n",
      "step: 1108 | Train: G_Loss: 1.045, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.660, D_sup_loss: 0.223, D_sup_acc: 94.24 Train acc: 93.377 Test acc: 93.990 \n",
      "step: 1109 | Train: G_Loss: 1.039, D_unsup_loss_real: 0.666, D_unsup_loss_fake: 0.716, D_sup_loss: 0.228, D_sup_acc: 94.07 Train acc: 93.397 Test acc: 94.030 \n",
      "step: 1110 | Train: G_Loss: 1.012, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.637, D_sup_loss: 0.226, D_sup_acc: 94.11 Train acc: 93.513 Test acc: 94.190 \n",
      "step: 1111 | Train: G_Loss: 1.089, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.674, D_sup_loss: 0.220, D_sup_acc: 94.26 Train acc: 93.628 Test acc: 94.200 \n",
      "step: 1112 | Train: G_Loss: 1.018, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.710, D_sup_loss: 0.217, D_sup_acc: 94.27 Train acc: 93.717 Test acc: 94.390 \n",
      "step: 1113 | Train: G_Loss: 1.072, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.632, D_sup_loss: 0.216, D_sup_acc: 94.46 Train acc: 93.572 Test acc: 94.160 \n",
      "step: 1114 | Train: G_Loss: 1.156, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.783, D_sup_loss: 0.218, D_sup_acc: 94.23 Train acc: 93.593 Test acc: 94.140 \n",
      "step: 1115 | Train: G_Loss: 1.038, D_unsup_loss_real: 0.686, D_unsup_loss_fake: 0.654, D_sup_loss: 0.217, D_sup_acc: 94.21 Train acc: 93.742 Test acc: 94.380 \n",
      "step: 1116 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.660, D_sup_loss: 0.217, D_sup_acc: 94.45 Train acc: 93.778 Test acc: 94.330 \n",
      "step: 1117 | Train: G_Loss: 1.128, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.694, D_sup_loss: 0.217, D_sup_acc: 94.40 Train acc: 93.843 Test acc: 94.260 \n",
      "step: 1118 | Train: G_Loss: 1.064, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.647, D_sup_loss: 0.219, D_sup_acc: 94.33 Train acc: 93.693 Test acc: 94.150 \n",
      "step: 1119 | Train: G_Loss: 1.074, D_unsup_loss_real: 0.716, D_unsup_loss_fake: 0.602, D_sup_loss: 0.222, D_sup_acc: 94.22 Train acc: 93.605 Test acc: 94.030 \n",
      "step: 1120 | Train: G_Loss: 1.115, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.656, D_sup_loss: 0.225, D_sup_acc: 94.11 Train acc: 93.528 Test acc: 94.040 \n",
      "step: 1121 | Train: G_Loss: 1.115, D_unsup_loss_real: 0.664, D_unsup_loss_fake: 0.649, D_sup_loss: 0.224, D_sup_acc: 94.12 Train acc: 93.668 Test acc: 94.110 \n",
      "step: 1122 | Train: G_Loss: 1.057, D_unsup_loss_real: 0.652, D_unsup_loss_fake: 0.588, D_sup_loss: 0.222, D_sup_acc: 94.18 Train acc: 93.733 Test acc: 94.130 \n",
      "step: 1123 | Train: G_Loss: 1.017, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.706, D_sup_loss: 0.219, D_sup_acc: 94.20 Train acc: 93.783 Test acc: 94.230 \n",
      "step: 1124 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.709, D_sup_loss: 0.216, D_sup_acc: 94.30 Train acc: 93.825 Test acc: 94.240 \n",
      "step: 1125 | Train: G_Loss: 1.075, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.664, D_sup_loss: 0.215, D_sup_acc: 94.31 Train acc: 93.787 Test acc: 94.190 \n",
      "step: 1126 | Train: G_Loss: 1.063, D_unsup_loss_real: 0.689, D_unsup_loss_fake: 0.640, D_sup_loss: 0.215, D_sup_acc: 94.26 Train acc: 93.743 Test acc: 94.210 \n",
      "step: 1127 | Train: G_Loss: 1.223, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.716, D_sup_loss: 0.217, D_sup_acc: 94.28 Train acc: 93.657 Test acc: 94.230 \n",
      "step: 1128 | Train: G_Loss: 1.052, D_unsup_loss_real: 0.681, D_unsup_loss_fake: 0.622, D_sup_loss: 0.219, D_sup_acc: 94.30 Train acc: 93.715 Test acc: 94.200 \n",
      "step: 1129 | Train: G_Loss: 1.100, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.654, D_sup_loss: 0.218, D_sup_acc: 94.27 Train acc: 93.758 Test acc: 94.250 \n",
      "step: 1130 | Train: G_Loss: 1.092, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.607, D_sup_loss: 0.214, D_sup_acc: 94.32 Train acc: 93.738 Test acc: 94.280 \n",
      "step: 1131 | Train: G_Loss: 1.061, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.634, D_sup_loss: 0.213, D_sup_acc: 94.35 Train acc: 93.790 Test acc: 94.280 \n",
      "step: 1132 | Train: G_Loss: 1.082, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.707, D_sup_loss: 0.212, D_sup_acc: 94.35 Train acc: 93.683 Test acc: 94.190 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1133 | Train: G_Loss: 1.057, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.554, D_sup_loss: 0.215, D_sup_acc: 94.26 Train acc: 93.660 Test acc: 94.200 \n",
      "step: 1134 | Train: G_Loss: 1.067, D_unsup_loss_real: 0.668, D_unsup_loss_fake: 0.714, D_sup_loss: 0.214, D_sup_acc: 94.27 Train acc: 93.617 Test acc: 94.090 \n",
      "step: 1135 | Train: G_Loss: 0.987, D_unsup_loss_real: 0.657, D_unsup_loss_fake: 0.675, D_sup_loss: 0.215, D_sup_acc: 94.16 Train acc: 93.705 Test acc: 94.290 \n",
      "step: 1136 | Train: G_Loss: 1.089, D_unsup_loss_real: 0.674, D_unsup_loss_fake: 0.744, D_sup_loss: 0.213, D_sup_acc: 94.36 Train acc: 93.600 Test acc: 94.290 \n",
      "step: 1137 | Train: G_Loss: 0.991, D_unsup_loss_real: 0.675, D_unsup_loss_fake: 0.687, D_sup_loss: 0.219, D_sup_acc: 94.36 Train acc: 93.408 Test acc: 94.010 \n",
      "step: 1138 | Train: G_Loss: 1.136, D_unsup_loss_real: 0.673, D_unsup_loss_fake: 0.789, D_sup_loss: 0.223, D_sup_acc: 94.09 Train acc: 93.523 Test acc: 93.970 \n",
      "step: 1139 | Train: G_Loss: 1.087, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.615, D_sup_loss: 0.218, D_sup_acc: 94.05 Train acc: 93.672 Test acc: 94.080 \n",
      "step: 1140 | Train: G_Loss: 1.070, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.708, D_sup_loss: 0.215, D_sup_acc: 94.15 Train acc: 93.798 Test acc: 94.320 \n",
      "step: 1141 | Train: G_Loss: 1.114, D_unsup_loss_real: 0.659, D_unsup_loss_fake: 0.683, D_sup_loss: 0.214, D_sup_acc: 94.39 Train acc: 93.745 Test acc: 94.330 \n",
      "step: 1142 | Train: G_Loss: 1.046, D_unsup_loss_real: 0.700, D_unsup_loss_fake: 0.645, D_sup_loss: 0.214, D_sup_acc: 94.40 Train acc: 93.790 Test acc: 94.350 \n",
      "step: 1143 | Train: G_Loss: 1.065, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.675, D_sup_loss: 0.214, D_sup_acc: 94.42 Train acc: 93.795 Test acc: 94.190 \n",
      "step: 1144 | Train: G_Loss: 1.065, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.652, D_sup_loss: 0.213, D_sup_acc: 94.26 Train acc: 93.850 Test acc: 94.220 \n",
      "step: 1145 | Train: G_Loss: 1.109, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.674, D_sup_loss: 0.212, D_sup_acc: 94.29 Train acc: 93.825 Test acc: 94.210 \n",
      "step: 1146 | Train: G_Loss: 1.025, D_unsup_loss_real: 0.704, D_unsup_loss_fake: 0.611, D_sup_loss: 0.212, D_sup_acc: 94.28 Train acc: 93.885 Test acc: 94.360 \n",
      "step: 1147 | Train: G_Loss: 1.046, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.704, D_sup_loss: 0.212, D_sup_acc: 94.43 Train acc: 93.843 Test acc: 94.390 \n",
      "step: 1148 | Train: G_Loss: 1.084, D_unsup_loss_real: 0.686, D_unsup_loss_fake: 0.687, D_sup_loss: 0.215, D_sup_acc: 94.46 Train acc: 93.755 Test acc: 94.250 \n",
      "step: 1149 | Train: G_Loss: 1.096, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.587, D_sup_loss: 0.219, D_sup_acc: 94.32 Train acc: 93.862 Test acc: 94.240 \n",
      "step: 1150 | Train: G_Loss: 1.065, D_unsup_loss_real: 0.656, D_unsup_loss_fake: 0.651, D_sup_loss: 0.216, D_sup_acc: 94.31 Train acc: 93.838 Test acc: 94.260 \n",
      "step: 1151 | Train: G_Loss: 1.025, D_unsup_loss_real: 0.650, D_unsup_loss_fake: 0.661, D_sup_loss: 0.215, D_sup_acc: 94.33 Train acc: 93.807 Test acc: 94.190 \n",
      "step: 1152 | Train: G_Loss: 1.126, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.657, D_sup_loss: 0.217, D_sup_acc: 94.26 Train acc: 93.800 Test acc: 94.300 \n",
      "step: 1153 | Train: G_Loss: 1.029, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.634, D_sup_loss: 0.216, D_sup_acc: 94.37 Train acc: 93.753 Test acc: 94.250 \n",
      "step: 1154 | Train: G_Loss: 1.094, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.660, D_sup_loss: 0.216, D_sup_acc: 94.32 Train acc: 93.848 Test acc: 94.250 \n",
      "step: 1155 | Train: G_Loss: 1.066, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.683, D_sup_loss: 0.214, D_sup_acc: 94.32 Train acc: 93.792 Test acc: 94.280 \n",
      "step: 1156 | Train: G_Loss: 1.074, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.661, D_sup_loss: 0.214, D_sup_acc: 94.35 Train acc: 93.802 Test acc: 94.430 \n",
      "step: 1157 | Train: G_Loss: 1.007, D_unsup_loss_real: 0.699, D_unsup_loss_fake: 0.669, D_sup_loss: 0.217, D_sup_acc: 94.50 Train acc: 93.745 Test acc: 94.290 \n",
      "step: 1158 | Train: G_Loss: 1.043, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.672, D_sup_loss: 0.218, D_sup_acc: 94.36 Train acc: 93.827 Test acc: 94.300 \n",
      "step: 1159 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.717, D_sup_loss: 0.215, D_sup_acc: 94.37 Train acc: 93.748 Test acc: 94.210 \n",
      "step: 1160 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.529, D_unsup_loss_fake: 0.604, D_sup_loss: 0.215, D_sup_acc: 94.28 Train acc: 93.700 Test acc: 94.150 \n",
      "step: 1161 | Train: G_Loss: 1.045, D_unsup_loss_real: 0.650, D_unsup_loss_fake: 0.627, D_sup_loss: 0.213, D_sup_acc: 94.22 Train acc: 93.725 Test acc: 94.110 \n",
      "step: 1162 | Train: G_Loss: 1.088, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.689, D_sup_loss: 0.212, D_sup_acc: 94.18 Train acc: 93.712 Test acc: 94.030 \n",
      "step: 1163 | Train: G_Loss: 1.127, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.710, D_sup_loss: 0.213, D_sup_acc: 94.11 Train acc: 93.490 Test acc: 93.910 \n",
      "step: 1164 | Train: G_Loss: 1.151, D_unsup_loss_real: 0.658, D_unsup_loss_fake: 0.606, D_sup_loss: 0.221, D_sup_acc: 93.99 Train acc: 93.403 Test acc: 93.770 \n",
      "step: 1165 | Train: G_Loss: 1.135, D_unsup_loss_real: 0.642, D_unsup_loss_fake: 0.624, D_sup_loss: 0.223, D_sup_acc: 93.85 Train acc: 93.492 Test acc: 93.940 \n",
      "step: 1166 | Train: G_Loss: 1.062, D_unsup_loss_real: 0.642, D_unsup_loss_fake: 0.742, D_sup_loss: 0.219, D_sup_acc: 94.02 Train acc: 93.507 Test acc: 93.880 \n",
      "step: 1167 | Train: G_Loss: 0.974, D_unsup_loss_real: 0.663, D_unsup_loss_fake: 0.643, D_sup_loss: 0.225, D_sup_acc: 93.96 Train acc: 93.710 Test acc: 94.150 \n",
      "step: 1168 | Train: G_Loss: 1.104, D_unsup_loss_real: 0.657, D_unsup_loss_fake: 0.748, D_sup_loss: 0.221, D_sup_acc: 94.22 Train acc: 93.588 Test acc: 94.010 \n",
      "step: 1169 | Train: G_Loss: 1.111, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.614, D_sup_loss: 0.224, D_sup_acc: 94.09 Train acc: 93.557 Test acc: 94.130 \n",
      "step: 1170 | Train: G_Loss: 1.014, D_unsup_loss_real: 0.728, D_unsup_loss_fake: 0.663, D_sup_loss: 0.224, D_sup_acc: 94.20 Train acc: 93.673 Test acc: 94.140 \n",
      "step: 1171 | Train: G_Loss: 1.068, D_unsup_loss_real: 0.666, D_unsup_loss_fake: 0.657, D_sup_loss: 0.224, D_sup_acc: 94.21 Train acc: 93.585 Test acc: 94.010 \n",
      "step: 1172 | Train: G_Loss: 1.014, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.714, D_sup_loss: 0.225, D_sup_acc: 94.09 Train acc: 93.525 Test acc: 93.950 \n",
      "step: 1173 | Train: G_Loss: 1.032, D_unsup_loss_real: 0.665, D_unsup_loss_fake: 0.629, D_sup_loss: 0.226, D_sup_acc: 94.03 Train acc: 93.545 Test acc: 94.000 \n",
      "step: 1174 | Train: G_Loss: 1.087, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.633, D_sup_loss: 0.227, D_sup_acc: 94.08 Train acc: 93.648 Test acc: 94.120 \n",
      "step: 1175 | Train: G_Loss: 1.067, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.662, D_sup_loss: 0.221, D_sup_acc: 94.19 Train acc: 93.678 Test acc: 94.090 \n",
      "step: 1176 | Train: G_Loss: 1.033, D_unsup_loss_real: 0.649, D_unsup_loss_fake: 0.670, D_sup_loss: 0.218, D_sup_acc: 94.16 Train acc: 93.732 Test acc: 94.180 \n",
      "step: 1177 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.637, D_sup_loss: 0.217, D_sup_acc: 94.25 Train acc: 93.683 Test acc: 94.140 \n",
      "step: 1178 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.728, D_sup_loss: 0.216, D_sup_acc: 94.21 Train acc: 93.708 Test acc: 94.160 \n",
      "step: 1179 | Train: G_Loss: 1.116, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.636, D_sup_loss: 0.221, D_sup_acc: 94.23 Train acc: 93.702 Test acc: 94.130 \n",
      "step: 1180 | Train: G_Loss: 1.065, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.611, D_sup_loss: 0.218, D_sup_acc: 94.20 Train acc: 93.697 Test acc: 94.170 \n",
      "step: 1181 | Train: G_Loss: 1.050, D_unsup_loss_real: 0.679, D_unsup_loss_fake: 0.671, D_sup_loss: 0.218, D_sup_acc: 94.24 Train acc: 93.637 Test acc: 94.070 \n",
      "step: 1182 | Train: G_Loss: 1.022, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.628, D_sup_loss: 0.221, D_sup_acc: 94.14 Train acc: 93.608 Test acc: 94.080 \n",
      "step: 1183 | Train: G_Loss: 1.064, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.777, D_sup_loss: 0.220, D_sup_acc: 94.15 Train acc: 93.645 Test acc: 94.050 \n",
      "step: 1184 | Train: G_Loss: 1.065, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.670, D_sup_loss: 0.220, D_sup_acc: 94.13 Train acc: 93.638 Test acc: 94.040 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1185 | Train: G_Loss: 1.036, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.633, D_sup_loss: 0.220, D_sup_acc: 94.12 Train acc: 93.635 Test acc: 94.070 \n",
      "step: 1186 | Train: G_Loss: 1.130, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.747, D_sup_loss: 0.219, D_sup_acc: 94.14 Train acc: 93.580 Test acc: 94.010 \n",
      "step: 1187 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.647, D_sup_loss: 0.220, D_sup_acc: 94.09 Train acc: 93.643 Test acc: 94.080 \n",
      "step: 1188 | Train: G_Loss: 0.966, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.620, D_sup_loss: 0.218, D_sup_acc: 94.15 Train acc: 93.688 Test acc: 94.150 \n",
      "step: 1189 | Train: G_Loss: 1.081, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.623, D_sup_loss: 0.217, D_sup_acc: 94.22 Train acc: 93.672 Test acc: 94.100 \n",
      "step: 1190 | Train: G_Loss: 1.175, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.646, D_sup_loss: 0.220, D_sup_acc: 94.17 Train acc: 93.682 Test acc: 94.100 \n",
      "step: 1191 | Train: G_Loss: 1.123, D_unsup_loss_real: 0.660, D_unsup_loss_fake: 0.600, D_sup_loss: 0.221, D_sup_acc: 94.17 Train acc: 93.795 Test acc: 94.200 \n",
      "step: 1192 | Train: G_Loss: 1.161, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.680, D_sup_loss: 0.215, D_sup_acc: 94.27 Train acc: 93.752 Test acc: 94.290 \n",
      "step: 1193 | Train: G_Loss: 1.089, D_unsup_loss_real: 0.651, D_unsup_loss_fake: 0.661, D_sup_loss: 0.215, D_sup_acc: 94.36 Train acc: 93.767 Test acc: 94.300 \n",
      "step: 1194 | Train: G_Loss: 1.045, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.622, D_sup_loss: 0.214, D_sup_acc: 94.37 Train acc: 93.928 Test acc: 94.360 \n",
      "step: 1195 | Train: G_Loss: 1.049, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.658, D_sup_loss: 0.212, D_sup_acc: 94.43 Train acc: 93.887 Test acc: 94.300 \n",
      "step: 1196 | Train: G_Loss: 1.086, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.685, D_sup_loss: 0.214, D_sup_acc: 94.37 Train acc: 93.818 Test acc: 94.220 \n",
      "step: 1197 | Train: G_Loss: 0.991, D_unsup_loss_real: 0.669, D_unsup_loss_fake: 0.598, D_sup_loss: 0.213, D_sup_acc: 94.29 Train acc: 93.815 Test acc: 94.270 \n",
      "step: 1198 | Train: G_Loss: 1.044, D_unsup_loss_real: 0.675, D_unsup_loss_fake: 0.632, D_sup_loss: 0.213, D_sup_acc: 94.34 Train acc: 93.753 Test acc: 94.190 \n",
      "step: 1199 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.664, D_unsup_loss_fake: 0.617, D_sup_loss: 0.214, D_sup_acc: 94.26 Train acc: 93.810 Test acc: 94.180 \n",
      "step: 1200 | Train: G_Loss: 1.107, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.644, D_sup_loss: 0.215, D_sup_acc: 94.25 Train acc: 93.898 Test acc: 94.370 \n",
      "Train Classifier Accuracy: 93.898%\n",
      "\n",
      "Test Classifier Accuracy: 94.370%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_1200.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_1200.h5\n",
      "step: 1201 | Train: G_Loss: 1.110, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.639, D_sup_loss: 0.214, D_sup_acc: 94.44 Train acc: 93.847 Test acc: 94.330 \n",
      "step: 1202 | Train: G_Loss: 1.050, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.664, D_sup_loss: 0.214, D_sup_acc: 94.40 Train acc: 93.833 Test acc: 94.230 \n",
      "step: 1203 | Train: G_Loss: 1.129, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.709, D_sup_loss: 0.211, D_sup_acc: 94.30 Train acc: 93.850 Test acc: 94.320 \n",
      "step: 1204 | Train: G_Loss: 1.088, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.662, D_sup_loss: 0.211, D_sup_acc: 94.39 Train acc: 93.942 Test acc: 94.440 \n",
      "step: 1205 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.674, D_unsup_loss_fake: 0.643, D_sup_loss: 0.209, D_sup_acc: 94.51 Train acc: 93.980 Test acc: 94.380 \n",
      "step: 1206 | Train: G_Loss: 1.100, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.685, D_sup_loss: 0.210, D_sup_acc: 94.45 Train acc: 93.965 Test acc: 94.360 \n",
      "step: 1207 | Train: G_Loss: 1.021, D_unsup_loss_real: 0.666, D_unsup_loss_fake: 0.727, D_sup_loss: 0.211, D_sup_acc: 94.43 Train acc: 93.910 Test acc: 94.320 \n",
      "step: 1208 | Train: G_Loss: 0.985, D_unsup_loss_real: 0.690, D_unsup_loss_fake: 0.693, D_sup_loss: 0.215, D_sup_acc: 94.39 Train acc: 93.835 Test acc: 94.330 \n",
      "step: 1209 | Train: G_Loss: 1.054, D_unsup_loss_real: 0.656, D_unsup_loss_fake: 0.668, D_sup_loss: 0.218, D_sup_acc: 94.40 Train acc: 93.763 Test acc: 94.320 \n",
      "step: 1210 | Train: G_Loss: 1.077, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.674, D_sup_loss: 0.221, D_sup_acc: 94.39 Train acc: 93.638 Test acc: 94.170 \n",
      "step: 1211 | Train: G_Loss: 1.050, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.668, D_sup_loss: 0.227, D_sup_acc: 94.24 Train acc: 93.700 Test acc: 94.190 \n",
      "step: 1212 | Train: G_Loss: 1.081, D_unsup_loss_real: 0.684, D_unsup_loss_fake: 0.643, D_sup_loss: 0.223, D_sup_acc: 94.26 Train acc: 93.860 Test acc: 94.450 \n",
      "step: 1213 | Train: G_Loss: 1.014, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.653, D_sup_loss: 0.216, D_sup_acc: 94.52 Train acc: 93.977 Test acc: 94.620 \n",
      "step: 1214 | Train: G_Loss: 1.004, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.703, D_sup_loss: 0.211, D_sup_acc: 94.69 Train acc: 93.992 Test acc: 94.580 \n",
      "step: 1215 | Train: G_Loss: 1.038, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.634, D_sup_loss: 0.211, D_sup_acc: 94.65 Train acc: 93.958 Test acc: 94.570 \n",
      "step: 1216 | Train: G_Loss: 1.004, D_unsup_loss_real: 0.666, D_unsup_loss_fake: 0.629, D_sup_loss: 0.215, D_sup_acc: 94.64 Train acc: 93.980 Test acc: 94.640 \n",
      "step: 1217 | Train: G_Loss: 1.023, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.654, D_sup_loss: 0.214, D_sup_acc: 94.71 Train acc: 94.037 Test acc: 94.590 \n",
      "step: 1218 | Train: G_Loss: 1.064, D_unsup_loss_real: 0.686, D_unsup_loss_fake: 0.637, D_sup_loss: 0.212, D_sup_acc: 94.66 Train acc: 94.013 Test acc: 94.590 \n",
      "step: 1219 | Train: G_Loss: 1.025, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.617, D_sup_loss: 0.214, D_sup_acc: 94.66 Train acc: 94.050 Test acc: 94.570 \n",
      "step: 1220 | Train: G_Loss: 1.063, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.635, D_sup_loss: 0.213, D_sup_acc: 94.64 Train acc: 93.995 Test acc: 94.500 \n",
      "step: 1221 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.657, D_unsup_loss_fake: 0.634, D_sup_loss: 0.213, D_sup_acc: 94.57 Train acc: 94.013 Test acc: 94.500 \n",
      "step: 1222 | Train: G_Loss: 1.060, D_unsup_loss_real: 0.675, D_unsup_loss_fake: 0.652, D_sup_loss: 0.211, D_sup_acc: 94.57 Train acc: 94.063 Test acc: 94.640 \n",
      "step: 1223 | Train: G_Loss: 1.028, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.641, D_sup_loss: 0.210, D_sup_acc: 94.71 Train acc: 94.008 Test acc: 94.540 \n",
      "step: 1224 | Train: G_Loss: 1.065, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.688, D_sup_loss: 0.214, D_sup_acc: 94.61 Train acc: 94.040 Test acc: 94.590 \n",
      "step: 1225 | Train: G_Loss: 1.032, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.656, D_sup_loss: 0.212, D_sup_acc: 94.66 Train acc: 94.048 Test acc: 94.630 \n",
      "step: 1226 | Train: G_Loss: 1.062, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.767, D_sup_loss: 0.211, D_sup_acc: 94.70 Train acc: 94.032 Test acc: 94.620 \n",
      "step: 1227 | Train: G_Loss: 1.062, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.581, D_sup_loss: 0.213, D_sup_acc: 94.69 Train acc: 94.063 Test acc: 94.660 \n",
      "step: 1228 | Train: G_Loss: 1.080, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.649, D_sup_loss: 0.212, D_sup_acc: 94.73 Train acc: 94.077 Test acc: 94.700 \n",
      "step: 1229 | Train: G_Loss: 1.025, D_unsup_loss_real: 0.639, D_unsup_loss_fake: 0.683, D_sup_loss: 0.210, D_sup_acc: 94.77 Train acc: 94.000 Test acc: 94.540 \n",
      "step: 1230 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.659, D_sup_loss: 0.211, D_sup_acc: 94.61 Train acc: 93.968 Test acc: 94.420 \n",
      "step: 1231 | Train: G_Loss: 1.086, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.580, D_sup_loss: 0.213, D_sup_acc: 94.49 Train acc: 94.083 Test acc: 94.610 \n",
      "step: 1232 | Train: G_Loss: 1.019, D_unsup_loss_real: 0.678, D_unsup_loss_fake: 0.698, D_sup_loss: 0.209, D_sup_acc: 94.68 Train acc: 94.063 Test acc: 94.660 \n",
      "step: 1233 | Train: G_Loss: 1.025, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.706, D_sup_loss: 0.211, D_sup_acc: 94.73 Train acc: 94.110 Test acc: 94.660 \n",
      "step: 1234 | Train: G_Loss: 1.022, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.617, D_sup_loss: 0.210, D_sup_acc: 94.73 Train acc: 94.008 Test acc: 94.480 \n",
      "step: 1235 | Train: G_Loss: 1.072, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.654, D_sup_loss: 0.211, D_sup_acc: 94.55 Train acc: 94.082 Test acc: 94.480 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1236 | Train: G_Loss: 1.125, D_unsup_loss_real: 0.642, D_unsup_loss_fake: 0.676, D_sup_loss: 0.210, D_sup_acc: 94.55 Train acc: 94.037 Test acc: 94.360 \n",
      "step: 1237 | Train: G_Loss: 1.107, D_unsup_loss_real: 0.635, D_unsup_loss_fake: 0.663, D_sup_loss: 0.210, D_sup_acc: 94.43 Train acc: 94.018 Test acc: 94.400 \n",
      "step: 1238 | Train: G_Loss: 1.086, D_unsup_loss_real: 0.545, D_unsup_loss_fake: 0.603, D_sup_loss: 0.211, D_sup_acc: 94.47 Train acc: 94.185 Test acc: 94.520 \n",
      "step: 1239 | Train: G_Loss: 1.088, D_unsup_loss_real: 0.655, D_unsup_loss_fake: 0.626, D_sup_loss: 0.207, D_sup_acc: 94.59 Train acc: 94.182 Test acc: 94.570 \n",
      "step: 1240 | Train: G_Loss: 1.086, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.656, D_sup_loss: 0.206, D_sup_acc: 94.64 Train acc: 94.093 Test acc: 94.440 \n",
      "step: 1241 | Train: G_Loss: 1.045, D_unsup_loss_real: 0.550, D_unsup_loss_fake: 0.601, D_sup_loss: 0.206, D_sup_acc: 94.51 Train acc: 94.005 Test acc: 94.380 \n",
      "step: 1242 | Train: G_Loss: 1.009, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.730, D_sup_loss: 0.209, D_sup_acc: 94.45 Train acc: 94.095 Test acc: 94.420 \n",
      "step: 1243 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.689, D_unsup_loss_fake: 0.730, D_sup_loss: 0.211, D_sup_acc: 94.49 Train acc: 94.103 Test acc: 94.480 \n",
      "step: 1244 | Train: G_Loss: 1.168, D_unsup_loss_real: 0.650, D_unsup_loss_fake: 0.622, D_sup_loss: 0.211, D_sup_acc: 94.55 Train acc: 94.105 Test acc: 94.400 \n",
      "step: 1245 | Train: G_Loss: 1.014, D_unsup_loss_real: 0.654, D_unsup_loss_fake: 0.637, D_sup_loss: 0.212, D_sup_acc: 94.47 Train acc: 94.078 Test acc: 94.450 \n",
      "step: 1246 | Train: G_Loss: 1.054, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.732, D_sup_loss: 0.209, D_sup_acc: 94.52 Train acc: 94.183 Test acc: 94.630 \n",
      "step: 1247 | Train: G_Loss: 1.048, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.595, D_sup_loss: 0.208, D_sup_acc: 94.70 Train acc: 94.245 Test acc: 94.660 \n",
      "step: 1248 | Train: G_Loss: 1.030, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.584, D_sup_loss: 0.206, D_sup_acc: 94.73 Train acc: 94.327 Test acc: 94.720 \n",
      "step: 1249 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.590, D_sup_loss: 0.205, D_sup_acc: 94.79 Train acc: 94.202 Test acc: 94.620 \n",
      "step: 1250 | Train: G_Loss: 1.011, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.601, D_sup_loss: 0.207, D_sup_acc: 94.69 Train acc: 94.232 Test acc: 94.630 \n",
      "step: 1251 | Train: G_Loss: 1.042, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.703, D_sup_loss: 0.204, D_sup_acc: 94.70 Train acc: 94.170 Test acc: 94.670 \n",
      "step: 1252 | Train: G_Loss: 1.045, D_unsup_loss_real: 0.695, D_unsup_loss_fake: 0.637, D_sup_loss: 0.206, D_sup_acc: 94.74 Train acc: 94.292 Test acc: 94.920 \n",
      "step: 1253 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.663, D_sup_loss: 0.203, D_sup_acc: 94.98 Train acc: 94.318 Test acc: 94.870 \n",
      "step: 1254 | Train: G_Loss: 1.082, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.620, D_sup_loss: 0.199, D_sup_acc: 94.93 Train acc: 94.323 Test acc: 94.930 \n",
      "step: 1255 | Train: G_Loss: 1.068, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.698, D_sup_loss: 0.198, D_sup_acc: 94.99 Train acc: 94.330 Test acc: 94.910 \n",
      "step: 1256 | Train: G_Loss: 1.094, D_unsup_loss_real: 0.648, D_unsup_loss_fake: 0.633, D_sup_loss: 0.197, D_sup_acc: 94.97 Train acc: 94.367 Test acc: 94.880 \n",
      "step: 1257 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.595, D_sup_loss: 0.197, D_sup_acc: 94.94 Train acc: 94.310 Test acc: 94.840 \n",
      "step: 1258 | Train: G_Loss: 1.149, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.775, D_sup_loss: 0.197, D_sup_acc: 94.91 Train acc: 94.277 Test acc: 94.740 \n",
      "step: 1259 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.736, D_unsup_loss_fake: 0.611, D_sup_loss: 0.201, D_sup_acc: 94.81 Train acc: 94.373 Test acc: 94.880 \n",
      "step: 1260 | Train: G_Loss: 1.088, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.677, D_sup_loss: 0.198, D_sup_acc: 94.94 Train acc: 94.332 Test acc: 94.870 \n",
      "step: 1261 | Train: G_Loss: 0.993, D_unsup_loss_real: 0.656, D_unsup_loss_fake: 0.621, D_sup_loss: 0.198, D_sup_acc: 94.93 Train acc: 94.345 Test acc: 94.840 \n",
      "step: 1262 | Train: G_Loss: 1.045, D_unsup_loss_real: 0.699, D_unsup_loss_fake: 0.647, D_sup_loss: 0.198, D_sup_acc: 94.91 Train acc: 94.288 Test acc: 94.820 \n",
      "step: 1263 | Train: G_Loss: 1.033, D_unsup_loss_real: 0.655, D_unsup_loss_fake: 0.631, D_sup_loss: 0.200, D_sup_acc: 94.89 Train acc: 94.173 Test acc: 94.700 \n",
      "step: 1264 | Train: G_Loss: 1.024, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.781, D_sup_loss: 0.201, D_sup_acc: 94.77 Train acc: 94.237 Test acc: 94.840 \n",
      "step: 1265 | Train: G_Loss: 1.187, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.683, D_sup_loss: 0.201, D_sup_acc: 94.91 Train acc: 94.325 Test acc: 94.770 \n",
      "step: 1266 | Train: G_Loss: 1.062, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.664, D_sup_loss: 0.201, D_sup_acc: 94.84 Train acc: 94.287 Test acc: 94.700 \n",
      "step: 1267 | Train: G_Loss: 1.023, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.624, D_sup_loss: 0.200, D_sup_acc: 94.77 Train acc: 94.285 Test acc: 94.640 \n",
      "step: 1268 | Train: G_Loss: 1.135, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.746, D_sup_loss: 0.199, D_sup_acc: 94.71 Train acc: 94.302 Test acc: 94.820 \n",
      "step: 1269 | Train: G_Loss: 1.097, D_unsup_loss_real: 0.699, D_unsup_loss_fake: 0.657, D_sup_loss: 0.199, D_sup_acc: 94.89 Train acc: 94.360 Test acc: 94.830 \n",
      "step: 1270 | Train: G_Loss: 1.076, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.697, D_sup_loss: 0.200, D_sup_acc: 94.90 Train acc: 94.392 Test acc: 94.830 \n",
      "step: 1271 | Train: G_Loss: 1.146, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.700, D_sup_loss: 0.201, D_sup_acc: 94.90 Train acc: 94.337 Test acc: 94.670 \n",
      "step: 1272 | Train: G_Loss: 0.981, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.618, D_sup_loss: 0.202, D_sup_acc: 94.74 Train acc: 94.278 Test acc: 94.740 \n",
      "step: 1273 | Train: G_Loss: 1.087, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.628, D_sup_loss: 0.199, D_sup_acc: 94.81 Train acc: 94.342 Test acc: 94.770 \n",
      "step: 1274 | Train: G_Loss: 1.027, D_unsup_loss_real: 0.639, D_unsup_loss_fake: 0.683, D_sup_loss: 0.198, D_sup_acc: 94.84 Train acc: 94.418 Test acc: 94.870 \n",
      "step: 1275 | Train: G_Loss: 1.079, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.630, D_sup_loss: 0.197, D_sup_acc: 94.93 Train acc: 94.347 Test acc: 94.860 \n",
      "step: 1276 | Train: G_Loss: 1.033, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.670, D_sup_loss: 0.197, D_sup_acc: 94.92 Train acc: 94.312 Test acc: 94.800 \n",
      "step: 1277 | Train: G_Loss: 1.053, D_unsup_loss_real: 0.675, D_unsup_loss_fake: 0.671, D_sup_loss: 0.200, D_sup_acc: 94.87 Train acc: 94.308 Test acc: 94.830 \n",
      "step: 1278 | Train: G_Loss: 1.126, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.674, D_sup_loss: 0.200, D_sup_acc: 94.90 Train acc: 94.295 Test acc: 94.670 \n",
      "step: 1279 | Train: G_Loss: 1.153, D_unsup_loss_real: 0.658, D_unsup_loss_fake: 0.575, D_sup_loss: 0.202, D_sup_acc: 94.74 Train acc: 94.297 Test acc: 94.580 \n",
      "step: 1280 | Train: G_Loss: 1.060, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.637, D_sup_loss: 0.201, D_sup_acc: 94.65 Train acc: 94.390 Test acc: 94.780 \n",
      "step: 1281 | Train: G_Loss: 1.066, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.598, D_sup_loss: 0.197, D_sup_acc: 94.85 Train acc: 94.375 Test acc: 94.750 \n",
      "step: 1282 | Train: G_Loss: 1.132, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.705, D_sup_loss: 0.195, D_sup_acc: 94.82 Train acc: 94.377 Test acc: 94.870 \n",
      "step: 1283 | Train: G_Loss: 1.091, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.696, D_sup_loss: 0.196, D_sup_acc: 94.93 Train acc: 94.350 Test acc: 94.750 \n",
      "step: 1284 | Train: G_Loss: 1.053, D_unsup_loss_real: 0.669, D_unsup_loss_fake: 0.748, D_sup_loss: 0.196, D_sup_acc: 94.82 Train acc: 94.360 Test acc: 94.880 \n",
      "step: 1285 | Train: G_Loss: 1.129, D_unsup_loss_real: 0.704, D_unsup_loss_fake: 0.706, D_sup_loss: 0.199, D_sup_acc: 94.94 Train acc: 94.332 Test acc: 94.760 \n",
      "step: 1286 | Train: G_Loss: 1.147, D_unsup_loss_real: 0.685, D_unsup_loss_fake: 0.623, D_sup_loss: 0.203, D_sup_acc: 94.83 Train acc: 94.360 Test acc: 94.790 \n",
      "step: 1287 | Train: G_Loss: 1.089, D_unsup_loss_real: 0.717, D_unsup_loss_fake: 0.654, D_sup_loss: 0.203, D_sup_acc: 94.86 Train acc: 94.385 Test acc: 94.870 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1288 | Train: G_Loss: 1.086, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.648, D_sup_loss: 0.204, D_sup_acc: 94.93 Train acc: 94.360 Test acc: 94.800 \n",
      "step: 1289 | Train: G_Loss: 1.059, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.603, D_sup_loss: 0.205, D_sup_acc: 94.87 Train acc: 94.465 Test acc: 94.920 \n",
      "step: 1290 | Train: G_Loss: 0.990, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.680, D_sup_loss: 0.199, D_sup_acc: 94.98 Train acc: 94.448 Test acc: 94.920 \n",
      "step: 1291 | Train: G_Loss: 1.020, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.666, D_sup_loss: 0.199, D_sup_acc: 94.98 Train acc: 94.448 Test acc: 94.820 \n",
      "step: 1292 | Train: G_Loss: 1.076, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.707, D_sup_loss: 0.196, D_sup_acc: 94.89 Train acc: 94.318 Test acc: 94.670 \n",
      "step: 1293 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.593, D_sup_loss: 0.200, D_sup_acc: 94.74 Train acc: 94.317 Test acc: 94.660 \n",
      "step: 1294 | Train: G_Loss: 1.087, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.629, D_sup_loss: 0.200, D_sup_acc: 94.73 Train acc: 94.293 Test acc: 94.630 \n",
      "step: 1295 | Train: G_Loss: 1.152, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.693, D_sup_loss: 0.201, D_sup_acc: 94.70 Train acc: 94.292 Test acc: 94.650 \n",
      "step: 1296 | Train: G_Loss: 1.139, D_unsup_loss_real: 0.651, D_unsup_loss_fake: 0.614, D_sup_loss: 0.203, D_sup_acc: 94.72 Train acc: 94.347 Test acc: 94.730 \n",
      "step: 1297 | Train: G_Loss: 1.051, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.612, D_sup_loss: 0.202, D_sup_acc: 94.80 Train acc: 94.267 Test acc: 94.620 \n",
      "step: 1298 | Train: G_Loss: 1.051, D_unsup_loss_real: 0.668, D_unsup_loss_fake: 0.658, D_sup_loss: 0.201, D_sup_acc: 94.69 Train acc: 94.227 Test acc: 94.700 \n",
      "step: 1299 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.586, D_sup_loss: 0.202, D_sup_acc: 94.77 Train acc: 94.240 Test acc: 94.640 \n",
      "step: 1300 | Train: G_Loss: 1.084, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.618, D_sup_loss: 0.202, D_sup_acc: 94.71 Train acc: 94.210 Test acc: 94.660 \n",
      "Train Classifier Accuracy: 94.210%\n",
      "\n",
      "Test Classifier Accuracy: 94.660%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_1300.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_1300.h5\n",
      "step: 1301 | Train: G_Loss: 1.115, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.606, D_sup_loss: 0.203, D_sup_acc: 94.73 Train acc: 94.192 Test acc: 94.590 \n",
      "step: 1302 | Train: G_Loss: 1.056, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.586, D_sup_loss: 0.203, D_sup_acc: 94.66 Train acc: 94.093 Test acc: 94.600 \n",
      "step: 1303 | Train: G_Loss: 1.043, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.643, D_sup_loss: 0.202, D_sup_acc: 94.67 Train acc: 94.215 Test acc: 94.670 \n",
      "step: 1304 | Train: G_Loss: 1.089, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.751, D_sup_loss: 0.200, D_sup_acc: 94.74 Train acc: 94.363 Test acc: 94.830 \n",
      "step: 1305 | Train: G_Loss: 1.066, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.726, D_sup_loss: 0.197, D_sup_acc: 94.90 Train acc: 94.375 Test acc: 94.940 \n",
      "step: 1306 | Train: G_Loss: 1.048, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.645, D_sup_loss: 0.200, D_sup_acc: 95.00 Train acc: 94.277 Test acc: 94.700 \n",
      "step: 1307 | Train: G_Loss: 1.167, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.575, D_sup_loss: 0.200, D_sup_acc: 94.77 Train acc: 94.308 Test acc: 94.700 \n",
      "step: 1308 | Train: G_Loss: 1.042, D_unsup_loss_real: 0.662, D_unsup_loss_fake: 0.622, D_sup_loss: 0.201, D_sup_acc: 94.77 Train acc: 94.443 Test acc: 94.730 \n",
      "step: 1309 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.641, D_sup_loss: 0.195, D_sup_acc: 94.80 Train acc: 94.475 Test acc: 94.830 \n",
      "step: 1310 | Train: G_Loss: 1.061, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.602, D_sup_loss: 0.192, D_sup_acc: 94.90 Train acc: 94.493 Test acc: 94.860 \n",
      "step: 1311 | Train: G_Loss: 1.112, D_unsup_loss_real: 0.652, D_unsup_loss_fake: 0.720, D_sup_loss: 0.189, D_sup_acc: 94.92 Train acc: 94.468 Test acc: 94.840 \n",
      "step: 1312 | Train: G_Loss: 1.153, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.702, D_sup_loss: 0.190, D_sup_acc: 94.91 Train acc: 94.472 Test acc: 94.940 \n",
      "step: 1313 | Train: G_Loss: 1.039, D_unsup_loss_real: 0.655, D_unsup_loss_fake: 0.712, D_sup_loss: 0.192, D_sup_acc: 95.00 Train acc: 94.490 Test acc: 94.910 \n",
      "step: 1314 | Train: G_Loss: 1.091, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.614, D_sup_loss: 0.194, D_sup_acc: 94.97 Train acc: 94.508 Test acc: 95.070 \n",
      "step: 1315 | Train: G_Loss: 1.104, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.687, D_sup_loss: 0.194, D_sup_acc: 95.13 Train acc: 94.463 Test acc: 95.070 \n",
      "step: 1316 | Train: G_Loss: 1.043, D_unsup_loss_real: 0.677, D_unsup_loss_fake: 0.717, D_sup_loss: 0.193, D_sup_acc: 95.12 Train acc: 94.458 Test acc: 95.050 \n",
      "step: 1317 | Train: G_Loss: 1.035, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.611, D_sup_loss: 0.195, D_sup_acc: 95.11 Train acc: 94.447 Test acc: 94.910 \n",
      "step: 1318 | Train: G_Loss: 1.063, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.668, D_sup_loss: 0.198, D_sup_acc: 94.97 Train acc: 94.503 Test acc: 95.000 \n",
      "step: 1319 | Train: G_Loss: 1.128, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.666, D_sup_loss: 0.197, D_sup_acc: 95.06 Train acc: 94.452 Test acc: 94.990 \n",
      "step: 1320 | Train: G_Loss: 1.086, D_unsup_loss_real: 0.689, D_unsup_loss_fake: 0.633, D_sup_loss: 0.198, D_sup_acc: 95.05 Train acc: 94.450 Test acc: 95.080 \n",
      "step: 1321 | Train: G_Loss: 1.195, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.601, D_sup_loss: 0.197, D_sup_acc: 95.14 Train acc: 94.495 Test acc: 94.980 \n",
      "step: 1322 | Train: G_Loss: 1.157, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.622, D_sup_loss: 0.197, D_sup_acc: 95.04 Train acc: 94.452 Test acc: 94.950 \n",
      "step: 1323 | Train: G_Loss: 1.067, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.602, D_sup_loss: 0.193, D_sup_acc: 95.01 Train acc: 94.518 Test acc: 94.980 \n",
      "step: 1324 | Train: G_Loss: 1.050, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.599, D_sup_loss: 0.191, D_sup_acc: 95.04 Train acc: 94.532 Test acc: 95.000 \n",
      "step: 1325 | Train: G_Loss: 1.106, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.590, D_sup_loss: 0.192, D_sup_acc: 95.06 Train acc: 94.492 Test acc: 94.960 \n",
      "step: 1326 | Train: G_Loss: 1.069, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.667, D_sup_loss: 0.194, D_sup_acc: 95.02 Train acc: 94.422 Test acc: 95.070 \n",
      "step: 1327 | Train: G_Loss: 1.083, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.663, D_sup_loss: 0.194, D_sup_acc: 95.13 Train acc: 94.462 Test acc: 95.080 \n",
      "step: 1328 | Train: G_Loss: 1.108, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.691, D_sup_loss: 0.196, D_sup_acc: 95.14 Train acc: 94.472 Test acc: 95.000 \n",
      "step: 1329 | Train: G_Loss: 1.080, D_unsup_loss_real: 0.671, D_unsup_loss_fake: 0.710, D_sup_loss: 0.195, D_sup_acc: 95.06 Train acc: 94.460 Test acc: 94.950 \n",
      "step: 1330 | Train: G_Loss: 1.136, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.705, D_sup_loss: 0.198, D_sup_acc: 95.01 Train acc: 94.383 Test acc: 94.910 \n",
      "step: 1331 | Train: G_Loss: 1.092, D_unsup_loss_real: 0.658, D_unsup_loss_fake: 0.623, D_sup_loss: 0.202, D_sup_acc: 94.97 Train acc: 94.423 Test acc: 94.970 \n",
      "step: 1332 | Train: G_Loss: 1.065, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.676, D_sup_loss: 0.197, D_sup_acc: 95.03 Train acc: 94.352 Test acc: 94.930 \n",
      "step: 1333 | Train: G_Loss: 1.096, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.630, D_sup_loss: 0.198, D_sup_acc: 94.99 Train acc: 94.395 Test acc: 94.810 \n",
      "step: 1334 | Train: G_Loss: 1.105, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.705, D_sup_loss: 0.199, D_sup_acc: 94.88 Train acc: 94.305 Test acc: 94.650 \n",
      "step: 1335 | Train: G_Loss: 1.072, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.638, D_sup_loss: 0.205, D_sup_acc: 94.72 Train acc: 94.397 Test acc: 94.790 \n",
      "step: 1336 | Train: G_Loss: 1.073, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.604, D_sup_loss: 0.199, D_sup_acc: 94.86 Train acc: 94.507 Test acc: 94.820 \n",
      "step: 1337 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.639, D_sup_loss: 0.196, D_sup_acc: 94.89 Train acc: 94.460 Test acc: 94.870 \n",
      "step: 1338 | Train: G_Loss: 1.187, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.610, D_sup_loss: 0.196, D_sup_acc: 94.93 Train acc: 94.427 Test acc: 94.840 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1339 | Train: G_Loss: 1.136, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.744, D_sup_loss: 0.196, D_sup_acc: 94.91 Train acc: 94.387 Test acc: 94.900 \n",
      "step: 1340 | Train: G_Loss: 1.160, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.659, D_sup_loss: 0.198, D_sup_acc: 94.96 Train acc: 94.377 Test acc: 94.830 \n",
      "step: 1341 | Train: G_Loss: 1.015, D_unsup_loss_real: 0.648, D_unsup_loss_fake: 0.532, D_sup_loss: 0.199, D_sup_acc: 94.90 Train acc: 94.440 Test acc: 94.830 \n",
      "step: 1342 | Train: G_Loss: 1.025, D_unsup_loss_real: 0.631, D_unsup_loss_fake: 0.686, D_sup_loss: 0.196, D_sup_acc: 94.90 Train acc: 94.470 Test acc: 94.800 \n",
      "step: 1343 | Train: G_Loss: 1.103, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.744, D_sup_loss: 0.196, D_sup_acc: 94.87 Train acc: 94.498 Test acc: 94.770 \n",
      "step: 1344 | Train: G_Loss: 1.074, D_unsup_loss_real: 0.661, D_unsup_loss_fake: 0.647, D_sup_loss: 0.196, D_sup_acc: 94.84 Train acc: 94.493 Test acc: 94.880 \n",
      "step: 1345 | Train: G_Loss: 1.091, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.698, D_sup_loss: 0.196, D_sup_acc: 94.94 Train acc: 94.575 Test acc: 94.880 \n",
      "step: 1346 | Train: G_Loss: 1.086, D_unsup_loss_real: 0.628, D_unsup_loss_fake: 0.646, D_sup_loss: 0.198, D_sup_acc: 94.94 Train acc: 94.497 Test acc: 94.960 \n",
      "step: 1347 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.624, D_sup_loss: 0.203, D_sup_acc: 95.02 Train acc: 94.493 Test acc: 94.880 \n",
      "step: 1348 | Train: G_Loss: 1.118, D_unsup_loss_real: 0.668, D_unsup_loss_fake: 0.649, D_sup_loss: 0.201, D_sup_acc: 94.94 Train acc: 94.468 Test acc: 94.850 \n",
      "step: 1349 | Train: G_Loss: 1.149, D_unsup_loss_real: 0.515, D_unsup_loss_fake: 0.605, D_sup_loss: 0.203, D_sup_acc: 94.92 Train acc: 94.410 Test acc: 94.810 \n",
      "step: 1350 | Train: G_Loss: 1.168, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.610, D_sup_loss: 0.202, D_sup_acc: 94.88 Train acc: 94.307 Test acc: 94.780 \n",
      "step: 1351 | Train: G_Loss: 1.087, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.647, D_sup_loss: 0.203, D_sup_acc: 94.85 Train acc: 94.423 Test acc: 94.930 \n",
      "step: 1352 | Train: G_Loss: 1.060, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.633, D_sup_loss: 0.198, D_sup_acc: 94.99 Train acc: 94.417 Test acc: 94.890 \n",
      "step: 1353 | Train: G_Loss: 1.043, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.606, D_sup_loss: 0.197, D_sup_acc: 94.95 Train acc: 94.370 Test acc: 94.830 \n",
      "step: 1354 | Train: G_Loss: 1.133, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.562, D_sup_loss: 0.201, D_sup_acc: 94.90 Train acc: 94.322 Test acc: 94.820 \n",
      "step: 1355 | Train: G_Loss: 1.046, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.598, D_sup_loss: 0.202, D_sup_acc: 94.89 Train acc: 94.395 Test acc: 94.900 \n",
      "step: 1356 | Train: G_Loss: 1.065, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.658, D_sup_loss: 0.198, D_sup_acc: 94.96 Train acc: 94.282 Test acc: 94.680 \n",
      "step: 1357 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.686, D_unsup_loss_fake: 0.709, D_sup_loss: 0.206, D_sup_acc: 94.75 Train acc: 94.108 Test acc: 94.540 \n",
      "step: 1358 | Train: G_Loss: 1.084, D_unsup_loss_real: 0.668, D_unsup_loss_fake: 0.639, D_sup_loss: 0.214, D_sup_acc: 94.61 Train acc: 94.052 Test acc: 94.500 \n",
      "step: 1359 | Train: G_Loss: 1.012, D_unsup_loss_real: 0.650, D_unsup_loss_fake: 0.649, D_sup_loss: 0.212, D_sup_acc: 94.57 Train acc: 94.352 Test acc: 94.800 \n",
      "step: 1360 | Train: G_Loss: 1.068, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.743, D_sup_loss: 0.204, D_sup_acc: 94.87 Train acc: 94.373 Test acc: 94.850 \n",
      "step: 1361 | Train: G_Loss: 1.197, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.686, D_sup_loss: 0.204, D_sup_acc: 94.92 Train acc: 94.350 Test acc: 94.790 \n",
      "step: 1362 | Train: G_Loss: 1.038, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.615, D_sup_loss: 0.203, D_sup_acc: 94.86 Train acc: 94.375 Test acc: 94.770 \n",
      "step: 1363 | Train: G_Loss: 1.033, D_unsup_loss_real: 0.649, D_unsup_loss_fake: 0.625, D_sup_loss: 0.201, D_sup_acc: 94.84 Train acc: 94.527 Test acc: 94.890 \n",
      "step: 1364 | Train: G_Loss: 1.203, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.718, D_sup_loss: 0.197, D_sup_acc: 94.95 Train acc: 94.423 Test acc: 94.950 \n",
      "step: 1365 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.610, D_sup_loss: 0.201, D_sup_acc: 95.01 Train acc: 94.330 Test acc: 94.600 \n",
      "step: 1366 | Train: G_Loss: 1.073, D_unsup_loss_real: 0.660, D_unsup_loss_fake: 0.645, D_sup_loss: 0.205, D_sup_acc: 94.67 Train acc: 94.255 Test acc: 94.640 \n",
      "step: 1367 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.696, D_sup_loss: 0.207, D_sup_acc: 94.71 Train acc: 94.225 Test acc: 94.670 \n",
      "step: 1368 | Train: G_Loss: 1.080, D_unsup_loss_real: 0.628, D_unsup_loss_fake: 0.601, D_sup_loss: 0.207, D_sup_acc: 94.74 Train acc: 94.302 Test acc: 94.710 \n",
      "step: 1369 | Train: G_Loss: 0.988, D_unsup_loss_real: 0.655, D_unsup_loss_fake: 0.641, D_sup_loss: 0.204, D_sup_acc: 94.78 Train acc: 94.345 Test acc: 94.830 \n",
      "step: 1370 | Train: G_Loss: 1.001, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.696, D_sup_loss: 0.201, D_sup_acc: 94.90 Train acc: 94.422 Test acc: 94.850 \n",
      "step: 1371 | Train: G_Loss: 0.994, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.645, D_sup_loss: 0.201, D_sup_acc: 94.92 Train acc: 94.432 Test acc: 94.760 \n",
      "step: 1372 | Train: G_Loss: 1.028, D_unsup_loss_real: 0.665, D_unsup_loss_fake: 0.597, D_sup_loss: 0.198, D_sup_acc: 94.83 Train acc: 94.527 Test acc: 94.880 \n",
      "step: 1373 | Train: G_Loss: 1.069, D_unsup_loss_real: 0.706, D_unsup_loss_fake: 0.683, D_sup_loss: 0.194, D_sup_acc: 94.94 Train acc: 94.515 Test acc: 94.970 \n",
      "step: 1374 | Train: G_Loss: 1.021, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.696, D_sup_loss: 0.194, D_sup_acc: 95.02 Train acc: 94.462 Test acc: 94.880 \n",
      "step: 1375 | Train: G_Loss: 1.119, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.697, D_sup_loss: 0.197, D_sup_acc: 94.94 Train acc: 94.380 Test acc: 94.770 \n",
      "step: 1376 | Train: G_Loss: 1.150, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.652, D_sup_loss: 0.201, D_sup_acc: 94.84 Train acc: 94.423 Test acc: 94.820 \n",
      "step: 1377 | Train: G_Loss: 1.051, D_unsup_loss_real: 0.688, D_unsup_loss_fake: 0.605, D_sup_loss: 0.199, D_sup_acc: 94.89 Train acc: 94.473 Test acc: 94.970 \n",
      "step: 1378 | Train: G_Loss: 1.062, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.632, D_sup_loss: 0.197, D_sup_acc: 95.03 Train acc: 94.502 Test acc: 94.890 \n",
      "step: 1379 | Train: G_Loss: 1.078, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.644, D_sup_loss: 0.193, D_sup_acc: 94.95 Train acc: 94.558 Test acc: 95.030 \n",
      "step: 1380 | Train: G_Loss: 1.045, D_unsup_loss_real: 0.533, D_unsup_loss_fake: 0.583, D_sup_loss: 0.192, D_sup_acc: 95.09 Train acc: 94.427 Test acc: 94.840 \n",
      "step: 1381 | Train: G_Loss: 1.181, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.625, D_sup_loss: 0.196, D_sup_acc: 94.91 Train acc: 94.388 Test acc: 94.830 \n",
      "step: 1382 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.711, D_sup_loss: 0.199, D_sup_acc: 94.90 Train acc: 94.347 Test acc: 94.730 \n",
      "step: 1383 | Train: G_Loss: 1.167, D_unsup_loss_real: 0.684, D_unsup_loss_fake: 0.675, D_sup_loss: 0.198, D_sup_acc: 94.80 Train acc: 94.388 Test acc: 94.780 \n",
      "step: 1384 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.601, D_sup_loss: 0.199, D_sup_acc: 94.85 Train acc: 94.352 Test acc: 94.740 \n",
      "step: 1385 | Train: G_Loss: 1.013, D_unsup_loss_real: 0.650, D_unsup_loss_fake: 0.687, D_sup_loss: 0.201, D_sup_acc: 94.81 Train acc: 94.378 Test acc: 94.700 \n",
      "step: 1386 | Train: G_Loss: 1.079, D_unsup_loss_real: 0.678, D_unsup_loss_fake: 0.681, D_sup_loss: 0.202, D_sup_acc: 94.77 Train acc: 94.325 Test acc: 94.700 \n",
      "step: 1387 | Train: G_Loss: 1.052, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.670, D_sup_loss: 0.204, D_sup_acc: 94.77 Train acc: 94.375 Test acc: 94.770 \n",
      "step: 1388 | Train: G_Loss: 1.140, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.591, D_sup_loss: 0.200, D_sup_acc: 94.84 Train acc: 94.373 Test acc: 94.850 \n",
      "step: 1389 | Train: G_Loss: 1.088, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.672, D_sup_loss: 0.199, D_sup_acc: 94.92 Train acc: 94.393 Test acc: 94.890 \n",
      "step: 1390 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.532, D_unsup_loss_fake: 0.588, D_sup_loss: 0.199, D_sup_acc: 94.95 Train acc: 94.382 Test acc: 94.860 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1391 | Train: G_Loss: 1.185, D_unsup_loss_real: 0.642, D_unsup_loss_fake: 0.613, D_sup_loss: 0.200, D_sup_acc: 94.92 Train acc: 94.253 Test acc: 94.720 \n",
      "step: 1392 | Train: G_Loss: 1.156, D_unsup_loss_real: 0.545, D_unsup_loss_fake: 0.556, D_sup_loss: 0.201, D_sup_acc: 94.79 Train acc: 94.210 Test acc: 94.660 \n",
      "step: 1393 | Train: G_Loss: 1.132, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.645, D_sup_loss: 0.202, D_sup_acc: 94.73 Train acc: 94.438 Test acc: 94.920 \n",
      "step: 1394 | Train: G_Loss: 1.116, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.619, D_sup_loss: 0.197, D_sup_acc: 94.98 Train acc: 94.427 Test acc: 94.920 \n",
      "step: 1395 | Train: G_Loss: 1.120, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.589, D_sup_loss: 0.196, D_sup_acc: 94.98 Train acc: 94.448 Test acc: 95.010 \n",
      "step: 1396 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.694, D_unsup_loss_fake: 0.622, D_sup_loss: 0.195, D_sup_acc: 95.07 Train acc: 94.450 Test acc: 94.970 \n",
      "step: 1397 | Train: G_Loss: 1.047, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.661, D_sup_loss: 0.194, D_sup_acc: 95.03 Train acc: 94.412 Test acc: 94.830 \n",
      "step: 1398 | Train: G_Loss: 1.012, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.707, D_sup_loss: 0.194, D_sup_acc: 94.90 Train acc: 94.480 Test acc: 95.010 \n",
      "step: 1399 | Train: G_Loss: 1.107, D_unsup_loss_real: 0.644, D_unsup_loss_fake: 0.646, D_sup_loss: 0.194, D_sup_acc: 95.07 Train acc: 94.537 Test acc: 94.960 \n",
      "step: 1400 | Train: G_Loss: 1.104, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.603, D_sup_loss: 0.193, D_sup_acc: 95.02 Train acc: 94.515 Test acc: 94.880 \n",
      "Train Classifier Accuracy: 94.515%\n",
      "\n",
      "Test Classifier Accuracy: 94.880%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_1400.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_1400.h5\n",
      "step: 1401 | Train: G_Loss: 1.052, D_unsup_loss_real: 0.663, D_unsup_loss_fake: 0.705, D_sup_loss: 0.192, D_sup_acc: 94.94 Train acc: 94.493 Test acc: 94.970 \n",
      "step: 1402 | Train: G_Loss: 1.039, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.577, D_sup_loss: 0.196, D_sup_acc: 95.03 Train acc: 94.450 Test acc: 95.010 \n",
      "step: 1403 | Train: G_Loss: 1.198, D_unsup_loss_real: 0.631, D_unsup_loss_fake: 0.608, D_sup_loss: 0.195, D_sup_acc: 95.07 Train acc: 94.423 Test acc: 94.910 \n",
      "step: 1404 | Train: G_Loss: 1.086, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.620, D_sup_loss: 0.197, D_sup_acc: 94.97 Train acc: 94.412 Test acc: 94.900 \n",
      "step: 1405 | Train: G_Loss: 1.037, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.716, D_sup_loss: 0.191, D_sup_acc: 94.96 Train acc: 94.398 Test acc: 94.780 \n",
      "step: 1406 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.659, D_unsup_loss_fake: 0.600, D_sup_loss: 0.192, D_sup_acc: 94.85 Train acc: 94.265 Test acc: 94.640 \n",
      "step: 1407 | Train: G_Loss: 1.119, D_unsup_loss_real: 0.513, D_unsup_loss_fake: 0.678, D_sup_loss: 0.193, D_sup_acc: 94.71 Train acc: 94.158 Test acc: 94.530 \n",
      "step: 1408 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.728, D_unsup_loss_fake: 0.643, D_sup_loss: 0.195, D_sup_acc: 94.60 Train acc: 94.318 Test acc: 94.760 \n",
      "step: 1409 | Train: G_Loss: 1.199, D_unsup_loss_real: 0.495, D_unsup_loss_fake: 0.588, D_sup_loss: 0.195, D_sup_acc: 94.83 Train acc: 94.390 Test acc: 94.820 \n",
      "step: 1410 | Train: G_Loss: 1.132, D_unsup_loss_real: 0.693, D_unsup_loss_fake: 0.666, D_sup_loss: 0.190, D_sup_acc: 94.89 Train acc: 94.430 Test acc: 94.860 \n",
      "step: 1411 | Train: G_Loss: 1.166, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.616, D_sup_loss: 0.190, D_sup_acc: 94.92 Train acc: 94.410 Test acc: 94.780 \n",
      "step: 1412 | Train: G_Loss: 1.077, D_unsup_loss_real: 0.673, D_unsup_loss_fake: 0.676, D_sup_loss: 0.190, D_sup_acc: 94.85 Train acc: 94.410 Test acc: 94.830 \n",
      "step: 1413 | Train: G_Loss: 1.141, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.614, D_sup_loss: 0.193, D_sup_acc: 94.90 Train acc: 94.228 Test acc: 94.770 \n",
      "step: 1414 | Train: G_Loss: 1.060, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.568, D_sup_loss: 0.195, D_sup_acc: 94.84 Train acc: 94.332 Test acc: 94.820 \n",
      "step: 1415 | Train: G_Loss: 1.064, D_unsup_loss_real: 0.680, D_unsup_loss_fake: 0.579, D_sup_loss: 0.192, D_sup_acc: 94.89 Train acc: 94.397 Test acc: 94.840 \n",
      "step: 1416 | Train: G_Loss: 1.113, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.643, D_sup_loss: 0.194, D_sup_acc: 94.91 Train acc: 94.435 Test acc: 94.930 \n",
      "step: 1417 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.645, D_sup_loss: 0.193, D_sup_acc: 94.99 Train acc: 94.468 Test acc: 94.860 \n",
      "step: 1418 | Train: G_Loss: 1.083, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.668, D_sup_loss: 0.191, D_sup_acc: 94.92 Train acc: 94.375 Test acc: 94.850 \n",
      "step: 1419 | Train: G_Loss: 1.184, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.657, D_sup_loss: 0.193, D_sup_acc: 94.92 Train acc: 94.592 Test acc: 95.030 \n",
      "step: 1420 | Train: G_Loss: 1.009, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.615, D_sup_loss: 0.191, D_sup_acc: 95.09 Train acc: 94.578 Test acc: 94.990 \n",
      "step: 1421 | Train: G_Loss: 1.051, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.679, D_sup_loss: 0.190, D_sup_acc: 95.05 Train acc: 94.590 Test acc: 94.990 \n",
      "step: 1422 | Train: G_Loss: 1.140, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.626, D_sup_loss: 0.191, D_sup_acc: 95.05 Train acc: 94.630 Test acc: 95.100 \n",
      "step: 1423 | Train: G_Loss: 1.071, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.629, D_sup_loss: 0.189, D_sup_acc: 95.16 Train acc: 94.725 Test acc: 95.120 \n",
      "step: 1424 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.575, D_sup_loss: 0.188, D_sup_acc: 95.18 Train acc: 94.677 Test acc: 94.960 \n",
      "step: 1425 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.678, D_sup_loss: 0.189, D_sup_acc: 95.02 Train acc: 94.683 Test acc: 94.970 \n",
      "step: 1426 | Train: G_Loss: 1.065, D_unsup_loss_real: 0.676, D_unsup_loss_fake: 0.631, D_sup_loss: 0.188, D_sup_acc: 95.03 Train acc: 94.687 Test acc: 94.890 \n",
      "step: 1427 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.628, D_unsup_loss_fake: 0.665, D_sup_loss: 0.188, D_sup_acc: 94.95 Train acc: 94.725 Test acc: 95.010 \n",
      "step: 1428 | Train: G_Loss: 1.020, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.590, D_sup_loss: 0.190, D_sup_acc: 95.07 Train acc: 94.692 Test acc: 95.000 \n",
      "step: 1429 | Train: G_Loss: 1.125, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.709, D_sup_loss: 0.186, D_sup_acc: 95.06 Train acc: 94.680 Test acc: 95.040 \n",
      "step: 1430 | Train: G_Loss: 1.047, D_unsup_loss_real: 0.644, D_unsup_loss_fake: 0.646, D_sup_loss: 0.189, D_sup_acc: 95.10 Train acc: 94.762 Test acc: 95.080 \n",
      "step: 1431 | Train: G_Loss: 1.039, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.649, D_sup_loss: 0.186, D_sup_acc: 95.14 Train acc: 94.653 Test acc: 94.990 \n",
      "step: 1432 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.631, D_unsup_loss_fake: 0.682, D_sup_loss: 0.186, D_sup_acc: 95.05 Train acc: 94.560 Test acc: 94.990 \n",
      "step: 1433 | Train: G_Loss: 1.147, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.626, D_sup_loss: 0.188, D_sup_acc: 95.05 Train acc: 94.603 Test acc: 95.070 \n",
      "step: 1434 | Train: G_Loss: 1.147, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.681, D_sup_loss: 0.190, D_sup_acc: 95.13 Train acc: 94.667 Test acc: 95.160 \n",
      "step: 1435 | Train: G_Loss: 1.076, D_unsup_loss_real: 0.676, D_unsup_loss_fake: 0.680, D_sup_loss: 0.188, D_sup_acc: 95.22 Train acc: 94.643 Test acc: 95.220 \n",
      "step: 1436 | Train: G_Loss: 1.060, D_unsup_loss_real: 0.665, D_unsup_loss_fake: 0.630, D_sup_loss: 0.188, D_sup_acc: 95.28 Train acc: 94.593 Test acc: 95.020 \n",
      "step: 1437 | Train: G_Loss: 1.129, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.663, D_sup_loss: 0.189, D_sup_acc: 95.08 Train acc: 94.382 Test acc: 94.750 \n",
      "step: 1438 | Train: G_Loss: 1.049, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.677, D_sup_loss: 0.196, D_sup_acc: 94.82 Train acc: 94.428 Test acc: 94.950 \n",
      "step: 1439 | Train: G_Loss: 1.041, D_unsup_loss_real: 0.706, D_unsup_loss_fake: 0.586, D_sup_loss: 0.201, D_sup_acc: 95.01 Train acc: 94.532 Test acc: 95.100 \n",
      "step: 1440 | Train: G_Loss: 1.133, D_unsup_loss_real: 0.718, D_unsup_loss_fake: 0.706, D_sup_loss: 0.197, D_sup_acc: 95.16 Train acc: 94.472 Test acc: 94.960 \n",
      "step: 1441 | Train: G_Loss: 1.074, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.607, D_sup_loss: 0.201, D_sup_acc: 95.02 Train acc: 94.448 Test acc: 94.900 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1442 | Train: G_Loss: 1.023, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.662, D_sup_loss: 0.203, D_sup_acc: 94.96 Train acc: 94.435 Test acc: 94.870 \n",
      "step: 1443 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.717, D_unsup_loss_fake: 0.631, D_sup_loss: 0.200, D_sup_acc: 94.93 Train acc: 94.638 Test acc: 95.190 \n",
      "step: 1444 | Train: G_Loss: 1.118, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.692, D_sup_loss: 0.192, D_sup_acc: 95.25 Train acc: 94.637 Test acc: 95.170 \n",
      "step: 1445 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.650, D_unsup_loss_fake: 0.664, D_sup_loss: 0.190, D_sup_acc: 95.23 Train acc: 94.632 Test acc: 95.110 \n",
      "step: 1446 | Train: G_Loss: 1.043, D_unsup_loss_real: 0.667, D_unsup_loss_fake: 0.693, D_sup_loss: 0.193, D_sup_acc: 95.17 Train acc: 94.482 Test acc: 95.040 \n",
      "step: 1447 | Train: G_Loss: 1.024, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.604, D_sup_loss: 0.193, D_sup_acc: 95.10 Train acc: 94.547 Test acc: 94.990 \n",
      "step: 1448 | Train: G_Loss: 1.073, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.673, D_sup_loss: 0.194, D_sup_acc: 95.05 Train acc: 94.560 Test acc: 95.010 \n",
      "step: 1449 | Train: G_Loss: 1.078, D_unsup_loss_real: 0.631, D_unsup_loss_fake: 0.611, D_sup_loss: 0.194, D_sup_acc: 95.07 Train acc: 94.667 Test acc: 95.150 \n",
      "step: 1450 | Train: G_Loss: 1.083, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.632, D_sup_loss: 0.194, D_sup_acc: 95.21 Train acc: 94.610 Test acc: 95.220 \n",
      "step: 1451 | Train: G_Loss: 1.076, D_unsup_loss_real: 0.646, D_unsup_loss_fake: 0.663, D_sup_loss: 0.190, D_sup_acc: 95.28 Train acc: 94.648 Test acc: 95.170 \n",
      "step: 1452 | Train: G_Loss: 1.052, D_unsup_loss_real: 0.639, D_unsup_loss_fake: 0.614, D_sup_loss: 0.192, D_sup_acc: 95.23 Train acc: 94.655 Test acc: 95.160 \n",
      "step: 1453 | Train: G_Loss: 1.107, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.676, D_sup_loss: 0.191, D_sup_acc: 95.22 Train acc: 94.625 Test acc: 95.040 \n",
      "step: 1454 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.636, D_sup_loss: 0.191, D_sup_acc: 95.10 Train acc: 94.643 Test acc: 95.120 \n",
      "step: 1455 | Train: G_Loss: 1.034, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.616, D_sup_loss: 0.192, D_sup_acc: 95.18 Train acc: 94.593 Test acc: 95.080 \n",
      "step: 1456 | Train: G_Loss: 1.175, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.616, D_sup_loss: 0.192, D_sup_acc: 95.14 Train acc: 94.557 Test acc: 95.040 \n",
      "step: 1457 | Train: G_Loss: 1.170, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.648, D_sup_loss: 0.195, D_sup_acc: 95.10 Train acc: 94.527 Test acc: 94.930 \n",
      "step: 1458 | Train: G_Loss: 1.117, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.690, D_sup_loss: 0.195, D_sup_acc: 94.99 Train acc: 94.583 Test acc: 95.070 \n",
      "step: 1459 | Train: G_Loss: 1.017, D_unsup_loss_real: 0.681, D_unsup_loss_fake: 0.661, D_sup_loss: 0.197, D_sup_acc: 95.13 Train acc: 94.658 Test acc: 95.090 \n",
      "step: 1460 | Train: G_Loss: 1.037, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.603, D_sup_loss: 0.193, D_sup_acc: 95.15 Train acc: 94.705 Test acc: 95.090 \n",
      "step: 1461 | Train: G_Loss: 1.030, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.656, D_sup_loss: 0.193, D_sup_acc: 95.15 Train acc: 94.748 Test acc: 95.130 \n",
      "step: 1462 | Train: G_Loss: 1.100, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.645, D_sup_loss: 0.193, D_sup_acc: 95.19 Train acc: 94.698 Test acc: 95.070 \n",
      "step: 1463 | Train: G_Loss: 1.035, D_unsup_loss_real: 0.683, D_unsup_loss_fake: 0.659, D_sup_loss: 0.189, D_sup_acc: 95.13 Train acc: 94.703 Test acc: 95.150 \n",
      "step: 1464 | Train: G_Loss: 1.083, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.717, D_sup_loss: 0.189, D_sup_acc: 95.21 Train acc: 94.705 Test acc: 95.160 \n",
      "step: 1465 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.679, D_sup_loss: 0.191, D_sup_acc: 95.22 Train acc: 94.610 Test acc: 95.000 \n",
      "step: 1466 | Train: G_Loss: 1.135, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.701, D_sup_loss: 0.194, D_sup_acc: 95.06 Train acc: 94.693 Test acc: 95.180 \n",
      "step: 1467 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.571, D_sup_loss: 0.193, D_sup_acc: 95.24 Train acc: 94.655 Test acc: 95.120 \n",
      "step: 1468 | Train: G_Loss: 1.258, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.663, D_sup_loss: 0.192, D_sup_acc: 95.18 Train acc: 94.597 Test acc: 95.020 \n",
      "step: 1469 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.613, D_sup_loss: 0.194, D_sup_acc: 95.08 Train acc: 94.663 Test acc: 95.200 \n",
      "step: 1470 | Train: G_Loss: 1.009, D_unsup_loss_real: 0.697, D_unsup_loss_fake: 0.595, D_sup_loss: 0.191, D_sup_acc: 95.26 Train acc: 94.663 Test acc: 95.130 \n",
      "step: 1471 | Train: G_Loss: 1.039, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.642, D_sup_loss: 0.191, D_sup_acc: 95.19 Train acc: 94.653 Test acc: 95.120 \n",
      "step: 1472 | Train: G_Loss: 1.143, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.656, D_sup_loss: 0.191, D_sup_acc: 95.18 Train acc: 94.653 Test acc: 95.050 \n",
      "step: 1473 | Train: G_Loss: 1.046, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.606, D_sup_loss: 0.190, D_sup_acc: 95.11 Train acc: 94.612 Test acc: 95.060 \n",
      "step: 1474 | Train: G_Loss: 1.132, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.595, D_sup_loss: 0.190, D_sup_acc: 95.12 Train acc: 94.668 Test acc: 95.160 \n",
      "step: 1475 | Train: G_Loss: 1.033, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.745, D_sup_loss: 0.189, D_sup_acc: 95.22 Train acc: 94.715 Test acc: 95.250 \n",
      "step: 1476 | Train: G_Loss: 1.090, D_unsup_loss_real: 0.660, D_unsup_loss_fake: 0.798, D_sup_loss: 0.189, D_sup_acc: 95.31 Train acc: 94.673 Test acc: 95.220 \n",
      "step: 1477 | Train: G_Loss: 1.067, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.577, D_sup_loss: 0.194, D_sup_acc: 95.28 Train acc: 94.652 Test acc: 95.070 \n",
      "step: 1478 | Train: G_Loss: 1.109, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.611, D_sup_loss: 0.193, D_sup_acc: 95.13 Train acc: 94.673 Test acc: 95.030 \n",
      "step: 1479 | Train: G_Loss: 1.135, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.668, D_sup_loss: 0.193, D_sup_acc: 95.09 Train acc: 94.728 Test acc: 95.190 \n",
      "step: 1480 | Train: G_Loss: 1.177, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.633, D_sup_loss: 0.192, D_sup_acc: 95.25 Train acc: 94.627 Test acc: 94.950 \n",
      "step: 1481 | Train: G_Loss: 1.169, D_unsup_loss_real: 0.648, D_unsup_loss_fake: 0.599, D_sup_loss: 0.193, D_sup_acc: 95.01 Train acc: 94.752 Test acc: 95.120 \n",
      "step: 1482 | Train: G_Loss: 1.089, D_unsup_loss_real: 0.693, D_unsup_loss_fake: 0.610, D_sup_loss: 0.191, D_sup_acc: 95.18 Train acc: 94.780 Test acc: 95.140 \n",
      "step: 1483 | Train: G_Loss: 1.031, D_unsup_loss_real: 0.720, D_unsup_loss_fake: 0.704, D_sup_loss: 0.193, D_sup_acc: 95.20 Train acc: 94.760 Test acc: 95.050 \n",
      "step: 1484 | Train: G_Loss: 1.069, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.679, D_sup_loss: 0.197, D_sup_acc: 95.11 Train acc: 94.717 Test acc: 95.050 \n",
      "step: 1485 | Train: G_Loss: 1.032, D_unsup_loss_real: 0.647, D_unsup_loss_fake: 0.678, D_sup_loss: 0.197, D_sup_acc: 95.11 Train acc: 94.762 Test acc: 95.130 \n",
      "step: 1486 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.677, D_unsup_loss_fake: 0.668, D_sup_loss: 0.196, D_sup_acc: 95.19 Train acc: 94.637 Test acc: 95.030 \n",
      "step: 1487 | Train: G_Loss: 1.057, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.647, D_sup_loss: 0.200, D_sup_acc: 95.09 Train acc: 94.717 Test acc: 95.100 \n",
      "step: 1488 | Train: G_Loss: 1.051, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.686, D_sup_loss: 0.200, D_sup_acc: 95.16 Train acc: 94.683 Test acc: 95.170 \n",
      "step: 1489 | Train: G_Loss: 1.065, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.691, D_sup_loss: 0.197, D_sup_acc: 95.23 Train acc: 94.538 Test acc: 94.850 \n",
      "step: 1490 | Train: G_Loss: 1.090, D_unsup_loss_real: 0.676, D_unsup_loss_fake: 0.693, D_sup_loss: 0.200, D_sup_acc: 94.92 Train acc: 94.580 Test acc: 94.990 \n",
      "step: 1491 | Train: G_Loss: 1.166, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.602, D_sup_loss: 0.197, D_sup_acc: 95.05 Train acc: 94.558 Test acc: 94.970 \n",
      "step: 1492 | Train: G_Loss: 1.170, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.616, D_sup_loss: 0.197, D_sup_acc: 95.03 Train acc: 94.500 Test acc: 94.880 \n",
      "step: 1493 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.691, D_unsup_loss_fake: 0.569, D_sup_loss: 0.197, D_sup_acc: 94.94 Train acc: 94.508 Test acc: 94.830 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1494 | Train: G_Loss: 1.088, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.638, D_sup_loss: 0.196, D_sup_acc: 94.90 Train acc: 94.543 Test acc: 95.000 \n",
      "step: 1495 | Train: G_Loss: 0.994, D_unsup_loss_real: 0.664, D_unsup_loss_fake: 0.634, D_sup_loss: 0.193, D_sup_acc: 95.06 Train acc: 94.565 Test acc: 95.110 \n",
      "step: 1496 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.679, D_sup_loss: 0.192, D_sup_acc: 95.17 Train acc: 94.475 Test acc: 94.980 \n",
      "step: 1497 | Train: G_Loss: 1.060, D_unsup_loss_real: 0.699, D_unsup_loss_fake: 0.703, D_sup_loss: 0.195, D_sup_acc: 95.04 Train acc: 94.470 Test acc: 94.950 \n",
      "step: 1498 | Train: G_Loss: 1.132, D_unsup_loss_real: 0.631, D_unsup_loss_fake: 0.641, D_sup_loss: 0.199, D_sup_acc: 95.01 Train acc: 94.570 Test acc: 95.110 \n",
      "step: 1499 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.668, D_unsup_loss_fake: 0.641, D_sup_loss: 0.195, D_sup_acc: 95.17 Train acc: 94.628 Test acc: 94.960 \n",
      "step: 1500 | Train: G_Loss: 1.069, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.581, D_sup_loss: 0.194, D_sup_acc: 95.02 Train acc: 94.693 Test acc: 95.150 \n",
      "Train Classifier Accuracy: 94.693%\n",
      "\n",
      "Test Classifier Accuracy: 95.150%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_1500.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_1500.h5\n",
      "step: 1501 | Train: G_Loss: 1.075, D_unsup_loss_real: 0.653, D_unsup_loss_fake: 0.675, D_sup_loss: 0.190, D_sup_acc: 95.21 Train acc: 94.690 Test acc: 95.060 \n",
      "step: 1502 | Train: G_Loss: 1.056, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.656, D_sup_loss: 0.194, D_sup_acc: 95.12 Train acc: 94.672 Test acc: 95.000 \n",
      "step: 1503 | Train: G_Loss: 1.092, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.655, D_sup_loss: 0.194, D_sup_acc: 95.06 Train acc: 94.605 Test acc: 94.960 \n",
      "step: 1504 | Train: G_Loss: 1.045, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.680, D_sup_loss: 0.196, D_sup_acc: 95.02 Train acc: 94.535 Test acc: 94.890 \n",
      "step: 1505 | Train: G_Loss: 1.127, D_unsup_loss_real: 0.691, D_unsup_loss_fake: 0.661, D_sup_loss: 0.198, D_sup_acc: 94.95 Train acc: 94.532 Test acc: 94.870 \n",
      "step: 1506 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.640, D_sup_loss: 0.197, D_sup_acc: 94.93 Train acc: 94.563 Test acc: 94.880 \n",
      "step: 1507 | Train: G_Loss: 1.084, D_unsup_loss_real: 0.665, D_unsup_loss_fake: 0.659, D_sup_loss: 0.195, D_sup_acc: 94.94 Train acc: 94.617 Test acc: 94.950 \n",
      "step: 1508 | Train: G_Loss: 1.038, D_unsup_loss_real: 0.672, D_unsup_loss_fake: 0.649, D_sup_loss: 0.195, D_sup_acc: 95.01 Train acc: 94.635 Test acc: 95.130 \n",
      "step: 1509 | Train: G_Loss: 1.060, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.642, D_sup_loss: 0.194, D_sup_acc: 95.19 Train acc: 94.627 Test acc: 94.970 \n",
      "step: 1510 | Train: G_Loss: 1.177, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.652, D_sup_loss: 0.193, D_sup_acc: 95.03 Train acc: 94.583 Test acc: 94.910 \n",
      "step: 1511 | Train: G_Loss: 1.069, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.604, D_sup_loss: 0.194, D_sup_acc: 94.97 Train acc: 94.692 Test acc: 95.060 \n",
      "step: 1512 | Train: G_Loss: 1.130, D_unsup_loss_real: 0.646, D_unsup_loss_fake: 0.569, D_sup_loss: 0.187, D_sup_acc: 95.12 Train acc: 94.575 Test acc: 94.910 \n",
      "step: 1513 | Train: G_Loss: 1.024, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.670, D_sup_loss: 0.189, D_sup_acc: 94.97 Train acc: 94.647 Test acc: 94.940 \n",
      "step: 1514 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.676, D_sup_loss: 0.188, D_sup_acc: 95.00 Train acc: 94.612 Test acc: 95.070 \n",
      "step: 1515 | Train: G_Loss: 1.173, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.633, D_sup_loss: 0.189, D_sup_acc: 95.13 Train acc: 94.553 Test acc: 94.880 \n",
      "step: 1516 | Train: G_Loss: 1.101, D_unsup_loss_real: 0.678, D_unsup_loss_fake: 0.653, D_sup_loss: 0.193, D_sup_acc: 94.94 Train acc: 94.628 Test acc: 95.120 \n",
      "step: 1517 | Train: G_Loss: 1.099, D_unsup_loss_real: 0.533, D_unsup_loss_fake: 0.621, D_sup_loss: 0.191, D_sup_acc: 95.18 Train acc: 94.560 Test acc: 94.950 \n",
      "step: 1518 | Train: G_Loss: 1.115, D_unsup_loss_real: 0.697, D_unsup_loss_fake: 0.684, D_sup_loss: 0.189, D_sup_acc: 95.01 Train acc: 94.608 Test acc: 95.020 \n",
      "step: 1519 | Train: G_Loss: 1.053, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.612, D_sup_loss: 0.190, D_sup_acc: 95.08 Train acc: 94.718 Test acc: 95.150 \n",
      "step: 1520 | Train: G_Loss: 1.055, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.549, D_sup_loss: 0.184, D_sup_acc: 95.21 Train acc: 94.757 Test acc: 95.200 \n",
      "step: 1521 | Train: G_Loss: 1.091, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.625, D_sup_loss: 0.183, D_sup_acc: 95.26 Train acc: 94.827 Test acc: 95.360 \n",
      "step: 1522 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.659, D_unsup_loss_fake: 0.690, D_sup_loss: 0.181, D_sup_acc: 95.42 Train acc: 94.868 Test acc: 95.340 \n",
      "step: 1523 | Train: G_Loss: 1.074, D_unsup_loss_real: 0.669, D_unsup_loss_fake: 0.605, D_sup_loss: 0.180, D_sup_acc: 95.40 Train acc: 94.763 Test acc: 95.240 \n",
      "step: 1524 | Train: G_Loss: 1.110, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.673, D_sup_loss: 0.185, D_sup_acc: 95.30 Train acc: 94.862 Test acc: 95.390 \n",
      "step: 1525 | Train: G_Loss: 1.087, D_unsup_loss_real: 0.652, D_unsup_loss_fake: 0.671, D_sup_loss: 0.183, D_sup_acc: 95.45 Train acc: 94.772 Test acc: 95.190 \n",
      "step: 1526 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.616, D_sup_loss: 0.186, D_sup_acc: 95.25 Train acc: 94.827 Test acc: 95.090 \n",
      "step: 1527 | Train: G_Loss: 1.034, D_unsup_loss_real: 0.649, D_unsup_loss_fake: 0.598, D_sup_loss: 0.187, D_sup_acc: 95.15 Train acc: 94.902 Test acc: 95.390 \n",
      "step: 1528 | Train: G_Loss: 1.173, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.677, D_sup_loss: 0.185, D_sup_acc: 95.45 Train acc: 94.897 Test acc: 95.450 \n",
      "step: 1529 | Train: G_Loss: 1.096, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.692, D_sup_loss: 0.183, D_sup_acc: 95.51 Train acc: 94.823 Test acc: 95.390 \n",
      "step: 1530 | Train: G_Loss: 1.026, D_unsup_loss_real: 0.670, D_unsup_loss_fake: 0.726, D_sup_loss: 0.183, D_sup_acc: 95.45 Train acc: 94.840 Test acc: 95.380 \n",
      "step: 1531 | Train: G_Loss: 1.120, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.729, D_sup_loss: 0.184, D_sup_acc: 95.44 Train acc: 94.862 Test acc: 95.390 \n",
      "step: 1532 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.676, D_unsup_loss_fake: 0.584, D_sup_loss: 0.184, D_sup_acc: 95.45 Train acc: 94.868 Test acc: 95.410 \n",
      "step: 1533 | Train: G_Loss: 1.092, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.644, D_sup_loss: 0.181, D_sup_acc: 95.47 Train acc: 94.890 Test acc: 95.480 \n",
      "step: 1534 | Train: G_Loss: 1.037, D_unsup_loss_real: 0.708, D_unsup_loss_fake: 0.638, D_sup_loss: 0.183, D_sup_acc: 95.54 Train acc: 94.927 Test acc: 95.470 \n",
      "step: 1535 | Train: G_Loss: 1.104, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.633, D_sup_loss: 0.182, D_sup_acc: 95.53 Train acc: 94.842 Test acc: 95.390 \n",
      "step: 1536 | Train: G_Loss: 1.074, D_unsup_loss_real: 0.675, D_unsup_loss_fake: 0.684, D_sup_loss: 0.183, D_sup_acc: 95.45 Train acc: 94.845 Test acc: 95.310 \n",
      "step: 1537 | Train: G_Loss: 1.015, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.693, D_sup_loss: 0.187, D_sup_acc: 95.37 Train acc: 94.992 Test acc: 95.480 \n",
      "step: 1538 | Train: G_Loss: 1.061, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.675, D_sup_loss: 0.182, D_sup_acc: 95.54 Train acc: 94.970 Test acc: 95.520 \n",
      "step: 1539 | Train: G_Loss: 1.042, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.633, D_sup_loss: 0.183, D_sup_acc: 95.58 Train acc: 94.907 Test acc: 95.360 \n",
      "step: 1540 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.633, D_sup_loss: 0.181, D_sup_acc: 95.42 Train acc: 94.903 Test acc: 95.370 \n",
      "step: 1541 | Train: G_Loss: 0.994, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.557, D_sup_loss: 0.180, D_sup_acc: 95.43 Train acc: 94.940 Test acc: 95.330 \n",
      "step: 1542 | Train: G_Loss: 1.034, D_unsup_loss_real: 0.654, D_unsup_loss_fake: 0.663, D_sup_loss: 0.181, D_sup_acc: 95.39 Train acc: 94.968 Test acc: 95.510 \n",
      "step: 1543 | Train: G_Loss: 1.209, D_unsup_loss_real: 0.535, D_unsup_loss_fake: 0.683, D_sup_loss: 0.182, D_sup_acc: 95.57 Train acc: 94.925 Test acc: 95.400 \n",
      "step: 1544 | Train: G_Loss: 1.069, D_unsup_loss_real: 0.642, D_unsup_loss_fake: 0.580, D_sup_loss: 0.182, D_sup_acc: 95.46 Train acc: 94.955 Test acc: 95.420 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1545 | Train: G_Loss: 1.089, D_unsup_loss_real: 0.519, D_unsup_loss_fake: 0.702, D_sup_loss: 0.180, D_sup_acc: 95.48 Train acc: 94.870 Test acc: 95.430 \n",
      "step: 1546 | Train: G_Loss: 1.076, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.658, D_sup_loss: 0.183, D_sup_acc: 95.49 Train acc: 94.897 Test acc: 95.430 \n",
      "step: 1547 | Train: G_Loss: 1.201, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.598, D_sup_loss: 0.178, D_sup_acc: 95.49 Train acc: 94.882 Test acc: 95.450 \n",
      "step: 1548 | Train: G_Loss: 1.197, D_unsup_loss_real: 0.761, D_unsup_loss_fake: 0.646, D_sup_loss: 0.178, D_sup_acc: 95.51 Train acc: 94.913 Test acc: 95.460 \n",
      "step: 1549 | Train: G_Loss: 1.072, D_unsup_loss_real: 0.704, D_unsup_loss_fake: 0.663, D_sup_loss: 0.182, D_sup_acc: 95.52 Train acc: 94.853 Test acc: 95.370 \n",
      "step: 1550 | Train: G_Loss: 1.110, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.592, D_sup_loss: 0.181, D_sup_acc: 95.43 Train acc: 94.777 Test acc: 95.260 \n",
      "step: 1551 | Train: G_Loss: 1.129, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.632, D_sup_loss: 0.179, D_sup_acc: 95.32 Train acc: 94.753 Test acc: 95.220 \n",
      "step: 1552 | Train: G_Loss: 1.106, D_unsup_loss_real: 0.672, D_unsup_loss_fake: 0.612, D_sup_loss: 0.180, D_sup_acc: 95.28 Train acc: 94.748 Test acc: 95.170 \n",
      "step: 1553 | Train: G_Loss: 1.045, D_unsup_loss_real: 0.696, D_unsup_loss_fake: 0.672, D_sup_loss: 0.180, D_sup_acc: 95.23 Train acc: 94.827 Test acc: 95.430 \n",
      "step: 1554 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.690, D_sup_loss: 0.180, D_sup_acc: 95.49 Train acc: 94.818 Test acc: 95.300 \n",
      "step: 1555 | Train: G_Loss: 1.062, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.638, D_sup_loss: 0.183, D_sup_acc: 95.36 Train acc: 94.767 Test acc: 95.330 \n",
      "step: 1556 | Train: G_Loss: 1.077, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.622, D_sup_loss: 0.183, D_sup_acc: 95.39 Train acc: 94.750 Test acc: 95.280 \n",
      "step: 1557 | Train: G_Loss: 1.117, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.598, D_sup_loss: 0.182, D_sup_acc: 95.34 Train acc: 94.767 Test acc: 95.270 \n",
      "step: 1558 | Train: G_Loss: 1.072, D_unsup_loss_real: 0.691, D_unsup_loss_fake: 0.665, D_sup_loss: 0.181, D_sup_acc: 95.33 Train acc: 94.760 Test acc: 95.190 \n",
      "step: 1559 | Train: G_Loss: 1.072, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.659, D_sup_loss: 0.184, D_sup_acc: 95.25 Train acc: 94.680 Test acc: 95.100 \n",
      "step: 1560 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.670, D_sup_loss: 0.188, D_sup_acc: 95.16 Train acc: 94.700 Test acc: 95.130 \n",
      "step: 1561 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.646, D_unsup_loss_fake: 0.638, D_sup_loss: 0.186, D_sup_acc: 95.19 Train acc: 94.800 Test acc: 95.150 \n",
      "step: 1562 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.655, D_unsup_loss_fake: 0.649, D_sup_loss: 0.185, D_sup_acc: 95.21 Train acc: 94.840 Test acc: 95.220 \n",
      "step: 1563 | Train: G_Loss: 1.055, D_unsup_loss_real: 0.668, D_unsup_loss_fake: 0.593, D_sup_loss: 0.185, D_sup_acc: 95.28 Train acc: 94.800 Test acc: 95.120 \n",
      "step: 1564 | Train: G_Loss: 1.122, D_unsup_loss_real: 0.542, D_unsup_loss_fake: 0.683, D_sup_loss: 0.186, D_sup_acc: 95.18 Train acc: 94.897 Test acc: 95.330 \n",
      "step: 1565 | Train: G_Loss: 0.992, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.557, D_sup_loss: 0.185, D_sup_acc: 95.39 Train acc: 94.955 Test acc: 95.250 \n",
      "step: 1566 | Train: G_Loss: 1.139, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.645, D_sup_loss: 0.183, D_sup_acc: 95.31 Train acc: 94.943 Test acc: 95.280 \n",
      "step: 1567 | Train: G_Loss: 1.046, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.563, D_sup_loss: 0.181, D_sup_acc: 95.34 Train acc: 94.968 Test acc: 95.340 \n",
      "step: 1568 | Train: G_Loss: 1.094, D_unsup_loss_real: 0.698, D_unsup_loss_fake: 0.635, D_sup_loss: 0.181, D_sup_acc: 95.40 Train acc: 94.915 Test acc: 95.220 \n",
      "step: 1569 | Train: G_Loss: 1.111, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.669, D_sup_loss: 0.182, D_sup_acc: 95.28 Train acc: 94.932 Test acc: 95.210 \n",
      "step: 1570 | Train: G_Loss: 1.119, D_unsup_loss_real: 0.646, D_unsup_loss_fake: 0.632, D_sup_loss: 0.181, D_sup_acc: 95.27 Train acc: 94.890 Test acc: 95.260 \n",
      "step: 1571 | Train: G_Loss: 1.085, D_unsup_loss_real: 0.536, D_unsup_loss_fake: 0.652, D_sup_loss: 0.182, D_sup_acc: 95.32 Train acc: 94.798 Test acc: 95.120 \n",
      "step: 1572 | Train: G_Loss: 1.117, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.678, D_sup_loss: 0.183, D_sup_acc: 95.18 Train acc: 94.780 Test acc: 95.160 \n",
      "step: 1573 | Train: G_Loss: 1.226, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.671, D_sup_loss: 0.185, D_sup_acc: 95.22 Train acc: 94.802 Test acc: 95.290 \n",
      "step: 1574 | Train: G_Loss: 1.060, D_unsup_loss_real: 0.655, D_unsup_loss_fake: 0.599, D_sup_loss: 0.183, D_sup_acc: 95.35 Train acc: 94.838 Test acc: 95.420 \n",
      "step: 1575 | Train: G_Loss: 1.120, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.646, D_sup_loss: 0.184, D_sup_acc: 95.48 Train acc: 94.862 Test acc: 95.440 \n",
      "step: 1576 | Train: G_Loss: 1.044, D_unsup_loss_real: 0.663, D_unsup_loss_fake: 0.632, D_sup_loss: 0.188, D_sup_acc: 95.50 Train acc: 94.813 Test acc: 95.340 \n",
      "step: 1577 | Train: G_Loss: 1.099, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.604, D_sup_loss: 0.188, D_sup_acc: 95.40 Train acc: 94.840 Test acc: 95.310 \n",
      "step: 1578 | Train: G_Loss: 0.994, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.659, D_sup_loss: 0.184, D_sup_acc: 95.37 Train acc: 94.795 Test acc: 95.270 \n",
      "step: 1579 | Train: G_Loss: 1.149, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.666, D_sup_loss: 0.185, D_sup_acc: 95.33 Train acc: 94.807 Test acc: 95.230 \n",
      "step: 1580 | Train: G_Loss: 1.140, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.659, D_sup_loss: 0.185, D_sup_acc: 95.29 Train acc: 94.843 Test acc: 95.320 \n",
      "step: 1581 | Train: G_Loss: 1.088, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.620, D_sup_loss: 0.184, D_sup_acc: 95.38 Train acc: 94.853 Test acc: 95.350 \n",
      "step: 1582 | Train: G_Loss: 1.050, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.627, D_sup_loss: 0.182, D_sup_acc: 95.41 Train acc: 94.840 Test acc: 95.360 \n",
      "step: 1583 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.576, D_sup_loss: 0.183, D_sup_acc: 95.42 Train acc: 94.840 Test acc: 95.300 \n",
      "step: 1584 | Train: G_Loss: 1.090, D_unsup_loss_real: 0.701, D_unsup_loss_fake: 0.648, D_sup_loss: 0.181, D_sup_acc: 95.36 Train acc: 94.790 Test acc: 95.210 \n",
      "step: 1585 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.672, D_sup_loss: 0.182, D_sup_acc: 95.27 Train acc: 94.800 Test acc: 95.350 \n",
      "step: 1586 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.651, D_unsup_loss_fake: 0.691, D_sup_loss: 0.184, D_sup_acc: 95.41 Train acc: 94.778 Test acc: 95.220 \n",
      "step: 1587 | Train: G_Loss: 1.171, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.620, D_sup_loss: 0.186, D_sup_acc: 95.28 Train acc: 94.692 Test acc: 95.280 \n",
      "step: 1588 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.637, D_sup_loss: 0.192, D_sup_acc: 95.34 Train acc: 94.800 Test acc: 95.400 \n",
      "step: 1589 | Train: G_Loss: 1.133, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.650, D_sup_loss: 0.187, D_sup_acc: 95.46 Train acc: 94.763 Test acc: 95.150 \n",
      "step: 1590 | Train: G_Loss: 1.073, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.627, D_sup_loss: 0.190, D_sup_acc: 95.21 Train acc: 94.655 Test acc: 95.030 \n",
      "step: 1591 | Train: G_Loss: 1.172, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.657, D_sup_loss: 0.188, D_sup_acc: 95.09 Train acc: 94.760 Test acc: 95.160 \n",
      "step: 1592 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.591, D_sup_loss: 0.186, D_sup_acc: 95.22 Train acc: 94.818 Test acc: 95.220 \n",
      "step: 1593 | Train: G_Loss: 1.132, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.724, D_sup_loss: 0.185, D_sup_acc: 95.28 Train acc: 94.783 Test acc: 95.170 \n",
      "step: 1594 | Train: G_Loss: 1.220, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.707, D_sup_loss: 0.188, D_sup_acc: 95.23 Train acc: 94.765 Test acc: 95.210 \n",
      "step: 1595 | Train: G_Loss: 1.214, D_unsup_loss_real: 0.665, D_unsup_loss_fake: 0.578, D_sup_loss: 0.188, D_sup_acc: 95.27 Train acc: 94.733 Test acc: 95.110 \n",
      "step: 1596 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.659, D_unsup_loss_fake: 0.617, D_sup_loss: 0.188, D_sup_acc: 95.17 Train acc: 94.712 Test acc: 95.060 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1597 | Train: G_Loss: 1.143, D_unsup_loss_real: 0.666, D_unsup_loss_fake: 0.686, D_sup_loss: 0.191, D_sup_acc: 95.12 Train acc: 94.552 Test acc: 94.930 \n",
      "step: 1598 | Train: G_Loss: 1.094, D_unsup_loss_real: 0.628, D_unsup_loss_fake: 0.562, D_sup_loss: 0.197, D_sup_acc: 94.99 Train acc: 94.725 Test acc: 95.140 \n",
      "step: 1599 | Train: G_Loss: 1.070, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.568, D_sup_loss: 0.189, D_sup_acc: 95.20 Train acc: 94.810 Test acc: 95.250 \n",
      "step: 1600 | Train: G_Loss: 1.048, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.671, D_sup_loss: 0.183, D_sup_acc: 95.31 Train acc: 94.743 Test acc: 95.220 \n",
      "Train Classifier Accuracy: 94.743%\n",
      "\n",
      "Test Classifier Accuracy: 95.220%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_1600.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_1600.h5\n",
      "step: 1601 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.501, D_unsup_loss_fake: 0.651, D_sup_loss: 0.184, D_sup_acc: 95.28 Train acc: 94.735 Test acc: 95.250 \n",
      "step: 1602 | Train: G_Loss: 1.010, D_unsup_loss_real: 0.672, D_unsup_loss_fake: 0.634, D_sup_loss: 0.186, D_sup_acc: 95.31 Train acc: 94.808 Test acc: 95.270 \n",
      "step: 1603 | Train: G_Loss: 1.162, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.730, D_sup_loss: 0.185, D_sup_acc: 95.33 Train acc: 94.832 Test acc: 95.230 \n",
      "step: 1604 | Train: G_Loss: 1.113, D_unsup_loss_real: 0.665, D_unsup_loss_fake: 0.694, D_sup_loss: 0.185, D_sup_acc: 95.29 Train acc: 94.768 Test acc: 95.090 \n",
      "step: 1605 | Train: G_Loss: 1.202, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.742, D_sup_loss: 0.189, D_sup_acc: 95.15 Train acc: 94.743 Test acc: 95.070 \n",
      "step: 1606 | Train: G_Loss: 1.189, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.595, D_sup_loss: 0.189, D_sup_acc: 95.13 Train acc: 94.802 Test acc: 95.200 \n",
      "step: 1607 | Train: G_Loss: 1.106, D_unsup_loss_real: 0.734, D_unsup_loss_fake: 0.661, D_sup_loss: 0.185, D_sup_acc: 95.26 Train acc: 94.800 Test acc: 95.350 \n",
      "step: 1608 | Train: G_Loss: 1.150, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.669, D_sup_loss: 0.183, D_sup_acc: 95.41 Train acc: 94.873 Test acc: 95.350 \n",
      "step: 1609 | Train: G_Loss: 1.117, D_unsup_loss_real: 0.674, D_unsup_loss_fake: 0.665, D_sup_loss: 0.183, D_sup_acc: 95.41 Train acc: 94.892 Test acc: 95.340 \n",
      "step: 1610 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.742, D_unsup_loss_fake: 0.637, D_sup_loss: 0.186, D_sup_acc: 95.40 Train acc: 94.872 Test acc: 95.330 \n",
      "step: 1611 | Train: G_Loss: 1.012, D_unsup_loss_real: 0.646, D_unsup_loss_fake: 0.655, D_sup_loss: 0.188, D_sup_acc: 95.39 Train acc: 94.855 Test acc: 95.240 \n",
      "step: 1612 | Train: G_Loss: 1.092, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.626, D_sup_loss: 0.191, D_sup_acc: 95.30 Train acc: 94.852 Test acc: 95.260 \n",
      "step: 1613 | Train: G_Loss: 1.090, D_unsup_loss_real: 0.658, D_unsup_loss_fake: 0.606, D_sup_loss: 0.191, D_sup_acc: 95.32 Train acc: 94.875 Test acc: 95.260 \n",
      "step: 1614 | Train: G_Loss: 1.048, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.557, D_sup_loss: 0.189, D_sup_acc: 95.32 Train acc: 94.850 Test acc: 95.260 \n",
      "step: 1615 | Train: G_Loss: 1.088, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.639, D_sup_loss: 0.187, D_sup_acc: 95.32 Train acc: 94.817 Test acc: 95.270 \n",
      "step: 1616 | Train: G_Loss: 1.132, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.697, D_sup_loss: 0.190, D_sup_acc: 95.33 Train acc: 94.868 Test acc: 95.330 \n",
      "step: 1617 | Train: G_Loss: 1.065, D_unsup_loss_real: 0.550, D_unsup_loss_fake: 0.676, D_sup_loss: 0.188, D_sup_acc: 95.39 Train acc: 94.860 Test acc: 95.350 \n",
      "step: 1618 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.617, D_sup_loss: 0.187, D_sup_acc: 95.41 Train acc: 94.848 Test acc: 95.380 \n",
      "step: 1619 | Train: G_Loss: 1.048, D_unsup_loss_real: 0.646, D_unsup_loss_fake: 0.600, D_sup_loss: 0.187, D_sup_acc: 95.44 Train acc: 94.943 Test acc: 95.340 \n",
      "step: 1620 | Train: G_Loss: 1.044, D_unsup_loss_real: 0.538, D_unsup_loss_fake: 0.646, D_sup_loss: 0.183, D_sup_acc: 95.40 Train acc: 94.777 Test acc: 95.330 \n",
      "step: 1621 | Train: G_Loss: 1.170, D_unsup_loss_real: 0.652, D_unsup_loss_fake: 0.673, D_sup_loss: 0.184, D_sup_acc: 95.39 Train acc: 94.862 Test acc: 95.320 \n",
      "step: 1622 | Train: G_Loss: 1.118, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.661, D_sup_loss: 0.184, D_sup_acc: 95.38 Train acc: 94.922 Test acc: 95.330 \n",
      "step: 1623 | Train: G_Loss: 1.056, D_unsup_loss_real: 0.673, D_unsup_loss_fake: 0.670, D_sup_loss: 0.185, D_sup_acc: 95.39 Train acc: 94.895 Test acc: 95.230 \n",
      "step: 1624 | Train: G_Loss: 1.087, D_unsup_loss_real: 0.683, D_unsup_loss_fake: 0.636, D_sup_loss: 0.188, D_sup_acc: 95.29 Train acc: 94.863 Test acc: 95.290 \n",
      "step: 1625 | Train: G_Loss: 1.034, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.635, D_sup_loss: 0.187, D_sup_acc: 95.35 Train acc: 94.872 Test acc: 95.270 \n",
      "step: 1626 | Train: G_Loss: 1.137, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.699, D_sup_loss: 0.186, D_sup_acc: 95.33 Train acc: 94.863 Test acc: 95.290 \n",
      "step: 1627 | Train: G_Loss: 1.048, D_unsup_loss_real: 0.651, D_unsup_loss_fake: 0.621, D_sup_loss: 0.185, D_sup_acc: 95.35 Train acc: 94.840 Test acc: 95.240 \n",
      "step: 1628 | Train: G_Loss: 1.162, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.630, D_sup_loss: 0.186, D_sup_acc: 95.30 Train acc: 94.837 Test acc: 95.180 \n",
      "step: 1629 | Train: G_Loss: 1.065, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.638, D_sup_loss: 0.186, D_sup_acc: 95.24 Train acc: 94.918 Test acc: 95.330 \n",
      "step: 1630 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.566, D_sup_loss: 0.182, D_sup_acc: 95.39 Train acc: 94.933 Test acc: 95.300 \n",
      "step: 1631 | Train: G_Loss: 1.081, D_unsup_loss_real: 0.642, D_unsup_loss_fake: 0.687, D_sup_loss: 0.182, D_sup_acc: 95.36 Train acc: 94.828 Test acc: 95.280 \n",
      "step: 1632 | Train: G_Loss: 1.108, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.573, D_sup_loss: 0.182, D_sup_acc: 95.34 Train acc: 94.795 Test acc: 95.250 \n",
      "step: 1633 | Train: G_Loss: 1.075, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.667, D_sup_loss: 0.184, D_sup_acc: 95.31 Train acc: 94.892 Test acc: 95.290 \n",
      "step: 1634 | Train: G_Loss: 1.155, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.648, D_sup_loss: 0.183, D_sup_acc: 95.35 Train acc: 94.877 Test acc: 95.300 \n",
      "step: 1635 | Train: G_Loss: 1.080, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.651, D_sup_loss: 0.181, D_sup_acc: 95.36 Train acc: 94.925 Test acc: 95.350 \n",
      "step: 1636 | Train: G_Loss: 1.028, D_unsup_loss_real: 0.647, D_unsup_loss_fake: 0.623, D_sup_loss: 0.180, D_sup_acc: 95.41 Train acc: 94.915 Test acc: 95.270 \n",
      "step: 1637 | Train: G_Loss: 1.096, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.586, D_sup_loss: 0.185, D_sup_acc: 95.33 Train acc: 94.883 Test acc: 95.210 \n",
      "step: 1638 | Train: G_Loss: 1.063, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.638, D_sup_loss: 0.183, D_sup_acc: 95.27 Train acc: 94.873 Test acc: 95.300 \n",
      "step: 1639 | Train: G_Loss: 1.143, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.677, D_sup_loss: 0.179, D_sup_acc: 95.36 Train acc: 94.907 Test acc: 95.210 \n",
      "step: 1640 | Train: G_Loss: 1.059, D_unsup_loss_real: 0.689, D_unsup_loss_fake: 0.597, D_sup_loss: 0.180, D_sup_acc: 95.27 Train acc: 94.923 Test acc: 95.190 \n",
      "step: 1641 | Train: G_Loss: 1.158, D_unsup_loss_real: 0.520, D_unsup_loss_fake: 0.621, D_sup_loss: 0.182, D_sup_acc: 95.25 Train acc: 94.865 Test acc: 95.250 \n",
      "step: 1642 | Train: G_Loss: 1.075, D_unsup_loss_real: 0.664, D_unsup_loss_fake: 0.682, D_sup_loss: 0.181, D_sup_acc: 95.31 Train acc: 94.807 Test acc: 95.220 \n",
      "step: 1643 | Train: G_Loss: 1.044, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.546, D_sup_loss: 0.182, D_sup_acc: 95.28 Train acc: 94.798 Test acc: 95.180 \n",
      "step: 1644 | Train: G_Loss: 1.078, D_unsup_loss_real: 0.666, D_unsup_loss_fake: 0.619, D_sup_loss: 0.182, D_sup_acc: 95.24 Train acc: 94.727 Test acc: 95.100 \n",
      "step: 1645 | Train: G_Loss: 1.137, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.591, D_sup_loss: 0.183, D_sup_acc: 95.16 Train acc: 94.745 Test acc: 95.180 \n",
      "step: 1646 | Train: G_Loss: 1.125, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.591, D_sup_loss: 0.180, D_sup_acc: 95.24 Train acc: 94.810 Test acc: 95.110 \n",
      "step: 1647 | Train: G_Loss: 1.048, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.563, D_sup_loss: 0.181, D_sup_acc: 95.17 Train acc: 94.842 Test acc: 95.280 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1648 | Train: G_Loss: 1.062, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.682, D_sup_loss: 0.178, D_sup_acc: 95.34 Train acc: 94.797 Test acc: 95.220 \n",
      "step: 1649 | Train: G_Loss: 1.128, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.604, D_sup_loss: 0.179, D_sup_acc: 95.28 Train acc: 94.767 Test acc: 95.170 \n",
      "step: 1650 | Train: G_Loss: 1.100, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.682, D_sup_loss: 0.182, D_sup_acc: 95.23 Train acc: 94.688 Test acc: 95.080 \n",
      "step: 1651 | Train: G_Loss: 1.132, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.689, D_sup_loss: 0.187, D_sup_acc: 95.14 Train acc: 94.568 Test acc: 94.860 \n",
      "step: 1652 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.673, D_unsup_loss_fake: 0.601, D_sup_loss: 0.193, D_sup_acc: 94.92 Train acc: 94.557 Test acc: 94.900 \n",
      "step: 1653 | Train: G_Loss: 1.065, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.632, D_sup_loss: 0.195, D_sup_acc: 94.96 Train acc: 94.693 Test acc: 95.060 \n",
      "step: 1654 | Train: G_Loss: 1.113, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.626, D_sup_loss: 0.188, D_sup_acc: 95.12 Train acc: 94.738 Test acc: 95.060 \n",
      "step: 1655 | Train: G_Loss: 1.126, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.701, D_sup_loss: 0.186, D_sup_acc: 95.12 Train acc: 94.795 Test acc: 95.080 \n",
      "step: 1656 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.554, D_sup_loss: 0.181, D_sup_acc: 95.14 Train acc: 94.848 Test acc: 95.210 \n",
      "step: 1657 | Train: G_Loss: 1.101, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.588, D_sup_loss: 0.181, D_sup_acc: 95.27 Train acc: 94.900 Test acc: 95.210 \n",
      "step: 1658 | Train: G_Loss: 1.067, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.600, D_sup_loss: 0.182, D_sup_acc: 95.27 Train acc: 94.902 Test acc: 95.210 \n",
      "step: 1659 | Train: G_Loss: 1.073, D_unsup_loss_real: 0.661, D_unsup_loss_fake: 0.607, D_sup_loss: 0.181, D_sup_acc: 95.27 Train acc: 94.865 Test acc: 95.220 \n",
      "step: 1660 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.634, D_sup_loss: 0.180, D_sup_acc: 95.28 Train acc: 94.860 Test acc: 95.160 \n",
      "step: 1661 | Train: G_Loss: 1.171, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.685, D_sup_loss: 0.182, D_sup_acc: 95.22 Train acc: 94.880 Test acc: 95.230 \n",
      "step: 1662 | Train: G_Loss: 1.184, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.647, D_sup_loss: 0.184, D_sup_acc: 95.29 Train acc: 94.908 Test acc: 95.290 \n",
      "step: 1663 | Train: G_Loss: 1.074, D_unsup_loss_real: 0.675, D_unsup_loss_fake: 0.603, D_sup_loss: 0.181, D_sup_acc: 95.35 Train acc: 94.973 Test acc: 95.370 \n",
      "step: 1664 | Train: G_Loss: 1.078, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.619, D_sup_loss: 0.179, D_sup_acc: 95.43 Train acc: 94.985 Test acc: 95.300 \n",
      "step: 1665 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.591, D_sup_loss: 0.177, D_sup_acc: 95.36 Train acc: 94.977 Test acc: 95.240 \n",
      "step: 1666 | Train: G_Loss: 1.119, D_unsup_loss_real: 0.650, D_unsup_loss_fake: 0.625, D_sup_loss: 0.178, D_sup_acc: 95.30 Train acc: 94.965 Test acc: 95.310 \n",
      "step: 1667 | Train: G_Loss: 1.125, D_unsup_loss_real: 0.644, D_unsup_loss_fake: 0.601, D_sup_loss: 0.179, D_sup_acc: 95.37 Train acc: 94.983 Test acc: 95.340 \n",
      "step: 1668 | Train: G_Loss: 1.079, D_unsup_loss_real: 0.642, D_unsup_loss_fake: 0.616, D_sup_loss: 0.182, D_sup_acc: 95.40 Train acc: 94.920 Test acc: 95.280 \n",
      "step: 1669 | Train: G_Loss: 1.216, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.618, D_sup_loss: 0.184, D_sup_acc: 95.34 Train acc: 94.885 Test acc: 95.220 \n",
      "step: 1670 | Train: G_Loss: 1.058, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.667, D_sup_loss: 0.185, D_sup_acc: 95.28 Train acc: 94.927 Test acc: 95.130 \n",
      "step: 1671 | Train: G_Loss: 1.171, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.607, D_sup_loss: 0.182, D_sup_acc: 95.19 Train acc: 94.963 Test acc: 95.220 \n",
      "step: 1672 | Train: G_Loss: 1.106, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.609, D_sup_loss: 0.180, D_sup_acc: 95.28 Train acc: 95.055 Test acc: 95.360 \n",
      "step: 1673 | Train: G_Loss: 1.122, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.631, D_sup_loss: 0.178, D_sup_acc: 95.42 Train acc: 94.963 Test acc: 95.250 \n",
      "step: 1674 | Train: G_Loss: 1.111, D_unsup_loss_real: 0.695, D_unsup_loss_fake: 0.668, D_sup_loss: 0.180, D_sup_acc: 95.31 Train acc: 94.968 Test acc: 95.280 \n",
      "step: 1675 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.662, D_unsup_loss_fake: 0.666, D_sup_loss: 0.179, D_sup_acc: 95.34 Train acc: 95.020 Test acc: 95.410 \n",
      "step: 1676 | Train: G_Loss: 1.053, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.567, D_sup_loss: 0.177, D_sup_acc: 95.47 Train acc: 95.022 Test acc: 95.420 \n",
      "step: 1677 | Train: G_Loss: 1.067, D_unsup_loss_real: 0.529, D_unsup_loss_fake: 0.611, D_sup_loss: 0.176, D_sup_acc: 95.48 Train acc: 95.032 Test acc: 95.410 \n",
      "step: 1678 | Train: G_Loss: 1.081, D_unsup_loss_real: 0.657, D_unsup_loss_fake: 0.577, D_sup_loss: 0.176, D_sup_acc: 95.47 Train acc: 95.097 Test acc: 95.430 \n",
      "step: 1679 | Train: G_Loss: 1.043, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.644, D_sup_loss: 0.173, D_sup_acc: 95.49 Train acc: 95.068 Test acc: 95.440 \n",
      "step: 1680 | Train: G_Loss: 1.158, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.627, D_sup_loss: 0.172, D_sup_acc: 95.50 Train acc: 95.035 Test acc: 95.400 \n",
      "step: 1681 | Train: G_Loss: 1.091, D_unsup_loss_real: 0.659, D_unsup_loss_fake: 0.588, D_sup_loss: 0.172, D_sup_acc: 95.46 Train acc: 95.058 Test acc: 95.460 \n",
      "step: 1682 | Train: G_Loss: 1.067, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.695, D_sup_loss: 0.171, D_sup_acc: 95.52 Train acc: 95.117 Test acc: 95.390 \n",
      "step: 1683 | Train: G_Loss: 1.011, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.669, D_sup_loss: 0.173, D_sup_acc: 95.45 Train acc: 95.110 Test acc: 95.510 \n",
      "step: 1684 | Train: G_Loss: 1.044, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.605, D_sup_loss: 0.174, D_sup_acc: 95.57 Train acc: 95.125 Test acc: 95.520 \n",
      "step: 1685 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.671, D_sup_loss: 0.174, D_sup_acc: 95.58 Train acc: 95.097 Test acc: 95.330 \n",
      "step: 1686 | Train: G_Loss: 1.152, D_unsup_loss_real: 0.541, D_unsup_loss_fake: 0.634, D_sup_loss: 0.181, D_sup_acc: 95.39 Train acc: 95.057 Test acc: 95.340 \n",
      "step: 1687 | Train: G_Loss: 1.193, D_unsup_loss_real: 0.724, D_unsup_loss_fake: 0.544, D_sup_loss: 0.182, D_sup_acc: 95.40 Train acc: 95.035 Test acc: 95.290 \n",
      "step: 1688 | Train: G_Loss: 1.129, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.641, D_sup_loss: 0.184, D_sup_acc: 95.35 Train acc: 94.983 Test acc: 95.290 \n",
      "step: 1689 | Train: G_Loss: 1.070, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.630, D_sup_loss: 0.184, D_sup_acc: 95.35 Train acc: 95.047 Test acc: 95.310 \n",
      "step: 1690 | Train: G_Loss: 1.168, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.635, D_sup_loss: 0.180, D_sup_acc: 95.37 Train acc: 94.903 Test acc: 95.140 \n",
      "step: 1691 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.658, D_sup_loss: 0.187, D_sup_acc: 95.20 Train acc: 94.890 Test acc: 95.210 \n",
      "step: 1692 | Train: G_Loss: 1.080, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.639, D_sup_loss: 0.186, D_sup_acc: 95.27 Train acc: 94.908 Test acc: 95.280 \n",
      "step: 1693 | Train: G_Loss: 1.020, D_unsup_loss_real: 0.644, D_unsup_loss_fake: 0.660, D_sup_loss: 0.188, D_sup_acc: 95.34 Train acc: 94.883 Test acc: 95.310 \n",
      "step: 1694 | Train: G_Loss: 1.044, D_unsup_loss_real: 0.682, D_unsup_loss_fake: 0.610, D_sup_loss: 0.193, D_sup_acc: 95.37 Train acc: 94.935 Test acc: 95.400 \n",
      "step: 1695 | Train: G_Loss: 1.137, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.606, D_sup_loss: 0.188, D_sup_acc: 95.46 Train acc: 94.903 Test acc: 95.250 \n",
      "step: 1696 | Train: G_Loss: 1.016, D_unsup_loss_real: 0.668, D_unsup_loss_fake: 0.598, D_sup_loss: 0.190, D_sup_acc: 95.31 Train acc: 94.975 Test acc: 95.420 \n",
      "step: 1697 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.699, D_sup_loss: 0.185, D_sup_acc: 95.48 Train acc: 94.992 Test acc: 95.370 \n",
      "step: 1698 | Train: G_Loss: 1.109, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.647, D_sup_loss: 0.185, D_sup_acc: 95.43 Train acc: 95.020 Test acc: 95.460 \n",
      "step: 1699 | Train: G_Loss: 1.088, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.602, D_sup_loss: 0.186, D_sup_acc: 95.52 Train acc: 95.057 Test acc: 95.480 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1700 | Train: G_Loss: 1.089, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.573, D_sup_loss: 0.184, D_sup_acc: 95.54 Train acc: 95.055 Test acc: 95.480 \n",
      "Train Classifier Accuracy: 95.055%\n",
      "\n",
      "Test Classifier Accuracy: 95.480%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_1700.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_1700.h5\n",
      "step: 1701 | Train: G_Loss: 1.057, D_unsup_loss_real: 0.648, D_unsup_loss_fake: 0.630, D_sup_loss: 0.180, D_sup_acc: 95.54 Train acc: 94.980 Test acc: 95.480 \n",
      "step: 1702 | Train: G_Loss: 1.082, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.634, D_sup_loss: 0.185, D_sup_acc: 95.54 Train acc: 94.890 Test acc: 95.340 \n",
      "step: 1703 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.657, D_sup_loss: 0.190, D_sup_acc: 95.40 Train acc: 94.917 Test acc: 95.220 \n",
      "step: 1704 | Train: G_Loss: 1.126, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.609, D_sup_loss: 0.190, D_sup_acc: 95.28 Train acc: 94.978 Test acc: 95.290 \n",
      "step: 1705 | Train: G_Loss: 1.187, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.647, D_sup_loss: 0.185, D_sup_acc: 95.35 Train acc: 94.902 Test acc: 95.270 \n",
      "step: 1706 | Train: G_Loss: 1.166, D_unsup_loss_real: 0.663, D_unsup_loss_fake: 0.601, D_sup_loss: 0.182, D_sup_acc: 95.33 Train acc: 94.902 Test acc: 95.340 \n",
      "step: 1707 | Train: G_Loss: 1.065, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.655, D_sup_loss: 0.182, D_sup_acc: 95.40 Train acc: 94.937 Test acc: 95.440 \n",
      "step: 1708 | Train: G_Loss: 1.127, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.611, D_sup_loss: 0.183, D_sup_acc: 95.50 Train acc: 94.892 Test acc: 95.500 \n",
      "step: 1709 | Train: G_Loss: 1.180, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.650, D_sup_loss: 0.185, D_sup_acc: 95.56 Train acc: 94.927 Test acc: 95.360 \n",
      "step: 1710 | Train: G_Loss: 1.070, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.709, D_sup_loss: 0.184, D_sup_acc: 95.42 Train acc: 94.880 Test acc: 95.370 \n",
      "step: 1711 | Train: G_Loss: 1.194, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.587, D_sup_loss: 0.185, D_sup_acc: 95.43 Train acc: 94.900 Test acc: 95.430 \n",
      "step: 1712 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.643, D_sup_loss: 0.183, D_sup_acc: 95.49 Train acc: 94.950 Test acc: 95.280 \n",
      "step: 1713 | Train: G_Loss: 1.067, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.626, D_sup_loss: 0.179, D_sup_acc: 95.34 Train acc: 94.987 Test acc: 95.350 \n",
      "step: 1714 | Train: G_Loss: 1.183, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.652, D_sup_loss: 0.183, D_sup_acc: 95.41 Train acc: 95.022 Test acc: 95.460 \n",
      "step: 1715 | Train: G_Loss: 1.090, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.588, D_sup_loss: 0.183, D_sup_acc: 95.52 Train acc: 94.945 Test acc: 95.510 \n",
      "step: 1716 | Train: G_Loss: 1.173, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.631, D_sup_loss: 0.184, D_sup_acc: 95.57 Train acc: 95.083 Test acc: 95.550 \n",
      "step: 1717 | Train: G_Loss: 1.085, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.635, D_sup_loss: 0.178, D_sup_acc: 95.61 Train acc: 95.135 Test acc: 95.490 \n",
      "step: 1718 | Train: G_Loss: 1.072, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.564, D_sup_loss: 0.177, D_sup_acc: 95.55 Train acc: 95.203 Test acc: 95.600 \n",
      "step: 1719 | Train: G_Loss: 1.140, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.617, D_sup_loss: 0.175, D_sup_acc: 95.66 Train acc: 95.138 Test acc: 95.540 \n",
      "step: 1720 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.687, D_unsup_loss_fake: 0.625, D_sup_loss: 0.173, D_sup_acc: 95.60 Train acc: 95.137 Test acc: 95.460 \n",
      "step: 1721 | Train: G_Loss: 1.087, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.755, D_sup_loss: 0.175, D_sup_acc: 95.52 Train acc: 95.142 Test acc: 95.590 \n",
      "step: 1722 | Train: G_Loss: 1.074, D_unsup_loss_real: 0.695, D_unsup_loss_fake: 0.637, D_sup_loss: 0.179, D_sup_acc: 95.65 Train acc: 94.963 Test acc: 95.390 \n",
      "step: 1723 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.607, D_sup_loss: 0.181, D_sup_acc: 95.45 Train acc: 95.032 Test acc: 95.520 \n",
      "step: 1724 | Train: G_Loss: 1.089, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.606, D_sup_loss: 0.177, D_sup_acc: 95.58 Train acc: 95.050 Test acc: 95.490 \n",
      "step: 1725 | Train: G_Loss: 1.055, D_unsup_loss_real: 0.653, D_unsup_loss_fake: 0.652, D_sup_loss: 0.176, D_sup_acc: 95.55 Train acc: 95.172 Test acc: 95.600 \n",
      "step: 1726 | Train: G_Loss: 1.039, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.656, D_sup_loss: 0.176, D_sup_acc: 95.66 Train acc: 95.083 Test acc: 95.440 \n",
      "step: 1727 | Train: G_Loss: 1.111, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.619, D_sup_loss: 0.181, D_sup_acc: 95.50 Train acc: 95.108 Test acc: 95.410 \n",
      "step: 1728 | Train: G_Loss: 1.157, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.650, D_sup_loss: 0.181, D_sup_acc: 95.47 Train acc: 95.130 Test acc: 95.400 \n",
      "step: 1729 | Train: G_Loss: 1.127, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.583, D_sup_loss: 0.179, D_sup_acc: 95.46 Train acc: 94.915 Test acc: 95.110 \n",
      "step: 1730 | Train: G_Loss: 1.208, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.637, D_sup_loss: 0.184, D_sup_acc: 95.17 Train acc: 94.877 Test acc: 95.280 \n",
      "step: 1731 | Train: G_Loss: 1.149, D_unsup_loss_real: 0.692, D_unsup_loss_fake: 0.635, D_sup_loss: 0.183, D_sup_acc: 95.34 Train acc: 94.965 Test acc: 95.340 \n",
      "step: 1732 | Train: G_Loss: 1.047, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.629, D_sup_loss: 0.184, D_sup_acc: 95.40 Train acc: 94.972 Test acc: 95.370 \n",
      "step: 1733 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.654, D_unsup_loss_fake: 0.588, D_sup_loss: 0.181, D_sup_acc: 95.43 Train acc: 94.868 Test acc: 95.270 \n",
      "step: 1734 | Train: G_Loss: 1.162, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.625, D_sup_loss: 0.182, D_sup_acc: 95.33 Train acc: 94.812 Test acc: 95.180 \n",
      "step: 1735 | Train: G_Loss: 1.053, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.625, D_sup_loss: 0.183, D_sup_acc: 95.24 Train acc: 94.830 Test acc: 95.340 \n",
      "step: 1736 | Train: G_Loss: 1.067, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.602, D_sup_loss: 0.183, D_sup_acc: 95.40 Train acc: 94.917 Test acc: 95.390 \n",
      "step: 1737 | Train: G_Loss: 1.123, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.694, D_sup_loss: 0.178, D_sup_acc: 95.45 Train acc: 94.945 Test acc: 95.450 \n",
      "step: 1738 | Train: G_Loss: 1.193, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.691, D_sup_loss: 0.178, D_sup_acc: 95.51 Train acc: 94.905 Test acc: 95.260 \n",
      "step: 1739 | Train: G_Loss: 1.082, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.658, D_sup_loss: 0.179, D_sup_acc: 95.32 Train acc: 94.945 Test acc: 95.230 \n",
      "step: 1740 | Train: G_Loss: 1.176, D_unsup_loss_real: 0.676, D_unsup_loss_fake: 0.613, D_sup_loss: 0.180, D_sup_acc: 95.29 Train acc: 94.963 Test acc: 95.320 \n",
      "step: 1741 | Train: G_Loss: 1.099, D_unsup_loss_real: 0.693, D_unsup_loss_fake: 0.669, D_sup_loss: 0.182, D_sup_acc: 95.38 Train acc: 94.908 Test acc: 95.290 \n",
      "step: 1742 | Train: G_Loss: 1.120, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.619, D_sup_loss: 0.185, D_sup_acc: 95.35 Train acc: 94.978 Test acc: 95.410 \n",
      "step: 1743 | Train: G_Loss: 1.118, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.562, D_sup_loss: 0.182, D_sup_acc: 95.47 Train acc: 94.957 Test acc: 95.350 \n",
      "step: 1744 | Train: G_Loss: 1.085, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.665, D_sup_loss: 0.182, D_sup_acc: 95.41 Train acc: 94.968 Test acc: 95.390 \n",
      "step: 1745 | Train: G_Loss: 1.084, D_unsup_loss_real: 0.653, D_unsup_loss_fake: 0.637, D_sup_loss: 0.181, D_sup_acc: 95.45 Train acc: 94.988 Test acc: 95.370 \n",
      "step: 1746 | Train: G_Loss: 1.017, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.572, D_sup_loss: 0.181, D_sup_acc: 95.43 Train acc: 94.988 Test acc: 95.320 \n",
      "step: 1747 | Train: G_Loss: 1.036, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.691, D_sup_loss: 0.183, D_sup_acc: 95.38 Train acc: 95.033 Test acc: 95.390 \n",
      "step: 1748 | Train: G_Loss: 1.120, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.614, D_sup_loss: 0.181, D_sup_acc: 95.45 Train acc: 95.035 Test acc: 95.410 \n",
      "step: 1749 | Train: G_Loss: 1.070, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.634, D_sup_loss: 0.181, D_sup_acc: 95.47 Train acc: 95.077 Test acc: 95.470 \n",
      "step: 1750 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.674, D_sup_loss: 0.180, D_sup_acc: 95.53 Train acc: 95.073 Test acc: 95.410 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1751 | Train: G_Loss: 1.078, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.689, D_sup_loss: 0.183, D_sup_acc: 95.47 Train acc: 95.043 Test acc: 95.470 \n",
      "step: 1752 | Train: G_Loss: 1.115, D_unsup_loss_real: 0.639, D_unsup_loss_fake: 0.695, D_sup_loss: 0.183, D_sup_acc: 95.53 Train acc: 95.118 Test acc: 95.410 \n",
      "step: 1753 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.696, D_sup_loss: 0.177, D_sup_acc: 95.47 Train acc: 94.920 Test acc: 95.370 \n",
      "step: 1754 | Train: G_Loss: 1.089, D_unsup_loss_real: 0.705, D_unsup_loss_fake: 0.610, D_sup_loss: 0.185, D_sup_acc: 95.43 Train acc: 94.965 Test acc: 95.420 \n",
      "step: 1755 | Train: G_Loss: 1.126, D_unsup_loss_real: 0.631, D_unsup_loss_fake: 0.604, D_sup_loss: 0.181, D_sup_acc: 95.48 Train acc: 95.052 Test acc: 95.460 \n",
      "step: 1756 | Train: G_Loss: 1.051, D_unsup_loss_real: 0.685, D_unsup_loss_fake: 0.583, D_sup_loss: 0.177, D_sup_acc: 95.52 Train acc: 95.002 Test acc: 95.480 \n",
      "step: 1757 | Train: G_Loss: 1.067, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.603, D_sup_loss: 0.177, D_sup_acc: 95.54 Train acc: 95.032 Test acc: 95.470 \n",
      "step: 1758 | Train: G_Loss: 1.032, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.615, D_sup_loss: 0.176, D_sup_acc: 95.53 Train acc: 95.123 Test acc: 95.530 \n",
      "step: 1759 | Train: G_Loss: 1.141, D_unsup_loss_real: 0.660, D_unsup_loss_fake: 0.676, D_sup_loss: 0.171, D_sup_acc: 95.59 Train acc: 95.170 Test acc: 95.610 \n",
      "step: 1760 | Train: G_Loss: 1.052, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.639, D_sup_loss: 0.174, D_sup_acc: 95.67 Train acc: 95.202 Test acc: 95.580 \n",
      "step: 1761 | Train: G_Loss: 1.081, D_unsup_loss_real: 0.515, D_unsup_loss_fake: 0.622, D_sup_loss: 0.173, D_sup_acc: 95.64 Train acc: 95.213 Test acc: 95.560 \n",
      "step: 1762 | Train: G_Loss: 1.127, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.666, D_sup_loss: 0.172, D_sup_acc: 95.62 Train acc: 95.195 Test acc: 95.550 \n",
      "step: 1763 | Train: G_Loss: 1.161, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.522, D_sup_loss: 0.175, D_sup_acc: 95.61 Train acc: 95.080 Test acc: 95.510 \n",
      "step: 1764 | Train: G_Loss: 1.096, D_unsup_loss_real: 0.644, D_unsup_loss_fake: 0.665, D_sup_loss: 0.176, D_sup_acc: 95.57 Train acc: 95.125 Test acc: 95.460 \n",
      "step: 1765 | Train: G_Loss: 1.078, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.610, D_sup_loss: 0.175, D_sup_acc: 95.52 Train acc: 95.158 Test acc: 95.370 \n",
      "step: 1766 | Train: G_Loss: 1.060, D_unsup_loss_real: 0.665, D_unsup_loss_fake: 0.714, D_sup_loss: 0.172, D_sup_acc: 95.43 Train acc: 95.122 Test acc: 95.460 \n",
      "step: 1767 | Train: G_Loss: 1.064, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.639, D_sup_loss: 0.174, D_sup_acc: 95.52 Train acc: 95.172 Test acc: 95.570 \n",
      "step: 1768 | Train: G_Loss: 1.112, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.641, D_sup_loss: 0.174, D_sup_acc: 95.63 Train acc: 95.247 Test acc: 95.720 \n",
      "step: 1769 | Train: G_Loss: 1.053, D_unsup_loss_real: 0.655, D_unsup_loss_fake: 0.693, D_sup_loss: 0.173, D_sup_acc: 95.77 Train acc: 95.145 Test acc: 95.520 \n",
      "step: 1770 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.585, D_sup_loss: 0.177, D_sup_acc: 95.58 Train acc: 95.202 Test acc: 95.590 \n",
      "step: 1771 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.623, D_sup_loss: 0.176, D_sup_acc: 95.65 Train acc: 95.262 Test acc: 95.660 \n",
      "step: 1772 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.570, D_sup_loss: 0.173, D_sup_acc: 95.71 Train acc: 95.162 Test acc: 95.630 \n",
      "step: 1773 | Train: G_Loss: 1.054, D_unsup_loss_real: 0.646, D_unsup_loss_fake: 0.636, D_sup_loss: 0.175, D_sup_acc: 95.69 Train acc: 95.142 Test acc: 95.630 \n",
      "step: 1774 | Train: G_Loss: 1.113, D_unsup_loss_real: 0.648, D_unsup_loss_fake: 0.639, D_sup_loss: 0.179, D_sup_acc: 95.69 Train acc: 95.152 Test acc: 95.690 \n",
      "step: 1775 | Train: G_Loss: 1.022, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.692, D_sup_loss: 0.179, D_sup_acc: 95.74 Train acc: 95.140 Test acc: 95.590 \n",
      "step: 1776 | Train: G_Loss: 1.119, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.640, D_sup_loss: 0.174, D_sup_acc: 95.65 Train acc: 95.153 Test acc: 95.700 \n",
      "step: 1777 | Train: G_Loss: 1.091, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.691, D_sup_loss: 0.177, D_sup_acc: 95.75 Train acc: 95.190 Test acc: 95.660 \n",
      "step: 1778 | Train: G_Loss: 1.137, D_unsup_loss_real: 0.639, D_unsup_loss_fake: 0.738, D_sup_loss: 0.176, D_sup_acc: 95.71 Train acc: 95.195 Test acc: 95.590 \n",
      "step: 1779 | Train: G_Loss: 1.127, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.675, D_sup_loss: 0.176, D_sup_acc: 95.65 Train acc: 95.165 Test acc: 95.610 \n",
      "step: 1780 | Train: G_Loss: 1.035, D_unsup_loss_real: 0.661, D_unsup_loss_fake: 0.598, D_sup_loss: 0.175, D_sup_acc: 95.67 Train acc: 95.143 Test acc: 95.680 \n",
      "step: 1781 | Train: G_Loss: 1.115, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.639, D_sup_loss: 0.173, D_sup_acc: 95.73 Train acc: 95.157 Test acc: 95.700 \n",
      "step: 1782 | Train: G_Loss: 1.058, D_unsup_loss_real: 0.694, D_unsup_loss_fake: 0.656, D_sup_loss: 0.171, D_sup_acc: 95.75 Train acc: 95.177 Test acc: 95.690 \n",
      "step: 1783 | Train: G_Loss: 1.117, D_unsup_loss_real: 0.673, D_unsup_loss_fake: 0.765, D_sup_loss: 0.171, D_sup_acc: 95.74 Train acc: 95.232 Test acc: 95.650 \n",
      "step: 1784 | Train: G_Loss: 0.992, D_unsup_loss_real: 0.666, D_unsup_loss_fake: 0.648, D_sup_loss: 0.173, D_sup_acc: 95.70 Train acc: 95.185 Test acc: 95.610 \n",
      "step: 1785 | Train: G_Loss: 1.040, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.680, D_sup_loss: 0.177, D_sup_acc: 95.67 Train acc: 95.220 Test acc: 95.650 \n",
      "step: 1786 | Train: G_Loss: 1.082, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.693, D_sup_loss: 0.177, D_sup_acc: 95.70 Train acc: 95.207 Test acc: 95.710 \n",
      "step: 1787 | Train: G_Loss: 1.079, D_unsup_loss_real: 0.680, D_unsup_loss_fake: 0.680, D_sup_loss: 0.180, D_sup_acc: 95.76 Train acc: 95.187 Test acc: 95.630 \n",
      "step: 1788 | Train: G_Loss: 1.120, D_unsup_loss_real: 0.647, D_unsup_loss_fake: 0.712, D_sup_loss: 0.185, D_sup_acc: 95.69 Train acc: 95.102 Test acc: 95.600 \n",
      "step: 1789 | Train: G_Loss: 1.086, D_unsup_loss_real: 0.653, D_unsup_loss_fake: 0.598, D_sup_loss: 0.184, D_sup_acc: 95.66 Train acc: 95.092 Test acc: 95.620 \n",
      "step: 1790 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.599, D_sup_loss: 0.182, D_sup_acc: 95.68 Train acc: 95.110 Test acc: 95.640 \n",
      "step: 1791 | Train: G_Loss: 1.035, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.592, D_sup_loss: 0.178, D_sup_acc: 95.70 Train acc: 95.147 Test acc: 95.640 \n",
      "step: 1792 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.709, D_sup_loss: 0.176, D_sup_acc: 95.70 Train acc: 95.113 Test acc: 95.570 \n",
      "step: 1793 | Train: G_Loss: 1.141, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.602, D_sup_loss: 0.178, D_sup_acc: 95.63 Train acc: 95.037 Test acc: 95.540 \n",
      "step: 1794 | Train: G_Loss: 1.013, D_unsup_loss_real: 0.753, D_unsup_loss_fake: 0.609, D_sup_loss: 0.181, D_sup_acc: 95.60 Train acc: 95.050 Test acc: 95.530 \n",
      "step: 1795 | Train: G_Loss: 1.037, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.623, D_sup_loss: 0.181, D_sup_acc: 95.59 Train acc: 95.080 Test acc: 95.510 \n",
      "step: 1796 | Train: G_Loss: 1.044, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.620, D_sup_loss: 0.178, D_sup_acc: 95.57 Train acc: 95.162 Test acc: 95.620 \n",
      "step: 1797 | Train: G_Loss: 1.189, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.666, D_sup_loss: 0.178, D_sup_acc: 95.68 Train acc: 95.175 Test acc: 95.680 \n",
      "step: 1798 | Train: G_Loss: 1.128, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.664, D_sup_loss: 0.180, D_sup_acc: 95.73 Train acc: 95.025 Test acc: 95.550 \n",
      "step: 1799 | Train: G_Loss: 1.168, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.637, D_sup_loss: 0.181, D_sup_acc: 95.61 Train acc: 94.970 Test acc: 95.620 \n",
      "step: 1800 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.545, D_unsup_loss_fake: 0.659, D_sup_loss: 0.184, D_sup_acc: 95.68 Train acc: 95.018 Test acc: 95.640 \n",
      "Train Classifier Accuracy: 95.018%\n",
      "\n",
      "Test Classifier Accuracy: 95.640%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_1800.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_1800.h5\n",
      "step: 1801 | Train: G_Loss: 1.157, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.606, D_sup_loss: 0.183, D_sup_acc: 95.70 Train acc: 95.098 Test acc: 95.600 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1802 | Train: G_Loss: 1.084, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.556, D_sup_loss: 0.177, D_sup_acc: 95.66 Train acc: 95.060 Test acc: 95.570 \n",
      "step: 1803 | Train: G_Loss: 1.091, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.575, D_sup_loss: 0.175, D_sup_acc: 95.63 Train acc: 95.057 Test acc: 95.480 \n",
      "step: 1804 | Train: G_Loss: 1.111, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.572, D_sup_loss: 0.175, D_sup_acc: 95.54 Train acc: 95.115 Test acc: 95.610 \n",
      "step: 1805 | Train: G_Loss: 1.053, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.611, D_sup_loss: 0.173, D_sup_acc: 95.67 Train acc: 95.097 Test acc: 95.510 \n",
      "step: 1806 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.644, D_sup_loss: 0.173, D_sup_acc: 95.57 Train acc: 95.108 Test acc: 95.610 \n",
      "step: 1807 | Train: G_Loss: 1.054, D_unsup_loss_real: 0.687, D_unsup_loss_fake: 0.671, D_sup_loss: 0.172, D_sup_acc: 95.67 Train acc: 95.153 Test acc: 95.660 \n",
      "step: 1808 | Train: G_Loss: 1.058, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.683, D_sup_loss: 0.174, D_sup_acc: 95.71 Train acc: 95.080 Test acc: 95.550 \n",
      "step: 1809 | Train: G_Loss: 1.167, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.606, D_sup_loss: 0.174, D_sup_acc: 95.61 Train acc: 95.132 Test acc: 95.590 \n",
      "step: 1810 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.674, D_sup_loss: 0.172, D_sup_acc: 95.65 Train acc: 95.182 Test acc: 95.620 \n",
      "step: 1811 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.545, D_sup_loss: 0.173, D_sup_acc: 95.68 Train acc: 95.155 Test acc: 95.610 \n",
      "step: 1812 | Train: G_Loss: 1.166, D_unsup_loss_real: 0.663, D_unsup_loss_fake: 0.575, D_sup_loss: 0.174, D_sup_acc: 95.67 Train acc: 95.117 Test acc: 95.540 \n",
      "step: 1813 | Train: G_Loss: 1.050, D_unsup_loss_real: 0.671, D_unsup_loss_fake: 0.620, D_sup_loss: 0.176, D_sup_acc: 95.60 Train acc: 95.227 Test acc: 95.610 \n",
      "step: 1814 | Train: G_Loss: 1.106, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.564, D_sup_loss: 0.171, D_sup_acc: 95.67 Train acc: 95.163 Test acc: 95.630 \n",
      "step: 1815 | Train: G_Loss: 1.107, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.532, D_sup_loss: 0.171, D_sup_acc: 95.69 Train acc: 95.235 Test acc: 95.640 \n",
      "step: 1816 | Train: G_Loss: 1.139, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.645, D_sup_loss: 0.170, D_sup_acc: 95.70 Train acc: 95.197 Test acc: 95.630 \n",
      "step: 1817 | Train: G_Loss: 1.212, D_unsup_loss_real: 0.523, D_unsup_loss_fake: 0.635, D_sup_loss: 0.169, D_sup_acc: 95.69 Train acc: 95.142 Test acc: 95.530 \n",
      "step: 1818 | Train: G_Loss: 1.117, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.630, D_sup_loss: 0.173, D_sup_acc: 95.59 Train acc: 95.105 Test acc: 95.580 \n",
      "step: 1819 | Train: G_Loss: 1.101, D_unsup_loss_real: 0.713, D_unsup_loss_fake: 0.675, D_sup_loss: 0.173, D_sup_acc: 95.64 Train acc: 95.017 Test acc: 95.560 \n",
      "step: 1820 | Train: G_Loss: 1.168, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.631, D_sup_loss: 0.177, D_sup_acc: 95.62 Train acc: 94.993 Test acc: 95.520 \n",
      "step: 1821 | Train: G_Loss: 1.163, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.625, D_sup_loss: 0.177, D_sup_acc: 95.58 Train acc: 94.992 Test acc: 95.420 \n",
      "step: 1822 | Train: G_Loss: 1.132, D_unsup_loss_real: 0.651, D_unsup_loss_fake: 0.672, D_sup_loss: 0.181, D_sup_acc: 95.48 Train acc: 94.948 Test acc: 95.310 \n",
      "step: 1823 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.549, D_sup_loss: 0.183, D_sup_acc: 95.37 Train acc: 94.965 Test acc: 95.450 \n",
      "step: 1824 | Train: G_Loss: 1.107, D_unsup_loss_real: 0.663, D_unsup_loss_fake: 0.751, D_sup_loss: 0.180, D_sup_acc: 95.51 Train acc: 94.988 Test acc: 95.530 \n",
      "step: 1825 | Train: G_Loss: 1.091, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.610, D_sup_loss: 0.181, D_sup_acc: 95.59 Train acc: 94.960 Test acc: 95.490 \n",
      "step: 1826 | Train: G_Loss: 1.168, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.700, D_sup_loss: 0.176, D_sup_acc: 95.55 Train acc: 95.033 Test acc: 95.600 \n",
      "step: 1827 | Train: G_Loss: 1.113, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.668, D_sup_loss: 0.176, D_sup_acc: 95.66 Train acc: 95.018 Test acc: 95.490 \n",
      "step: 1828 | Train: G_Loss: 1.168, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.595, D_sup_loss: 0.180, D_sup_acc: 95.55 Train acc: 95.072 Test acc: 95.560 \n",
      "step: 1829 | Train: G_Loss: 1.084, D_unsup_loss_real: 0.666, D_unsup_loss_fake: 0.694, D_sup_loss: 0.177, D_sup_acc: 95.62 Train acc: 95.070 Test acc: 95.510 \n",
      "step: 1830 | Train: G_Loss: 1.030, D_unsup_loss_real: 0.665, D_unsup_loss_fake: 0.567, D_sup_loss: 0.180, D_sup_acc: 95.57 Train acc: 95.065 Test acc: 95.530 \n",
      "step: 1831 | Train: G_Loss: 1.064, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.599, D_sup_loss: 0.178, D_sup_acc: 95.59 Train acc: 95.047 Test acc: 95.540 \n",
      "step: 1832 | Train: G_Loss: 1.063, D_unsup_loss_real: 0.716, D_unsup_loss_fake: 0.603, D_sup_loss: 0.180, D_sup_acc: 95.60 Train acc: 94.932 Test acc: 95.480 \n",
      "step: 1833 | Train: G_Loss: 1.097, D_unsup_loss_real: 0.515, D_unsup_loss_fake: 0.689, D_sup_loss: 0.185, D_sup_acc: 95.54 Train acc: 95.010 Test acc: 95.530 \n",
      "step: 1834 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.708, D_unsup_loss_fake: 0.628, D_sup_loss: 0.183, D_sup_acc: 95.59 Train acc: 94.962 Test acc: 95.430 \n",
      "step: 1835 | Train: G_Loss: 1.094, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.586, D_sup_loss: 0.185, D_sup_acc: 95.49 Train acc: 94.995 Test acc: 95.530 \n",
      "step: 1836 | Train: G_Loss: 1.129, D_unsup_loss_real: 0.654, D_unsup_loss_fake: 0.603, D_sup_loss: 0.181, D_sup_acc: 95.59 Train acc: 94.993 Test acc: 95.590 \n",
      "step: 1837 | Train: G_Loss: 1.059, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.611, D_sup_loss: 0.177, D_sup_acc: 95.65 Train acc: 95.037 Test acc: 95.660 \n",
      "step: 1838 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.666, D_unsup_loss_fake: 0.708, D_sup_loss: 0.173, D_sup_acc: 95.71 Train acc: 95.083 Test acc: 95.570 \n",
      "step: 1839 | Train: G_Loss: 1.153, D_unsup_loss_real: 0.541, D_unsup_loss_fake: 0.603, D_sup_loss: 0.173, D_sup_acc: 95.63 Train acc: 95.103 Test acc: 95.630 \n",
      "step: 1840 | Train: G_Loss: 1.065, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.668, D_sup_loss: 0.172, D_sup_acc: 95.69 Train acc: 95.110 Test acc: 95.670 \n",
      "step: 1841 | Train: G_Loss: 1.003, D_unsup_loss_real: 0.683, D_unsup_loss_fake: 0.683, D_sup_loss: 0.174, D_sup_acc: 95.72 Train acc: 95.183 Test acc: 95.640 \n",
      "step: 1842 | Train: G_Loss: 1.123, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.568, D_sup_loss: 0.176, D_sup_acc: 95.70 Train acc: 95.203 Test acc: 95.630 \n",
      "step: 1843 | Train: G_Loss: 1.122, D_unsup_loss_real: 0.685, D_unsup_loss_fake: 0.681, D_sup_loss: 0.175, D_sup_acc: 95.69 Train acc: 95.162 Test acc: 95.700 \n",
      "step: 1844 | Train: G_Loss: 1.116, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.632, D_sup_loss: 0.179, D_sup_acc: 95.75 Train acc: 95.178 Test acc: 95.660 \n",
      "step: 1845 | Train: G_Loss: 1.081, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.616, D_sup_loss: 0.173, D_sup_acc: 95.71 Train acc: 95.223 Test acc: 95.710 \n",
      "step: 1846 | Train: G_Loss: 1.083, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.597, D_sup_loss: 0.170, D_sup_acc: 95.76 Train acc: 95.242 Test acc: 95.760 \n",
      "step: 1847 | Train: G_Loss: 1.040, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.621, D_sup_loss: 0.169, D_sup_acc: 95.81 Train acc: 95.278 Test acc: 95.810 \n",
      "step: 1848 | Train: G_Loss: 1.080, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.730, D_sup_loss: 0.169, D_sup_acc: 95.86 Train acc: 95.320 Test acc: 95.620 \n",
      "step: 1849 | Train: G_Loss: 1.048, D_unsup_loss_real: 0.648, D_unsup_loss_fake: 0.688, D_sup_loss: 0.173, D_sup_acc: 95.68 Train acc: 95.253 Test acc: 95.610 \n",
      "step: 1850 | Train: G_Loss: 1.113, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.638, D_sup_loss: 0.176, D_sup_acc: 95.67 Train acc: 95.275 Test acc: 95.580 \n",
      "step: 1851 | Train: G_Loss: 1.077, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.665, D_sup_loss: 0.176, D_sup_acc: 95.64 Train acc: 95.272 Test acc: 95.580 \n",
      "step: 1852 | Train: G_Loss: 1.071, D_unsup_loss_real: 0.655, D_unsup_loss_fake: 0.624, D_sup_loss: 0.177, D_sup_acc: 95.64 Train acc: 95.312 Test acc: 95.740 \n",
      "step: 1853 | Train: G_Loss: 1.024, D_unsup_loss_real: 0.656, D_unsup_loss_fake: 0.588, D_sup_loss: 0.177, D_sup_acc: 95.79 Train acc: 95.297 Test acc: 95.740 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1854 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.639, D_unsup_loss_fake: 0.560, D_sup_loss: 0.176, D_sup_acc: 95.79 Train acc: 95.258 Test acc: 95.740 \n",
      "step: 1855 | Train: G_Loss: 1.132, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.569, D_sup_loss: 0.173, D_sup_acc: 95.79 Train acc: 95.318 Test acc: 95.830 \n",
      "step: 1856 | Train: G_Loss: 1.047, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.673, D_sup_loss: 0.169, D_sup_acc: 95.88 Train acc: 95.270 Test acc: 95.760 \n",
      "step: 1857 | Train: G_Loss: 1.113, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.634, D_sup_loss: 0.171, D_sup_acc: 95.81 Train acc: 95.210 Test acc: 95.750 \n",
      "step: 1858 | Train: G_Loss: 1.062, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.607, D_sup_loss: 0.171, D_sup_acc: 95.80 Train acc: 95.207 Test acc: 95.610 \n",
      "step: 1859 | Train: G_Loss: 1.008, D_unsup_loss_real: 0.647, D_unsup_loss_fake: 0.708, D_sup_loss: 0.173, D_sup_acc: 95.67 Train acc: 95.278 Test acc: 95.720 \n",
      "step: 1860 | Train: G_Loss: 1.049, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.690, D_sup_loss: 0.173, D_sup_acc: 95.77 Train acc: 95.212 Test acc: 95.700 \n",
      "step: 1861 | Train: G_Loss: 1.114, D_unsup_loss_real: 0.631, D_unsup_loss_fake: 0.620, D_sup_loss: 0.173, D_sup_acc: 95.75 Train acc: 95.178 Test acc: 95.690 \n",
      "step: 1862 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.648, D_unsup_loss_fake: 0.610, D_sup_loss: 0.176, D_sup_acc: 95.74 Train acc: 95.203 Test acc: 95.710 \n",
      "step: 1863 | Train: G_Loss: 1.136, D_unsup_loss_real: 0.665, D_unsup_loss_fake: 0.643, D_sup_loss: 0.174, D_sup_acc: 95.76 Train acc: 95.170 Test acc: 95.730 \n",
      "step: 1864 | Train: G_Loss: 1.081, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.594, D_sup_loss: 0.175, D_sup_acc: 95.78 Train acc: 95.210 Test acc: 95.740 \n",
      "step: 1865 | Train: G_Loss: 1.074, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.600, D_sup_loss: 0.173, D_sup_acc: 95.79 Train acc: 95.263 Test acc: 95.760 \n",
      "step: 1866 | Train: G_Loss: 1.104, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.577, D_sup_loss: 0.172, D_sup_acc: 95.81 Train acc: 95.253 Test acc: 95.790 \n",
      "step: 1867 | Train: G_Loss: 1.071, D_unsup_loss_real: 0.545, D_unsup_loss_fake: 0.609, D_sup_loss: 0.174, D_sup_acc: 95.84 Train acc: 95.212 Test acc: 95.780 \n",
      "step: 1868 | Train: G_Loss: 1.088, D_unsup_loss_real: 0.628, D_unsup_loss_fake: 0.646, D_sup_loss: 0.174, D_sup_acc: 95.83 Train acc: 95.225 Test acc: 95.630 \n",
      "step: 1869 | Train: G_Loss: 1.087, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.665, D_sup_loss: 0.174, D_sup_acc: 95.69 Train acc: 95.153 Test acc: 95.540 \n",
      "step: 1870 | Train: G_Loss: 1.140, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.642, D_sup_loss: 0.179, D_sup_acc: 95.60 Train acc: 95.182 Test acc: 95.480 \n",
      "step: 1871 | Train: G_Loss: 0.971, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.685, D_sup_loss: 0.177, D_sup_acc: 95.54 Train acc: 95.150 Test acc: 95.520 \n",
      "step: 1872 | Train: G_Loss: 1.116, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.605, D_sup_loss: 0.177, D_sup_acc: 95.58 Train acc: 95.153 Test acc: 95.480 \n",
      "step: 1873 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.697, D_unsup_loss_fake: 0.551, D_sup_loss: 0.176, D_sup_acc: 95.54 Train acc: 95.132 Test acc: 95.430 \n",
      "step: 1874 | Train: G_Loss: 1.096, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.658, D_sup_loss: 0.177, D_sup_acc: 95.49 Train acc: 95.160 Test acc: 95.490 \n",
      "step: 1875 | Train: G_Loss: 1.165, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.600, D_sup_loss: 0.177, D_sup_acc: 95.55 Train acc: 95.218 Test acc: 95.540 \n",
      "step: 1876 | Train: G_Loss: 1.117, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.617, D_sup_loss: 0.175, D_sup_acc: 95.60 Train acc: 95.273 Test acc: 95.530 \n",
      "step: 1877 | Train: G_Loss: 1.065, D_unsup_loss_real: 0.651, D_unsup_loss_fake: 0.623, D_sup_loss: 0.173, D_sup_acc: 95.59 Train acc: 95.282 Test acc: 95.670 \n",
      "step: 1878 | Train: G_Loss: 1.037, D_unsup_loss_real: 0.669, D_unsup_loss_fake: 0.663, D_sup_loss: 0.174, D_sup_acc: 95.72 Train acc: 95.210 Test acc: 95.520 \n",
      "step: 1879 | Train: G_Loss: 1.097, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.670, D_sup_loss: 0.178, D_sup_acc: 95.58 Train acc: 95.237 Test acc: 95.550 \n",
      "step: 1880 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.578, D_sup_loss: 0.181, D_sup_acc: 95.61 Train acc: 95.228 Test acc: 95.630 \n",
      "step: 1881 | Train: G_Loss: 1.040, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.643, D_sup_loss: 0.180, D_sup_acc: 95.69 Train acc: 95.287 Test acc: 95.630 \n",
      "step: 1882 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.655, D_unsup_loss_fake: 0.673, D_sup_loss: 0.175, D_sup_acc: 95.69 Train acc: 95.145 Test acc: 95.440 \n",
      "step: 1883 | Train: G_Loss: 1.180, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.703, D_sup_loss: 0.180, D_sup_acc: 95.50 Train acc: 95.155 Test acc: 95.580 \n",
      "step: 1884 | Train: G_Loss: 1.046, D_unsup_loss_real: 0.646, D_unsup_loss_fake: 0.633, D_sup_loss: 0.182, D_sup_acc: 95.64 Train acc: 95.053 Test acc: 95.440 \n",
      "step: 1885 | Train: G_Loss: 1.158, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.599, D_sup_loss: 0.186, D_sup_acc: 95.50 Train acc: 95.203 Test acc: 95.500 \n",
      "step: 1886 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.529, D_sup_loss: 0.176, D_sup_acc: 95.56 Train acc: 95.265 Test acc: 95.610 \n",
      "step: 1887 | Train: G_Loss: 1.120, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.595, D_sup_loss: 0.170, D_sup_acc: 95.67 Train acc: 95.365 Test acc: 95.700 \n",
      "step: 1888 | Train: G_Loss: 1.008, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.594, D_sup_loss: 0.167, D_sup_acc: 95.75 Train acc: 95.383 Test acc: 95.770 \n",
      "step: 1889 | Train: G_Loss: 0.996, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.724, D_sup_loss: 0.165, D_sup_acc: 95.82 Train acc: 95.348 Test acc: 95.790 \n",
      "step: 1890 | Train: G_Loss: 1.106, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.589, D_sup_loss: 0.166, D_sup_acc: 95.84 Train acc: 95.317 Test acc: 95.750 \n",
      "step: 1891 | Train: G_Loss: 1.064, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.666, D_sup_loss: 0.167, D_sup_acc: 95.80 Train acc: 95.305 Test acc: 95.670 \n",
      "step: 1892 | Train: G_Loss: 1.112, D_unsup_loss_real: 0.667, D_unsup_loss_fake: 0.639, D_sup_loss: 0.168, D_sup_acc: 95.72 Train acc: 95.310 Test acc: 95.680 \n",
      "step: 1893 | Train: G_Loss: 1.028, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.566, D_sup_loss: 0.169, D_sup_acc: 95.73 Train acc: 95.245 Test acc: 95.760 \n",
      "step: 1894 | Train: G_Loss: 1.015, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.670, D_sup_loss: 0.168, D_sup_acc: 95.81 Train acc: 95.262 Test acc: 95.710 \n",
      "step: 1895 | Train: G_Loss: 1.128, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.605, D_sup_loss: 0.168, D_sup_acc: 95.76 Train acc: 95.330 Test acc: 95.730 \n",
      "step: 1896 | Train: G_Loss: 1.136, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.719, D_sup_loss: 0.168, D_sup_acc: 95.78 Train acc: 95.317 Test acc: 95.710 \n",
      "step: 1897 | Train: G_Loss: 1.096, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.681, D_sup_loss: 0.172, D_sup_acc: 95.76 Train acc: 95.332 Test acc: 95.690 \n",
      "step: 1898 | Train: G_Loss: 1.082, D_unsup_loss_real: 0.674, D_unsup_loss_fake: 0.575, D_sup_loss: 0.171, D_sup_acc: 95.74 Train acc: 95.343 Test acc: 95.690 \n",
      "step: 1899 | Train: G_Loss: 1.140, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.628, D_sup_loss: 0.169, D_sup_acc: 95.74 Train acc: 95.295 Test acc: 95.710 \n",
      "step: 1900 | Train: G_Loss: 1.014, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.640, D_sup_loss: 0.167, D_sup_acc: 95.76 Train acc: 95.288 Test acc: 95.700 \n",
      "Train Classifier Accuracy: 95.288%\n",
      "\n",
      "Test Classifier Accuracy: 95.700%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_1900.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_1900.h5\n",
      "step: 1901 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.683, D_sup_loss: 0.168, D_sup_acc: 95.75 Train acc: 95.198 Test acc: 95.620 \n",
      "step: 1902 | Train: G_Loss: 1.154, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.635, D_sup_loss: 0.170, D_sup_acc: 95.68 Train acc: 95.220 Test acc: 95.650 \n",
      "step: 1903 | Train: G_Loss: 1.106, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.599, D_sup_loss: 0.174, D_sup_acc: 95.70 Train acc: 95.318 Test acc: 95.800 \n",
      "step: 1904 | Train: G_Loss: 1.130, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.656, D_sup_loss: 0.169, D_sup_acc: 95.85 Train acc: 95.288 Test acc: 95.720 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1905 | Train: G_Loss: 1.100, D_unsup_loss_real: 0.667, D_unsup_loss_fake: 0.639, D_sup_loss: 0.169, D_sup_acc: 95.77 Train acc: 95.325 Test acc: 95.690 \n",
      "step: 1906 | Train: G_Loss: 1.111, D_unsup_loss_real: 0.657, D_unsup_loss_fake: 0.634, D_sup_loss: 0.170, D_sup_acc: 95.74 Train acc: 95.378 Test acc: 95.780 \n",
      "step: 1907 | Train: G_Loss: 1.113, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.609, D_sup_loss: 0.169, D_sup_acc: 95.83 Train acc: 95.338 Test acc: 95.760 \n",
      "step: 1908 | Train: G_Loss: 1.038, D_unsup_loss_real: 0.644, D_unsup_loss_fake: 0.652, D_sup_loss: 0.168, D_sup_acc: 95.81 Train acc: 95.323 Test acc: 95.790 \n",
      "step: 1909 | Train: G_Loss: 1.081, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.683, D_sup_loss: 0.170, D_sup_acc: 95.84 Train acc: 95.247 Test acc: 95.660 \n",
      "step: 1910 | Train: G_Loss: 1.033, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.595, D_sup_loss: 0.171, D_sup_acc: 95.71 Train acc: 95.242 Test acc: 95.690 \n",
      "step: 1911 | Train: G_Loss: 1.165, D_unsup_loss_real: 0.537, D_unsup_loss_fake: 0.636, D_sup_loss: 0.172, D_sup_acc: 95.74 Train acc: 95.233 Test acc: 95.660 \n",
      "step: 1912 | Train: G_Loss: 1.062, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.653, D_sup_loss: 0.170, D_sup_acc: 95.71 Train acc: 95.268 Test acc: 95.610 \n",
      "step: 1913 | Train: G_Loss: 1.080, D_unsup_loss_real: 0.674, D_unsup_loss_fake: 0.633, D_sup_loss: 0.172, D_sup_acc: 95.67 Train acc: 95.138 Test acc: 95.470 \n",
      "step: 1914 | Train: G_Loss: 1.070, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.625, D_sup_loss: 0.176, D_sup_acc: 95.53 Train acc: 95.330 Test acc: 95.780 \n",
      "step: 1915 | Train: G_Loss: 1.057, D_unsup_loss_real: 0.675, D_unsup_loss_fake: 0.758, D_sup_loss: 0.170, D_sup_acc: 95.83 Train acc: 95.348 Test acc: 95.830 \n",
      "step: 1916 | Train: G_Loss: 0.939, D_unsup_loss_real: 0.665, D_unsup_loss_fake: 0.715, D_sup_loss: 0.171, D_sup_acc: 95.88 Train acc: 95.385 Test acc: 95.800 \n",
      "step: 1917 | Train: G_Loss: 1.046, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.712, D_sup_loss: 0.171, D_sup_acc: 95.85 Train acc: 95.405 Test acc: 95.760 \n",
      "step: 1918 | Train: G_Loss: 1.181, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.671, D_sup_loss: 0.168, D_sup_acc: 95.81 Train acc: 95.407 Test acc: 95.770 \n",
      "step: 1919 | Train: G_Loss: 1.123, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.645, D_sup_loss: 0.171, D_sup_acc: 95.82 Train acc: 95.365 Test acc: 95.710 \n",
      "step: 1920 | Train: G_Loss: 1.128, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.612, D_sup_loss: 0.172, D_sup_acc: 95.76 Train acc: 95.318 Test acc: 95.610 \n",
      "step: 1921 | Train: G_Loss: 1.146, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.546, D_sup_loss: 0.176, D_sup_acc: 95.67 Train acc: 95.302 Test acc: 95.610 \n",
      "step: 1922 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.647, D_unsup_loss_fake: 0.646, D_sup_loss: 0.172, D_sup_acc: 95.67 Train acc: 95.263 Test acc: 95.480 \n",
      "step: 1923 | Train: G_Loss: 1.117, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.583, D_sup_loss: 0.172, D_sup_acc: 95.54 Train acc: 95.350 Test acc: 95.610 \n",
      "step: 1924 | Train: G_Loss: 1.114, D_unsup_loss_real: 0.682, D_unsup_loss_fake: 0.610, D_sup_loss: 0.172, D_sup_acc: 95.67 Train acc: 95.425 Test acc: 95.740 \n",
      "step: 1925 | Train: G_Loss: 1.129, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.638, D_sup_loss: 0.171, D_sup_acc: 95.79 Train acc: 95.347 Test acc: 95.780 \n",
      "step: 1926 | Train: G_Loss: 1.116, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.649, D_sup_loss: 0.174, D_sup_acc: 95.83 Train acc: 95.330 Test acc: 95.750 \n",
      "step: 1927 | Train: G_Loss: 1.089, D_unsup_loss_real: 0.664, D_unsup_loss_fake: 0.552, D_sup_loss: 0.179, D_sup_acc: 95.80 Train acc: 95.278 Test acc: 95.630 \n",
      "step: 1928 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.631, D_unsup_loss_fake: 0.639, D_sup_loss: 0.177, D_sup_acc: 95.69 Train acc: 95.355 Test acc: 95.780 \n",
      "step: 1929 | Train: G_Loss: 1.064, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.596, D_sup_loss: 0.172, D_sup_acc: 95.83 Train acc: 95.337 Test acc: 95.760 \n",
      "step: 1930 | Train: G_Loss: 1.064, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.616, D_sup_loss: 0.171, D_sup_acc: 95.81 Train acc: 95.328 Test acc: 95.670 \n",
      "step: 1931 | Train: G_Loss: 1.110, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.580, D_sup_loss: 0.171, D_sup_acc: 95.72 Train acc: 95.402 Test acc: 95.660 \n",
      "step: 1932 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.603, D_sup_loss: 0.168, D_sup_acc: 95.71 Train acc: 95.282 Test acc: 95.590 \n",
      "step: 1933 | Train: G_Loss: 1.151, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.650, D_sup_loss: 0.169, D_sup_acc: 95.65 Train acc: 95.268 Test acc: 95.620 \n",
      "step: 1934 | Train: G_Loss: 1.097, D_unsup_loss_real: 0.670, D_unsup_loss_fake: 0.656, D_sup_loss: 0.170, D_sup_acc: 95.68 Train acc: 95.145 Test acc: 95.590 \n",
      "step: 1935 | Train: G_Loss: 1.139, D_unsup_loss_real: 0.665, D_unsup_loss_fake: 0.623, D_sup_loss: 0.174, D_sup_acc: 95.65 Train acc: 95.152 Test acc: 95.590 \n",
      "step: 1936 | Train: G_Loss: 1.132, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.692, D_sup_loss: 0.175, D_sup_acc: 95.65 Train acc: 95.235 Test acc: 95.620 \n",
      "step: 1937 | Train: G_Loss: 1.129, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.569, D_sup_loss: 0.178, D_sup_acc: 95.68 Train acc: 95.273 Test acc: 95.710 \n",
      "step: 1938 | Train: G_Loss: 1.091, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.618, D_sup_loss: 0.175, D_sup_acc: 95.76 Train acc: 95.352 Test acc: 95.800 \n",
      "step: 1939 | Train: G_Loss: 1.152, D_unsup_loss_real: 0.477, D_unsup_loss_fake: 0.620, D_sup_loss: 0.170, D_sup_acc: 95.85 Train acc: 95.385 Test acc: 95.840 \n",
      "step: 1940 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.682, D_sup_loss: 0.168, D_sup_acc: 95.89 Train acc: 95.340 Test acc: 95.840 \n",
      "step: 1941 | Train: G_Loss: 1.096, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.604, D_sup_loss: 0.170, D_sup_acc: 95.89 Train acc: 95.390 Test acc: 95.710 \n",
      "step: 1942 | Train: G_Loss: 1.173, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.555, D_sup_loss: 0.172, D_sup_acc: 95.76 Train acc: 95.320 Test acc: 95.730 \n",
      "step: 1943 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.684, D_sup_loss: 0.173, D_sup_acc: 95.78 Train acc: 95.425 Test acc: 95.800 \n",
      "step: 1944 | Train: G_Loss: 0.994, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.605, D_sup_loss: 0.170, D_sup_acc: 95.85 Train acc: 95.312 Test acc: 95.730 \n",
      "step: 1945 | Train: G_Loss: 1.089, D_unsup_loss_real: 0.724, D_unsup_loss_fake: 0.660, D_sup_loss: 0.167, D_sup_acc: 95.78 Train acc: 95.272 Test acc: 95.640 \n",
      "step: 1946 | Train: G_Loss: 1.080, D_unsup_loss_real: 0.677, D_unsup_loss_fake: 0.661, D_sup_loss: 0.170, D_sup_acc: 95.70 Train acc: 95.170 Test acc: 95.670 \n",
      "step: 1947 | Train: G_Loss: 1.068, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.657, D_sup_loss: 0.171, D_sup_acc: 95.72 Train acc: 95.223 Test acc: 95.730 \n",
      "step: 1948 | Train: G_Loss: 1.078, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.645, D_sup_loss: 0.170, D_sup_acc: 95.78 Train acc: 95.327 Test acc: 95.760 \n",
      "step: 1949 | Train: G_Loss: 1.052, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.605, D_sup_loss: 0.169, D_sup_acc: 95.81 Train acc: 95.408 Test acc: 95.760 \n",
      "step: 1950 | Train: G_Loss: 1.113, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.653, D_sup_loss: 0.169, D_sup_acc: 95.81 Train acc: 95.393 Test acc: 95.660 \n",
      "step: 1951 | Train: G_Loss: 1.022, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.605, D_sup_loss: 0.169, D_sup_acc: 95.71 Train acc: 95.422 Test acc: 95.720 \n",
      "step: 1952 | Train: G_Loss: 1.020, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.708, D_sup_loss: 0.167, D_sup_acc: 95.77 Train acc: 95.337 Test acc: 95.600 \n",
      "step: 1953 | Train: G_Loss: 1.137, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.621, D_sup_loss: 0.174, D_sup_acc: 95.66 Train acc: 95.330 Test acc: 95.660 \n",
      "step: 1954 | Train: G_Loss: 1.088, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.618, D_sup_loss: 0.174, D_sup_acc: 95.71 Train acc: 95.430 Test acc: 95.730 \n",
      "step: 1955 | Train: G_Loss: 1.083, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.639, D_sup_loss: 0.169, D_sup_acc: 95.78 Train acc: 95.403 Test acc: 95.670 \n",
      "step: 1956 | Train: G_Loss: 1.087, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.656, D_sup_loss: 0.168, D_sup_acc: 95.72 Train acc: 95.432 Test acc: 95.740 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1957 | Train: G_Loss: 1.132, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.648, D_sup_loss: 0.168, D_sup_acc: 95.79 Train acc: 95.432 Test acc: 95.800 \n",
      "step: 1958 | Train: G_Loss: 1.067, D_unsup_loss_real: 0.635, D_unsup_loss_fake: 0.533, D_sup_loss: 0.169, D_sup_acc: 95.85 Train acc: 95.448 Test acc: 95.800 \n",
      "step: 1959 | Train: G_Loss: 1.054, D_unsup_loss_real: 0.654, D_unsup_loss_fake: 0.655, D_sup_loss: 0.165, D_sup_acc: 95.85 Train acc: 95.395 Test acc: 95.800 \n",
      "step: 1960 | Train: G_Loss: 1.051, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.711, D_sup_loss: 0.171, D_sup_acc: 95.85 Train acc: 95.417 Test acc: 95.680 \n",
      "step: 1961 | Train: G_Loss: 1.062, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.629, D_sup_loss: 0.171, D_sup_acc: 95.73 Train acc: 95.467 Test acc: 95.790 \n",
      "step: 1962 | Train: G_Loss: 1.099, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.623, D_sup_loss: 0.169, D_sup_acc: 95.84 Train acc: 95.385 Test acc: 95.690 \n",
      "step: 1963 | Train: G_Loss: 1.015, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.675, D_sup_loss: 0.173, D_sup_acc: 95.74 Train acc: 95.277 Test acc: 95.730 \n",
      "step: 1964 | Train: G_Loss: 1.149, D_unsup_loss_real: 0.646, D_unsup_loss_fake: 0.535, D_sup_loss: 0.177, D_sup_acc: 95.78 Train acc: 95.243 Test acc: 95.630 \n",
      "step: 1965 | Train: G_Loss: 1.080, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.606, D_sup_loss: 0.177, D_sup_acc: 95.69 Train acc: 95.358 Test acc: 95.790 \n",
      "step: 1966 | Train: G_Loss: 1.058, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.592, D_sup_loss: 0.173, D_sup_acc: 95.84 Train acc: 95.348 Test acc: 95.770 \n",
      "step: 1967 | Train: G_Loss: 1.056, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.630, D_sup_loss: 0.172, D_sup_acc: 95.82 Train acc: 95.408 Test acc: 95.780 \n",
      "step: 1968 | Train: G_Loss: 1.090, D_unsup_loss_real: 0.675, D_unsup_loss_fake: 0.661, D_sup_loss: 0.173, D_sup_acc: 95.83 Train acc: 95.382 Test acc: 95.790 \n",
      "step: 1969 | Train: G_Loss: 1.018, D_unsup_loss_real: 0.545, D_unsup_loss_fake: 0.584, D_sup_loss: 0.172, D_sup_acc: 95.84 Train acc: 95.415 Test acc: 95.830 \n",
      "step: 1970 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.635, D_unsup_loss_fake: 0.600, D_sup_loss: 0.173, D_sup_acc: 95.88 Train acc: 95.350 Test acc: 95.810 \n",
      "step: 1971 | Train: G_Loss: 1.075, D_unsup_loss_real: 0.720, D_unsup_loss_fake: 0.702, D_sup_loss: 0.172, D_sup_acc: 95.86 Train acc: 95.360 Test acc: 95.740 \n",
      "step: 1972 | Train: G_Loss: 1.071, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.691, D_sup_loss: 0.172, D_sup_acc: 95.79 Train acc: 95.345 Test acc: 95.850 \n",
      "step: 1973 | Train: G_Loss: 1.050, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.565, D_sup_loss: 0.174, D_sup_acc: 95.90 Train acc: 95.423 Test acc: 95.860 \n",
      "step: 1974 | Train: G_Loss: 1.111, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.618, D_sup_loss: 0.168, D_sup_acc: 95.91 Train acc: 95.417 Test acc: 95.740 \n",
      "step: 1975 | Train: G_Loss: 1.107, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.602, D_sup_loss: 0.167, D_sup_acc: 95.79 Train acc: 95.435 Test acc: 95.770 \n",
      "step: 1976 | Train: G_Loss: 1.271, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.697, D_sup_loss: 0.165, D_sup_acc: 95.82 Train acc: 95.388 Test acc: 95.780 \n",
      "step: 1977 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.649, D_unsup_loss_fake: 0.557, D_sup_loss: 0.166, D_sup_acc: 95.83 Train acc: 95.402 Test acc: 95.770 \n",
      "step: 1978 | Train: G_Loss: 1.084, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.641, D_sup_loss: 0.165, D_sup_acc: 95.82 Train acc: 95.462 Test acc: 95.860 \n",
      "step: 1979 | Train: G_Loss: 1.107, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.592, D_sup_loss: 0.163, D_sup_acc: 95.91 Train acc: 95.453 Test acc: 95.860 \n",
      "step: 1980 | Train: G_Loss: 1.086, D_unsup_loss_real: 0.647, D_unsup_loss_fake: 0.779, D_sup_loss: 0.163, D_sup_acc: 95.91 Train acc: 95.420 Test acc: 95.820 \n",
      "step: 1981 | Train: G_Loss: 1.081, D_unsup_loss_real: 0.656, D_unsup_loss_fake: 0.629, D_sup_loss: 0.165, D_sup_acc: 95.87 Train acc: 95.397 Test acc: 95.730 \n",
      "step: 1982 | Train: G_Loss: 1.023, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.635, D_sup_loss: 0.170, D_sup_acc: 95.78 Train acc: 95.332 Test acc: 95.680 \n",
      "step: 1983 | Train: G_Loss: 1.082, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.625, D_sup_loss: 0.172, D_sup_acc: 95.73 Train acc: 95.378 Test acc: 95.600 \n",
      "step: 1984 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.636, D_sup_loss: 0.174, D_sup_acc: 95.66 Train acc: 95.372 Test acc: 95.680 \n",
      "step: 1985 | Train: G_Loss: 1.179, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.623, D_sup_loss: 0.173, D_sup_acc: 95.73 Train acc: 95.357 Test acc: 95.680 \n",
      "step: 1986 | Train: G_Loss: 1.156, D_unsup_loss_real: 0.499, D_unsup_loss_fake: 0.558, D_sup_loss: 0.173, D_sup_acc: 95.73 Train acc: 95.385 Test acc: 95.820 \n",
      "step: 1987 | Train: G_Loss: 1.156, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.585, D_sup_loss: 0.169, D_sup_acc: 95.87 Train acc: 95.465 Test acc: 95.810 \n",
      "step: 1988 | Train: G_Loss: 1.178, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.602, D_sup_loss: 0.168, D_sup_acc: 95.86 Train acc: 95.410 Test acc: 95.820 \n",
      "step: 1989 | Train: G_Loss: 1.191, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.573, D_sup_loss: 0.167, D_sup_acc: 95.87 Train acc: 95.405 Test acc: 95.730 \n",
      "step: 1990 | Train: G_Loss: 1.208, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.604, D_sup_loss: 0.163, D_sup_acc: 95.78 Train acc: 95.390 Test acc: 95.740 \n",
      "step: 1991 | Train: G_Loss: 1.097, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.642, D_sup_loss: 0.164, D_sup_acc: 95.79 Train acc: 95.337 Test acc: 95.630 \n",
      "step: 1992 | Train: G_Loss: 1.090, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.611, D_sup_loss: 0.167, D_sup_acc: 95.69 Train acc: 95.345 Test acc: 95.630 \n",
      "step: 1993 | Train: G_Loss: 1.199, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.644, D_sup_loss: 0.167, D_sup_acc: 95.69 Train acc: 95.223 Test acc: 95.620 \n",
      "step: 1994 | Train: G_Loss: 1.222, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.527, D_sup_loss: 0.171, D_sup_acc: 95.68 Train acc: 95.242 Test acc: 95.610 \n",
      "step: 1995 | Train: G_Loss: 1.130, D_unsup_loss_real: 0.761, D_unsup_loss_fake: 0.667, D_sup_loss: 0.171, D_sup_acc: 95.67 Train acc: 95.355 Test acc: 95.680 \n",
      "step: 1996 | Train: G_Loss: 1.059, D_unsup_loss_real: 0.656, D_unsup_loss_fake: 0.636, D_sup_loss: 0.170, D_sup_acc: 95.73 Train acc: 95.335 Test acc: 95.710 \n",
      "step: 1997 | Train: G_Loss: 1.051, D_unsup_loss_real: 0.692, D_unsup_loss_fake: 0.654, D_sup_loss: 0.170, D_sup_acc: 95.76 Train acc: 95.338 Test acc: 95.760 \n",
      "step: 1998 | Train: G_Loss: 1.034, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.608, D_sup_loss: 0.171, D_sup_acc: 95.81 Train acc: 95.362 Test acc: 95.750 \n",
      "step: 1999 | Train: G_Loss: 1.133, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.622, D_sup_loss: 0.172, D_sup_acc: 95.80 Train acc: 95.430 Test acc: 95.810 \n",
      "step: 2000 | Train: G_Loss: 1.087, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.684, D_sup_loss: 0.167, D_sup_acc: 95.86 Train acc: 95.535 Test acc: 95.890 \n",
      "Train Classifier Accuracy: 95.535%\n",
      "\n",
      "Test Classifier Accuracy: 95.890%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_2000.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_2000.h5\n",
      "step: 2001 | Train: G_Loss: 0.986, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.672, D_sup_loss: 0.164, D_sup_acc: 95.94 Train acc: 95.538 Test acc: 95.850 \n",
      "step: 2002 | Train: G_Loss: 1.055, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.715, D_sup_loss: 0.166, D_sup_acc: 95.90 Train acc: 95.592 Test acc: 95.900 \n",
      "step: 2003 | Train: G_Loss: 1.084, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.667, D_sup_loss: 0.166, D_sup_acc: 95.95 Train acc: 95.493 Test acc: 95.860 \n",
      "step: 2004 | Train: G_Loss: 1.119, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.596, D_sup_loss: 0.167, D_sup_acc: 95.91 Train acc: 95.475 Test acc: 95.780 \n",
      "step: 2005 | Train: G_Loss: 1.147, D_unsup_loss_real: 0.714, D_unsup_loss_fake: 0.618, D_sup_loss: 0.169, D_sup_acc: 95.83 Train acc: 95.473 Test acc: 95.850 \n",
      "step: 2006 | Train: G_Loss: 1.015, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.700, D_sup_loss: 0.173, D_sup_acc: 95.90 Train acc: 95.452 Test acc: 95.840 \n",
      "step: 2007 | Train: G_Loss: 1.059, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.632, D_sup_loss: 0.172, D_sup_acc: 95.89 Train acc: 95.405 Test acc: 95.820 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2008 | Train: G_Loss: 1.029, D_unsup_loss_real: 0.656, D_unsup_loss_fake: 0.624, D_sup_loss: 0.170, D_sup_acc: 95.87 Train acc: 95.467 Test acc: 95.900 \n",
      "step: 2009 | Train: G_Loss: 1.096, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.670, D_sup_loss: 0.168, D_sup_acc: 95.95 Train acc: 95.508 Test acc: 95.890 \n",
      "step: 2010 | Train: G_Loss: 1.025, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.632, D_sup_loss: 0.170, D_sup_acc: 95.94 Train acc: 95.437 Test acc: 95.820 \n",
      "step: 2011 | Train: G_Loss: 1.139, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.586, D_sup_loss: 0.168, D_sup_acc: 95.87 Train acc: 95.358 Test acc: 95.760 \n",
      "step: 2012 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.534, D_unsup_loss_fake: 0.604, D_sup_loss: 0.170, D_sup_acc: 95.81 Train acc: 95.383 Test acc: 95.790 \n",
      "step: 2013 | Train: G_Loss: 1.018, D_unsup_loss_real: 0.669, D_unsup_loss_fake: 0.602, D_sup_loss: 0.168, D_sup_acc: 95.84 Train acc: 95.323 Test acc: 95.740 \n",
      "step: 2014 | Train: G_Loss: 1.052, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.608, D_sup_loss: 0.169, D_sup_acc: 95.79 Train acc: 95.383 Test acc: 95.900 \n",
      "step: 2015 | Train: G_Loss: 1.008, D_unsup_loss_real: 0.654, D_unsup_loss_fake: 0.600, D_sup_loss: 0.169, D_sup_acc: 95.95 Train acc: 95.520 Test acc: 95.880 \n",
      "step: 2016 | Train: G_Loss: 1.083, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.672, D_sup_loss: 0.164, D_sup_acc: 95.93 Train acc: 95.413 Test acc: 95.780 \n",
      "step: 2017 | Train: G_Loss: 1.000, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.616, D_sup_loss: 0.167, D_sup_acc: 95.83 Train acc: 95.313 Test acc: 95.620 \n",
      "step: 2018 | Train: G_Loss: 0.969, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.604, D_sup_loss: 0.170, D_sup_acc: 95.68 Train acc: 95.353 Test acc: 95.700 \n",
      "step: 2019 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.678, D_sup_loss: 0.168, D_sup_acc: 95.75 Train acc: 95.415 Test acc: 95.790 \n",
      "step: 2020 | Train: G_Loss: 1.133, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.653, D_sup_loss: 0.167, D_sup_acc: 95.84 Train acc: 95.432 Test acc: 95.790 \n",
      "step: 2021 | Train: G_Loss: 1.169, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.592, D_sup_loss: 0.167, D_sup_acc: 95.84 Train acc: 95.525 Test acc: 95.900 \n",
      "step: 2022 | Train: G_Loss: 1.119, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.550, D_sup_loss: 0.166, D_sup_acc: 95.95 Train acc: 95.578 Test acc: 95.920 \n",
      "step: 2023 | Train: G_Loss: 1.034, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.637, D_sup_loss: 0.163, D_sup_acc: 95.97 Train acc: 95.590 Test acc: 95.910 \n",
      "step: 2024 | Train: G_Loss: 1.115, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.564, D_sup_loss: 0.162, D_sup_acc: 95.96 Train acc: 95.578 Test acc: 95.860 \n",
      "step: 2025 | Train: G_Loss: 1.008, D_unsup_loss_real: 0.659, D_unsup_loss_fake: 0.644, D_sup_loss: 0.161, D_sup_acc: 95.91 Train acc: 95.595 Test acc: 95.890 \n",
      "step: 2026 | Train: G_Loss: 1.085, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.645, D_sup_loss: 0.162, D_sup_acc: 95.94 Train acc: 95.485 Test acc: 95.810 \n",
      "step: 2027 | Train: G_Loss: 0.942, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.621, D_sup_loss: 0.169, D_sup_acc: 95.86 Train acc: 95.497 Test acc: 95.870 \n",
      "step: 2028 | Train: G_Loss: 1.086, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.699, D_sup_loss: 0.167, D_sup_acc: 95.92 Train acc: 95.455 Test acc: 95.840 \n",
      "step: 2029 | Train: G_Loss: 1.123, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.577, D_sup_loss: 0.168, D_sup_acc: 95.89 Train acc: 95.515 Test acc: 95.870 \n",
      "step: 2030 | Train: G_Loss: 1.078, D_unsup_loss_real: 0.545, D_unsup_loss_fake: 0.631, D_sup_loss: 0.164, D_sup_acc: 95.92 Train acc: 95.478 Test acc: 95.810 \n",
      "step: 2031 | Train: G_Loss: 1.068, D_unsup_loss_real: 0.653, D_unsup_loss_fake: 0.642, D_sup_loss: 0.165, D_sup_acc: 95.86 Train acc: 95.485 Test acc: 95.780 \n",
      "step: 2032 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.694, D_sup_loss: 0.167, D_sup_acc: 95.83 Train acc: 95.187 Test acc: 95.690 \n",
      "step: 2033 | Train: G_Loss: 1.056, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.569, D_sup_loss: 0.181, D_sup_acc: 95.74 Train acc: 95.350 Test acc: 95.650 \n",
      "step: 2034 | Train: G_Loss: 1.066, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.639, D_sup_loss: 0.172, D_sup_acc: 95.70 Train acc: 95.370 Test acc: 95.780 \n",
      "step: 2035 | Train: G_Loss: 1.122, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.699, D_sup_loss: 0.170, D_sup_acc: 95.83 Train acc: 95.450 Test acc: 95.910 \n",
      "step: 2036 | Train: G_Loss: 1.139, D_unsup_loss_real: 0.657, D_unsup_loss_fake: 0.655, D_sup_loss: 0.170, D_sup_acc: 95.96 Train acc: 95.393 Test acc: 95.680 \n",
      "step: 2037 | Train: G_Loss: 1.133, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.613, D_sup_loss: 0.168, D_sup_acc: 95.73 Train acc: 95.515 Test acc: 95.800 \n",
      "step: 2038 | Train: G_Loss: 1.128, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.641, D_sup_loss: 0.165, D_sup_acc: 95.85 Train acc: 95.458 Test acc: 95.830 \n",
      "step: 2039 | Train: G_Loss: 1.029, D_unsup_loss_real: 0.707, D_unsup_loss_fake: 0.656, D_sup_loss: 0.169, D_sup_acc: 95.88 Train acc: 95.430 Test acc: 95.770 \n",
      "step: 2040 | Train: G_Loss: 1.033, D_unsup_loss_real: 0.661, D_unsup_loss_fake: 0.598, D_sup_loss: 0.172, D_sup_acc: 95.82 Train acc: 95.512 Test acc: 95.840 \n",
      "step: 2041 | Train: G_Loss: 1.080, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.693, D_sup_loss: 0.167, D_sup_acc: 95.89 Train acc: 95.522 Test acc: 95.910 \n",
      "step: 2042 | Train: G_Loss: 1.136, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.730, D_sup_loss: 0.168, D_sup_acc: 95.96 Train acc: 95.422 Test acc: 95.800 \n",
      "step: 2043 | Train: G_Loss: 1.111, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.630, D_sup_loss: 0.170, D_sup_acc: 95.85 Train acc: 95.445 Test acc: 95.860 \n",
      "step: 2044 | Train: G_Loss: 1.052, D_unsup_loss_real: 0.667, D_unsup_loss_fake: 0.693, D_sup_loss: 0.167, D_sup_acc: 95.91 Train acc: 95.417 Test acc: 95.880 \n",
      "step: 2045 | Train: G_Loss: 1.083, D_unsup_loss_real: 0.713, D_unsup_loss_fake: 0.636, D_sup_loss: 0.168, D_sup_acc: 95.93 Train acc: 95.407 Test acc: 95.830 \n",
      "step: 2046 | Train: G_Loss: 1.088, D_unsup_loss_real: 0.695, D_unsup_loss_fake: 0.614, D_sup_loss: 0.170, D_sup_acc: 95.88 Train acc: 95.408 Test acc: 95.840 \n",
      "step: 2047 | Train: G_Loss: 0.918, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.614, D_sup_loss: 0.170, D_sup_acc: 95.89 Train acc: 95.410 Test acc: 95.830 \n",
      "step: 2048 | Train: G_Loss: 1.035, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.612, D_sup_loss: 0.171, D_sup_acc: 95.88 Train acc: 95.395 Test acc: 95.800 \n",
      "step: 2049 | Train: G_Loss: 1.056, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.627, D_sup_loss: 0.170, D_sup_acc: 95.85 Train acc: 95.435 Test acc: 95.830 \n",
      "step: 2050 | Train: G_Loss: 1.011, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.624, D_sup_loss: 0.169, D_sup_acc: 95.88 Train acc: 95.470 Test acc: 95.880 \n",
      "step: 2051 | Train: G_Loss: 1.107, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.684, D_sup_loss: 0.169, D_sup_acc: 95.93 Train acc: 95.420 Test acc: 95.820 \n",
      "step: 2052 | Train: G_Loss: 1.057, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.652, D_sup_loss: 0.175, D_sup_acc: 95.87 Train acc: 95.425 Test acc: 95.780 \n",
      "step: 2053 | Train: G_Loss: 1.200, D_unsup_loss_real: 0.662, D_unsup_loss_fake: 0.658, D_sup_loss: 0.172, D_sup_acc: 95.83 Train acc: 95.388 Test acc: 95.710 \n",
      "step: 2054 | Train: G_Loss: 1.067, D_unsup_loss_real: 0.656, D_unsup_loss_fake: 0.626, D_sup_loss: 0.175, D_sup_acc: 95.76 Train acc: 95.370 Test acc: 95.630 \n",
      "step: 2055 | Train: G_Loss: 1.119, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.700, D_sup_loss: 0.178, D_sup_acc: 95.69 Train acc: 95.292 Test acc: 95.590 \n",
      "step: 2056 | Train: G_Loss: 1.059, D_unsup_loss_real: 0.673, D_unsup_loss_fake: 0.618, D_sup_loss: 0.179, D_sup_acc: 95.65 Train acc: 95.363 Test acc: 95.580 \n",
      "step: 2057 | Train: G_Loss: 1.044, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.541, D_sup_loss: 0.176, D_sup_acc: 95.64 Train acc: 95.430 Test acc: 95.730 \n",
      "step: 2058 | Train: G_Loss: 1.112, D_unsup_loss_real: 0.507, D_unsup_loss_fake: 0.585, D_sup_loss: 0.167, D_sup_acc: 95.78 Train acc: 95.523 Test acc: 95.760 \n",
      "step: 2059 | Train: G_Loss: 1.110, D_unsup_loss_real: 0.628, D_unsup_loss_fake: 0.648, D_sup_loss: 0.164, D_sup_acc: 95.81 Train acc: 95.548 Test acc: 95.830 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2060 | Train: G_Loss: 1.068, D_unsup_loss_real: 0.662, D_unsup_loss_fake: 0.790, D_sup_loss: 0.163, D_sup_acc: 95.88 Train acc: 95.560 Test acc: 95.890 \n",
      "step: 2061 | Train: G_Loss: 1.125, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.693, D_sup_loss: 0.168, D_sup_acc: 95.94 Train acc: 95.592 Test acc: 95.830 \n",
      "step: 2062 | Train: G_Loss: 1.039, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.645, D_sup_loss: 0.171, D_sup_acc: 95.88 Train acc: 95.617 Test acc: 95.940 \n",
      "step: 2063 | Train: G_Loss: 1.150, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.672, D_sup_loss: 0.171, D_sup_acc: 95.99 Train acc: 95.542 Test acc: 95.910 \n",
      "step: 2064 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.634, D_sup_loss: 0.176, D_sup_acc: 95.96 Train acc: 95.497 Test acc: 95.860 \n",
      "step: 2065 | Train: G_Loss: 1.070, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.659, D_sup_loss: 0.172, D_sup_acc: 95.91 Train acc: 95.515 Test acc: 95.890 \n",
      "step: 2066 | Train: G_Loss: 1.111, D_unsup_loss_real: 0.650, D_unsup_loss_fake: 0.604, D_sup_loss: 0.173, D_sup_acc: 95.94 Train acc: 95.515 Test acc: 95.910 \n",
      "step: 2067 | Train: G_Loss: 1.090, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.611, D_sup_loss: 0.171, D_sup_acc: 95.96 Train acc: 95.348 Test acc: 95.780 \n",
      "step: 2068 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.646, D_unsup_loss_fake: 0.599, D_sup_loss: 0.178, D_sup_acc: 95.83 Train acc: 95.420 Test acc: 95.840 \n",
      "step: 2069 | Train: G_Loss: 1.042, D_unsup_loss_real: 0.663, D_unsup_loss_fake: 0.661, D_sup_loss: 0.173, D_sup_acc: 95.89 Train acc: 95.538 Test acc: 95.870 \n",
      "step: 2070 | Train: G_Loss: 1.056, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.636, D_sup_loss: 0.166, D_sup_acc: 95.92 Train acc: 95.568 Test acc: 95.940 \n",
      "step: 2071 | Train: G_Loss: 1.048, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.719, D_sup_loss: 0.163, D_sup_acc: 95.99 Train acc: 95.558 Test acc: 95.900 \n",
      "step: 2072 | Train: G_Loss: 1.072, D_unsup_loss_real: 0.660, D_unsup_loss_fake: 0.673, D_sup_loss: 0.164, D_sup_acc: 95.95 Train acc: 95.430 Test acc: 95.670 \n",
      "step: 2073 | Train: G_Loss: 1.058, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.635, D_sup_loss: 0.171, D_sup_acc: 95.72 Train acc: 95.435 Test acc: 95.640 \n",
      "step: 2074 | Train: G_Loss: 1.034, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.618, D_sup_loss: 0.171, D_sup_acc: 95.70 Train acc: 95.373 Test acc: 95.650 \n",
      "step: 2075 | Train: G_Loss: 1.129, D_unsup_loss_real: 0.691, D_unsup_loss_fake: 0.652, D_sup_loss: 0.172, D_sup_acc: 95.70 Train acc: 95.415 Test acc: 95.670 \n",
      "step: 2076 | Train: G_Loss: 1.080, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.609, D_sup_loss: 0.174, D_sup_acc: 95.72 Train acc: 95.297 Test acc: 95.520 \n",
      "step: 2077 | Train: G_Loss: 1.066, D_unsup_loss_real: 0.659, D_unsup_loss_fake: 0.622, D_sup_loss: 0.177, D_sup_acc: 95.58 Train acc: 95.455 Test acc: 95.700 \n",
      "step: 2078 | Train: G_Loss: 1.049, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.637, D_sup_loss: 0.169, D_sup_acc: 95.75 Train acc: 95.483 Test acc: 95.690 \n",
      "step: 2079 | Train: G_Loss: 1.088, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.536, D_sup_loss: 0.167, D_sup_acc: 95.74 Train acc: 95.477 Test acc: 95.760 \n",
      "step: 2080 | Train: G_Loss: 1.039, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.636, D_sup_loss: 0.165, D_sup_acc: 95.81 Train acc: 95.460 Test acc: 95.850 \n",
      "step: 2081 | Train: G_Loss: 1.058, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.652, D_sup_loss: 0.166, D_sup_acc: 95.90 Train acc: 95.333 Test acc: 95.750 \n",
      "step: 2082 | Train: G_Loss: 1.079, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.645, D_sup_loss: 0.174, D_sup_acc: 95.80 Train acc: 95.348 Test acc: 95.670 \n",
      "step: 2083 | Train: G_Loss: 1.014, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.611, D_sup_loss: 0.170, D_sup_acc: 95.72 Train acc: 95.408 Test acc: 95.670 \n",
      "step: 2084 | Train: G_Loss: 1.021, D_unsup_loss_real: 0.646, D_unsup_loss_fake: 0.625, D_sup_loss: 0.169, D_sup_acc: 95.72 Train acc: 95.413 Test acc: 95.630 \n",
      "step: 2085 | Train: G_Loss: 1.042, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.631, D_sup_loss: 0.168, D_sup_acc: 95.69 Train acc: 95.515 Test acc: 95.810 \n",
      "step: 2086 | Train: G_Loss: 1.038, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.636, D_sup_loss: 0.166, D_sup_acc: 95.86 Train acc: 95.480 Test acc: 95.740 \n",
      "step: 2087 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.652, D_unsup_loss_fake: 0.626, D_sup_loss: 0.168, D_sup_acc: 95.79 Train acc: 95.435 Test acc: 95.690 \n",
      "step: 2088 | Train: G_Loss: 1.091, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.608, D_sup_loss: 0.169, D_sup_acc: 95.74 Train acc: 95.462 Test acc: 95.740 \n",
      "step: 2089 | Train: G_Loss: 1.076, D_unsup_loss_real: 0.651, D_unsup_loss_fake: 0.602, D_sup_loss: 0.167, D_sup_acc: 95.79 Train acc: 95.462 Test acc: 95.740 \n",
      "step: 2090 | Train: G_Loss: 1.064, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.571, D_sup_loss: 0.168, D_sup_acc: 95.79 Train acc: 95.482 Test acc: 95.710 \n",
      "step: 2091 | Train: G_Loss: 0.981, D_unsup_loss_real: 0.659, D_unsup_loss_fake: 0.643, D_sup_loss: 0.168, D_sup_acc: 95.76 Train acc: 95.308 Test acc: 95.600 \n",
      "step: 2092 | Train: G_Loss: 1.030, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.648, D_sup_loss: 0.173, D_sup_acc: 95.66 Train acc: 95.402 Test acc: 95.600 \n",
      "step: 2093 | Train: G_Loss: 1.059, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.646, D_sup_loss: 0.171, D_sup_acc: 95.66 Train acc: 95.313 Test acc: 95.620 \n",
      "step: 2094 | Train: G_Loss: 1.045, D_unsup_loss_real: 0.646, D_unsup_loss_fake: 0.656, D_sup_loss: 0.173, D_sup_acc: 95.68 Train acc: 95.452 Test acc: 95.710 \n",
      "step: 2095 | Train: G_Loss: 1.051, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.674, D_sup_loss: 0.166, D_sup_acc: 95.76 Train acc: 95.487 Test acc: 95.700 \n",
      "step: 2096 | Train: G_Loss: 1.020, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.605, D_sup_loss: 0.164, D_sup_acc: 95.75 Train acc: 95.428 Test acc: 95.760 \n",
      "step: 2097 | Train: G_Loss: 1.031, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.622, D_sup_loss: 0.165, D_sup_acc: 95.81 Train acc: 95.363 Test acc: 95.710 \n",
      "step: 2098 | Train: G_Loss: 1.030, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.609, D_sup_loss: 0.171, D_sup_acc: 95.76 Train acc: 95.378 Test acc: 95.670 \n",
      "step: 2099 | Train: G_Loss: 1.074, D_unsup_loss_real: 0.663, D_unsup_loss_fake: 0.642, D_sup_loss: 0.168, D_sup_acc: 95.72 Train acc: 95.467 Test acc: 95.780 \n",
      "step: 2100 | Train: G_Loss: 1.152, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.577, D_sup_loss: 0.167, D_sup_acc: 95.83 Train acc: 95.437 Test acc: 95.670 \n",
      "Train Classifier Accuracy: 95.437%\n",
      "\n",
      "Test Classifier Accuracy: 95.670%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_2100.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_2100.h5\n",
      "step: 2101 | Train: G_Loss: 1.045, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.604, D_sup_loss: 0.168, D_sup_acc: 95.72 Train acc: 95.285 Test acc: 95.550 \n",
      "step: 2102 | Train: G_Loss: 1.067, D_unsup_loss_real: 0.668, D_unsup_loss_fake: 0.629, D_sup_loss: 0.165, D_sup_acc: 95.61 Train acc: 95.408 Test acc: 95.550 \n",
      "step: 2103 | Train: G_Loss: 1.085, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.619, D_sup_loss: 0.171, D_sup_acc: 95.61 Train acc: 95.287 Test acc: 95.690 \n",
      "step: 2104 | Train: G_Loss: 0.997, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.691, D_sup_loss: 0.174, D_sup_acc: 95.74 Train acc: 95.275 Test acc: 95.650 \n",
      "step: 2105 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.620, D_sup_loss: 0.176, D_sup_acc: 95.70 Train acc: 95.308 Test acc: 95.610 \n",
      "step: 2106 | Train: G_Loss: 1.028, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.710, D_sup_loss: 0.172, D_sup_acc: 95.67 Train acc: 95.288 Test acc: 95.530 \n",
      "step: 2107 | Train: G_Loss: 1.167, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.654, D_sup_loss: 0.172, D_sup_acc: 95.59 Train acc: 95.342 Test acc: 95.640 \n",
      "step: 2108 | Train: G_Loss: 1.076, D_unsup_loss_real: 0.687, D_unsup_loss_fake: 0.597, D_sup_loss: 0.173, D_sup_acc: 95.70 Train acc: 95.347 Test acc: 95.580 \n",
      "step: 2109 | Train: G_Loss: 1.025, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.616, D_sup_loss: 0.174, D_sup_acc: 95.64 Train acc: 95.333 Test acc: 95.620 \n",
      "step: 2110 | Train: G_Loss: 1.083, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.600, D_sup_loss: 0.176, D_sup_acc: 95.68 Train acc: 95.300 Test acc: 95.540 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2111 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.610, D_sup_loss: 0.175, D_sup_acc: 95.60 Train acc: 95.323 Test acc: 95.600 \n",
      "step: 2112 | Train: G_Loss: 1.128, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.586, D_sup_loss: 0.173, D_sup_acc: 95.66 Train acc: 95.398 Test acc: 95.690 \n",
      "step: 2113 | Train: G_Loss: 1.007, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.640, D_sup_loss: 0.169, D_sup_acc: 95.74 Train acc: 95.467 Test acc: 95.830 \n",
      "step: 2114 | Train: G_Loss: 1.037, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.649, D_sup_loss: 0.167, D_sup_acc: 95.88 Train acc: 95.475 Test acc: 95.750 \n",
      "step: 2115 | Train: G_Loss: 1.056, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.608, D_sup_loss: 0.170, D_sup_acc: 95.80 Train acc: 95.577 Test acc: 95.750 \n",
      "step: 2116 | Train: G_Loss: 1.116, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.656, D_sup_loss: 0.165, D_sup_acc: 95.80 Train acc: 95.597 Test acc: 95.820 \n",
      "step: 2117 | Train: G_Loss: 1.040, D_unsup_loss_real: 0.657, D_unsup_loss_fake: 0.655, D_sup_loss: 0.165, D_sup_acc: 95.87 Train acc: 95.552 Test acc: 95.670 \n",
      "step: 2118 | Train: G_Loss: 1.216, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.726, D_sup_loss: 0.169, D_sup_acc: 95.72 Train acc: 95.387 Test acc: 95.650 \n",
      "step: 2119 | Train: G_Loss: 1.092, D_unsup_loss_real: 0.697, D_unsup_loss_fake: 0.710, D_sup_loss: 0.176, D_sup_acc: 95.70 Train acc: 95.445 Test acc: 95.640 \n",
      "step: 2120 | Train: G_Loss: 1.013, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.585, D_sup_loss: 0.175, D_sup_acc: 95.70 Train acc: 95.433 Test acc: 95.720 \n",
      "step: 2121 | Train: G_Loss: 1.012, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.585, D_sup_loss: 0.172, D_sup_acc: 95.77 Train acc: 95.502 Test acc: 95.920 \n",
      "step: 2122 | Train: G_Loss: 1.034, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.724, D_sup_loss: 0.172, D_sup_acc: 95.97 Train acc: 95.417 Test acc: 95.790 \n",
      "step: 2123 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.697, D_sup_loss: 0.171, D_sup_acc: 95.84 Train acc: 95.410 Test acc: 95.780 \n",
      "step: 2124 | Train: G_Loss: 1.100, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.678, D_sup_loss: 0.173, D_sup_acc: 95.83 Train acc: 95.358 Test acc: 95.810 \n",
      "step: 2125 | Train: G_Loss: 1.233, D_unsup_loss_real: 0.646, D_unsup_loss_fake: 0.617, D_sup_loss: 0.175, D_sup_acc: 95.86 Train acc: 95.432 Test acc: 95.790 \n",
      "step: 2126 | Train: G_Loss: 1.081, D_unsup_loss_real: 0.648, D_unsup_loss_fake: 0.533, D_sup_loss: 0.169, D_sup_acc: 95.84 Train acc: 95.302 Test acc: 95.810 \n",
      "step: 2127 | Train: G_Loss: 1.132, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.599, D_sup_loss: 0.174, D_sup_acc: 95.86 Train acc: 95.410 Test acc: 95.830 \n",
      "step: 2128 | Train: G_Loss: 1.133, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.639, D_sup_loss: 0.167, D_sup_acc: 95.88 Train acc: 95.350 Test acc: 95.840 \n",
      "step: 2129 | Train: G_Loss: 1.056, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.643, D_sup_loss: 0.172, D_sup_acc: 95.89 Train acc: 95.473 Test acc: 95.800 \n",
      "step: 2130 | Train: G_Loss: 1.081, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.632, D_sup_loss: 0.167, D_sup_acc: 95.85 Train acc: 95.493 Test acc: 95.860 \n",
      "step: 2131 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.634, D_sup_loss: 0.167, D_sup_acc: 95.91 Train acc: 95.383 Test acc: 95.560 \n",
      "step: 2132 | Train: G_Loss: 1.168, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.632, D_sup_loss: 0.170, D_sup_acc: 95.62 Train acc: 95.535 Test acc: 95.810 \n",
      "step: 2133 | Train: G_Loss: 1.080, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.695, D_sup_loss: 0.168, D_sup_acc: 95.86 Train acc: 95.430 Test acc: 95.790 \n",
      "step: 2134 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.744, D_unsup_loss_fake: 0.712, D_sup_loss: 0.172, D_sup_acc: 95.84 Train acc: 95.343 Test acc: 95.750 \n",
      "step: 2135 | Train: G_Loss: 1.052, D_unsup_loss_real: 0.675, D_unsup_loss_fake: 0.673, D_sup_loss: 0.180, D_sup_acc: 95.80 Train acc: 95.358 Test acc: 95.630 \n",
      "step: 2136 | Train: G_Loss: 1.028, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.570, D_sup_loss: 0.174, D_sup_acc: 95.69 Train acc: 95.320 Test acc: 95.610 \n",
      "step: 2137 | Train: G_Loss: 1.086, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.645, D_sup_loss: 0.176, D_sup_acc: 95.67 Train acc: 95.407 Test acc: 95.730 \n",
      "step: 2138 | Train: G_Loss: 1.094, D_unsup_loss_real: 0.652, D_unsup_loss_fake: 0.633, D_sup_loss: 0.167, D_sup_acc: 95.78 Train acc: 95.393 Test acc: 95.810 \n",
      "step: 2139 | Train: G_Loss: 1.022, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.590, D_sup_loss: 0.174, D_sup_acc: 95.86 Train acc: 95.440 Test acc: 95.850 \n",
      "step: 2140 | Train: G_Loss: 1.040, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.644, D_sup_loss: 0.172, D_sup_acc: 95.90 Train acc: 95.492 Test acc: 95.880 \n",
      "step: 2141 | Train: G_Loss: 1.157, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.615, D_sup_loss: 0.170, D_sup_acc: 95.93 Train acc: 95.513 Test acc: 95.810 \n",
      "step: 2142 | Train: G_Loss: 1.069, D_unsup_loss_real: 0.660, D_unsup_loss_fake: 0.605, D_sup_loss: 0.169, D_sup_acc: 95.86 Train acc: 95.490 Test acc: 95.850 \n",
      "step: 2143 | Train: G_Loss: 1.033, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.634, D_sup_loss: 0.169, D_sup_acc: 95.90 Train acc: 95.487 Test acc: 95.790 \n",
      "step: 2144 | Train: G_Loss: 1.015, D_unsup_loss_real: 0.649, D_unsup_loss_fake: 0.657, D_sup_loss: 0.166, D_sup_acc: 95.84 Train acc: 95.488 Test acc: 95.890 \n",
      "step: 2145 | Train: G_Loss: 1.132, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.614, D_sup_loss: 0.168, D_sup_acc: 95.94 Train acc: 95.403 Test acc: 95.750 \n",
      "step: 2146 | Train: G_Loss: 1.161, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.572, D_sup_loss: 0.165, D_sup_acc: 95.80 Train acc: 95.260 Test acc: 95.560 \n",
      "step: 2147 | Train: G_Loss: 1.166, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.622, D_sup_loss: 0.171, D_sup_acc: 95.62 Train acc: 95.313 Test acc: 95.540 \n",
      "step: 2148 | Train: G_Loss: 1.048, D_unsup_loss_real: 0.628, D_unsup_loss_fake: 0.586, D_sup_loss: 0.169, D_sup_acc: 95.60 Train acc: 95.385 Test acc: 95.670 \n",
      "step: 2149 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.530, D_unsup_loss_fake: 0.627, D_sup_loss: 0.167, D_sup_acc: 95.72 Train acc: 95.390 Test acc: 95.720 \n",
      "step: 2150 | Train: G_Loss: 1.088, D_unsup_loss_real: 0.648, D_unsup_loss_fake: 0.683, D_sup_loss: 0.169, D_sup_acc: 95.77 Train acc: 95.432 Test acc: 95.670 \n",
      "step: 2151 | Train: G_Loss: 0.999, D_unsup_loss_real: 0.688, D_unsup_loss_fake: 0.625, D_sup_loss: 0.167, D_sup_acc: 95.72 Train acc: 95.477 Test acc: 95.760 \n",
      "step: 2152 | Train: G_Loss: 1.069, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.698, D_sup_loss: 0.170, D_sup_acc: 95.81 Train acc: 95.562 Test acc: 95.880 \n",
      "step: 2153 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.567, D_sup_loss: 0.165, D_sup_acc: 95.93 Train acc: 95.587 Test acc: 95.860 \n",
      "step: 2154 | Train: G_Loss: 1.114, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.632, D_sup_loss: 0.160, D_sup_acc: 95.91 Train acc: 95.652 Test acc: 95.870 \n",
      "step: 2155 | Train: G_Loss: 1.130, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.632, D_sup_loss: 0.160, D_sup_acc: 95.92 Train acc: 95.592 Test acc: 95.820 \n",
      "step: 2156 | Train: G_Loss: 1.060, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.660, D_sup_loss: 0.166, D_sup_acc: 95.87 Train acc: 95.522 Test acc: 95.840 \n",
      "step: 2157 | Train: G_Loss: 1.146, D_unsup_loss_real: 0.652, D_unsup_loss_fake: 0.586, D_sup_loss: 0.168, D_sup_acc: 95.89 Train acc: 95.627 Test acc: 95.950 \n",
      "step: 2158 | Train: G_Loss: 1.085, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.698, D_sup_loss: 0.163, D_sup_acc: 96.00 Train acc: 95.512 Test acc: 95.810 \n",
      "step: 2159 | Train: G_Loss: 1.059, D_unsup_loss_real: 0.650, D_unsup_loss_fake: 0.621, D_sup_loss: 0.168, D_sup_acc: 95.86 Train acc: 95.437 Test acc: 95.870 \n",
      "step: 2160 | Train: G_Loss: 1.171, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.612, D_sup_loss: 0.173, D_sup_acc: 95.92 Train acc: 95.520 Test acc: 95.830 \n",
      "step: 2161 | Train: G_Loss: 1.126, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.676, D_sup_loss: 0.168, D_sup_acc: 95.88 Train acc: 95.573 Test acc: 95.890 \n",
      "step: 2162 | Train: G_Loss: 1.060, D_unsup_loss_real: 0.679, D_unsup_loss_fake: 0.571, D_sup_loss: 0.168, D_sup_acc: 95.94 Train acc: 95.635 Test acc: 95.940 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2163 | Train: G_Loss: 1.070, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.558, D_sup_loss: 0.163, D_sup_acc: 95.99 Train acc: 95.655 Test acc: 95.970 \n",
      "step: 2164 | Train: G_Loss: 1.110, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.645, D_sup_loss: 0.159, D_sup_acc: 96.02 Train acc: 95.638 Test acc: 95.920 \n",
      "step: 2165 | Train: G_Loss: 1.008, D_unsup_loss_real: 0.672, D_unsup_loss_fake: 0.561, D_sup_loss: 0.161, D_sup_acc: 95.97 Train acc: 95.605 Test acc: 95.970 \n",
      "step: 2166 | Train: G_Loss: 1.036, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.665, D_sup_loss: 0.162, D_sup_acc: 96.02 Train acc: 95.600 Test acc: 95.920 \n",
      "step: 2167 | Train: G_Loss: 1.051, D_unsup_loss_real: 0.515, D_unsup_loss_fake: 0.683, D_sup_loss: 0.164, D_sup_acc: 95.97 Train acc: 95.565 Test acc: 95.870 \n",
      "step: 2168 | Train: G_Loss: 1.094, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.667, D_sup_loss: 0.163, D_sup_acc: 95.92 Train acc: 95.498 Test acc: 95.800 \n",
      "step: 2169 | Train: G_Loss: 1.094, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.633, D_sup_loss: 0.167, D_sup_acc: 95.85 Train acc: 95.447 Test acc: 95.680 \n",
      "step: 2170 | Train: G_Loss: 1.110, D_unsup_loss_real: 0.668, D_unsup_loss_fake: 0.578, D_sup_loss: 0.167, D_sup_acc: 95.73 Train acc: 95.437 Test acc: 95.720 \n",
      "step: 2171 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.681, D_unsup_loss_fake: 0.658, D_sup_loss: 0.172, D_sup_acc: 95.77 Train acc: 95.442 Test acc: 95.650 \n",
      "step: 2172 | Train: G_Loss: 1.062, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.595, D_sup_loss: 0.171, D_sup_acc: 95.70 Train acc: 95.523 Test acc: 95.710 \n",
      "step: 2173 | Train: G_Loss: 1.198, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.591, D_sup_loss: 0.165, D_sup_acc: 95.76 Train acc: 95.553 Test acc: 95.750 \n",
      "step: 2174 | Train: G_Loss: 1.075, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.622, D_sup_loss: 0.164, D_sup_acc: 95.80 Train acc: 95.563 Test acc: 95.740 \n",
      "step: 2175 | Train: G_Loss: 1.116, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.589, D_sup_loss: 0.162, D_sup_acc: 95.79 Train acc: 95.617 Test acc: 95.810 \n",
      "step: 2176 | Train: G_Loss: 1.191, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.572, D_sup_loss: 0.161, D_sup_acc: 95.86 Train acc: 95.557 Test acc: 95.890 \n",
      "step: 2177 | Train: G_Loss: 1.104, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.633, D_sup_loss: 0.160, D_sup_acc: 95.94 Train acc: 95.527 Test acc: 96.010 \n",
      "step: 2178 | Train: G_Loss: 1.118, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.644, D_sup_loss: 0.163, D_sup_acc: 96.06 Train acc: 95.562 Test acc: 95.840 \n",
      "step: 2179 | Train: G_Loss: 1.062, D_unsup_loss_real: 0.693, D_unsup_loss_fake: 0.638, D_sup_loss: 0.160, D_sup_acc: 95.89 Train acc: 95.555 Test acc: 95.970 \n",
      "step: 2180 | Train: G_Loss: 1.128, D_unsup_loss_real: 0.652, D_unsup_loss_fake: 0.600, D_sup_loss: 0.164, D_sup_acc: 96.02 Train acc: 95.553 Test acc: 95.940 \n",
      "step: 2181 | Train: G_Loss: 1.199, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.590, D_sup_loss: 0.163, D_sup_acc: 95.99 Train acc: 95.472 Test acc: 95.830 \n",
      "step: 2182 | Train: G_Loss: 1.060, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.677, D_sup_loss: 0.165, D_sup_acc: 95.88 Train acc: 95.473 Test acc: 95.790 \n",
      "step: 2183 | Train: G_Loss: 1.061, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.627, D_sup_loss: 0.163, D_sup_acc: 95.84 Train acc: 95.352 Test acc: 95.810 \n",
      "step: 2184 | Train: G_Loss: 1.061, D_unsup_loss_real: 0.672, D_unsup_loss_fake: 0.620, D_sup_loss: 0.166, D_sup_acc: 95.86 Train acc: 95.478 Test acc: 95.820 \n",
      "step: 2185 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.666, D_sup_loss: 0.164, D_sup_acc: 95.87 Train acc: 95.520 Test acc: 95.940 \n",
      "step: 2186 | Train: G_Loss: 1.140, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.580, D_sup_loss: 0.165, D_sup_acc: 95.99 Train acc: 95.488 Test acc: 95.910 \n",
      "step: 2187 | Train: G_Loss: 1.051, D_unsup_loss_real: 0.653, D_unsup_loss_fake: 0.642, D_sup_loss: 0.162, D_sup_acc: 95.96 Train acc: 95.512 Test acc: 95.950 \n",
      "step: 2188 | Train: G_Loss: 1.045, D_unsup_loss_real: 0.672, D_unsup_loss_fake: 0.574, D_sup_loss: 0.166, D_sup_acc: 96.00 Train acc: 95.595 Test acc: 96.010 \n",
      "step: 2189 | Train: G_Loss: 1.108, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.638, D_sup_loss: 0.164, D_sup_acc: 96.06 Train acc: 95.565 Test acc: 95.920 \n",
      "step: 2190 | Train: G_Loss: 1.059, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.728, D_sup_loss: 0.166, D_sup_acc: 95.97 Train acc: 95.493 Test acc: 95.780 \n",
      "step: 2191 | Train: G_Loss: 1.083, D_unsup_loss_real: 0.746, D_unsup_loss_fake: 0.632, D_sup_loss: 0.166, D_sup_acc: 95.83 Train acc: 95.452 Test acc: 95.840 \n",
      "step: 2192 | Train: G_Loss: 1.083, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.637, D_sup_loss: 0.174, D_sup_acc: 95.89 Train acc: 95.380 Test acc: 95.610 \n",
      "step: 2193 | Train: G_Loss: 1.062, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.585, D_sup_loss: 0.176, D_sup_acc: 95.67 Train acc: 95.483 Test acc: 95.760 \n",
      "step: 2194 | Train: G_Loss: 1.118, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.600, D_sup_loss: 0.168, D_sup_acc: 95.81 Train acc: 95.462 Test acc: 95.800 \n",
      "step: 2195 | Train: G_Loss: 1.156, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.613, D_sup_loss: 0.166, D_sup_acc: 95.85 Train acc: 95.452 Test acc: 95.790 \n",
      "step: 2196 | Train: G_Loss: 1.042, D_unsup_loss_real: 0.649, D_unsup_loss_fake: 0.642, D_sup_loss: 0.168, D_sup_acc: 95.84 Train acc: 95.422 Test acc: 95.750 \n",
      "step: 2197 | Train: G_Loss: 1.073, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.664, D_sup_loss: 0.169, D_sup_acc: 95.80 Train acc: 95.313 Test acc: 95.720 \n",
      "step: 2198 | Train: G_Loss: 1.141, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.594, D_sup_loss: 0.174, D_sup_acc: 95.77 Train acc: 95.382 Test acc: 95.810 \n",
      "step: 2199 | Train: G_Loss: 1.139, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.690, D_sup_loss: 0.170, D_sup_acc: 95.86 Train acc: 95.398 Test acc: 95.850 \n",
      "step: 2200 | Train: G_Loss: 1.160, D_unsup_loss_real: 0.544, D_unsup_loss_fake: 0.640, D_sup_loss: 0.167, D_sup_acc: 95.90 Train acc: 95.290 Test acc: 95.800 \n",
      "Train Classifier Accuracy: 95.290%\n",
      "\n",
      "Test Classifier Accuracy: 95.800%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_2200.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_2200.h5\n",
      "step: 2201 | Train: G_Loss: 1.056, D_unsup_loss_real: 0.661, D_unsup_loss_fake: 0.615, D_sup_loss: 0.173, D_sup_acc: 95.85 Train acc: 95.310 Test acc: 95.860 \n",
      "step: 2202 | Train: G_Loss: 1.123, D_unsup_loss_real: 0.497, D_unsup_loss_fake: 0.583, D_sup_loss: 0.172, D_sup_acc: 95.91 Train acc: 95.368 Test acc: 95.950 \n",
      "step: 2203 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.657, D_unsup_loss_fake: 0.546, D_sup_loss: 0.167, D_sup_acc: 96.00 Train acc: 95.540 Test acc: 96.060 \n",
      "step: 2204 | Train: G_Loss: 1.077, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.655, D_sup_loss: 0.163, D_sup_acc: 96.11 Train acc: 95.420 Test acc: 95.770 \n",
      "step: 2205 | Train: G_Loss: 1.041, D_unsup_loss_real: 0.653, D_unsup_loss_fake: 0.650, D_sup_loss: 0.170, D_sup_acc: 95.82 Train acc: 95.302 Test acc: 95.830 \n",
      "step: 2206 | Train: G_Loss: 1.090, D_unsup_loss_real: 0.635, D_unsup_loss_fake: 0.635, D_sup_loss: 0.172, D_sup_acc: 95.88 Train acc: 95.320 Test acc: 95.870 \n",
      "step: 2207 | Train: G_Loss: 1.061, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.629, D_sup_loss: 0.172, D_sup_acc: 95.92 Train acc: 95.372 Test acc: 95.870 \n",
      "step: 2208 | Train: G_Loss: 1.006, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.622, D_sup_loss: 0.169, D_sup_acc: 95.92 Train acc: 95.470 Test acc: 96.180 \n",
      "step: 2209 | Train: G_Loss: 1.072, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.582, D_sup_loss: 0.167, D_sup_acc: 96.23 Train acc: 95.455 Test acc: 95.940 \n",
      "step: 2210 | Train: G_Loss: 1.132, D_unsup_loss_real: 0.686, D_unsup_loss_fake: 0.711, D_sup_loss: 0.170, D_sup_acc: 95.99 Train acc: 95.487 Test acc: 95.800 \n",
      "step: 2211 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.594, D_sup_loss: 0.171, D_sup_acc: 95.85 Train acc: 95.465 Test acc: 95.740 \n",
      "step: 2212 | Train: G_Loss: 1.077, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.679, D_sup_loss: 0.171, D_sup_acc: 95.79 Train acc: 95.378 Test acc: 95.840 \n",
      "step: 2213 | Train: G_Loss: 1.068, D_unsup_loss_real: 0.646, D_unsup_loss_fake: 0.663, D_sup_loss: 0.174, D_sup_acc: 95.89 Train acc: 95.253 Test acc: 95.780 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2214 | Train: G_Loss: 1.055, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.591, D_sup_loss: 0.180, D_sup_acc: 95.83 Train acc: 95.370 Test acc: 95.760 \n",
      "step: 2215 | Train: G_Loss: 0.971, D_unsup_loss_real: 0.653, D_unsup_loss_fake: 0.655, D_sup_loss: 0.175, D_sup_acc: 95.81 Train acc: 95.513 Test acc: 95.880 \n",
      "step: 2216 | Train: G_Loss: 0.995, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.588, D_sup_loss: 0.170, D_sup_acc: 95.93 Train acc: 95.517 Test acc: 95.920 \n",
      "step: 2217 | Train: G_Loss: 1.129, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.672, D_sup_loss: 0.166, D_sup_acc: 95.97 Train acc: 95.462 Test acc: 95.910 \n",
      "step: 2218 | Train: G_Loss: 1.099, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.642, D_sup_loss: 0.173, D_sup_acc: 95.96 Train acc: 95.548 Test acc: 95.940 \n",
      "step: 2219 | Train: G_Loss: 1.043, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.643, D_sup_loss: 0.172, D_sup_acc: 95.99 Train acc: 95.503 Test acc: 95.990 \n",
      "step: 2220 | Train: G_Loss: 1.110, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.696, D_sup_loss: 0.173, D_sup_acc: 96.04 Train acc: 95.420 Test acc: 95.850 \n",
      "step: 2221 | Train: G_Loss: 1.051, D_unsup_loss_real: 0.667, D_unsup_loss_fake: 0.594, D_sup_loss: 0.179, D_sup_acc: 95.90 Train acc: 95.472 Test acc: 95.890 \n",
      "step: 2222 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.660, D_unsup_loss_fake: 0.625, D_sup_loss: 0.173, D_sup_acc: 95.94 Train acc: 95.593 Test acc: 96.100 \n",
      "step: 2223 | Train: G_Loss: 1.083, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.618, D_sup_loss: 0.168, D_sup_acc: 96.15 Train acc: 95.615 Test acc: 96.130 \n",
      "step: 2224 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.583, D_sup_loss: 0.166, D_sup_acc: 96.18 Train acc: 95.643 Test acc: 96.060 \n",
      "step: 2225 | Train: G_Loss: 1.104, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.570, D_sup_loss: 0.163, D_sup_acc: 96.11 Train acc: 95.678 Test acc: 96.110 \n",
      "step: 2226 | Train: G_Loss: 1.040, D_unsup_loss_real: 0.665, D_unsup_loss_fake: 0.663, D_sup_loss: 0.161, D_sup_acc: 96.16 Train acc: 95.665 Test acc: 96.020 \n",
      "step: 2227 | Train: G_Loss: 0.985, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.581, D_sup_loss: 0.161, D_sup_acc: 96.07 Train acc: 95.638 Test acc: 96.090 \n",
      "step: 2228 | Train: G_Loss: 1.135, D_unsup_loss_real: 0.670, D_unsup_loss_fake: 0.655, D_sup_loss: 0.160, D_sup_acc: 96.14 Train acc: 95.565 Test acc: 95.930 \n",
      "step: 2229 | Train: G_Loss: 0.994, D_unsup_loss_real: 0.650, D_unsup_loss_fake: 0.707, D_sup_loss: 0.159, D_sup_acc: 95.98 Train acc: 95.657 Test acc: 96.080 \n",
      "step: 2230 | Train: G_Loss: 1.133, D_unsup_loss_real: 0.678, D_unsup_loss_fake: 0.717, D_sup_loss: 0.160, D_sup_acc: 96.13 Train acc: 95.698 Test acc: 96.120 \n",
      "step: 2231 | Train: G_Loss: 1.113, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.641, D_sup_loss: 0.162, D_sup_acc: 96.17 Train acc: 95.503 Test acc: 95.930 \n",
      "step: 2232 | Train: G_Loss: 1.051, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.549, D_sup_loss: 0.169, D_sup_acc: 95.98 Train acc: 95.723 Test acc: 96.120 \n",
      "step: 2233 | Train: G_Loss: 1.070, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.679, D_sup_loss: 0.158, D_sup_acc: 96.17 Train acc: 95.700 Test acc: 96.150 \n",
      "step: 2234 | Train: G_Loss: 1.151, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.601, D_sup_loss: 0.158, D_sup_acc: 96.20 Train acc: 95.672 Test acc: 96.170 \n",
      "step: 2235 | Train: G_Loss: 1.025, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.636, D_sup_loss: 0.158, D_sup_acc: 96.22 Train acc: 95.670 Test acc: 96.140 \n",
      "step: 2236 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.646, D_unsup_loss_fake: 0.648, D_sup_loss: 0.158, D_sup_acc: 96.19 Train acc: 95.600 Test acc: 96.070 \n",
      "step: 2237 | Train: G_Loss: 1.044, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.616, D_sup_loss: 0.162, D_sup_acc: 96.12 Train acc: 95.680 Test acc: 96.140 \n",
      "step: 2238 | Train: G_Loss: 1.077, D_unsup_loss_real: 0.668, D_unsup_loss_fake: 0.585, D_sup_loss: 0.158, D_sup_acc: 96.19 Train acc: 95.670 Test acc: 96.040 \n",
      "step: 2239 | Train: G_Loss: 1.106, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.760, D_sup_loss: 0.157, D_sup_acc: 96.09 Train acc: 95.668 Test acc: 95.980 \n",
      "step: 2240 | Train: G_Loss: 1.086, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.651, D_sup_loss: 0.162, D_sup_acc: 96.03 Train acc: 95.678 Test acc: 96.080 \n",
      "step: 2241 | Train: G_Loss: 1.090, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.601, D_sup_loss: 0.161, D_sup_acc: 96.13 Train acc: 95.633 Test acc: 96.050 \n",
      "step: 2242 | Train: G_Loss: 1.043, D_unsup_loss_real: 0.686, D_unsup_loss_fake: 0.625, D_sup_loss: 0.161, D_sup_acc: 96.10 Train acc: 95.593 Test acc: 95.900 \n",
      "step: 2243 | Train: G_Loss: 1.119, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.655, D_sup_loss: 0.160, D_sup_acc: 95.95 Train acc: 95.703 Test acc: 96.040 \n",
      "step: 2244 | Train: G_Loss: 0.975, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.671, D_sup_loss: 0.158, D_sup_acc: 96.09 Train acc: 95.647 Test acc: 96.080 \n",
      "step: 2245 | Train: G_Loss: 1.056, D_unsup_loss_real: 0.680, D_unsup_loss_fake: 0.613, D_sup_loss: 0.160, D_sup_acc: 96.13 Train acc: 95.725 Test acc: 96.190 \n",
      "step: 2246 | Train: G_Loss: 1.094, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.614, D_sup_loss: 0.160, D_sup_acc: 96.24 Train acc: 95.668 Test acc: 96.160 \n",
      "step: 2247 | Train: G_Loss: 1.128, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.667, D_sup_loss: 0.163, D_sup_acc: 96.21 Train acc: 95.718 Test acc: 96.170 \n",
      "step: 2248 | Train: G_Loss: 1.118, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.564, D_sup_loss: 0.159, D_sup_acc: 96.22 Train acc: 95.757 Test acc: 96.170 \n",
      "step: 2249 | Train: G_Loss: 1.035, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.696, D_sup_loss: 0.155, D_sup_acc: 96.22 Train acc: 95.665 Test acc: 96.200 \n",
      "step: 2250 | Train: G_Loss: 1.065, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.638, D_sup_loss: 0.160, D_sup_acc: 96.25 Train acc: 95.727 Test acc: 96.280 \n",
      "step: 2251 | Train: G_Loss: 1.047, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.676, D_sup_loss: 0.157, D_sup_acc: 96.33 Train acc: 95.787 Test acc: 96.240 \n",
      "step: 2252 | Train: G_Loss: 1.015, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.655, D_sup_loss: 0.157, D_sup_acc: 96.29 Train acc: 95.695 Test acc: 96.280 \n",
      "step: 2253 | Train: G_Loss: 1.116, D_unsup_loss_real: 0.674, D_unsup_loss_fake: 0.717, D_sup_loss: 0.164, D_sup_acc: 96.33 Train acc: 95.777 Test acc: 96.330 \n",
      "step: 2254 | Train: G_Loss: 1.065, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.651, D_sup_loss: 0.164, D_sup_acc: 96.38 Train acc: 95.693 Test acc: 96.170 \n",
      "step: 2255 | Train: G_Loss: 1.104, D_unsup_loss_real: 0.672, D_unsup_loss_fake: 0.571, D_sup_loss: 0.164, D_sup_acc: 96.22 Train acc: 95.617 Test acc: 96.070 \n",
      "step: 2256 | Train: G_Loss: 1.094, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.662, D_sup_loss: 0.163, D_sup_acc: 96.12 Train acc: 95.602 Test acc: 96.050 \n",
      "step: 2257 | Train: G_Loss: 1.103, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.624, D_sup_loss: 0.164, D_sup_acc: 96.10 Train acc: 95.747 Test acc: 96.180 \n",
      "step: 2258 | Train: G_Loss: 1.184, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.609, D_sup_loss: 0.161, D_sup_acc: 96.23 Train acc: 95.775 Test acc: 96.230 \n",
      "step: 2259 | Train: G_Loss: 1.094, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.652, D_sup_loss: 0.161, D_sup_acc: 96.28 Train acc: 95.642 Test acc: 96.160 \n",
      "step: 2260 | Train: G_Loss: 1.079, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.600, D_sup_loss: 0.164, D_sup_acc: 96.21 Train acc: 95.772 Test acc: 96.170 \n",
      "step: 2261 | Train: G_Loss: 1.172, D_unsup_loss_real: 0.648, D_unsup_loss_fake: 0.650, D_sup_loss: 0.159, D_sup_acc: 96.22 Train acc: 95.833 Test acc: 96.210 \n",
      "step: 2262 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.654, D_sup_loss: 0.159, D_sup_acc: 96.26 Train acc: 95.803 Test acc: 96.110 \n",
      "step: 2263 | Train: G_Loss: 1.083, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.619, D_sup_loss: 0.160, D_sup_acc: 96.16 Train acc: 95.808 Test acc: 96.170 \n",
      "step: 2264 | Train: G_Loss: 1.156, D_unsup_loss_real: 0.699, D_unsup_loss_fake: 0.600, D_sup_loss: 0.160, D_sup_acc: 96.22 Train acc: 95.822 Test acc: 96.180 \n",
      "step: 2265 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.649, D_sup_loss: 0.160, D_sup_acc: 96.23 Train acc: 95.837 Test acc: 96.190 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2266 | Train: G_Loss: 1.140, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.655, D_sup_loss: 0.156, D_sup_acc: 96.24 Train acc: 95.807 Test acc: 96.190 \n",
      "step: 2267 | Train: G_Loss: 1.126, D_unsup_loss_real: 0.722, D_unsup_loss_fake: 0.656, D_sup_loss: 0.159, D_sup_acc: 96.24 Train acc: 95.723 Test acc: 96.080 \n",
      "step: 2268 | Train: G_Loss: 1.085, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.636, D_sup_loss: 0.162, D_sup_acc: 96.13 Train acc: 95.753 Test acc: 96.220 \n",
      "step: 2269 | Train: G_Loss: 1.029, D_unsup_loss_real: 0.663, D_unsup_loss_fake: 0.661, D_sup_loss: 0.161, D_sup_acc: 96.27 Train acc: 95.805 Test acc: 96.140 \n",
      "step: 2270 | Train: G_Loss: 1.016, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.593, D_sup_loss: 0.160, D_sup_acc: 96.19 Train acc: 95.762 Test acc: 96.140 \n",
      "step: 2271 | Train: G_Loss: 1.057, D_unsup_loss_real: 0.653, D_unsup_loss_fake: 0.603, D_sup_loss: 0.161, D_sup_acc: 96.19 Train acc: 95.733 Test acc: 96.090 \n",
      "step: 2272 | Train: G_Loss: 1.115, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.613, D_sup_loss: 0.165, D_sup_acc: 96.14 Train acc: 95.783 Test acc: 96.230 \n",
      "step: 2273 | Train: G_Loss: 1.073, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.627, D_sup_loss: 0.160, D_sup_acc: 96.28 Train acc: 95.690 Test acc: 96.140 \n",
      "step: 2274 | Train: G_Loss: 1.082, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.623, D_sup_loss: 0.158, D_sup_acc: 96.19 Train acc: 95.748 Test acc: 96.250 \n",
      "step: 2275 | Train: G_Loss: 1.009, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.624, D_sup_loss: 0.159, D_sup_acc: 96.30 Train acc: 95.748 Test acc: 96.140 \n",
      "step: 2276 | Train: G_Loss: 1.048, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.624, D_sup_loss: 0.161, D_sup_acc: 96.19 Train acc: 95.680 Test acc: 96.110 \n",
      "step: 2277 | Train: G_Loss: 1.109, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.674, D_sup_loss: 0.164, D_sup_acc: 96.16 Train acc: 95.502 Test acc: 96.120 \n",
      "step: 2278 | Train: G_Loss: 1.066, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.570, D_sup_loss: 0.173, D_sup_acc: 96.17 Train acc: 95.665 Test acc: 96.120 \n",
      "step: 2279 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.604, D_sup_loss: 0.161, D_sup_acc: 96.17 Train acc: 95.658 Test acc: 96.090 \n",
      "step: 2280 | Train: G_Loss: 1.076, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.630, D_sup_loss: 0.164, D_sup_acc: 96.14 Train acc: 95.733 Test acc: 96.240 \n",
      "step: 2281 | Train: G_Loss: 1.062, D_unsup_loss_real: 0.674, D_unsup_loss_fake: 0.661, D_sup_loss: 0.158, D_sup_acc: 96.29 Train acc: 95.768 Test acc: 96.240 \n",
      "step: 2282 | Train: G_Loss: 1.096, D_unsup_loss_real: 0.657, D_unsup_loss_fake: 0.580, D_sup_loss: 0.161, D_sup_acc: 96.29 Train acc: 95.632 Test acc: 96.090 \n",
      "step: 2283 | Train: G_Loss: 1.066, D_unsup_loss_real: 0.650, D_unsup_loss_fake: 0.648, D_sup_loss: 0.163, D_sup_acc: 96.14 Train acc: 95.812 Test acc: 96.250 \n",
      "step: 2284 | Train: G_Loss: 1.173, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.666, D_sup_loss: 0.162, D_sup_acc: 96.30 Train acc: 95.757 Test acc: 96.040 \n",
      "step: 2285 | Train: G_Loss: 1.111, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.661, D_sup_loss: 0.163, D_sup_acc: 96.09 Train acc: 95.695 Test acc: 96.050 \n",
      "step: 2286 | Train: G_Loss: 1.076, D_unsup_loss_real: 0.685, D_unsup_loss_fake: 0.600, D_sup_loss: 0.167, D_sup_acc: 96.10 Train acc: 95.732 Test acc: 96.110 \n",
      "step: 2287 | Train: G_Loss: 1.126, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.577, D_sup_loss: 0.166, D_sup_acc: 96.16 Train acc: 95.793 Test acc: 96.270 \n",
      "step: 2288 | Train: G_Loss: 1.063, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.685, D_sup_loss: 0.166, D_sup_acc: 96.32 Train acc: 95.765 Test acc: 96.170 \n",
      "step: 2289 | Train: G_Loss: 1.105, D_unsup_loss_real: 0.631, D_unsup_loss_fake: 0.686, D_sup_loss: 0.164, D_sup_acc: 96.22 Train acc: 95.630 Test acc: 96.050 \n",
      "step: 2290 | Train: G_Loss: 1.135, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.616, D_sup_loss: 0.170, D_sup_acc: 96.10 Train acc: 95.670 Test acc: 96.110 \n",
      "step: 2291 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.593, D_sup_loss: 0.169, D_sup_acc: 96.16 Train acc: 95.780 Test acc: 96.190 \n",
      "step: 2292 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.567, D_sup_loss: 0.164, D_sup_acc: 96.24 Train acc: 95.822 Test acc: 96.200 \n",
      "step: 2293 | Train: G_Loss: 1.083, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.633, D_sup_loss: 0.159, D_sup_acc: 96.25 Train acc: 95.833 Test acc: 96.200 \n",
      "step: 2294 | Train: G_Loss: 1.110, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.598, D_sup_loss: 0.162, D_sup_acc: 96.25 Train acc: 95.807 Test acc: 96.210 \n",
      "step: 2295 | Train: G_Loss: 1.146, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.593, D_sup_loss: 0.159, D_sup_acc: 96.26 Train acc: 95.822 Test acc: 96.210 \n",
      "step: 2296 | Train: G_Loss: 1.096, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.609, D_sup_loss: 0.159, D_sup_acc: 96.26 Train acc: 95.803 Test acc: 96.200 \n",
      "step: 2297 | Train: G_Loss: 1.088, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.515, D_sup_loss: 0.159, D_sup_acc: 96.25 Train acc: 95.782 Test acc: 96.130 \n",
      "step: 2298 | Train: G_Loss: 1.097, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.586, D_sup_loss: 0.159, D_sup_acc: 96.18 Train acc: 95.788 Test acc: 96.180 \n",
      "step: 2299 | Train: G_Loss: 1.077, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.656, D_sup_loss: 0.161, D_sup_acc: 96.23 Train acc: 95.810 Test acc: 96.230 \n",
      "step: 2300 | Train: G_Loss: 1.015, D_unsup_loss_real: 0.661, D_unsup_loss_fake: 0.555, D_sup_loss: 0.159, D_sup_acc: 96.28 Train acc: 95.697 Test acc: 96.040 \n",
      "Train Classifier Accuracy: 95.697%\n",
      "\n",
      "Test Classifier Accuracy: 96.040%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_2300.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_2300.h5\n",
      "step: 2301 | Train: G_Loss: 1.037, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.560, D_sup_loss: 0.161, D_sup_acc: 96.09 Train acc: 95.808 Test acc: 96.210 \n",
      "step: 2302 | Train: G_Loss: 1.086, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.723, D_sup_loss: 0.156, D_sup_acc: 96.26 Train acc: 95.722 Test acc: 96.010 \n",
      "step: 2303 | Train: G_Loss: 1.086, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.687, D_sup_loss: 0.160, D_sup_acc: 96.06 Train acc: 95.660 Test acc: 95.900 \n",
      "step: 2304 | Train: G_Loss: 1.179, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.575, D_sup_loss: 0.161, D_sup_acc: 95.95 Train acc: 95.678 Test acc: 95.940 \n",
      "step: 2305 | Train: G_Loss: 1.179, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.579, D_sup_loss: 0.161, D_sup_acc: 95.99 Train acc: 95.777 Test acc: 96.080 \n",
      "step: 2306 | Train: G_Loss: 1.152, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.618, D_sup_loss: 0.160, D_sup_acc: 96.13 Train acc: 95.735 Test acc: 96.150 \n",
      "step: 2307 | Train: G_Loss: 1.181, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.653, D_sup_loss: 0.162, D_sup_acc: 96.20 Train acc: 95.645 Test acc: 96.090 \n",
      "step: 2308 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.674, D_unsup_loss_fake: 0.619, D_sup_loss: 0.165, D_sup_acc: 96.14 Train acc: 95.675 Test acc: 96.070 \n",
      "step: 2309 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.701, D_unsup_loss_fake: 0.587, D_sup_loss: 0.160, D_sup_acc: 96.12 Train acc: 95.605 Test acc: 96.110 \n",
      "step: 2310 | Train: G_Loss: 1.106, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.648, D_sup_loss: 0.161, D_sup_acc: 96.16 Train acc: 95.570 Test acc: 96.000 \n",
      "step: 2311 | Train: G_Loss: 1.090, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.633, D_sup_loss: 0.164, D_sup_acc: 96.05 Train acc: 95.625 Test acc: 95.940 \n",
      "step: 2312 | Train: G_Loss: 0.997, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.623, D_sup_loss: 0.165, D_sup_acc: 95.99 Train acc: 95.605 Test acc: 96.020 \n",
      "step: 2313 | Train: G_Loss: 1.111, D_unsup_loss_real: 0.523, D_unsup_loss_fake: 0.644, D_sup_loss: 0.167, D_sup_acc: 96.07 Train acc: 95.548 Test acc: 95.950 \n",
      "step: 2314 | Train: G_Loss: 1.137, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.656, D_sup_loss: 0.169, D_sup_acc: 96.00 Train acc: 95.468 Test acc: 95.920 \n",
      "step: 2315 | Train: G_Loss: 1.118, D_unsup_loss_real: 0.631, D_unsup_loss_fake: 0.647, D_sup_loss: 0.168, D_sup_acc: 95.97 Train acc: 95.515 Test acc: 95.910 \n",
      "step: 2316 | Train: G_Loss: 0.982, D_unsup_loss_real: 0.666, D_unsup_loss_fake: 0.618, D_sup_loss: 0.167, D_sup_acc: 95.96 Train acc: 95.562 Test acc: 96.010 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2317 | Train: G_Loss: 1.050, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.636, D_sup_loss: 0.164, D_sup_acc: 96.06 Train acc: 95.508 Test acc: 95.800 \n",
      "step: 2318 | Train: G_Loss: 1.088, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.633, D_sup_loss: 0.161, D_sup_acc: 95.85 Train acc: 95.613 Test acc: 95.990 \n",
      "step: 2319 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.729, D_sup_loss: 0.159, D_sup_acc: 96.04 Train acc: 95.592 Test acc: 95.980 \n",
      "step: 2320 | Train: G_Loss: 1.021, D_unsup_loss_real: 0.661, D_unsup_loss_fake: 0.611, D_sup_loss: 0.160, D_sup_acc: 96.03 Train acc: 95.543 Test acc: 95.940 \n",
      "step: 2321 | Train: G_Loss: 1.073, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.588, D_sup_loss: 0.163, D_sup_acc: 95.99 Train acc: 95.427 Test acc: 95.770 \n",
      "step: 2322 | Train: G_Loss: 1.078, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.683, D_sup_loss: 0.168, D_sup_acc: 95.82 Train acc: 95.360 Test acc: 95.830 \n",
      "step: 2323 | Train: G_Loss: 1.174, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.600, D_sup_loss: 0.170, D_sup_acc: 95.88 Train acc: 95.585 Test acc: 95.960 \n",
      "step: 2324 | Train: G_Loss: 1.070, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.614, D_sup_loss: 0.165, D_sup_acc: 96.01 Train acc: 95.533 Test acc: 95.970 \n",
      "step: 2325 | Train: G_Loss: 1.045, D_unsup_loss_real: 0.650, D_unsup_loss_fake: 0.636, D_sup_loss: 0.166, D_sup_acc: 96.02 Train acc: 95.583 Test acc: 96.000 \n",
      "step: 2326 | Train: G_Loss: 1.006, D_unsup_loss_real: 0.635, D_unsup_loss_fake: 0.604, D_sup_loss: 0.170, D_sup_acc: 96.05 Train acc: 95.562 Test acc: 95.950 \n",
      "step: 2327 | Train: G_Loss: 1.088, D_unsup_loss_real: 0.628, D_unsup_loss_fake: 0.620, D_sup_loss: 0.170, D_sup_acc: 96.00 Train acc: 95.620 Test acc: 96.040 \n",
      "step: 2328 | Train: G_Loss: 1.064, D_unsup_loss_real: 0.642, D_unsup_loss_fake: 0.556, D_sup_loss: 0.166, D_sup_acc: 96.09 Train acc: 95.695 Test acc: 96.030 \n",
      "step: 2329 | Train: G_Loss: 1.025, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.609, D_sup_loss: 0.159, D_sup_acc: 96.08 Train acc: 95.770 Test acc: 96.150 \n",
      "step: 2330 | Train: G_Loss: 1.114, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.559, D_sup_loss: 0.157, D_sup_acc: 96.20 Train acc: 95.765 Test acc: 96.180 \n",
      "step: 2331 | Train: G_Loss: 1.033, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.777, D_sup_loss: 0.157, D_sup_acc: 96.23 Train acc: 95.748 Test acc: 96.140 \n",
      "step: 2332 | Train: G_Loss: 1.031, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.578, D_sup_loss: 0.162, D_sup_acc: 96.19 Train acc: 95.773 Test acc: 96.260 \n",
      "step: 2333 | Train: G_Loss: 1.044, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.602, D_sup_loss: 0.160, D_sup_acc: 96.31 Train acc: 95.725 Test acc: 96.130 \n",
      "step: 2334 | Train: G_Loss: 1.057, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.641, D_sup_loss: 0.161, D_sup_acc: 96.18 Train acc: 95.730 Test acc: 96.030 \n",
      "step: 2335 | Train: G_Loss: 1.074, D_unsup_loss_real: 0.668, D_unsup_loss_fake: 0.605, D_sup_loss: 0.164, D_sup_acc: 96.08 Train acc: 95.720 Test acc: 96.000 \n",
      "step: 2336 | Train: G_Loss: 1.117, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.580, D_sup_loss: 0.161, D_sup_acc: 96.05 Train acc: 95.657 Test acc: 96.140 \n",
      "step: 2337 | Train: G_Loss: 1.072, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.605, D_sup_loss: 0.166, D_sup_acc: 96.19 Train acc: 95.638 Test acc: 96.100 \n",
      "step: 2338 | Train: G_Loss: 1.078, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.618, D_sup_loss: 0.169, D_sup_acc: 96.15 Train acc: 95.538 Test acc: 95.970 \n",
      "step: 2339 | Train: G_Loss: 1.010, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.686, D_sup_loss: 0.173, D_sup_acc: 96.02 Train acc: 95.522 Test acc: 96.000 \n",
      "step: 2340 | Train: G_Loss: 1.015, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.634, D_sup_loss: 0.174, D_sup_acc: 96.05 Train acc: 95.620 Test acc: 96.110 \n",
      "step: 2341 | Train: G_Loss: 1.029, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.666, D_sup_loss: 0.170, D_sup_acc: 96.16 Train acc: 95.705 Test acc: 96.100 \n",
      "step: 2342 | Train: G_Loss: 1.156, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.643, D_sup_loss: 0.163, D_sup_acc: 96.15 Train acc: 95.675 Test acc: 96.010 \n",
      "step: 2343 | Train: G_Loss: 1.079, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.608, D_sup_loss: 0.163, D_sup_acc: 96.06 Train acc: 95.695 Test acc: 96.110 \n",
      "step: 2344 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.677, D_unsup_loss_fake: 0.656, D_sup_loss: 0.165, D_sup_acc: 96.16 Train acc: 95.708 Test acc: 96.120 \n",
      "step: 2345 | Train: G_Loss: 1.075, D_unsup_loss_real: 0.694, D_unsup_loss_fake: 0.631, D_sup_loss: 0.164, D_sup_acc: 96.17 Train acc: 95.660 Test acc: 96.020 \n",
      "step: 2346 | Train: G_Loss: 1.072, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.586, D_sup_loss: 0.172, D_sup_acc: 96.07 Train acc: 95.723 Test acc: 96.080 \n",
      "step: 2347 | Train: G_Loss: 1.023, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.599, D_sup_loss: 0.166, D_sup_acc: 96.13 Train acc: 95.765 Test acc: 96.060 \n",
      "step: 2348 | Train: G_Loss: 1.072, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.582, D_sup_loss: 0.161, D_sup_acc: 96.11 Train acc: 95.758 Test acc: 96.030 \n",
      "step: 2349 | Train: G_Loss: 1.116, D_unsup_loss_real: 0.642, D_unsup_loss_fake: 0.686, D_sup_loss: 0.162, D_sup_acc: 96.08 Train acc: 95.682 Test acc: 95.980 \n",
      "step: 2350 | Train: G_Loss: 1.078, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.633, D_sup_loss: 0.164, D_sup_acc: 96.03 Train acc: 95.700 Test acc: 95.880 \n",
      "step: 2351 | Train: G_Loss: 1.101, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.579, D_sup_loss: 0.167, D_sup_acc: 95.93 Train acc: 95.720 Test acc: 96.060 \n",
      "step: 2352 | Train: G_Loss: 1.083, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.624, D_sup_loss: 0.167, D_sup_acc: 96.11 Train acc: 95.720 Test acc: 95.990 \n",
      "step: 2353 | Train: G_Loss: 1.067, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.671, D_sup_loss: 0.164, D_sup_acc: 96.04 Train acc: 95.760 Test acc: 96.100 \n",
      "step: 2354 | Train: G_Loss: 1.089, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.644, D_sup_loss: 0.160, D_sup_acc: 96.15 Train acc: 95.812 Test acc: 96.090 \n",
      "step: 2355 | Train: G_Loss: 1.051, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.682, D_sup_loss: 0.157, D_sup_acc: 96.14 Train acc: 95.910 Test acc: 96.210 \n",
      "step: 2356 | Train: G_Loss: 1.099, D_unsup_loss_real: 0.672, D_unsup_loss_fake: 0.618, D_sup_loss: 0.155, D_sup_acc: 96.26 Train acc: 95.855 Test acc: 96.120 \n",
      "step: 2357 | Train: G_Loss: 1.088, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.613, D_sup_loss: 0.159, D_sup_acc: 96.17 Train acc: 95.950 Test acc: 96.270 \n",
      "step: 2358 | Train: G_Loss: 1.189, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.663, D_sup_loss: 0.159, D_sup_acc: 96.32 Train acc: 95.973 Test acc: 96.280 \n",
      "step: 2359 | Train: G_Loss: 1.041, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.609, D_sup_loss: 0.161, D_sup_acc: 96.33 Train acc: 95.932 Test acc: 96.170 \n",
      "step: 2360 | Train: G_Loss: 1.040, D_unsup_loss_real: 0.631, D_unsup_loss_fake: 0.652, D_sup_loss: 0.164, D_sup_acc: 96.22 Train acc: 95.868 Test acc: 96.240 \n",
      "step: 2361 | Train: G_Loss: 1.032, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.660, D_sup_loss: 0.167, D_sup_acc: 96.29 Train acc: 95.805 Test acc: 96.120 \n",
      "step: 2362 | Train: G_Loss: 1.045, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.640, D_sup_loss: 0.164, D_sup_acc: 96.17 Train acc: 95.870 Test acc: 96.280 \n",
      "step: 2363 | Train: G_Loss: 1.027, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.575, D_sup_loss: 0.158, D_sup_acc: 96.33 Train acc: 95.880 Test acc: 96.220 \n",
      "step: 2364 | Train: G_Loss: 1.137, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.613, D_sup_loss: 0.162, D_sup_acc: 96.27 Train acc: 95.882 Test acc: 96.250 \n",
      "step: 2365 | Train: G_Loss: 1.003, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.611, D_sup_loss: 0.160, D_sup_acc: 96.30 Train acc: 95.888 Test acc: 96.260 \n",
      "step: 2366 | Train: G_Loss: 1.160, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.604, D_sup_loss: 0.160, D_sup_acc: 96.31 Train acc: 95.855 Test acc: 96.280 \n",
      "step: 2367 | Train: G_Loss: 1.190, D_unsup_loss_real: 0.536, D_unsup_loss_fake: 0.579, D_sup_loss: 0.162, D_sup_acc: 96.33 Train acc: 95.935 Test acc: 96.290 \n",
      "step: 2368 | Train: G_Loss: 1.061, D_unsup_loss_real: 0.702, D_unsup_loss_fake: 0.612, D_sup_loss: 0.155, D_sup_acc: 96.34 Train acc: 95.875 Test acc: 96.200 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2369 | Train: G_Loss: 1.185, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.677, D_sup_loss: 0.156, D_sup_acc: 96.25 Train acc: 95.908 Test acc: 96.310 \n",
      "step: 2370 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.636, D_sup_loss: 0.155, D_sup_acc: 96.36 Train acc: 95.882 Test acc: 96.330 \n",
      "step: 2371 | Train: G_Loss: 1.169, D_unsup_loss_real: 0.673, D_unsup_loss_fake: 0.603, D_sup_loss: 0.156, D_sup_acc: 96.38 Train acc: 95.853 Test acc: 96.300 \n",
      "step: 2372 | Train: G_Loss: 1.073, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.597, D_sup_loss: 0.160, D_sup_acc: 96.35 Train acc: 95.877 Test acc: 96.320 \n",
      "step: 2373 | Train: G_Loss: 1.078, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.637, D_sup_loss: 0.158, D_sup_acc: 96.37 Train acc: 95.892 Test acc: 96.400 \n",
      "step: 2374 | Train: G_Loss: 1.178, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.650, D_sup_loss: 0.156, D_sup_acc: 96.45 Train acc: 95.920 Test acc: 96.320 \n",
      "step: 2375 | Train: G_Loss: 1.069, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.674, D_sup_loss: 0.158, D_sup_acc: 96.37 Train acc: 95.860 Test acc: 96.250 \n",
      "step: 2376 | Train: G_Loss: 1.051, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.665, D_sup_loss: 0.160, D_sup_acc: 96.30 Train acc: 95.758 Test acc: 96.170 \n",
      "step: 2377 | Train: G_Loss: 1.085, D_unsup_loss_real: 0.667, D_unsup_loss_fake: 0.618, D_sup_loss: 0.168, D_sup_acc: 96.22 Train acc: 95.760 Test acc: 96.180 \n",
      "step: 2378 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.633, D_sup_loss: 0.166, D_sup_acc: 96.23 Train acc: 95.558 Test acc: 96.070 \n",
      "step: 2379 | Train: G_Loss: 1.110, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.648, D_sup_loss: 0.179, D_sup_acc: 96.12 Train acc: 95.647 Test acc: 96.030 \n",
      "step: 2380 | Train: G_Loss: 1.082, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.591, D_sup_loss: 0.172, D_sup_acc: 96.08 Train acc: 95.692 Test acc: 95.970 \n",
      "step: 2381 | Train: G_Loss: 1.149, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.622, D_sup_loss: 0.169, D_sup_acc: 96.02 Train acc: 95.640 Test acc: 95.960 \n",
      "step: 2382 | Train: G_Loss: 1.031, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.608, D_sup_loss: 0.169, D_sup_acc: 96.01 Train acc: 95.755 Test acc: 96.120 \n",
      "step: 2383 | Train: G_Loss: 1.150, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.615, D_sup_loss: 0.160, D_sup_acc: 96.17 Train acc: 95.643 Test acc: 96.040 \n",
      "step: 2384 | Train: G_Loss: 1.090, D_unsup_loss_real: 0.657, D_unsup_loss_fake: 0.631, D_sup_loss: 0.169, D_sup_acc: 96.09 Train acc: 95.705 Test acc: 96.170 \n",
      "step: 2385 | Train: G_Loss: 1.036, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.589, D_sup_loss: 0.160, D_sup_acc: 96.22 Train acc: 95.822 Test acc: 96.180 \n",
      "step: 2386 | Train: G_Loss: 1.119, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.627, D_sup_loss: 0.157, D_sup_acc: 96.23 Train acc: 95.862 Test acc: 96.200 \n",
      "step: 2387 | Train: G_Loss: 1.086, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.595, D_sup_loss: 0.163, D_sup_acc: 96.25 Train acc: 95.877 Test acc: 96.210 \n",
      "step: 2388 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.606, D_sup_loss: 0.158, D_sup_acc: 96.26 Train acc: 95.847 Test acc: 96.180 \n",
      "step: 2389 | Train: G_Loss: 1.036, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.712, D_sup_loss: 0.155, D_sup_acc: 96.23 Train acc: 95.852 Test acc: 96.150 \n",
      "step: 2390 | Train: G_Loss: 1.195, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.621, D_sup_loss: 0.159, D_sup_acc: 96.20 Train acc: 95.825 Test acc: 96.080 \n",
      "step: 2391 | Train: G_Loss: 1.133, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.592, D_sup_loss: 0.159, D_sup_acc: 96.13 Train acc: 95.837 Test acc: 96.160 \n",
      "step: 2392 | Train: G_Loss: 1.115, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.609, D_sup_loss: 0.157, D_sup_acc: 96.21 Train acc: 95.862 Test acc: 96.170 \n",
      "step: 2393 | Train: G_Loss: 1.064, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.612, D_sup_loss: 0.157, D_sup_acc: 96.22 Train acc: 95.742 Test acc: 96.160 \n",
      "step: 2394 | Train: G_Loss: 1.171, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.665, D_sup_loss: 0.157, D_sup_acc: 96.21 Train acc: 95.728 Test acc: 96.140 \n",
      "step: 2395 | Train: G_Loss: 1.104, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.674, D_sup_loss: 0.158, D_sup_acc: 96.19 Train acc: 95.673 Test acc: 96.070 \n",
      "step: 2396 | Train: G_Loss: 0.979, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.672, D_sup_loss: 0.166, D_sup_acc: 96.12 Train acc: 95.722 Test acc: 96.190 \n",
      "step: 2397 | Train: G_Loss: 1.120, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.734, D_sup_loss: 0.164, D_sup_acc: 96.24 Train acc: 95.710 Test acc: 96.120 \n",
      "step: 2398 | Train: G_Loss: 1.104, D_unsup_loss_real: 0.666, D_unsup_loss_fake: 0.600, D_sup_loss: 0.163, D_sup_acc: 96.17 Train acc: 95.658 Test acc: 96.060 \n",
      "step: 2399 | Train: G_Loss: 1.171, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.665, D_sup_loss: 0.168, D_sup_acc: 96.11 Train acc: 95.727 Test acc: 96.170 \n",
      "step: 2400 | Train: G_Loss: 1.200, D_unsup_loss_real: 0.669, D_unsup_loss_fake: 0.635, D_sup_loss: 0.166, D_sup_acc: 96.22 Train acc: 95.670 Test acc: 95.970 \n",
      "Train Classifier Accuracy: 95.670%\n",
      "\n",
      "Test Classifier Accuracy: 95.970%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_2400.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_2400.h5\n",
      "step: 2401 | Train: G_Loss: 1.074, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.620, D_sup_loss: 0.165, D_sup_acc: 96.02 Train acc: 95.743 Test acc: 96.180 \n",
      "step: 2402 | Train: G_Loss: 0.998, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.621, D_sup_loss: 0.163, D_sup_acc: 96.23 Train acc: 95.692 Test acc: 96.290 \n",
      "step: 2403 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.567, D_sup_loss: 0.164, D_sup_acc: 96.34 Train acc: 95.822 Test acc: 96.280 \n",
      "step: 2404 | Train: G_Loss: 1.137, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.748, D_sup_loss: 0.158, D_sup_acc: 96.33 Train acc: 95.812 Test acc: 96.260 \n",
      "step: 2405 | Train: G_Loss: 1.122, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.582, D_sup_loss: 0.162, D_sup_acc: 96.31 Train acc: 95.860 Test acc: 96.300 \n",
      "step: 2406 | Train: G_Loss: 1.147, D_unsup_loss_real: 0.655, D_unsup_loss_fake: 0.654, D_sup_loss: 0.161, D_sup_acc: 96.35 Train acc: 95.838 Test acc: 96.130 \n",
      "step: 2407 | Train: G_Loss: 1.057, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.637, D_sup_loss: 0.165, D_sup_acc: 96.18 Train acc: 95.848 Test acc: 96.250 \n",
      "step: 2408 | Train: G_Loss: 1.163, D_unsup_loss_real: 0.525, D_unsup_loss_fake: 0.618, D_sup_loss: 0.164, D_sup_acc: 96.30 Train acc: 95.872 Test acc: 96.170 \n",
      "step: 2409 | Train: G_Loss: 1.159, D_unsup_loss_real: 0.690, D_unsup_loss_fake: 0.568, D_sup_loss: 0.162, D_sup_acc: 96.22 Train acc: 95.932 Test acc: 96.320 \n",
      "step: 2410 | Train: G_Loss: 1.114, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.714, D_sup_loss: 0.165, D_sup_acc: 96.37 Train acc: 95.983 Test acc: 96.370 \n",
      "step: 2411 | Train: G_Loss: 1.106, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.573, D_sup_loss: 0.164, D_sup_acc: 96.42 Train acc: 95.940 Test acc: 96.290 \n",
      "step: 2412 | Train: G_Loss: 1.166, D_unsup_loss_real: 0.635, D_unsup_loss_fake: 0.602, D_sup_loss: 0.160, D_sup_acc: 96.34 Train acc: 95.918 Test acc: 96.260 \n",
      "step: 2413 | Train: G_Loss: 1.023, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.626, D_sup_loss: 0.159, D_sup_acc: 96.31 Train acc: 95.892 Test acc: 96.280 \n",
      "step: 2414 | Train: G_Loss: 1.109, D_unsup_loss_real: 0.649, D_unsup_loss_fake: 0.597, D_sup_loss: 0.158, D_sup_acc: 96.33 Train acc: 95.927 Test acc: 96.350 \n",
      "step: 2415 | Train: G_Loss: 0.988, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.615, D_sup_loss: 0.158, D_sup_acc: 96.40 Train acc: 95.918 Test acc: 96.230 \n",
      "step: 2416 | Train: G_Loss: 1.200, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.682, D_sup_loss: 0.163, D_sup_acc: 96.28 Train acc: 95.902 Test acc: 96.240 \n",
      "step: 2417 | Train: G_Loss: 1.060, D_unsup_loss_real: 0.652, D_unsup_loss_fake: 0.631, D_sup_loss: 0.163, D_sup_acc: 96.29 Train acc: 95.895 Test acc: 96.250 \n",
      "step: 2418 | Train: G_Loss: 1.090, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.605, D_sup_loss: 0.162, D_sup_acc: 96.30 Train acc: 95.927 Test acc: 96.330 \n",
      "step: 2419 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.673, D_unsup_loss_fake: 0.596, D_sup_loss: 0.158, D_sup_acc: 96.38 Train acc: 95.920 Test acc: 96.240 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2420 | Train: G_Loss: 1.103, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.612, D_sup_loss: 0.157, D_sup_acc: 96.29 Train acc: 95.772 Test acc: 96.090 \n",
      "step: 2421 | Train: G_Loss: 1.037, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.659, D_sup_loss: 0.165, D_sup_acc: 96.14 Train acc: 95.945 Test acc: 96.360 \n",
      "step: 2422 | Train: G_Loss: 1.071, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.668, D_sup_loss: 0.157, D_sup_acc: 96.41 Train acc: 95.888 Test acc: 96.320 \n",
      "step: 2423 | Train: G_Loss: 1.027, D_unsup_loss_real: 0.525, D_unsup_loss_fake: 0.632, D_sup_loss: 0.154, D_sup_acc: 96.37 Train acc: 96.002 Test acc: 96.380 \n",
      "step: 2424 | Train: G_Loss: 1.126, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.605, D_sup_loss: 0.151, D_sup_acc: 96.43 Train acc: 96.020 Test acc: 96.360 \n",
      "step: 2425 | Train: G_Loss: 1.173, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.606, D_sup_loss: 0.156, D_sup_acc: 96.41 Train acc: 95.965 Test acc: 96.290 \n",
      "step: 2426 | Train: G_Loss: 1.096, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.532, D_sup_loss: 0.155, D_sup_acc: 96.34 Train acc: 95.908 Test acc: 96.240 \n",
      "step: 2427 | Train: G_Loss: 1.091, D_unsup_loss_real: 0.655, D_unsup_loss_fake: 0.624, D_sup_loss: 0.156, D_sup_acc: 96.29 Train acc: 95.922 Test acc: 96.230 \n",
      "step: 2428 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.602, D_sup_loss: 0.155, D_sup_acc: 96.28 Train acc: 95.815 Test acc: 96.150 \n",
      "step: 2429 | Train: G_Loss: 1.135, D_unsup_loss_real: 0.650, D_unsup_loss_fake: 0.559, D_sup_loss: 0.156, D_sup_acc: 96.20 Train acc: 95.707 Test acc: 96.080 \n",
      "step: 2430 | Train: G_Loss: 1.156, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.649, D_sup_loss: 0.159, D_sup_acc: 96.13 Train acc: 95.765 Test acc: 96.240 \n",
      "step: 2431 | Train: G_Loss: 1.030, D_unsup_loss_real: 0.657, D_unsup_loss_fake: 0.733, D_sup_loss: 0.163, D_sup_acc: 96.29 Train acc: 95.762 Test acc: 96.220 \n",
      "step: 2432 | Train: G_Loss: 1.016, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.587, D_sup_loss: 0.165, D_sup_acc: 96.27 Train acc: 95.797 Test acc: 96.260 \n",
      "step: 2433 | Train: G_Loss: 1.077, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.657, D_sup_loss: 0.163, D_sup_acc: 96.31 Train acc: 95.875 Test acc: 96.180 \n",
      "step: 2434 | Train: G_Loss: 1.096, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.647, D_sup_loss: 0.154, D_sup_acc: 96.23 Train acc: 95.852 Test acc: 96.180 \n",
      "step: 2435 | Train: G_Loss: 1.042, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.595, D_sup_loss: 0.156, D_sup_acc: 96.23 Train acc: 95.842 Test acc: 96.170 \n",
      "step: 2436 | Train: G_Loss: 1.077, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.623, D_sup_loss: 0.154, D_sup_acc: 96.22 Train acc: 95.755 Test acc: 96.250 \n",
      "step: 2437 | Train: G_Loss: 1.160, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.620, D_sup_loss: 0.158, D_sup_acc: 96.30 Train acc: 95.687 Test acc: 95.970 \n",
      "step: 2438 | Train: G_Loss: 1.149, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.619, D_sup_loss: 0.157, D_sup_acc: 96.02 Train acc: 95.723 Test acc: 96.090 \n",
      "step: 2439 | Train: G_Loss: 1.119, D_unsup_loss_real: 0.674, D_unsup_loss_fake: 0.629, D_sup_loss: 0.159, D_sup_acc: 96.14 Train acc: 95.790 Test acc: 96.100 \n",
      "step: 2440 | Train: G_Loss: 1.063, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.602, D_sup_loss: 0.156, D_sup_acc: 96.15 Train acc: 95.710 Test acc: 96.030 \n",
      "step: 2441 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.690, D_unsup_loss_fake: 0.623, D_sup_loss: 0.163, D_sup_acc: 96.08 Train acc: 95.553 Test acc: 96.000 \n",
      "step: 2442 | Train: G_Loss: 1.180, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.639, D_sup_loss: 0.167, D_sup_acc: 96.05 Train acc: 95.775 Test acc: 96.000 \n",
      "step: 2443 | Train: G_Loss: 1.070, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.583, D_sup_loss: 0.158, D_sup_acc: 96.05 Train acc: 95.768 Test acc: 96.060 \n",
      "step: 2444 | Train: G_Loss: 1.140, D_unsup_loss_real: 0.642, D_unsup_loss_fake: 0.602, D_sup_loss: 0.159, D_sup_acc: 96.11 Train acc: 95.813 Test acc: 96.110 \n",
      "step: 2445 | Train: G_Loss: 1.065, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.684, D_sup_loss: 0.156, D_sup_acc: 96.16 Train acc: 95.750 Test acc: 96.040 \n",
      "step: 2446 | Train: G_Loss: 1.199, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.599, D_sup_loss: 0.164, D_sup_acc: 96.09 Train acc: 95.767 Test acc: 95.960 \n",
      "step: 2447 | Train: G_Loss: 1.133, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.626, D_sup_loss: 0.159, D_sup_acc: 96.01 Train acc: 95.718 Test acc: 95.990 \n",
      "step: 2448 | Train: G_Loss: 1.153, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.648, D_sup_loss: 0.163, D_sup_acc: 96.04 Train acc: 95.683 Test acc: 95.970 \n",
      "step: 2449 | Train: G_Loss: 1.213, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.620, D_sup_loss: 0.162, D_sup_acc: 96.02 Train acc: 95.802 Test acc: 96.120 \n",
      "step: 2450 | Train: G_Loss: 1.168, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.593, D_sup_loss: 0.156, D_sup_acc: 96.17 Train acc: 95.788 Test acc: 96.010 \n",
      "step: 2451 | Train: G_Loss: 1.052, D_unsup_loss_real: 0.635, D_unsup_loss_fake: 0.592, D_sup_loss: 0.159, D_sup_acc: 96.06 Train acc: 95.878 Test acc: 96.090 \n",
      "step: 2452 | Train: G_Loss: 1.130, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.646, D_sup_loss: 0.155, D_sup_acc: 96.14 Train acc: 95.878 Test acc: 96.170 \n",
      "step: 2453 | Train: G_Loss: 1.064, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.687, D_sup_loss: 0.156, D_sup_acc: 96.22 Train acc: 95.883 Test acc: 96.200 \n",
      "step: 2454 | Train: G_Loss: 1.188, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.670, D_sup_loss: 0.157, D_sup_acc: 96.25 Train acc: 95.892 Test acc: 96.220 \n",
      "step: 2455 | Train: G_Loss: 1.060, D_unsup_loss_real: 0.699, D_unsup_loss_fake: 0.618, D_sup_loss: 0.155, D_sup_acc: 96.27 Train acc: 95.913 Test acc: 96.250 \n",
      "step: 2456 | Train: G_Loss: 1.066, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.624, D_sup_loss: 0.156, D_sup_acc: 96.30 Train acc: 95.815 Test acc: 96.210 \n",
      "step: 2457 | Train: G_Loss: 1.005, D_unsup_loss_real: 0.721, D_unsup_loss_fake: 0.575, D_sup_loss: 0.156, D_sup_acc: 96.26 Train acc: 95.903 Test acc: 96.240 \n",
      "step: 2458 | Train: G_Loss: 1.058, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.680, D_sup_loss: 0.156, D_sup_acc: 96.29 Train acc: 95.837 Test acc: 96.130 \n",
      "step: 2459 | Train: G_Loss: 1.211, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.627, D_sup_loss: 0.156, D_sup_acc: 96.18 Train acc: 95.830 Test acc: 96.180 \n",
      "step: 2460 | Train: G_Loss: 1.135, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.591, D_sup_loss: 0.157, D_sup_acc: 96.23 Train acc: 95.873 Test acc: 96.170 \n",
      "step: 2461 | Train: G_Loss: 1.070, D_unsup_loss_real: 0.648, D_unsup_loss_fake: 0.648, D_sup_loss: 0.157, D_sup_acc: 96.22 Train acc: 95.793 Test acc: 96.060 \n",
      "step: 2462 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.644, D_unsup_loss_fake: 0.700, D_sup_loss: 0.163, D_sup_acc: 96.11 Train acc: 95.835 Test acc: 96.100 \n",
      "step: 2463 | Train: G_Loss: 1.101, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.621, D_sup_loss: 0.163, D_sup_acc: 96.15 Train acc: 95.870 Test acc: 96.170 \n",
      "step: 2464 | Train: G_Loss: 1.046, D_unsup_loss_real: 0.688, D_unsup_loss_fake: 0.584, D_sup_loss: 0.163, D_sup_acc: 96.22 Train acc: 95.930 Test acc: 96.230 \n",
      "step: 2465 | Train: G_Loss: 0.980, D_unsup_loss_real: 0.656, D_unsup_loss_fake: 0.776, D_sup_loss: 0.158, D_sup_acc: 96.28 Train acc: 95.800 Test acc: 96.250 \n",
      "step: 2466 | Train: G_Loss: 1.063, D_unsup_loss_real: 0.661, D_unsup_loss_fake: 0.647, D_sup_loss: 0.167, D_sup_acc: 96.30 Train acc: 95.878 Test acc: 96.300 \n",
      "step: 2467 | Train: G_Loss: 1.103, D_unsup_loss_real: 0.538, D_unsup_loss_fake: 0.677, D_sup_loss: 0.162, D_sup_acc: 96.35 Train acc: 95.898 Test acc: 96.320 \n",
      "step: 2468 | Train: G_Loss: 1.108, D_unsup_loss_real: 0.655, D_unsup_loss_fake: 0.662, D_sup_loss: 0.159, D_sup_acc: 96.37 Train acc: 95.760 Test acc: 96.210 \n",
      "step: 2469 | Train: G_Loss: 1.021, D_unsup_loss_real: 0.666, D_unsup_loss_fake: 0.602, D_sup_loss: 0.166, D_sup_acc: 96.26 Train acc: 95.760 Test acc: 96.230 \n",
      "step: 2470 | Train: G_Loss: 1.024, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.618, D_sup_loss: 0.170, D_sup_acc: 96.28 Train acc: 95.825 Test acc: 96.200 \n",
      "step: 2471 | Train: G_Loss: 1.106, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.596, D_sup_loss: 0.166, D_sup_acc: 96.25 Train acc: 95.923 Test acc: 96.300 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2472 | Train: G_Loss: 1.068, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.592, D_sup_loss: 0.159, D_sup_acc: 96.35 Train acc: 95.975 Test acc: 96.370 \n",
      "step: 2473 | Train: G_Loss: 1.044, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.653, D_sup_loss: 0.157, D_sup_acc: 96.42 Train acc: 95.955 Test acc: 96.340 \n",
      "step: 2474 | Train: G_Loss: 1.044, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.648, D_sup_loss: 0.157, D_sup_acc: 96.39 Train acc: 95.968 Test acc: 96.290 \n",
      "step: 2475 | Train: G_Loss: 1.054, D_unsup_loss_real: 0.683, D_unsup_loss_fake: 0.626, D_sup_loss: 0.160, D_sup_acc: 96.34 Train acc: 96.013 Test acc: 96.330 \n",
      "step: 2476 | Train: G_Loss: 1.118, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.597, D_sup_loss: 0.159, D_sup_acc: 96.38 Train acc: 95.987 Test acc: 96.290 \n",
      "step: 2477 | Train: G_Loss: 1.118, D_unsup_loss_real: 0.665, D_unsup_loss_fake: 0.684, D_sup_loss: 0.158, D_sup_acc: 96.34 Train acc: 95.773 Test acc: 96.010 \n",
      "step: 2478 | Train: G_Loss: 1.056, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.625, D_sup_loss: 0.164, D_sup_acc: 96.06 Train acc: 95.657 Test acc: 95.920 \n",
      "step: 2479 | Train: G_Loss: 1.166, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.674, D_sup_loss: 0.164, D_sup_acc: 95.97 Train acc: 95.877 Test acc: 96.200 \n",
      "step: 2480 | Train: G_Loss: 1.047, D_unsup_loss_real: 0.668, D_unsup_loss_fake: 0.587, D_sup_loss: 0.159, D_sup_acc: 96.25 Train acc: 95.875 Test acc: 96.340 \n",
      "step: 2481 | Train: G_Loss: 1.003, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.593, D_sup_loss: 0.164, D_sup_acc: 96.39 Train acc: 95.942 Test acc: 96.360 \n",
      "step: 2482 | Train: G_Loss: 1.025, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.561, D_sup_loss: 0.158, D_sup_acc: 96.41 Train acc: 95.848 Test acc: 96.260 \n",
      "step: 2483 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.627, D_sup_loss: 0.161, D_sup_acc: 96.31 Train acc: 95.802 Test acc: 96.180 \n",
      "step: 2484 | Train: G_Loss: 1.081, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.643, D_sup_loss: 0.161, D_sup_acc: 96.23 Train acc: 95.823 Test acc: 96.140 \n",
      "step: 2485 | Train: G_Loss: 1.004, D_unsup_loss_real: 0.669, D_unsup_loss_fake: 0.656, D_sup_loss: 0.156, D_sup_acc: 96.19 Train acc: 95.857 Test acc: 96.270 \n",
      "step: 2486 | Train: G_Loss: 1.043, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.625, D_sup_loss: 0.159, D_sup_acc: 96.32 Train acc: 95.803 Test acc: 96.210 \n",
      "step: 2487 | Train: G_Loss: 1.150, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.648, D_sup_loss: 0.159, D_sup_acc: 96.26 Train acc: 95.868 Test acc: 96.300 \n",
      "step: 2488 | Train: G_Loss: 1.088, D_unsup_loss_real: 0.628, D_unsup_loss_fake: 0.637, D_sup_loss: 0.162, D_sup_acc: 96.35 Train acc: 95.972 Test acc: 96.360 \n",
      "step: 2489 | Train: G_Loss: 1.052, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.626, D_sup_loss: 0.156, D_sup_acc: 96.41 Train acc: 95.985 Test acc: 96.320 \n",
      "step: 2490 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.627, D_sup_loss: 0.152, D_sup_acc: 96.37 Train acc: 95.842 Test acc: 96.280 \n",
      "step: 2491 | Train: G_Loss: 1.107, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.651, D_sup_loss: 0.157, D_sup_acc: 96.33 Train acc: 95.923 Test acc: 96.300 \n",
      "step: 2492 | Train: G_Loss: 1.113, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.608, D_sup_loss: 0.154, D_sup_acc: 96.35 Train acc: 95.945 Test acc: 96.230 \n",
      "step: 2493 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.697, D_unsup_loss_fake: 0.644, D_sup_loss: 0.152, D_sup_acc: 96.28 Train acc: 95.963 Test acc: 96.210 \n",
      "step: 2494 | Train: G_Loss: 1.064, D_unsup_loss_real: 0.674, D_unsup_loss_fake: 0.591, D_sup_loss: 0.153, D_sup_acc: 96.26 Train acc: 95.915 Test acc: 96.260 \n",
      "step: 2495 | Train: G_Loss: 1.065, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.625, D_sup_loss: 0.155, D_sup_acc: 96.31 Train acc: 95.920 Test acc: 96.210 \n",
      "step: 2496 | Train: G_Loss: 1.123, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.671, D_sup_loss: 0.155, D_sup_acc: 96.26 Train acc: 95.855 Test acc: 96.240 \n",
      "step: 2497 | Train: G_Loss: 1.086, D_unsup_loss_real: 0.666, D_unsup_loss_fake: 0.577, D_sup_loss: 0.156, D_sup_acc: 96.29 Train acc: 95.840 Test acc: 96.220 \n",
      "step: 2498 | Train: G_Loss: 1.040, D_unsup_loss_real: 0.644, D_unsup_loss_fake: 0.580, D_sup_loss: 0.159, D_sup_acc: 96.27 Train acc: 95.877 Test acc: 96.190 \n",
      "step: 2499 | Train: G_Loss: 1.063, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.627, D_sup_loss: 0.155, D_sup_acc: 96.24 Train acc: 95.900 Test acc: 96.270 \n",
      "step: 2500 | Train: G_Loss: 1.071, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.654, D_sup_loss: 0.155, D_sup_acc: 96.32 Train acc: 95.925 Test acc: 96.260 \n",
      "Train Classifier Accuracy: 95.925%\n",
      "\n",
      "Test Classifier Accuracy: 96.260%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_2500.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_2500.h5\n",
      "step: 2501 | Train: G_Loss: 1.141, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.609, D_sup_loss: 0.154, D_sup_acc: 96.31 Train acc: 95.908 Test acc: 96.180 \n",
      "step: 2502 | Train: G_Loss: 1.080, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.692, D_sup_loss: 0.155, D_sup_acc: 96.23 Train acc: 95.932 Test acc: 96.240 \n",
      "step: 2503 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.703, D_sup_loss: 0.160, D_sup_acc: 96.29 Train acc: 95.797 Test acc: 96.170 \n",
      "step: 2504 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.672, D_sup_loss: 0.165, D_sup_acc: 96.22 Train acc: 95.750 Test acc: 96.160 \n",
      "step: 2505 | Train: G_Loss: 1.127, D_unsup_loss_real: 0.655, D_unsup_loss_fake: 0.659, D_sup_loss: 0.164, D_sup_acc: 96.21 Train acc: 95.768 Test acc: 96.190 \n",
      "step: 2506 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.700, D_sup_loss: 0.160, D_sup_acc: 96.24 Train acc: 95.847 Test acc: 96.130 \n",
      "step: 2507 | Train: G_Loss: 1.107, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.618, D_sup_loss: 0.160, D_sup_acc: 96.18 Train acc: 95.768 Test acc: 96.110 \n",
      "step: 2508 | Train: G_Loss: 1.064, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.651, D_sup_loss: 0.160, D_sup_acc: 96.16 Train acc: 95.783 Test acc: 96.180 \n",
      "step: 2509 | Train: G_Loss: 1.043, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.622, D_sup_loss: 0.160, D_sup_acc: 96.23 Train acc: 95.835 Test acc: 96.160 \n",
      "step: 2510 | Train: G_Loss: 1.043, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.613, D_sup_loss: 0.157, D_sup_acc: 96.21 Train acc: 95.787 Test acc: 96.200 \n",
      "step: 2511 | Train: G_Loss: 1.125, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.716, D_sup_loss: 0.159, D_sup_acc: 96.25 Train acc: 95.830 Test acc: 96.170 \n",
      "step: 2512 | Train: G_Loss: 1.081, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.595, D_sup_loss: 0.158, D_sup_acc: 96.22 Train acc: 95.840 Test acc: 96.160 \n",
      "step: 2513 | Train: G_Loss: 1.082, D_unsup_loss_real: 0.664, D_unsup_loss_fake: 0.660, D_sup_loss: 0.161, D_sup_acc: 96.21 Train acc: 95.763 Test acc: 96.080 \n",
      "step: 2514 | Train: G_Loss: 1.076, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.593, D_sup_loss: 0.167, D_sup_acc: 96.13 Train acc: 95.805 Test acc: 96.110 \n",
      "step: 2515 | Train: G_Loss: 1.055, D_unsup_loss_real: 0.673, D_unsup_loss_fake: 0.644, D_sup_loss: 0.161, D_sup_acc: 96.16 Train acc: 95.755 Test acc: 96.080 \n",
      "step: 2516 | Train: G_Loss: 1.045, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.611, D_sup_loss: 0.161, D_sup_acc: 96.13 Train acc: 95.748 Test acc: 96.030 \n",
      "step: 2517 | Train: G_Loss: 1.082, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.560, D_sup_loss: 0.162, D_sup_acc: 96.08 Train acc: 95.802 Test acc: 96.110 \n",
      "step: 2518 | Train: G_Loss: 1.107, D_unsup_loss_real: 0.526, D_unsup_loss_fake: 0.648, D_sup_loss: 0.158, D_sup_acc: 96.16 Train acc: 95.827 Test acc: 96.040 \n",
      "step: 2519 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.639, D_sup_loss: 0.155, D_sup_acc: 96.09 Train acc: 95.805 Test acc: 96.090 \n",
      "step: 2520 | Train: G_Loss: 1.026, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.673, D_sup_loss: 0.159, D_sup_acc: 96.14 Train acc: 95.750 Test acc: 96.090 \n",
      "step: 2521 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.635, D_sup_loss: 0.161, D_sup_acc: 96.14 Train acc: 95.797 Test acc: 96.100 \n",
      "step: 2522 | Train: G_Loss: 1.030, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.647, D_sup_loss: 0.162, D_sup_acc: 96.15 Train acc: 95.773 Test acc: 96.100 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2523 | Train: G_Loss: 1.152, D_unsup_loss_real: 0.710, D_unsup_loss_fake: 0.608, D_sup_loss: 0.163, D_sup_acc: 96.15 Train acc: 95.700 Test acc: 96.100 \n",
      "step: 2524 | Train: G_Loss: 1.016, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.623, D_sup_loss: 0.164, D_sup_acc: 96.15 Train acc: 95.712 Test acc: 96.070 \n",
      "step: 2525 | Train: G_Loss: 1.071, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.667, D_sup_loss: 0.163, D_sup_acc: 96.12 Train acc: 95.802 Test acc: 96.220 \n",
      "step: 2526 | Train: G_Loss: 1.087, D_unsup_loss_real: 0.526, D_unsup_loss_fake: 0.634, D_sup_loss: 0.158, D_sup_acc: 96.27 Train acc: 95.813 Test acc: 96.130 \n",
      "step: 2527 | Train: G_Loss: 0.995, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.642, D_sup_loss: 0.158, D_sup_acc: 96.18 Train acc: 95.825 Test acc: 96.240 \n",
      "step: 2528 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.665, D_unsup_loss_fake: 0.571, D_sup_loss: 0.159, D_sup_acc: 96.29 Train acc: 95.752 Test acc: 96.140 \n",
      "step: 2529 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.536, D_unsup_loss_fake: 0.671, D_sup_loss: 0.161, D_sup_acc: 96.19 Train acc: 95.820 Test acc: 96.260 \n",
      "step: 2530 | Train: G_Loss: 1.050, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.605, D_sup_loss: 0.160, D_sup_acc: 96.31 Train acc: 95.872 Test acc: 96.220 \n",
      "step: 2531 | Train: G_Loss: 1.082, D_unsup_loss_real: 0.684, D_unsup_loss_fake: 0.625, D_sup_loss: 0.159, D_sup_acc: 96.27 Train acc: 95.855 Test acc: 96.160 \n",
      "step: 2532 | Train: G_Loss: 1.100, D_unsup_loss_real: 0.693, D_unsup_loss_fake: 0.564, D_sup_loss: 0.159, D_sup_acc: 96.21 Train acc: 95.870 Test acc: 96.250 \n",
      "step: 2533 | Train: G_Loss: 1.126, D_unsup_loss_real: 0.524, D_unsup_loss_fake: 0.550, D_sup_loss: 0.157, D_sup_acc: 96.30 Train acc: 95.938 Test acc: 96.260 \n",
      "step: 2534 | Train: G_Loss: 1.099, D_unsup_loss_real: 0.519, D_unsup_loss_fake: 0.648, D_sup_loss: 0.149, D_sup_acc: 96.31 Train acc: 95.900 Test acc: 96.280 \n",
      "step: 2535 | Train: G_Loss: 1.160, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.557, D_sup_loss: 0.150, D_sup_acc: 96.33 Train acc: 95.915 Test acc: 96.320 \n",
      "step: 2536 | Train: G_Loss: 0.994, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.576, D_sup_loss: 0.153, D_sup_acc: 96.37 Train acc: 95.893 Test acc: 96.420 \n",
      "step: 2537 | Train: G_Loss: 1.045, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.641, D_sup_loss: 0.155, D_sup_acc: 96.47 Train acc: 95.808 Test acc: 96.320 \n",
      "step: 2538 | Train: G_Loss: 1.090, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.656, D_sup_loss: 0.160, D_sup_acc: 96.37 Train acc: 95.767 Test acc: 96.180 \n",
      "step: 2539 | Train: G_Loss: 1.032, D_unsup_loss_real: 0.664, D_unsup_loss_fake: 0.632, D_sup_loss: 0.157, D_sup_acc: 96.23 Train acc: 95.660 Test acc: 96.100 \n",
      "step: 2540 | Train: G_Loss: 1.076, D_unsup_loss_real: 0.656, D_unsup_loss_fake: 0.635, D_sup_loss: 0.168, D_sup_acc: 96.15 Train acc: 95.713 Test acc: 96.110 \n",
      "step: 2541 | Train: G_Loss: 1.062, D_unsup_loss_real: 0.647, D_unsup_loss_fake: 0.677, D_sup_loss: 0.163, D_sup_acc: 96.16 Train acc: 95.663 Test acc: 96.070 \n",
      "step: 2542 | Train: G_Loss: 1.057, D_unsup_loss_real: 0.671, D_unsup_loss_fake: 0.646, D_sup_loss: 0.167, D_sup_acc: 96.12 Train acc: 95.623 Test acc: 96.070 \n",
      "step: 2543 | Train: G_Loss: 1.037, D_unsup_loss_real: 0.628, D_unsup_loss_fake: 0.620, D_sup_loss: 0.166, D_sup_acc: 96.12 Train acc: 95.752 Test acc: 96.260 \n",
      "step: 2544 | Train: G_Loss: 1.105, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.565, D_sup_loss: 0.158, D_sup_acc: 96.31 Train acc: 95.775 Test acc: 96.250 \n",
      "step: 2545 | Train: G_Loss: 1.008, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.627, D_sup_loss: 0.156, D_sup_acc: 96.30 Train acc: 95.768 Test acc: 96.270 \n",
      "step: 2546 | Train: G_Loss: 1.013, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.618, D_sup_loss: 0.156, D_sup_acc: 96.32 Train acc: 95.698 Test acc: 96.240 \n",
      "step: 2547 | Train: G_Loss: 1.091, D_unsup_loss_real: 0.650, D_unsup_loss_fake: 0.633, D_sup_loss: 0.161, D_sup_acc: 96.29 Train acc: 95.808 Test acc: 96.260 \n",
      "step: 2548 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.622, D_sup_loss: 0.158, D_sup_acc: 96.31 Train acc: 95.805 Test acc: 96.300 \n",
      "step: 2549 | Train: G_Loss: 1.038, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.609, D_sup_loss: 0.159, D_sup_acc: 96.35 Train acc: 95.777 Test acc: 96.200 \n",
      "step: 2550 | Train: G_Loss: 1.052, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.595, D_sup_loss: 0.160, D_sup_acc: 96.25 Train acc: 95.835 Test acc: 96.310 \n",
      "step: 2551 | Train: G_Loss: 1.073, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.562, D_sup_loss: 0.153, D_sup_acc: 96.36 Train acc: 95.888 Test acc: 96.350 \n",
      "step: 2552 | Train: G_Loss: 1.062, D_unsup_loss_real: 0.672, D_unsup_loss_fake: 0.645, D_sup_loss: 0.153, D_sup_acc: 96.40 Train acc: 95.868 Test acc: 96.400 \n",
      "step: 2553 | Train: G_Loss: 1.116, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.626, D_sup_loss: 0.154, D_sup_acc: 96.45 Train acc: 95.903 Test acc: 96.290 \n",
      "step: 2554 | Train: G_Loss: 1.050, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.643, D_sup_loss: 0.154, D_sup_acc: 96.34 Train acc: 95.828 Test acc: 96.140 \n",
      "step: 2555 | Train: G_Loss: 1.005, D_unsup_loss_real: 0.658, D_unsup_loss_fake: 0.682, D_sup_loss: 0.158, D_sup_acc: 96.19 Train acc: 95.750 Test acc: 96.230 \n",
      "step: 2556 | Train: G_Loss: 1.061, D_unsup_loss_real: 0.635, D_unsup_loss_fake: 0.614, D_sup_loss: 0.166, D_sup_acc: 96.28 Train acc: 95.838 Test acc: 96.180 \n",
      "step: 2557 | Train: G_Loss: 1.136, D_unsup_loss_real: 0.505, D_unsup_loss_fake: 0.665, D_sup_loss: 0.161, D_sup_acc: 96.23 Train acc: 95.762 Test acc: 96.090 \n",
      "step: 2558 | Train: G_Loss: 1.087, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.660, D_sup_loss: 0.162, D_sup_acc: 96.14 Train acc: 95.863 Test acc: 96.250 \n",
      "step: 2559 | Train: G_Loss: 1.044, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.577, D_sup_loss: 0.158, D_sup_acc: 96.30 Train acc: 95.872 Test acc: 96.250 \n",
      "step: 2560 | Train: G_Loss: 1.085, D_unsup_loss_real: 0.687, D_unsup_loss_fake: 0.688, D_sup_loss: 0.155, D_sup_acc: 96.30 Train acc: 95.782 Test acc: 96.070 \n",
      "step: 2561 | Train: G_Loss: 1.143, D_unsup_loss_real: 0.668, D_unsup_loss_fake: 0.611, D_sup_loss: 0.164, D_sup_acc: 96.12 Train acc: 95.718 Test acc: 95.920 \n",
      "step: 2562 | Train: G_Loss: 1.016, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.542, D_sup_loss: 0.166, D_sup_acc: 95.97 Train acc: 95.782 Test acc: 96.220 \n",
      "step: 2563 | Train: G_Loss: 1.116, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.688, D_sup_loss: 0.167, D_sup_acc: 96.27 Train acc: 95.830 Test acc: 96.300 \n",
      "step: 2564 | Train: G_Loss: 1.023, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.596, D_sup_loss: 0.164, D_sup_acc: 96.35 Train acc: 95.898 Test acc: 96.320 \n",
      "step: 2565 | Train: G_Loss: 1.141, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.588, D_sup_loss: 0.159, D_sup_acc: 96.37 Train acc: 95.902 Test acc: 96.410 \n",
      "step: 2566 | Train: G_Loss: 1.250, D_unsup_loss_real: 0.639, D_unsup_loss_fake: 0.607, D_sup_loss: 0.152, D_sup_acc: 96.46 Train acc: 95.868 Test acc: 96.140 \n",
      "step: 2567 | Train: G_Loss: 1.024, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.631, D_sup_loss: 0.155, D_sup_acc: 96.19 Train acc: 95.882 Test acc: 96.210 \n",
      "step: 2568 | Train: G_Loss: 1.109, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.579, D_sup_loss: 0.154, D_sup_acc: 96.26 Train acc: 95.973 Test acc: 96.360 \n",
      "step: 2569 | Train: G_Loss: 1.091, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.678, D_sup_loss: 0.155, D_sup_acc: 96.41 Train acc: 95.960 Test acc: 96.340 \n",
      "step: 2570 | Train: G_Loss: 1.191, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.656, D_sup_loss: 0.158, D_sup_acc: 96.39 Train acc: 95.865 Test acc: 96.180 \n",
      "step: 2571 | Train: G_Loss: 1.122, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.569, D_sup_loss: 0.162, D_sup_acc: 96.23 Train acc: 95.975 Test acc: 96.310 \n",
      "step: 2572 | Train: G_Loss: 1.059, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.637, D_sup_loss: 0.158, D_sup_acc: 96.36 Train acc: 95.968 Test acc: 96.370 \n",
      "step: 2573 | Train: G_Loss: 1.107, D_unsup_loss_real: 0.644, D_unsup_loss_fake: 0.645, D_sup_loss: 0.159, D_sup_acc: 96.42 Train acc: 95.982 Test acc: 96.370 \n",
      "step: 2574 | Train: G_Loss: 1.109, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.574, D_sup_loss: 0.158, D_sup_acc: 96.42 Train acc: 96.045 Test acc: 96.320 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2575 | Train: G_Loss: 1.091, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.598, D_sup_loss: 0.153, D_sup_acc: 96.37 Train acc: 96.095 Test acc: 96.440 \n",
      "step: 2576 | Train: G_Loss: 0.993, D_unsup_loss_real: 0.668, D_unsup_loss_fake: 0.630, D_sup_loss: 0.152, D_sup_acc: 96.48 Train acc: 96.002 Test acc: 96.370 \n",
      "step: 2577 | Train: G_Loss: 1.120, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.596, D_sup_loss: 0.156, D_sup_acc: 96.42 Train acc: 96.000 Test acc: 96.420 \n",
      "step: 2578 | Train: G_Loss: 1.050, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.606, D_sup_loss: 0.156, D_sup_acc: 96.47 Train acc: 96.018 Test acc: 96.320 \n",
      "step: 2579 | Train: G_Loss: 1.048, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.655, D_sup_loss: 0.155, D_sup_acc: 96.37 Train acc: 95.920 Test acc: 96.350 \n",
      "step: 2580 | Train: G_Loss: 1.046, D_unsup_loss_real: 0.628, D_unsup_loss_fake: 0.621, D_sup_loss: 0.156, D_sup_acc: 96.40 Train acc: 95.840 Test acc: 96.320 \n",
      "step: 2581 | Train: G_Loss: 1.056, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.604, D_sup_loss: 0.164, D_sup_acc: 96.37 Train acc: 95.908 Test acc: 96.380 \n",
      "step: 2582 | Train: G_Loss: 1.064, D_unsup_loss_real: 0.649, D_unsup_loss_fake: 0.659, D_sup_loss: 0.153, D_sup_acc: 96.43 Train acc: 95.932 Test acc: 96.320 \n",
      "step: 2583 | Train: G_Loss: 1.065, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.600, D_sup_loss: 0.155, D_sup_acc: 96.37 Train acc: 95.867 Test acc: 96.240 \n",
      "step: 2584 | Train: G_Loss: 1.062, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.746, D_sup_loss: 0.160, D_sup_acc: 96.29 Train acc: 95.972 Test acc: 96.350 \n",
      "step: 2585 | Train: G_Loss: 1.081, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.643, D_sup_loss: 0.159, D_sup_acc: 96.40 Train acc: 95.903 Test acc: 96.350 \n",
      "step: 2586 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.579, D_sup_loss: 0.164, D_sup_acc: 96.40 Train acc: 95.957 Test acc: 96.360 \n",
      "step: 2587 | Train: G_Loss: 1.109, D_unsup_loss_real: 0.685, D_unsup_loss_fake: 0.597, D_sup_loss: 0.159, D_sup_acc: 96.41 Train acc: 95.880 Test acc: 96.180 \n",
      "step: 2588 | Train: G_Loss: 1.053, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.635, D_sup_loss: 0.162, D_sup_acc: 96.23 Train acc: 95.867 Test acc: 96.260 \n",
      "step: 2589 | Train: G_Loss: 1.183, D_unsup_loss_real: 0.529, D_unsup_loss_fake: 0.555, D_sup_loss: 0.164, D_sup_acc: 96.31 Train acc: 95.903 Test acc: 96.380 \n",
      "step: 2590 | Train: G_Loss: 1.055, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.582, D_sup_loss: 0.163, D_sup_acc: 96.43 Train acc: 95.948 Test acc: 96.420 \n",
      "step: 2591 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.594, D_sup_loss: 0.161, D_sup_acc: 96.47 Train acc: 96.058 Test acc: 96.530 \n",
      "step: 2592 | Train: G_Loss: 1.143, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.594, D_sup_loss: 0.152, D_sup_acc: 96.57 Train acc: 95.890 Test acc: 96.230 \n",
      "step: 2593 | Train: G_Loss: 1.122, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.626, D_sup_loss: 0.152, D_sup_acc: 96.28 Train acc: 95.922 Test acc: 96.370 \n",
      "step: 2594 | Train: G_Loss: 1.070, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.627, D_sup_loss: 0.149, D_sup_acc: 96.42 Train acc: 96.003 Test acc: 96.490 \n",
      "step: 2595 | Train: G_Loss: 1.092, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.701, D_sup_loss: 0.148, D_sup_acc: 96.53 Train acc: 95.970 Test acc: 96.320 \n",
      "step: 2596 | Train: G_Loss: 1.079, D_unsup_loss_real: 0.654, D_unsup_loss_fake: 0.612, D_sup_loss: 0.158, D_sup_acc: 96.37 Train acc: 95.965 Test acc: 96.300 \n",
      "step: 2597 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.616, D_sup_loss: 0.163, D_sup_acc: 96.35 Train acc: 96.027 Test acc: 96.380 \n",
      "step: 2598 | Train: G_Loss: 1.074, D_unsup_loss_real: 0.668, D_unsup_loss_fake: 0.557, D_sup_loss: 0.155, D_sup_acc: 96.43 Train acc: 95.993 Test acc: 96.440 \n",
      "step: 2599 | Train: G_Loss: 1.126, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.643, D_sup_loss: 0.155, D_sup_acc: 96.48 Train acc: 96.035 Test acc: 96.430 \n",
      "step: 2600 | Train: G_Loss: 1.089, D_unsup_loss_real: 0.647, D_unsup_loss_fake: 0.671, D_sup_loss: 0.151, D_sup_acc: 96.48 Train acc: 96.008 Test acc: 96.500 \n",
      "Train Classifier Accuracy: 96.008%\n",
      "\n",
      "Test Classifier Accuracy: 96.500%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_2600.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_2600.h5\n",
      "step: 2601 | Train: G_Loss: 1.157, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.578, D_sup_loss: 0.152, D_sup_acc: 96.54 Train acc: 96.025 Test acc: 96.380 \n",
      "step: 2602 | Train: G_Loss: 1.071, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.620, D_sup_loss: 0.148, D_sup_acc: 96.43 Train acc: 96.015 Test acc: 96.450 \n",
      "step: 2603 | Train: G_Loss: 1.107, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.675, D_sup_loss: 0.151, D_sup_acc: 96.49 Train acc: 96.010 Test acc: 96.430 \n",
      "step: 2604 | Train: G_Loss: 1.067, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.634, D_sup_loss: 0.154, D_sup_acc: 96.48 Train acc: 95.903 Test acc: 96.360 \n",
      "step: 2605 | Train: G_Loss: 0.991, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.591, D_sup_loss: 0.155, D_sup_acc: 96.41 Train acc: 95.928 Test acc: 96.340 \n",
      "step: 2606 | Train: G_Loss: 1.062, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.571, D_sup_loss: 0.153, D_sup_acc: 96.39 Train acc: 95.950 Test acc: 96.370 \n",
      "step: 2607 | Train: G_Loss: 1.066, D_unsup_loss_real: 0.537, D_unsup_loss_fake: 0.727, D_sup_loss: 0.152, D_sup_acc: 96.42 Train acc: 95.970 Test acc: 96.160 \n",
      "step: 2608 | Train: G_Loss: 1.206, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.547, D_sup_loss: 0.151, D_sup_acc: 96.21 Train acc: 95.998 Test acc: 96.290 \n",
      "step: 2609 | Train: G_Loss: 1.018, D_unsup_loss_real: 0.647, D_unsup_loss_fake: 0.669, D_sup_loss: 0.149, D_sup_acc: 96.34 Train acc: 95.928 Test acc: 96.260 \n",
      "step: 2610 | Train: G_Loss: 1.163, D_unsup_loss_real: 0.673, D_unsup_loss_fake: 0.645, D_sup_loss: 0.155, D_sup_acc: 96.31 Train acc: 95.963 Test acc: 96.300 \n",
      "step: 2611 | Train: G_Loss: 1.135, D_unsup_loss_real: 0.684, D_unsup_loss_fake: 0.607, D_sup_loss: 0.153, D_sup_acc: 96.35 Train acc: 95.977 Test acc: 96.300 \n",
      "step: 2612 | Train: G_Loss: 1.074, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.564, D_sup_loss: 0.155, D_sup_acc: 96.35 Train acc: 95.945 Test acc: 96.280 \n",
      "step: 2613 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.587, D_sup_loss: 0.156, D_sup_acc: 96.33 Train acc: 95.987 Test acc: 96.340 \n",
      "step: 2614 | Train: G_Loss: 1.158, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.715, D_sup_loss: 0.152, D_sup_acc: 96.39 Train acc: 95.975 Test acc: 96.270 \n",
      "step: 2615 | Train: G_Loss: 1.133, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.604, D_sup_loss: 0.153, D_sup_acc: 96.32 Train acc: 95.922 Test acc: 96.270 \n",
      "step: 2616 | Train: G_Loss: 1.052, D_unsup_loss_real: 0.691, D_unsup_loss_fake: 0.638, D_sup_loss: 0.159, D_sup_acc: 96.32 Train acc: 95.820 Test acc: 96.170 \n",
      "step: 2617 | Train: G_Loss: 1.045, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.632, D_sup_loss: 0.163, D_sup_acc: 96.22 Train acc: 95.757 Test acc: 95.970 \n",
      "step: 2618 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.652, D_unsup_loss_fake: 0.563, D_sup_loss: 0.167, D_sup_acc: 96.02 Train acc: 95.817 Test acc: 96.170 \n",
      "step: 2619 | Train: G_Loss: 0.993, D_unsup_loss_real: 0.666, D_unsup_loss_fake: 0.623, D_sup_loss: 0.158, D_sup_acc: 96.22 Train acc: 95.883 Test acc: 96.320 \n",
      "step: 2620 | Train: G_Loss: 1.069, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.629, D_sup_loss: 0.154, D_sup_acc: 96.37 Train acc: 95.948 Test acc: 96.360 \n",
      "step: 2621 | Train: G_Loss: 1.052, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.652, D_sup_loss: 0.151, D_sup_acc: 96.41 Train acc: 95.805 Test acc: 96.320 \n",
      "step: 2622 | Train: G_Loss: 0.967, D_unsup_loss_real: 0.661, D_unsup_loss_fake: 0.628, D_sup_loss: 0.153, D_sup_acc: 96.37 Train acc: 95.853 Test acc: 96.340 \n",
      "step: 2623 | Train: G_Loss: 1.076, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.614, D_sup_loss: 0.156, D_sup_acc: 96.39 Train acc: 95.823 Test acc: 96.350 \n",
      "step: 2624 | Train: G_Loss: 1.079, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.583, D_sup_loss: 0.155, D_sup_acc: 96.40 Train acc: 95.848 Test acc: 96.330 \n",
      "step: 2625 | Train: G_Loss: 1.110, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.644, D_sup_loss: 0.155, D_sup_acc: 96.38 Train acc: 95.908 Test acc: 96.320 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2626 | Train: G_Loss: 1.019, D_unsup_loss_real: 0.644, D_unsup_loss_fake: 0.617, D_sup_loss: 0.156, D_sup_acc: 96.37 Train acc: 95.775 Test acc: 96.130 \n",
      "step: 2627 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.669, D_unsup_loss_fake: 0.624, D_sup_loss: 0.161, D_sup_acc: 96.18 Train acc: 95.828 Test acc: 96.130 \n",
      "step: 2628 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.518, D_unsup_loss_fake: 0.570, D_sup_loss: 0.161, D_sup_acc: 96.18 Train acc: 95.898 Test acc: 96.180 \n",
      "step: 2629 | Train: G_Loss: 1.045, D_unsup_loss_real: 0.628, D_unsup_loss_fake: 0.641, D_sup_loss: 0.160, D_sup_acc: 96.23 Train acc: 95.855 Test acc: 96.200 \n",
      "step: 2630 | Train: G_Loss: 1.082, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.662, D_sup_loss: 0.159, D_sup_acc: 96.25 Train acc: 95.857 Test acc: 96.110 \n",
      "step: 2631 | Train: G_Loss: 1.104, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.621, D_sup_loss: 0.161, D_sup_acc: 96.16 Train acc: 95.865 Test acc: 96.230 \n",
      "step: 2632 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.715, D_sup_loss: 0.160, D_sup_acc: 96.28 Train acc: 95.855 Test acc: 96.140 \n",
      "step: 2633 | Train: G_Loss: 1.198, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.620, D_sup_loss: 0.165, D_sup_acc: 96.19 Train acc: 95.680 Test acc: 95.920 \n",
      "step: 2634 | Train: G_Loss: 1.041, D_unsup_loss_real: 0.654, D_unsup_loss_fake: 0.607, D_sup_loss: 0.170, D_sup_acc: 95.97 Train acc: 95.888 Test acc: 96.060 \n",
      "step: 2635 | Train: G_Loss: 1.201, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.658, D_sup_loss: 0.165, D_sup_acc: 96.11 Train acc: 95.917 Test acc: 96.180 \n",
      "step: 2636 | Train: G_Loss: 1.170, D_unsup_loss_real: 0.655, D_unsup_loss_fake: 0.588, D_sup_loss: 0.162, D_sup_acc: 96.23 Train acc: 95.933 Test acc: 96.110 \n",
      "step: 2637 | Train: G_Loss: 1.038, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.582, D_sup_loss: 0.159, D_sup_acc: 96.16 Train acc: 95.965 Test acc: 96.290 \n",
      "step: 2638 | Train: G_Loss: 1.089, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.598, D_sup_loss: 0.153, D_sup_acc: 96.34 Train acc: 95.957 Test acc: 96.300 \n",
      "step: 2639 | Train: G_Loss: 1.143, D_unsup_loss_real: 0.535, D_unsup_loss_fake: 0.698, D_sup_loss: 0.156, D_sup_acc: 96.35 Train acc: 95.997 Test acc: 96.310 \n",
      "step: 2640 | Train: G_Loss: 1.077, D_unsup_loss_real: 0.651, D_unsup_loss_fake: 0.647, D_sup_loss: 0.153, D_sup_acc: 96.36 Train acc: 95.962 Test acc: 96.180 \n",
      "step: 2641 | Train: G_Loss: 1.097, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.636, D_sup_loss: 0.156, D_sup_acc: 96.23 Train acc: 95.990 Test acc: 96.360 \n",
      "step: 2642 | Train: G_Loss: 1.049, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.682, D_sup_loss: 0.156, D_sup_acc: 96.41 Train acc: 95.993 Test acc: 96.280 \n",
      "step: 2643 | Train: G_Loss: 1.232, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.635, D_sup_loss: 0.156, D_sup_acc: 96.33 Train acc: 95.975 Test acc: 96.250 \n",
      "step: 2644 | Train: G_Loss: 1.076, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.562, D_sup_loss: 0.156, D_sup_acc: 96.30 Train acc: 95.973 Test acc: 96.330 \n",
      "step: 2645 | Train: G_Loss: 0.994, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.529, D_sup_loss: 0.157, D_sup_acc: 96.38 Train acc: 96.025 Test acc: 96.330 \n",
      "step: 2646 | Train: G_Loss: 1.108, D_unsup_loss_real: 0.525, D_unsup_loss_fake: 0.652, D_sup_loss: 0.152, D_sup_acc: 96.38 Train acc: 95.872 Test acc: 96.270 \n",
      "step: 2647 | Train: G_Loss: 1.091, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.668, D_sup_loss: 0.156, D_sup_acc: 96.32 Train acc: 95.923 Test acc: 96.300 \n",
      "step: 2648 | Train: G_Loss: 1.073, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.635, D_sup_loss: 0.156, D_sup_acc: 96.35 Train acc: 96.018 Test acc: 96.350 \n",
      "step: 2649 | Train: G_Loss: 1.175, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.592, D_sup_loss: 0.152, D_sup_acc: 96.40 Train acc: 95.982 Test acc: 96.340 \n",
      "step: 2650 | Train: G_Loss: 1.080, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.546, D_sup_loss: 0.153, D_sup_acc: 96.39 Train acc: 96.003 Test acc: 96.380 \n",
      "step: 2651 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.626, D_sup_loss: 0.151, D_sup_acc: 96.43 Train acc: 96.080 Test acc: 96.470 \n",
      "step: 2652 | Train: G_Loss: 1.081, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.560, D_sup_loss: 0.148, D_sup_acc: 96.51 Train acc: 96.105 Test acc: 96.480 \n",
      "step: 2653 | Train: G_Loss: 1.063, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.593, D_sup_loss: 0.149, D_sup_acc: 96.52 Train acc: 96.093 Test acc: 96.420 \n",
      "step: 2654 | Train: G_Loss: 1.045, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.688, D_sup_loss: 0.146, D_sup_acc: 96.47 Train acc: 96.100 Test acc: 96.510 \n",
      "step: 2655 | Train: G_Loss: 1.038, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.672, D_sup_loss: 0.150, D_sup_acc: 96.55 Train acc: 96.138 Test acc: 96.480 \n",
      "step: 2656 | Train: G_Loss: 1.092, D_unsup_loss_real: 0.646, D_unsup_loss_fake: 0.601, D_sup_loss: 0.147, D_sup_acc: 96.52 Train acc: 96.090 Test acc: 96.430 \n",
      "step: 2657 | Train: G_Loss: 1.080, D_unsup_loss_real: 0.664, D_unsup_loss_fake: 0.642, D_sup_loss: 0.149, D_sup_acc: 96.48 Train acc: 96.042 Test acc: 96.450 \n",
      "step: 2658 | Train: G_Loss: 1.074, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.671, D_sup_loss: 0.148, D_sup_acc: 96.49 Train acc: 95.952 Test acc: 96.330 \n",
      "step: 2659 | Train: G_Loss: 1.069, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.582, D_sup_loss: 0.157, D_sup_acc: 96.38 Train acc: 96.000 Test acc: 96.370 \n",
      "step: 2660 | Train: G_Loss: 1.030, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.511, D_sup_loss: 0.151, D_sup_acc: 96.42 Train acc: 96.032 Test acc: 96.320 \n",
      "step: 2661 | Train: G_Loss: 1.038, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.602, D_sup_loss: 0.150, D_sup_acc: 96.37 Train acc: 96.032 Test acc: 96.310 \n",
      "step: 2662 | Train: G_Loss: 1.087, D_unsup_loss_real: 0.516, D_unsup_loss_fake: 0.646, D_sup_loss: 0.153, D_sup_acc: 96.36 Train acc: 96.015 Test acc: 96.370 \n",
      "step: 2663 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.659, D_unsup_loss_fake: 0.695, D_sup_loss: 0.151, D_sup_acc: 96.42 Train acc: 96.093 Test acc: 96.350 \n",
      "step: 2664 | Train: G_Loss: 1.119, D_unsup_loss_real: 0.687, D_unsup_loss_fake: 0.605, D_sup_loss: 0.151, D_sup_acc: 96.40 Train acc: 96.127 Test acc: 96.390 \n",
      "step: 2665 | Train: G_Loss: 1.091, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.605, D_sup_loss: 0.148, D_sup_acc: 96.44 Train acc: 96.088 Test acc: 96.370 \n",
      "step: 2666 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.616, D_sup_loss: 0.153, D_sup_acc: 96.42 Train acc: 96.070 Test acc: 96.350 \n",
      "step: 2667 | Train: G_Loss: 1.081, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.610, D_sup_loss: 0.153, D_sup_acc: 96.40 Train acc: 96.118 Test acc: 96.440 \n",
      "step: 2668 | Train: G_Loss: 1.060, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.642, D_sup_loss: 0.147, D_sup_acc: 96.48 Train acc: 96.105 Test acc: 96.380 \n",
      "step: 2669 | Train: G_Loss: 1.150, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.567, D_sup_loss: 0.152, D_sup_acc: 96.43 Train acc: 96.153 Test acc: 96.390 \n",
      "step: 2670 | Train: G_Loss: 1.127, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.681, D_sup_loss: 0.150, D_sup_acc: 96.44 Train acc: 96.143 Test acc: 96.390 \n",
      "step: 2671 | Train: G_Loss: 1.133, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.672, D_sup_loss: 0.153, D_sup_acc: 96.44 Train acc: 96.133 Test acc: 96.390 \n",
      "step: 2672 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.622, D_sup_loss: 0.152, D_sup_acc: 96.44 Train acc: 96.047 Test acc: 96.530 \n",
      "step: 2673 | Train: G_Loss: 1.125, D_unsup_loss_real: 0.679, D_unsup_loss_fake: 0.674, D_sup_loss: 0.155, D_sup_acc: 96.57 Train acc: 95.985 Test acc: 96.390 \n",
      "step: 2674 | Train: G_Loss: 1.137, D_unsup_loss_real: 0.628, D_unsup_loss_fake: 0.548, D_sup_loss: 0.156, D_sup_acc: 96.44 Train acc: 96.072 Test acc: 96.450 \n",
      "step: 2675 | Train: G_Loss: 1.122, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.660, D_sup_loss: 0.153, D_sup_acc: 96.49 Train acc: 96.120 Test acc: 96.550 \n",
      "step: 2676 | Train: G_Loss: 1.139, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.636, D_sup_loss: 0.147, D_sup_acc: 96.59 Train acc: 96.118 Test acc: 96.520 \n",
      "step: 2677 | Train: G_Loss: 1.037, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.584, D_sup_loss: 0.147, D_sup_acc: 96.56 Train acc: 96.113 Test acc: 96.410 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2678 | Train: G_Loss: 1.061, D_unsup_loss_real: 0.659, D_unsup_loss_fake: 0.636, D_sup_loss: 0.144, D_sup_acc: 96.46 Train acc: 96.067 Test acc: 96.510 \n",
      "step: 2679 | Train: G_Loss: 1.034, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.629, D_sup_loss: 0.148, D_sup_acc: 96.55 Train acc: 96.058 Test acc: 96.560 \n",
      "step: 2680 | Train: G_Loss: 1.078, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.643, D_sup_loss: 0.150, D_sup_acc: 96.60 Train acc: 96.053 Test acc: 96.520 \n",
      "step: 2681 | Train: G_Loss: 1.208, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.652, D_sup_loss: 0.150, D_sup_acc: 96.56 Train acc: 96.023 Test acc: 96.500 \n",
      "step: 2682 | Train: G_Loss: 1.178, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.642, D_sup_loss: 0.153, D_sup_acc: 96.54 Train acc: 95.923 Test acc: 96.430 \n",
      "step: 2683 | Train: G_Loss: 1.176, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.585, D_sup_loss: 0.155, D_sup_acc: 96.48 Train acc: 95.995 Test acc: 96.490 \n",
      "step: 2684 | Train: G_Loss: 1.179, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.699, D_sup_loss: 0.152, D_sup_acc: 96.53 Train acc: 96.102 Test acc: 96.450 \n",
      "step: 2685 | Train: G_Loss: 1.060, D_unsup_loss_real: 0.660, D_unsup_loss_fake: 0.585, D_sup_loss: 0.155, D_sup_acc: 96.49 Train acc: 96.022 Test acc: 96.370 \n",
      "step: 2686 | Train: G_Loss: 1.122, D_unsup_loss_real: 0.667, D_unsup_loss_fake: 0.587, D_sup_loss: 0.155, D_sup_acc: 96.42 Train acc: 96.048 Test acc: 96.260 \n",
      "step: 2687 | Train: G_Loss: 1.108, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.573, D_sup_loss: 0.154, D_sup_acc: 96.31 Train acc: 96.060 Test acc: 96.360 \n",
      "step: 2688 | Train: G_Loss: 1.074, D_unsup_loss_real: 0.676, D_unsup_loss_fake: 0.630, D_sup_loss: 0.149, D_sup_acc: 96.41 Train acc: 95.935 Test acc: 96.120 \n",
      "step: 2689 | Train: G_Loss: 0.999, D_unsup_loss_real: 0.671, D_unsup_loss_fake: 0.629, D_sup_loss: 0.155, D_sup_acc: 96.17 Train acc: 96.047 Test acc: 96.350 \n",
      "step: 2690 | Train: G_Loss: 1.072, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.590, D_sup_loss: 0.151, D_sup_acc: 96.40 Train acc: 96.142 Test acc: 96.360 \n",
      "step: 2691 | Train: G_Loss: 1.085, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.663, D_sup_loss: 0.149, D_sup_acc: 96.41 Train acc: 95.992 Test acc: 96.310 \n",
      "step: 2692 | Train: G_Loss: 1.126, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.654, D_sup_loss: 0.156, D_sup_acc: 96.36 Train acc: 96.107 Test acc: 96.360 \n",
      "step: 2693 | Train: G_Loss: 1.043, D_unsup_loss_real: 0.653, D_unsup_loss_fake: 0.648, D_sup_loss: 0.148, D_sup_acc: 96.41 Train acc: 96.075 Test acc: 96.380 \n",
      "step: 2694 | Train: G_Loss: 1.065, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.558, D_sup_loss: 0.149, D_sup_acc: 96.43 Train acc: 96.060 Test acc: 96.320 \n",
      "step: 2695 | Train: G_Loss: 1.211, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.613, D_sup_loss: 0.149, D_sup_acc: 96.37 Train acc: 96.162 Test acc: 96.510 \n",
      "step: 2696 | Train: G_Loss: 1.169, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.579, D_sup_loss: 0.145, D_sup_acc: 96.55 Train acc: 96.192 Test acc: 96.680 \n",
      "step: 2697 | Train: G_Loss: 1.092, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.619, D_sup_loss: 0.143, D_sup_acc: 96.72 Train acc: 96.098 Test acc: 96.540 \n",
      "step: 2698 | Train: G_Loss: 1.122, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.562, D_sup_loss: 0.149, D_sup_acc: 96.58 Train acc: 96.228 Test acc: 96.590 \n",
      "step: 2699 | Train: G_Loss: 1.064, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.580, D_sup_loss: 0.144, D_sup_acc: 96.63 Train acc: 96.182 Test acc: 96.480 \n",
      "step: 2700 | Train: G_Loss: 1.228, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.589, D_sup_loss: 0.145, D_sup_acc: 96.52 Train acc: 96.120 Test acc: 96.470 \n",
      "Train Classifier Accuracy: 96.120%\n",
      "\n",
      "Test Classifier Accuracy: 96.470%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_2700.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_2700.h5\n",
      "step: 2701 | Train: G_Loss: 1.103, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.642, D_sup_loss: 0.144, D_sup_acc: 96.51 Train acc: 96.192 Test acc: 96.610 \n",
      "step: 2702 | Train: G_Loss: 1.069, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.612, D_sup_loss: 0.144, D_sup_acc: 96.65 Train acc: 96.215 Test acc: 96.630 \n",
      "step: 2703 | Train: G_Loss: 1.092, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.542, D_sup_loss: 0.144, D_sup_acc: 96.67 Train acc: 96.198 Test acc: 96.570 \n",
      "step: 2704 | Train: G_Loss: 1.126, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.624, D_sup_loss: 0.146, D_sup_acc: 96.61 Train acc: 96.112 Test acc: 96.510 \n",
      "step: 2705 | Train: G_Loss: 1.162, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.616, D_sup_loss: 0.149, D_sup_acc: 96.55 Train acc: 96.120 Test acc: 96.620 \n",
      "step: 2706 | Train: G_Loss: 1.158, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.546, D_sup_loss: 0.147, D_sup_acc: 96.66 Train acc: 96.178 Test acc: 96.710 \n",
      "step: 2707 | Train: G_Loss: 1.211, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.593, D_sup_loss: 0.143, D_sup_acc: 96.75 Train acc: 96.233 Test acc: 96.760 \n",
      "step: 2708 | Train: G_Loss: 1.031, D_unsup_loss_real: 0.639, D_unsup_loss_fake: 0.687, D_sup_loss: 0.138, D_sup_acc: 96.80 Train acc: 95.928 Test acc: 96.480 \n",
      "step: 2709 | Train: G_Loss: 1.128, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.616, D_sup_loss: 0.159, D_sup_acc: 96.52 Train acc: 96.107 Test acc: 96.610 \n",
      "step: 2710 | Train: G_Loss: 1.168, D_unsup_loss_real: 0.542, D_unsup_loss_fake: 0.625, D_sup_loss: 0.146, D_sup_acc: 96.65 Train acc: 96.132 Test acc: 96.650 \n",
      "step: 2711 | Train: G_Loss: 1.032, D_unsup_loss_real: 0.660, D_unsup_loss_fake: 0.559, D_sup_loss: 0.144, D_sup_acc: 96.69 Train acc: 96.097 Test acc: 96.660 \n",
      "step: 2712 | Train: G_Loss: 1.044, D_unsup_loss_real: 0.628, D_unsup_loss_fake: 0.675, D_sup_loss: 0.147, D_sup_acc: 96.70 Train acc: 96.047 Test acc: 96.530 \n",
      "step: 2713 | Train: G_Loss: 1.172, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.595, D_sup_loss: 0.150, D_sup_acc: 96.57 Train acc: 96.000 Test acc: 96.500 \n",
      "step: 2714 | Train: G_Loss: 1.049, D_unsup_loss_real: 0.675, D_unsup_loss_fake: 0.663, D_sup_loss: 0.149, D_sup_acc: 96.54 Train acc: 95.952 Test acc: 96.450 \n",
      "step: 2715 | Train: G_Loss: 1.074, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.731, D_sup_loss: 0.153, D_sup_acc: 96.49 Train acc: 95.780 Test acc: 96.260 \n",
      "step: 2716 | Train: G_Loss: 1.034, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.582, D_sup_loss: 0.162, D_sup_acc: 96.31 Train acc: 95.838 Test acc: 96.410 \n",
      "step: 2717 | Train: G_Loss: 1.101, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.615, D_sup_loss: 0.155, D_sup_acc: 96.46 Train acc: 95.930 Test acc: 96.350 \n",
      "step: 2718 | Train: G_Loss: 1.069, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.641, D_sup_loss: 0.152, D_sup_acc: 96.40 Train acc: 95.822 Test acc: 96.270 \n",
      "step: 2719 | Train: G_Loss: 1.059, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.625, D_sup_loss: 0.162, D_sup_acc: 96.32 Train acc: 96.010 Test acc: 96.460 \n",
      "step: 2720 | Train: G_Loss: 1.125, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.631, D_sup_loss: 0.154, D_sup_acc: 96.50 Train acc: 96.060 Test acc: 96.490 \n",
      "step: 2721 | Train: G_Loss: 1.103, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.573, D_sup_loss: 0.154, D_sup_acc: 96.53 Train acc: 96.098 Test acc: 96.500 \n",
      "step: 2722 | Train: G_Loss: 1.074, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.601, D_sup_loss: 0.145, D_sup_acc: 96.54 Train acc: 96.108 Test acc: 96.560 \n",
      "step: 2723 | Train: G_Loss: 1.080, D_unsup_loss_real: 0.651, D_unsup_loss_fake: 0.649, D_sup_loss: 0.147, D_sup_acc: 96.60 Train acc: 96.063 Test acc: 96.440 \n",
      "step: 2724 | Train: G_Loss: 1.074, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.596, D_sup_loss: 0.151, D_sup_acc: 96.48 Train acc: 95.930 Test acc: 96.280 \n",
      "step: 2725 | Train: G_Loss: 1.157, D_unsup_loss_real: 0.668, D_unsup_loss_fake: 0.718, D_sup_loss: 0.153, D_sup_acc: 96.33 Train acc: 96.053 Test acc: 96.320 \n",
      "step: 2726 | Train: G_Loss: 1.159, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.556, D_sup_loss: 0.153, D_sup_acc: 96.37 Train acc: 95.993 Test acc: 96.340 \n",
      "step: 2727 | Train: G_Loss: 1.118, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.609, D_sup_loss: 0.151, D_sup_acc: 96.39 Train acc: 96.088 Test acc: 96.380 \n",
      "step: 2728 | Train: G_Loss: 1.083, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.597, D_sup_loss: 0.148, D_sup_acc: 96.43 Train acc: 96.152 Test acc: 96.490 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2729 | Train: G_Loss: 1.094, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.640, D_sup_loss: 0.147, D_sup_acc: 96.53 Train acc: 96.088 Test acc: 96.410 \n",
      "step: 2730 | Train: G_Loss: 1.036, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.595, D_sup_loss: 0.147, D_sup_acc: 96.46 Train acc: 96.063 Test acc: 96.470 \n",
      "step: 2731 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.696, D_unsup_loss_fake: 0.643, D_sup_loss: 0.147, D_sup_acc: 96.51 Train acc: 96.018 Test acc: 96.460 \n",
      "step: 2732 | Train: G_Loss: 1.029, D_unsup_loss_real: 0.536, D_unsup_loss_fake: 0.649, D_sup_loss: 0.149, D_sup_acc: 96.50 Train acc: 96.040 Test acc: 96.560 \n",
      "step: 2733 | Train: G_Loss: 1.169, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.598, D_sup_loss: 0.147, D_sup_acc: 96.60 Train acc: 96.037 Test acc: 96.470 \n",
      "step: 2734 | Train: G_Loss: 1.114, D_unsup_loss_real: 0.719, D_unsup_loss_fake: 0.637, D_sup_loss: 0.145, D_sup_acc: 96.51 Train acc: 96.037 Test acc: 96.440 \n",
      "step: 2735 | Train: G_Loss: 1.091, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.712, D_sup_loss: 0.151, D_sup_acc: 96.48 Train acc: 96.002 Test acc: 96.540 \n",
      "step: 2736 | Train: G_Loss: 1.033, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.616, D_sup_loss: 0.153, D_sup_acc: 96.58 Train acc: 96.022 Test acc: 96.360 \n",
      "step: 2737 | Train: G_Loss: 1.076, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.652, D_sup_loss: 0.155, D_sup_acc: 96.41 Train acc: 95.745 Test acc: 96.200 \n",
      "step: 2738 | Train: G_Loss: 1.181, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.635, D_sup_loss: 0.168, D_sup_acc: 96.25 Train acc: 95.955 Test acc: 96.440 \n",
      "step: 2739 | Train: G_Loss: 1.080, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.588, D_sup_loss: 0.155, D_sup_acc: 96.48 Train acc: 96.023 Test acc: 96.580 \n",
      "step: 2740 | Train: G_Loss: 1.158, D_unsup_loss_real: 0.658, D_unsup_loss_fake: 0.614, D_sup_loss: 0.153, D_sup_acc: 96.62 Train acc: 96.042 Test acc: 96.600 \n",
      "step: 2741 | Train: G_Loss: 1.115, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.644, D_sup_loss: 0.155, D_sup_acc: 96.64 Train acc: 96.033 Test acc: 96.530 \n",
      "step: 2742 | Train: G_Loss: 1.091, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.639, D_sup_loss: 0.154, D_sup_acc: 96.57 Train acc: 95.903 Test acc: 96.400 \n",
      "step: 2743 | Train: G_Loss: 1.033, D_unsup_loss_real: 0.734, D_unsup_loss_fake: 0.614, D_sup_loss: 0.161, D_sup_acc: 96.45 Train acc: 95.882 Test acc: 96.400 \n",
      "step: 2744 | Train: G_Loss: 1.132, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.608, D_sup_loss: 0.161, D_sup_acc: 96.45 Train acc: 96.020 Test acc: 96.460 \n",
      "step: 2745 | Train: G_Loss: 1.040, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.645, D_sup_loss: 0.156, D_sup_acc: 96.50 Train acc: 96.068 Test acc: 96.430 \n",
      "step: 2746 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.617, D_sup_loss: 0.156, D_sup_acc: 96.48 Train acc: 96.117 Test acc: 96.390 \n",
      "step: 2747 | Train: G_Loss: 1.101, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.655, D_sup_loss: 0.155, D_sup_acc: 96.44 Train acc: 96.008 Test acc: 96.220 \n",
      "step: 2748 | Train: G_Loss: 1.041, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.585, D_sup_loss: 0.156, D_sup_acc: 96.27 Train acc: 96.022 Test acc: 96.250 \n",
      "step: 2749 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.657, D_sup_loss: 0.158, D_sup_acc: 96.30 Train acc: 95.937 Test acc: 96.220 \n",
      "step: 2750 | Train: G_Loss: 1.047, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.581, D_sup_loss: 0.164, D_sup_acc: 96.27 Train acc: 96.022 Test acc: 96.390 \n",
      "step: 2751 | Train: G_Loss: 1.163, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.565, D_sup_loss: 0.160, D_sup_acc: 96.44 Train acc: 96.098 Test acc: 96.390 \n",
      "step: 2752 | Train: G_Loss: 1.068, D_unsup_loss_real: 0.687, D_unsup_loss_fake: 0.618, D_sup_loss: 0.156, D_sup_acc: 96.44 Train acc: 96.007 Test acc: 96.290 \n",
      "step: 2753 | Train: G_Loss: 0.987, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.604, D_sup_loss: 0.159, D_sup_acc: 96.34 Train acc: 96.105 Test acc: 96.340 \n",
      "step: 2754 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.614, D_sup_loss: 0.155, D_sup_acc: 96.39 Train acc: 96.032 Test acc: 96.380 \n",
      "step: 2755 | Train: G_Loss: 1.033, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.613, D_sup_loss: 0.156, D_sup_acc: 96.43 Train acc: 96.013 Test acc: 96.410 \n",
      "step: 2756 | Train: G_Loss: 1.092, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.602, D_sup_loss: 0.157, D_sup_acc: 96.46 Train acc: 96.035 Test acc: 96.290 \n",
      "step: 2757 | Train: G_Loss: 1.077, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.622, D_sup_loss: 0.155, D_sup_acc: 96.34 Train acc: 95.952 Test acc: 96.140 \n",
      "step: 2758 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.637, D_sup_loss: 0.160, D_sup_acc: 96.19 Train acc: 96.038 Test acc: 96.250 \n",
      "step: 2759 | Train: G_Loss: 1.032, D_unsup_loss_real: 0.660, D_unsup_loss_fake: 0.654, D_sup_loss: 0.157, D_sup_acc: 96.30 Train acc: 96.092 Test acc: 96.430 \n",
      "step: 2760 | Train: G_Loss: 1.147, D_unsup_loss_real: 0.648, D_unsup_loss_fake: 0.634, D_sup_loss: 0.157, D_sup_acc: 96.48 Train acc: 96.087 Test acc: 96.430 \n",
      "step: 2761 | Train: G_Loss: 0.935, D_unsup_loss_real: 0.659, D_unsup_loss_fake: 0.602, D_sup_loss: 0.153, D_sup_acc: 96.48 Train acc: 96.055 Test acc: 96.420 \n",
      "step: 2762 | Train: G_Loss: 1.192, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.650, D_sup_loss: 0.152, D_sup_acc: 96.47 Train acc: 96.053 Test acc: 96.400 \n",
      "step: 2763 | Train: G_Loss: 1.170, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.609, D_sup_loss: 0.150, D_sup_acc: 96.45 Train acc: 96.168 Test acc: 96.550 \n",
      "step: 2764 | Train: G_Loss: 1.065, D_unsup_loss_real: 0.656, D_unsup_loss_fake: 0.625, D_sup_loss: 0.149, D_sup_acc: 96.59 Train acc: 96.165 Test acc: 96.590 \n",
      "step: 2765 | Train: G_Loss: 1.114, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.640, D_sup_loss: 0.147, D_sup_acc: 96.63 Train acc: 96.047 Test acc: 96.630 \n",
      "step: 2766 | Train: G_Loss: 1.096, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.650, D_sup_loss: 0.152, D_sup_acc: 96.67 Train acc: 95.940 Test acc: 96.470 \n",
      "step: 2767 | Train: G_Loss: 1.032, D_unsup_loss_real: 0.666, D_unsup_loss_fake: 0.575, D_sup_loss: 0.155, D_sup_acc: 96.51 Train acc: 95.953 Test acc: 96.490 \n",
      "step: 2768 | Train: G_Loss: 1.029, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.610, D_sup_loss: 0.153, D_sup_acc: 96.53 Train acc: 96.013 Test acc: 96.590 \n",
      "step: 2769 | Train: G_Loss: 1.007, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.610, D_sup_loss: 0.153, D_sup_acc: 96.63 Train acc: 96.128 Test acc: 96.640 \n",
      "step: 2770 | Train: G_Loss: 1.056, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.610, D_sup_loss: 0.146, D_sup_acc: 96.68 Train acc: 96.128 Test acc: 96.600 \n",
      "step: 2771 | Train: G_Loss: 1.139, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.676, D_sup_loss: 0.143, D_sup_acc: 96.64 Train acc: 96.097 Test acc: 96.610 \n",
      "step: 2772 | Train: G_Loss: 1.120, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.646, D_sup_loss: 0.145, D_sup_acc: 96.65 Train acc: 95.820 Test acc: 96.100 \n",
      "step: 2773 | Train: G_Loss: 1.115, D_unsup_loss_real: 0.709, D_unsup_loss_fake: 0.604, D_sup_loss: 0.160, D_sup_acc: 96.15 Train acc: 95.760 Test acc: 96.110 \n",
      "step: 2774 | Train: G_Loss: 1.076, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.619, D_sup_loss: 0.165, D_sup_acc: 96.16 Train acc: 95.878 Test acc: 96.170 \n",
      "step: 2775 | Train: G_Loss: 1.066, D_unsup_loss_real: 0.678, D_unsup_loss_fake: 0.593, D_sup_loss: 0.160, D_sup_acc: 96.22 Train acc: 96.063 Test acc: 96.270 \n",
      "step: 2776 | Train: G_Loss: 0.967, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.673, D_sup_loss: 0.153, D_sup_acc: 96.32 Train acc: 96.123 Test acc: 96.460 \n",
      "step: 2777 | Train: G_Loss: 1.056, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.588, D_sup_loss: 0.153, D_sup_acc: 96.50 Train acc: 96.192 Test acc: 96.530 \n",
      "step: 2778 | Train: G_Loss: 1.008, D_unsup_loss_real: 0.525, D_unsup_loss_fake: 0.660, D_sup_loss: 0.149, D_sup_acc: 96.57 Train acc: 96.178 Test acc: 96.570 \n",
      "step: 2779 | Train: G_Loss: 0.927, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.615, D_sup_loss: 0.148, D_sup_acc: 96.61 Train acc: 96.125 Test acc: 96.480 \n",
      "step: 2780 | Train: G_Loss: 1.147, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.630, D_sup_loss: 0.154, D_sup_acc: 96.52 Train acc: 96.055 Test acc: 96.470 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2781 | Train: G_Loss: 1.090, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.718, D_sup_loss: 0.154, D_sup_acc: 96.51 Train acc: 95.882 Test acc: 96.210 \n",
      "step: 2782 | Train: G_Loss: 1.035, D_unsup_loss_real: 0.675, D_unsup_loss_fake: 0.612, D_sup_loss: 0.164, D_sup_acc: 96.26 Train acc: 95.888 Test acc: 96.270 \n",
      "step: 2783 | Train: G_Loss: 1.004, D_unsup_loss_real: 0.657, D_unsup_loss_fake: 0.634, D_sup_loss: 0.162, D_sup_acc: 96.32 Train acc: 95.955 Test acc: 96.330 \n",
      "step: 2784 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.669, D_sup_loss: 0.163, D_sup_acc: 96.38 Train acc: 95.942 Test acc: 96.210 \n",
      "step: 2785 | Train: G_Loss: 1.043, D_unsup_loss_real: 0.534, D_unsup_loss_fake: 0.604, D_sup_loss: 0.165, D_sup_acc: 96.26 Train acc: 95.977 Test acc: 96.270 \n",
      "step: 2786 | Train: G_Loss: 1.104, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.547, D_sup_loss: 0.159, D_sup_acc: 96.32 Train acc: 95.950 Test acc: 96.230 \n",
      "step: 2787 | Train: G_Loss: 1.101, D_unsup_loss_real: 0.686, D_unsup_loss_fake: 0.689, D_sup_loss: 0.157, D_sup_acc: 96.28 Train acc: 95.867 Test acc: 96.220 \n",
      "step: 2788 | Train: G_Loss: 1.038, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.618, D_sup_loss: 0.168, D_sup_acc: 96.27 Train acc: 95.957 Test acc: 96.340 \n",
      "step: 2789 | Train: G_Loss: 1.087, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.629, D_sup_loss: 0.159, D_sup_acc: 96.39 Train acc: 96.018 Test acc: 96.410 \n",
      "step: 2790 | Train: G_Loss: 1.130, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.602, D_sup_loss: 0.156, D_sup_acc: 96.46 Train acc: 95.998 Test acc: 96.370 \n",
      "step: 2791 | Train: G_Loss: 1.160, D_unsup_loss_real: 0.651, D_unsup_loss_fake: 0.627, D_sup_loss: 0.154, D_sup_acc: 96.42 Train acc: 96.003 Test acc: 96.390 \n",
      "step: 2792 | Train: G_Loss: 1.122, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.638, D_sup_loss: 0.159, D_sup_acc: 96.44 Train acc: 95.930 Test acc: 96.410 \n",
      "step: 2793 | Train: G_Loss: 1.055, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.591, D_sup_loss: 0.165, D_sup_acc: 96.46 Train acc: 95.915 Test acc: 96.330 \n",
      "step: 2794 | Train: G_Loss: 1.108, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.578, D_sup_loss: 0.161, D_sup_acc: 96.38 Train acc: 95.763 Test acc: 96.250 \n",
      "step: 2795 | Train: G_Loss: 1.106, D_unsup_loss_real: 0.639, D_unsup_loss_fake: 0.569, D_sup_loss: 0.165, D_sup_acc: 96.30 Train acc: 95.825 Test acc: 96.290 \n",
      "step: 2796 | Train: G_Loss: 0.993, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.559, D_sup_loss: 0.165, D_sup_acc: 96.34 Train acc: 95.980 Test acc: 96.470 \n",
      "step: 2797 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.692, D_sup_loss: 0.161, D_sup_acc: 96.51 Train acc: 96.108 Test acc: 96.600 \n",
      "step: 2798 | Train: G_Loss: 1.034, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.603, D_sup_loss: 0.152, D_sup_acc: 96.64 Train acc: 96.153 Test acc: 96.610 \n",
      "step: 2799 | Train: G_Loss: 1.071, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.558, D_sup_loss: 0.149, D_sup_acc: 96.65 Train acc: 96.112 Test acc: 96.510 \n",
      "step: 2800 | Train: G_Loss: 1.041, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.678, D_sup_loss: 0.148, D_sup_acc: 96.55 Train acc: 96.065 Test acc: 96.500 \n",
      "Train Classifier Accuracy: 96.065%\n",
      "\n",
      "Test Classifier Accuracy: 96.500%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_2800.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_2800.h5\n",
      "step: 2801 | Train: G_Loss: 1.105, D_unsup_loss_real: 0.684, D_unsup_loss_fake: 0.586, D_sup_loss: 0.152, D_sup_acc: 96.54 Train acc: 95.848 Test acc: 96.280 \n",
      "step: 2802 | Train: G_Loss: 1.071, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.636, D_sup_loss: 0.164, D_sup_acc: 96.33 Train acc: 95.860 Test acc: 96.250 \n",
      "step: 2803 | Train: G_Loss: 1.038, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.615, D_sup_loss: 0.164, D_sup_acc: 96.30 Train acc: 95.832 Test acc: 96.240 \n",
      "step: 2804 | Train: G_Loss: 1.060, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.613, D_sup_loss: 0.166, D_sup_acc: 96.29 Train acc: 95.955 Test acc: 96.410 \n",
      "step: 2805 | Train: G_Loss: 1.101, D_unsup_loss_real: 0.649, D_unsup_loss_fake: 0.635, D_sup_loss: 0.163, D_sup_acc: 96.46 Train acc: 95.887 Test acc: 96.350 \n",
      "step: 2806 | Train: G_Loss: 0.977, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.596, D_sup_loss: 0.162, D_sup_acc: 96.40 Train acc: 96.073 Test acc: 96.490 \n",
      "step: 2807 | Train: G_Loss: 1.149, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.697, D_sup_loss: 0.153, D_sup_acc: 96.53 Train acc: 96.145 Test acc: 96.470 \n",
      "step: 2808 | Train: G_Loss: 1.192, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.624, D_sup_loss: 0.151, D_sup_acc: 96.51 Train acc: 96.073 Test acc: 96.370 \n",
      "step: 2809 | Train: G_Loss: 1.165, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.588, D_sup_loss: 0.153, D_sup_acc: 96.42 Train acc: 96.027 Test acc: 96.350 \n",
      "step: 2810 | Train: G_Loss: 1.097, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.615, D_sup_loss: 0.158, D_sup_acc: 96.40 Train acc: 95.812 Test acc: 96.220 \n",
      "step: 2811 | Train: G_Loss: 1.112, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.604, D_sup_loss: 0.171, D_sup_acc: 96.27 Train acc: 96.003 Test acc: 96.340 \n",
      "step: 2812 | Train: G_Loss: 1.012, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.689, D_sup_loss: 0.159, D_sup_acc: 96.39 Train acc: 95.932 Test acc: 96.410 \n",
      "step: 2813 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.690, D_unsup_loss_fake: 0.656, D_sup_loss: 0.165, D_sup_acc: 96.46 Train acc: 96.047 Test acc: 96.520 \n",
      "step: 2814 | Train: G_Loss: 1.024, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.670, D_sup_loss: 0.160, D_sup_acc: 96.56 Train acc: 95.968 Test acc: 96.370 \n",
      "step: 2815 | Train: G_Loss: 1.126, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.715, D_sup_loss: 0.164, D_sup_acc: 96.42 Train acc: 95.962 Test acc: 96.330 \n",
      "step: 2816 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.595, D_sup_loss: 0.167, D_sup_acc: 96.38 Train acc: 96.213 Test acc: 96.490 \n",
      "step: 2817 | Train: G_Loss: 1.062, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.653, D_sup_loss: 0.152, D_sup_acc: 96.53 Train acc: 96.175 Test acc: 96.460 \n",
      "step: 2818 | Train: G_Loss: 1.154, D_unsup_loss_real: 0.680, D_unsup_loss_fake: 0.654, D_sup_loss: 0.155, D_sup_acc: 96.50 Train acc: 96.158 Test acc: 96.570 \n",
      "step: 2819 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.683, D_unsup_loss_fake: 0.545, D_sup_loss: 0.152, D_sup_acc: 96.61 Train acc: 96.218 Test acc: 96.540 \n",
      "step: 2820 | Train: G_Loss: 1.025, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.605, D_sup_loss: 0.151, D_sup_acc: 96.58 Train acc: 96.193 Test acc: 96.520 \n",
      "step: 2821 | Train: G_Loss: 1.019, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.662, D_sup_loss: 0.149, D_sup_acc: 96.56 Train acc: 96.062 Test acc: 96.400 \n",
      "step: 2822 | Train: G_Loss: 1.096, D_unsup_loss_real: 0.535, D_unsup_loss_fake: 0.697, D_sup_loss: 0.149, D_sup_acc: 96.45 Train acc: 96.108 Test acc: 96.470 \n",
      "step: 2823 | Train: G_Loss: 1.039, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.628, D_sup_loss: 0.153, D_sup_acc: 96.51 Train acc: 96.030 Test acc: 96.470 \n",
      "step: 2824 | Train: G_Loss: 1.071, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.594, D_sup_loss: 0.155, D_sup_acc: 96.51 Train acc: 95.947 Test acc: 96.400 \n",
      "step: 2825 | Train: G_Loss: 1.147, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.628, D_sup_loss: 0.161, D_sup_acc: 96.45 Train acc: 96.142 Test acc: 96.520 \n",
      "step: 2826 | Train: G_Loss: 1.167, D_unsup_loss_real: 0.656, D_unsup_loss_fake: 0.624, D_sup_loss: 0.151, D_sup_acc: 96.56 Train acc: 96.115 Test acc: 96.420 \n",
      "step: 2827 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.615, D_sup_loss: 0.152, D_sup_acc: 96.47 Train acc: 96.152 Test acc: 96.590 \n",
      "step: 2828 | Train: G_Loss: 1.120, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.609, D_sup_loss: 0.151, D_sup_acc: 96.63 Train acc: 96.187 Test acc: 96.630 \n",
      "step: 2829 | Train: G_Loss: 1.107, D_unsup_loss_real: 0.642, D_unsup_loss_fake: 0.581, D_sup_loss: 0.150, D_sup_acc: 96.67 Train acc: 96.177 Test acc: 96.530 \n",
      "step: 2830 | Train: G_Loss: 1.092, D_unsup_loss_real: 0.651, D_unsup_loss_fake: 0.615, D_sup_loss: 0.149, D_sup_acc: 96.57 Train acc: 96.147 Test acc: 96.550 \n",
      "step: 2831 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.607, D_sup_loss: 0.149, D_sup_acc: 96.59 Train acc: 96.108 Test acc: 96.530 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2832 | Train: G_Loss: 1.097, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.599, D_sup_loss: 0.152, D_sup_acc: 96.57 Train acc: 96.050 Test acc: 96.460 \n",
      "step: 2833 | Train: G_Loss: 1.099, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.601, D_sup_loss: 0.156, D_sup_acc: 96.50 Train acc: 95.988 Test acc: 96.480 \n",
      "step: 2834 | Train: G_Loss: 1.084, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.636, D_sup_loss: 0.157, D_sup_acc: 96.52 Train acc: 95.828 Test acc: 96.410 \n",
      "step: 2835 | Train: G_Loss: 1.101, D_unsup_loss_real: 0.682, D_unsup_loss_fake: 0.591, D_sup_loss: 0.164, D_sup_acc: 96.46 Train acc: 96.037 Test acc: 96.550 \n",
      "step: 2836 | Train: G_Loss: 1.151, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.607, D_sup_loss: 0.151, D_sup_acc: 96.59 Train acc: 96.128 Test acc: 96.540 \n",
      "step: 2837 | Train: G_Loss: 1.136, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.666, D_sup_loss: 0.147, D_sup_acc: 96.58 Train acc: 96.058 Test acc: 96.450 \n",
      "step: 2838 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.744, D_sup_loss: 0.155, D_sup_acc: 96.49 Train acc: 96.158 Test acc: 96.460 \n",
      "step: 2839 | Train: G_Loss: 1.059, D_unsup_loss_real: 0.628, D_unsup_loss_fake: 0.609, D_sup_loss: 0.148, D_sup_acc: 96.50 Train acc: 96.037 Test acc: 96.370 \n",
      "step: 2840 | Train: G_Loss: 1.031, D_unsup_loss_real: 0.652, D_unsup_loss_fake: 0.595, D_sup_loss: 0.152, D_sup_acc: 96.42 Train acc: 96.185 Test acc: 96.560 \n",
      "step: 2841 | Train: G_Loss: 1.208, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.676, D_sup_loss: 0.143, D_sup_acc: 96.60 Train acc: 96.210 Test acc: 96.580 \n",
      "step: 2842 | Train: G_Loss: 1.083, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.629, D_sup_loss: 0.140, D_sup_acc: 96.62 Train acc: 96.182 Test acc: 96.570 \n",
      "step: 2843 | Train: G_Loss: 1.165, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.633, D_sup_loss: 0.144, D_sup_acc: 96.61 Train acc: 96.130 Test acc: 96.450 \n",
      "step: 2844 | Train: G_Loss: 1.082, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.608, D_sup_loss: 0.147, D_sup_acc: 96.49 Train acc: 96.080 Test acc: 96.400 \n",
      "step: 2845 | Train: G_Loss: 1.069, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.583, D_sup_loss: 0.151, D_sup_acc: 96.45 Train acc: 96.108 Test acc: 96.430 \n",
      "step: 2846 | Train: G_Loss: 1.099, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.578, D_sup_loss: 0.154, D_sup_acc: 96.48 Train acc: 95.955 Test acc: 96.360 \n",
      "step: 2847 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.541, D_sup_loss: 0.157, D_sup_acc: 96.41 Train acc: 96.023 Test acc: 96.360 \n",
      "step: 2848 | Train: G_Loss: 1.175, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.685, D_sup_loss: 0.154, D_sup_acc: 96.41 Train acc: 95.958 Test acc: 96.470 \n",
      "step: 2849 | Train: G_Loss: 1.069, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.631, D_sup_loss: 0.153, D_sup_acc: 96.51 Train acc: 96.102 Test acc: 96.560 \n",
      "step: 2850 | Train: G_Loss: 1.055, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.682, D_sup_loss: 0.147, D_sup_acc: 96.60 Train acc: 96.037 Test acc: 96.530 \n",
      "step: 2851 | Train: G_Loss: 1.141, D_unsup_loss_real: 0.684, D_unsup_loss_fake: 0.604, D_sup_loss: 0.151, D_sup_acc: 96.57 Train acc: 96.048 Test acc: 96.470 \n",
      "step: 2852 | Train: G_Loss: 1.052, D_unsup_loss_real: 0.677, D_unsup_loss_fake: 0.659, D_sup_loss: 0.153, D_sup_acc: 96.51 Train acc: 96.042 Test acc: 96.510 \n",
      "step: 2853 | Train: G_Loss: 1.137, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.634, D_sup_loss: 0.155, D_sup_acc: 96.55 Train acc: 96.127 Test acc: 96.590 \n",
      "step: 2854 | Train: G_Loss: 1.135, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.687, D_sup_loss: 0.151, D_sup_acc: 96.63 Train acc: 96.097 Test acc: 96.500 \n",
      "step: 2855 | Train: G_Loss: 1.182, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.603, D_sup_loss: 0.151, D_sup_acc: 96.54 Train acc: 96.168 Test acc: 96.590 \n",
      "step: 2856 | Train: G_Loss: 1.143, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.658, D_sup_loss: 0.142, D_sup_acc: 96.63 Train acc: 96.095 Test acc: 96.600 \n",
      "step: 2857 | Train: G_Loss: 1.078, D_unsup_loss_real: 0.649, D_unsup_loss_fake: 0.657, D_sup_loss: 0.149, D_sup_acc: 96.64 Train acc: 96.085 Test acc: 96.510 \n",
      "step: 2858 | Train: G_Loss: 1.056, D_unsup_loss_real: 0.647, D_unsup_loss_fake: 0.609, D_sup_loss: 0.153, D_sup_acc: 96.55 Train acc: 95.988 Test acc: 96.470 \n",
      "step: 2859 | Train: G_Loss: 1.046, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.561, D_sup_loss: 0.157, D_sup_acc: 96.51 Train acc: 96.105 Test acc: 96.550 \n",
      "step: 2860 | Train: G_Loss: 1.143, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.622, D_sup_loss: 0.148, D_sup_acc: 96.59 Train acc: 96.073 Test acc: 96.570 \n",
      "step: 2861 | Train: G_Loss: 1.111, D_unsup_loss_real: 0.666, D_unsup_loss_fake: 0.661, D_sup_loss: 0.151, D_sup_acc: 96.61 Train acc: 96.157 Test acc: 96.610 \n",
      "step: 2862 | Train: G_Loss: 1.019, D_unsup_loss_real: 0.671, D_unsup_loss_fake: 0.582, D_sup_loss: 0.148, D_sup_acc: 96.65 Train acc: 96.127 Test acc: 96.650 \n",
      "step: 2863 | Train: G_Loss: 0.982, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.590, D_sup_loss: 0.150, D_sup_acc: 96.69 Train acc: 96.092 Test acc: 96.670 \n",
      "step: 2864 | Train: G_Loss: 1.100, D_unsup_loss_real: 0.707, D_unsup_loss_fake: 0.645, D_sup_loss: 0.149, D_sup_acc: 96.71 Train acc: 96.042 Test acc: 96.510 \n",
      "step: 2865 | Train: G_Loss: 1.043, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.608, D_sup_loss: 0.153, D_sup_acc: 96.55 Train acc: 95.982 Test acc: 96.500 \n",
      "step: 2866 | Train: G_Loss: 1.078, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.690, D_sup_loss: 0.154, D_sup_acc: 96.54 Train acc: 96.187 Test acc: 96.660 \n",
      "step: 2867 | Train: G_Loss: 1.070, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.631, D_sup_loss: 0.145, D_sup_acc: 96.70 Train acc: 96.048 Test acc: 96.620 \n",
      "step: 2868 | Train: G_Loss: 1.143, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.637, D_sup_loss: 0.148, D_sup_acc: 96.66 Train acc: 96.028 Test acc: 96.600 \n",
      "step: 2869 | Train: G_Loss: 1.005, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.686, D_sup_loss: 0.153, D_sup_acc: 96.64 Train acc: 96.080 Test acc: 96.650 \n",
      "step: 2870 | Train: G_Loss: 1.209, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.644, D_sup_loss: 0.149, D_sup_acc: 96.69 Train acc: 96.177 Test acc: 96.510 \n",
      "step: 2871 | Train: G_Loss: 1.054, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.629, D_sup_loss: 0.146, D_sup_acc: 96.55 Train acc: 96.188 Test acc: 96.560 \n",
      "step: 2872 | Train: G_Loss: 1.174, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.667, D_sup_loss: 0.146, D_sup_acc: 96.60 Train acc: 96.182 Test acc: 96.580 \n",
      "step: 2873 | Train: G_Loss: 1.040, D_unsup_loss_real: 0.695, D_unsup_loss_fake: 0.602, D_sup_loss: 0.149, D_sup_acc: 96.62 Train acc: 96.037 Test acc: 96.430 \n",
      "step: 2874 | Train: G_Loss: 1.031, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.651, D_sup_loss: 0.156, D_sup_acc: 96.48 Train acc: 96.040 Test acc: 96.490 \n",
      "step: 2875 | Train: G_Loss: 1.152, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.629, D_sup_loss: 0.152, D_sup_acc: 96.53 Train acc: 96.140 Test acc: 96.600 \n",
      "step: 2876 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.569, D_sup_loss: 0.146, D_sup_acc: 96.64 Train acc: 96.092 Test acc: 96.540 \n",
      "step: 2877 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.619, D_sup_loss: 0.146, D_sup_acc: 96.58 Train acc: 96.122 Test acc: 96.640 \n",
      "step: 2878 | Train: G_Loss: 1.167, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.613, D_sup_loss: 0.146, D_sup_acc: 96.68 Train acc: 96.213 Test acc: 96.590 \n",
      "step: 2879 | Train: G_Loss: 1.081, D_unsup_loss_real: 0.647, D_unsup_loss_fake: 0.527, D_sup_loss: 0.147, D_sup_acc: 96.63 Train acc: 96.138 Test acc: 96.630 \n",
      "step: 2880 | Train: G_Loss: 1.080, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.570, D_sup_loss: 0.149, D_sup_acc: 96.67 Train acc: 96.248 Test acc: 96.640 \n",
      "step: 2881 | Train: G_Loss: 1.203, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.611, D_sup_loss: 0.142, D_sup_acc: 96.68 Train acc: 96.208 Test acc: 96.610 \n",
      "step: 2882 | Train: G_Loss: 1.180, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.638, D_sup_loss: 0.144, D_sup_acc: 96.65 Train acc: 96.213 Test acc: 96.620 \n",
      "step: 2883 | Train: G_Loss: 1.166, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.666, D_sup_loss: 0.146, D_sup_acc: 96.66 Train acc: 96.268 Test acc: 96.660 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2884 | Train: G_Loss: 1.180, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.616, D_sup_loss: 0.146, D_sup_acc: 96.70 Train acc: 96.267 Test acc: 96.620 \n",
      "step: 2885 | Train: G_Loss: 1.222, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.707, D_sup_loss: 0.144, D_sup_acc: 96.66 Train acc: 96.207 Test acc: 96.650 \n",
      "step: 2886 | Train: G_Loss: 1.216, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.607, D_sup_loss: 0.147, D_sup_acc: 96.69 Train acc: 96.260 Test acc: 96.610 \n",
      "step: 2887 | Train: G_Loss: 1.194, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.615, D_sup_loss: 0.146, D_sup_acc: 96.65 Train acc: 96.312 Test acc: 96.650 \n",
      "step: 2888 | Train: G_Loss: 1.074, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.597, D_sup_loss: 0.145, D_sup_acc: 96.69 Train acc: 96.367 Test acc: 96.640 \n",
      "step: 2889 | Train: G_Loss: 1.180, D_unsup_loss_real: 0.681, D_unsup_loss_fake: 0.582, D_sup_loss: 0.144, D_sup_acc: 96.68 Train acc: 96.363 Test acc: 96.680 \n",
      "step: 2890 | Train: G_Loss: 1.139, D_unsup_loss_real: 0.535, D_unsup_loss_fake: 0.589, D_sup_loss: 0.144, D_sup_acc: 96.72 Train acc: 96.215 Test acc: 96.560 \n",
      "step: 2891 | Train: G_Loss: 1.107, D_unsup_loss_real: 0.647, D_unsup_loss_fake: 0.597, D_sup_loss: 0.150, D_sup_acc: 96.60 Train acc: 96.175 Test acc: 96.500 \n",
      "step: 2892 | Train: G_Loss: 1.161, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.530, D_sup_loss: 0.151, D_sup_acc: 96.54 Train acc: 96.182 Test acc: 96.560 \n",
      "step: 2893 | Train: G_Loss: 1.099, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.669, D_sup_loss: 0.149, D_sup_acc: 96.60 Train acc: 96.137 Test acc: 96.540 \n",
      "step: 2894 | Train: G_Loss: 1.046, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.645, D_sup_loss: 0.151, D_sup_acc: 96.58 Train acc: 96.190 Test acc: 96.520 \n",
      "step: 2895 | Train: G_Loss: 1.123, D_unsup_loss_real: 0.659, D_unsup_loss_fake: 0.615, D_sup_loss: 0.149, D_sup_acc: 96.56 Train acc: 96.207 Test acc: 96.540 \n",
      "step: 2896 | Train: G_Loss: 1.091, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.625, D_sup_loss: 0.146, D_sup_acc: 96.58 Train acc: 96.198 Test acc: 96.550 \n",
      "step: 2897 | Train: G_Loss: 1.118, D_unsup_loss_real: 0.537, D_unsup_loss_fake: 0.572, D_sup_loss: 0.146, D_sup_acc: 96.59 Train acc: 96.213 Test acc: 96.580 \n",
      "step: 2898 | Train: G_Loss: 1.125, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.644, D_sup_loss: 0.142, D_sup_acc: 96.62 Train acc: 96.212 Test acc: 96.610 \n",
      "step: 2899 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.566, D_sup_loss: 0.142, D_sup_acc: 96.65 Train acc: 96.193 Test acc: 96.580 \n",
      "step: 2900 | Train: G_Loss: 1.185, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.648, D_sup_loss: 0.144, D_sup_acc: 96.62 Train acc: 96.222 Test acc: 96.580 \n",
      "Train Classifier Accuracy: 96.222%\n",
      "\n",
      "Test Classifier Accuracy: 96.580%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_2900.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_2900.h5\n",
      "step: 2901 | Train: G_Loss: 1.136, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.635, D_sup_loss: 0.145, D_sup_acc: 96.62 Train acc: 96.205 Test acc: 96.570 \n",
      "step: 2902 | Train: G_Loss: 1.119, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.586, D_sup_loss: 0.146, D_sup_acc: 96.61 Train acc: 96.268 Test acc: 96.540 \n",
      "step: 2903 | Train: G_Loss: 1.077, D_unsup_loss_real: 0.704, D_unsup_loss_fake: 0.632, D_sup_loss: 0.141, D_sup_acc: 96.58 Train acc: 96.217 Test acc: 96.570 \n",
      "step: 2904 | Train: G_Loss: 1.020, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.564, D_sup_loss: 0.146, D_sup_acc: 96.61 Train acc: 96.255 Test acc: 96.630 \n",
      "step: 2905 | Train: G_Loss: 1.089, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.613, D_sup_loss: 0.142, D_sup_acc: 96.67 Train acc: 96.107 Test acc: 96.510 \n",
      "step: 2906 | Train: G_Loss: 1.139, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.696, D_sup_loss: 0.150, D_sup_acc: 96.55 Train acc: 96.190 Test acc: 96.550 \n",
      "step: 2907 | Train: G_Loss: 1.084, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.610, D_sup_loss: 0.141, D_sup_acc: 96.59 Train acc: 96.220 Test acc: 96.560 \n",
      "step: 2908 | Train: G_Loss: 1.127, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.622, D_sup_loss: 0.140, D_sup_acc: 96.60 Train acc: 96.200 Test acc: 96.480 \n",
      "step: 2909 | Train: G_Loss: 1.146, D_unsup_loss_real: 0.631, D_unsup_loss_fake: 0.653, D_sup_loss: 0.143, D_sup_acc: 96.52 Train acc: 96.167 Test acc: 96.580 \n",
      "step: 2910 | Train: G_Loss: 1.165, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.648, D_sup_loss: 0.147, D_sup_acc: 96.62 Train acc: 96.158 Test acc: 96.480 \n",
      "step: 2911 | Train: G_Loss: 1.170, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.576, D_sup_loss: 0.149, D_sup_acc: 96.52 Train acc: 96.147 Test acc: 96.590 \n",
      "step: 2912 | Train: G_Loss: 1.162, D_unsup_loss_real: 0.651, D_unsup_loss_fake: 0.597, D_sup_loss: 0.146, D_sup_acc: 96.63 Train acc: 96.178 Test acc: 96.630 \n",
      "step: 2913 | Train: G_Loss: 1.081, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.600, D_sup_loss: 0.145, D_sup_acc: 96.67 Train acc: 96.127 Test acc: 96.610 \n",
      "step: 2914 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.669, D_sup_loss: 0.145, D_sup_acc: 96.65 Train acc: 96.200 Test acc: 96.610 \n",
      "step: 2915 | Train: G_Loss: 1.101, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.642, D_sup_loss: 0.142, D_sup_acc: 96.65 Train acc: 96.153 Test acc: 96.580 \n",
      "step: 2916 | Train: G_Loss: 1.146, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.603, D_sup_loss: 0.146, D_sup_acc: 96.62 Train acc: 96.130 Test acc: 96.510 \n",
      "step: 2917 | Train: G_Loss: 1.119, D_unsup_loss_real: 0.648, D_unsup_loss_fake: 0.658, D_sup_loss: 0.147, D_sup_acc: 96.55 Train acc: 96.048 Test acc: 96.380 \n",
      "step: 2918 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.657, D_unsup_loss_fake: 0.625, D_sup_loss: 0.149, D_sup_acc: 96.43 Train acc: 96.002 Test acc: 96.460 \n",
      "step: 2919 | Train: G_Loss: 1.120, D_unsup_loss_real: 0.659, D_unsup_loss_fake: 0.586, D_sup_loss: 0.154, D_sup_acc: 96.50 Train acc: 96.098 Test acc: 96.560 \n",
      "step: 2920 | Train: G_Loss: 1.071, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.610, D_sup_loss: 0.147, D_sup_acc: 96.60 Train acc: 96.082 Test acc: 96.500 \n",
      "step: 2921 | Train: G_Loss: 1.133, D_unsup_loss_real: 0.702, D_unsup_loss_fake: 0.671, D_sup_loss: 0.150, D_sup_acc: 96.54 Train acc: 96.128 Test acc: 96.530 \n",
      "step: 2922 | Train: G_Loss: 1.118, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.573, D_sup_loss: 0.149, D_sup_acc: 96.57 Train acc: 96.123 Test acc: 96.480 \n",
      "step: 2923 | Train: G_Loss: 1.059, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.566, D_sup_loss: 0.149, D_sup_acc: 96.52 Train acc: 96.132 Test acc: 96.470 \n",
      "step: 2924 | Train: G_Loss: 1.021, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.616, D_sup_loss: 0.149, D_sup_acc: 96.51 Train acc: 96.075 Test acc: 96.490 \n",
      "step: 2925 | Train: G_Loss: 1.090, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.623, D_sup_loss: 0.153, D_sup_acc: 96.53 Train acc: 96.092 Test acc: 96.450 \n",
      "step: 2926 | Train: G_Loss: 1.057, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.577, D_sup_loss: 0.154, D_sup_acc: 96.49 Train acc: 96.125 Test acc: 96.580 \n",
      "step: 2927 | Train: G_Loss: 1.071, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.625, D_sup_loss: 0.148, D_sup_acc: 96.62 Train acc: 96.113 Test acc: 96.540 \n",
      "step: 2928 | Train: G_Loss: 0.998, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.683, D_sup_loss: 0.149, D_sup_acc: 96.58 Train acc: 96.128 Test acc: 96.580 \n",
      "step: 2929 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.661, D_sup_loss: 0.148, D_sup_acc: 96.62 Train acc: 96.185 Test acc: 96.540 \n",
      "step: 2930 | Train: G_Loss: 1.020, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.609, D_sup_loss: 0.147, D_sup_acc: 96.58 Train acc: 96.232 Test acc: 96.630 \n",
      "step: 2931 | Train: G_Loss: 1.047, D_unsup_loss_real: 0.697, D_unsup_loss_fake: 0.695, D_sup_loss: 0.144, D_sup_acc: 96.67 Train acc: 96.217 Test acc: 96.640 \n",
      "step: 2932 | Train: G_Loss: 1.101, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.653, D_sup_loss: 0.145, D_sup_acc: 96.68 Train acc: 96.253 Test acc: 96.660 \n",
      "step: 2933 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.643, D_sup_loss: 0.148, D_sup_acc: 96.70 Train acc: 96.303 Test acc: 96.670 \n",
      "step: 2934 | Train: G_Loss: 1.163, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.656, D_sup_loss: 0.142, D_sup_acc: 96.71 Train acc: 96.193 Test acc: 96.570 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2935 | Train: G_Loss: 1.207, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.618, D_sup_loss: 0.149, D_sup_acc: 96.61 Train acc: 96.223 Test acc: 96.580 \n",
      "step: 2936 | Train: G_Loss: 1.117, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.619, D_sup_loss: 0.148, D_sup_acc: 96.62 Train acc: 96.270 Test acc: 96.570 \n",
      "step: 2937 | Train: G_Loss: 1.140, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.625, D_sup_loss: 0.147, D_sup_acc: 96.61 Train acc: 96.223 Test acc: 96.610 \n",
      "step: 2938 | Train: G_Loss: 1.056, D_unsup_loss_real: 0.716, D_unsup_loss_fake: 0.605, D_sup_loss: 0.152, D_sup_acc: 96.65 Train acc: 96.222 Test acc: 96.650 \n",
      "step: 2939 | Train: G_Loss: 1.129, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.598, D_sup_loss: 0.153, D_sup_acc: 96.69 Train acc: 96.292 Test acc: 96.750 \n",
      "step: 2940 | Train: G_Loss: 1.082, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.727, D_sup_loss: 0.147, D_sup_acc: 96.79 Train acc: 96.273 Test acc: 96.800 \n",
      "step: 2941 | Train: G_Loss: 1.100, D_unsup_loss_real: 0.646, D_unsup_loss_fake: 0.715, D_sup_loss: 0.147, D_sup_acc: 96.84 Train acc: 96.262 Test acc: 96.680 \n",
      "step: 2942 | Train: G_Loss: 1.118, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.600, D_sup_loss: 0.150, D_sup_acc: 96.72 Train acc: 96.170 Test acc: 96.550 \n",
      "step: 2943 | Train: G_Loss: 1.053, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.572, D_sup_loss: 0.152, D_sup_acc: 96.59 Train acc: 96.140 Test acc: 96.640 \n",
      "step: 2944 | Train: G_Loss: 1.029, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.609, D_sup_loss: 0.149, D_sup_acc: 96.68 Train acc: 96.278 Test acc: 96.680 \n",
      "step: 2945 | Train: G_Loss: 1.185, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.681, D_sup_loss: 0.145, D_sup_acc: 96.72 Train acc: 96.290 Test acc: 96.740 \n",
      "step: 2946 | Train: G_Loss: 1.123, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.604, D_sup_loss: 0.151, D_sup_acc: 96.78 Train acc: 96.195 Test acc: 96.650 \n",
      "step: 2947 | Train: G_Loss: 1.150, D_unsup_loss_real: 0.650, D_unsup_loss_fake: 0.632, D_sup_loss: 0.148, D_sup_acc: 96.69 Train acc: 96.257 Test acc: 96.730 \n",
      "step: 2948 | Train: G_Loss: 1.082, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.612, D_sup_loss: 0.150, D_sup_acc: 96.77 Train acc: 96.262 Test acc: 96.750 \n",
      "step: 2949 | Train: G_Loss: 1.115, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.658, D_sup_loss: 0.148, D_sup_acc: 96.79 Train acc: 96.292 Test acc: 96.780 \n",
      "step: 2950 | Train: G_Loss: 1.074, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.542, D_sup_loss: 0.146, D_sup_acc: 96.82 Train acc: 96.285 Test acc: 96.670 \n",
      "step: 2951 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.609, D_sup_loss: 0.141, D_sup_acc: 96.71 Train acc: 96.293 Test acc: 96.700 \n",
      "step: 2952 | Train: G_Loss: 1.137, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.647, D_sup_loss: 0.140, D_sup_acc: 96.74 Train acc: 96.302 Test acc: 96.730 \n",
      "step: 2953 | Train: G_Loss: 1.099, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.526, D_sup_loss: 0.142, D_sup_acc: 96.77 Train acc: 96.308 Test acc: 96.660 \n",
      "step: 2954 | Train: G_Loss: 1.136, D_unsup_loss_real: 0.524, D_unsup_loss_fake: 0.613, D_sup_loss: 0.141, D_sup_acc: 96.70 Train acc: 96.292 Test acc: 96.650 \n",
      "step: 2955 | Train: G_Loss: 1.021, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.576, D_sup_loss: 0.141, D_sup_acc: 96.69 Train acc: 96.292 Test acc: 96.670 \n",
      "step: 2956 | Train: G_Loss: 1.003, D_unsup_loss_real: 0.660, D_unsup_loss_fake: 0.635, D_sup_loss: 0.142, D_sup_acc: 96.71 Train acc: 96.278 Test acc: 96.700 \n",
      "step: 2957 | Train: G_Loss: 1.030, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.612, D_sup_loss: 0.143, D_sup_acc: 96.74 Train acc: 96.268 Test acc: 96.660 \n",
      "step: 2958 | Train: G_Loss: 1.108, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.598, D_sup_loss: 0.145, D_sup_acc: 96.70 Train acc: 96.188 Test acc: 96.580 \n",
      "step: 2959 | Train: G_Loss: 1.058, D_unsup_loss_real: 0.672, D_unsup_loss_fake: 0.617, D_sup_loss: 0.149, D_sup_acc: 96.62 Train acc: 96.255 Test acc: 96.660 \n",
      "step: 2960 | Train: G_Loss: 1.196, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.650, D_sup_loss: 0.145, D_sup_acc: 96.70 Train acc: 96.235 Test acc: 96.560 \n",
      "step: 2961 | Train: G_Loss: 1.074, D_unsup_loss_real: 0.661, D_unsup_loss_fake: 0.619, D_sup_loss: 0.149, D_sup_acc: 96.60 Train acc: 96.168 Test acc: 96.550 \n",
      "step: 2962 | Train: G_Loss: 1.078, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.579, D_sup_loss: 0.157, D_sup_acc: 96.59 Train acc: 96.090 Test acc: 96.330 \n",
      "step: 2963 | Train: G_Loss: 1.055, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.595, D_sup_loss: 0.159, D_sup_acc: 96.38 Train acc: 96.092 Test acc: 96.440 \n",
      "step: 2964 | Train: G_Loss: 1.058, D_unsup_loss_real: 0.530, D_unsup_loss_fake: 0.545, D_sup_loss: 0.160, D_sup_acc: 96.48 Train acc: 96.173 Test acc: 96.510 \n",
      "step: 2965 | Train: G_Loss: 1.168, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.577, D_sup_loss: 0.154, D_sup_acc: 96.55 Train acc: 96.162 Test acc: 96.560 \n",
      "step: 2966 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.531, D_unsup_loss_fake: 0.615, D_sup_loss: 0.153, D_sup_acc: 96.60 Train acc: 96.212 Test acc: 96.670 \n",
      "step: 2967 | Train: G_Loss: 1.097, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.603, D_sup_loss: 0.150, D_sup_acc: 96.71 Train acc: 96.258 Test acc: 96.580 \n",
      "step: 2968 | Train: G_Loss: 1.096, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.619, D_sup_loss: 0.146, D_sup_acc: 96.62 Train acc: 96.197 Test acc: 96.640 \n",
      "step: 2969 | Train: G_Loss: 1.126, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.534, D_sup_loss: 0.147, D_sup_acc: 96.68 Train acc: 96.292 Test acc: 96.680 \n",
      "step: 2970 | Train: G_Loss: 1.143, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.523, D_sup_loss: 0.144, D_sup_acc: 96.72 Train acc: 96.273 Test acc: 96.560 \n",
      "step: 2971 | Train: G_Loss: 1.088, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.622, D_sup_loss: 0.140, D_sup_acc: 96.60 Train acc: 96.272 Test acc: 96.590 \n",
      "step: 2972 | Train: G_Loss: 1.127, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.551, D_sup_loss: 0.142, D_sup_acc: 96.63 Train acc: 96.275 Test acc: 96.600 \n",
      "step: 2973 | Train: G_Loss: 1.038, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.624, D_sup_loss: 0.141, D_sup_acc: 96.64 Train acc: 96.238 Test acc: 96.450 \n",
      "step: 2974 | Train: G_Loss: 0.989, D_unsup_loss_real: 0.664, D_unsup_loss_fake: 0.642, D_sup_loss: 0.146, D_sup_acc: 96.49 Train acc: 96.238 Test acc: 96.510 \n",
      "step: 2975 | Train: G_Loss: 1.084, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.675, D_sup_loss: 0.148, D_sup_acc: 96.55 Train acc: 96.300 Test acc: 96.650 \n",
      "step: 2976 | Train: G_Loss: 1.090, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.632, D_sup_loss: 0.147, D_sup_acc: 96.69 Train acc: 96.280 Test acc: 96.490 \n",
      "step: 2977 | Train: G_Loss: 1.101, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.526, D_sup_loss: 0.151, D_sup_acc: 96.53 Train acc: 96.330 Test acc: 96.710 \n",
      "step: 2978 | Train: G_Loss: 1.111, D_unsup_loss_real: 0.671, D_unsup_loss_fake: 0.642, D_sup_loss: 0.146, D_sup_acc: 96.75 Train acc: 96.232 Test acc: 96.530 \n",
      "step: 2979 | Train: G_Loss: 1.112, D_unsup_loss_real: 0.651, D_unsup_loss_fake: 0.551, D_sup_loss: 0.150, D_sup_acc: 96.57 Train acc: 96.202 Test acc: 96.610 \n",
      "step: 2980 | Train: G_Loss: 1.109, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.642, D_sup_loss: 0.151, D_sup_acc: 96.65 Train acc: 96.203 Test acc: 96.660 \n",
      "step: 2981 | Train: G_Loss: 1.128, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.616, D_sup_loss: 0.152, D_sup_acc: 96.70 Train acc: 96.103 Test acc: 96.570 \n",
      "step: 2982 | Train: G_Loss: 1.050, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.628, D_sup_loss: 0.154, D_sup_acc: 96.61 Train acc: 96.080 Test acc: 96.440 \n",
      "step: 2983 | Train: G_Loss: 1.063, D_unsup_loss_real: 0.675, D_unsup_loss_fake: 0.590, D_sup_loss: 0.155, D_sup_acc: 96.48 Train acc: 96.158 Test acc: 96.460 \n",
      "step: 2984 | Train: G_Loss: 1.059, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.608, D_sup_loss: 0.152, D_sup_acc: 96.50 Train acc: 96.325 Test acc: 96.740 \n",
      "step: 2985 | Train: G_Loss: 1.070, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.611, D_sup_loss: 0.148, D_sup_acc: 96.78 Train acc: 96.280 Test acc: 96.800 \n",
      "step: 2986 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.628, D_unsup_loss_fake: 0.616, D_sup_loss: 0.147, D_sup_acc: 96.84 Train acc: 96.377 Test acc: 96.650 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 2987 | Train: G_Loss: 1.139, D_unsup_loss_real: 0.651, D_unsup_loss_fake: 0.654, D_sup_loss: 0.142, D_sup_acc: 96.69 Train acc: 96.375 Test acc: 96.780 \n",
      "step: 2988 | Train: G_Loss: 1.065, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.630, D_sup_loss: 0.142, D_sup_acc: 96.82 Train acc: 96.398 Test acc: 96.800 \n",
      "step: 2989 | Train: G_Loss: 1.066, D_unsup_loss_real: 0.642, D_unsup_loss_fake: 0.608, D_sup_loss: 0.140, D_sup_acc: 96.84 Train acc: 96.408 Test acc: 96.840 \n",
      "step: 2990 | Train: G_Loss: 1.012, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.621, D_sup_loss: 0.144, D_sup_acc: 96.88 Train acc: 96.397 Test acc: 96.670 \n",
      "step: 2991 | Train: G_Loss: 1.141, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.615, D_sup_loss: 0.140, D_sup_acc: 96.71 Train acc: 96.350 Test acc: 96.760 \n",
      "step: 2992 | Train: G_Loss: 1.123, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.642, D_sup_loss: 0.144, D_sup_acc: 96.80 Train acc: 96.327 Test acc: 96.590 \n",
      "step: 2993 | Train: G_Loss: 1.056, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.609, D_sup_loss: 0.147, D_sup_acc: 96.63 Train acc: 96.302 Test acc: 96.720 \n",
      "step: 2994 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.649, D_sup_loss: 0.147, D_sup_acc: 96.76 Train acc: 96.262 Test acc: 96.520 \n",
      "step: 2995 | Train: G_Loss: 1.000, D_unsup_loss_real: 0.653, D_unsup_loss_fake: 0.606, D_sup_loss: 0.149, D_sup_acc: 96.56 Train acc: 96.365 Test acc: 96.760 \n",
      "step: 2996 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.642, D_sup_loss: 0.144, D_sup_acc: 96.80 Train acc: 96.230 Test acc: 96.540 \n",
      "step: 2997 | Train: G_Loss: 1.049, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.578, D_sup_loss: 0.148, D_sup_acc: 96.58 Train acc: 96.297 Test acc: 96.770 \n",
      "step: 2998 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.644, D_sup_loss: 0.145, D_sup_acc: 96.81 Train acc: 96.365 Test acc: 96.880 \n",
      "step: 2999 | Train: G_Loss: 1.064, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.616, D_sup_loss: 0.142, D_sup_acc: 96.92 Train acc: 96.310 Test acc: 96.630 \n",
      "step: 3000 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.650, D_unsup_loss_fake: 0.631, D_sup_loss: 0.144, D_sup_acc: 96.67 Train acc: 96.285 Test acc: 96.780 \n",
      "Train Classifier Accuracy: 96.285%\n",
      "\n",
      "Test Classifier Accuracy: 96.780%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_3000.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_3000.h5\n",
      "step: 3001 | Train: G_Loss: 1.156, D_unsup_loss_real: 0.522, D_unsup_loss_fake: 0.607, D_sup_loss: 0.144, D_sup_acc: 96.82 Train acc: 96.285 Test acc: 96.760 \n",
      "step: 3002 | Train: G_Loss: 1.181, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.572, D_sup_loss: 0.144, D_sup_acc: 96.80 Train acc: 96.250 Test acc: 96.650 \n",
      "step: 3003 | Train: G_Loss: 1.175, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.660, D_sup_loss: 0.150, D_sup_acc: 96.69 Train acc: 96.205 Test acc: 96.620 \n",
      "step: 3004 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.741, D_unsup_loss_fake: 0.615, D_sup_loss: 0.149, D_sup_acc: 96.66 Train acc: 96.270 Test acc: 96.610 \n",
      "step: 3005 | Train: G_Loss: 1.035, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.653, D_sup_loss: 0.146, D_sup_acc: 96.65 Train acc: 96.315 Test acc: 96.760 \n",
      "step: 3006 | Train: G_Loss: 1.128, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.613, D_sup_loss: 0.144, D_sup_acc: 96.80 Train acc: 96.277 Test acc: 96.670 \n",
      "step: 3007 | Train: G_Loss: 1.056, D_unsup_loss_real: 0.653, D_unsup_loss_fake: 0.660, D_sup_loss: 0.144, D_sup_acc: 96.71 Train acc: 96.260 Test acc: 96.660 \n",
      "step: 3008 | Train: G_Loss: 1.115, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.617, D_sup_loss: 0.147, D_sup_acc: 96.70 Train acc: 96.310 Test acc: 96.700 \n",
      "step: 3009 | Train: G_Loss: 1.076, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.649, D_sup_loss: 0.141, D_sup_acc: 96.74 Train acc: 96.283 Test acc: 96.630 \n",
      "step: 3010 | Train: G_Loss: 1.081, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.669, D_sup_loss: 0.142, D_sup_acc: 96.67 Train acc: 96.303 Test acc: 96.640 \n",
      "step: 3011 | Train: G_Loss: 1.151, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.666, D_sup_loss: 0.149, D_sup_acc: 96.68 Train acc: 96.278 Test acc: 96.690 \n",
      "step: 3012 | Train: G_Loss: 1.107, D_unsup_loss_real: 0.702, D_unsup_loss_fake: 0.597, D_sup_loss: 0.149, D_sup_acc: 96.73 Train acc: 96.335 Test acc: 96.710 \n",
      "step: 3013 | Train: G_Loss: 1.091, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.631, D_sup_loss: 0.140, D_sup_acc: 96.75 Train acc: 96.363 Test acc: 96.690 \n",
      "step: 3014 | Train: G_Loss: 1.089, D_unsup_loss_real: 0.537, D_unsup_loss_fake: 0.599, D_sup_loss: 0.141, D_sup_acc: 96.73 Train acc: 96.252 Test acc: 96.650 \n",
      "step: 3015 | Train: G_Loss: 1.050, D_unsup_loss_real: 0.670, D_unsup_loss_fake: 0.603, D_sup_loss: 0.150, D_sup_acc: 96.69 Train acc: 96.227 Test acc: 96.600 \n",
      "step: 3016 | Train: G_Loss: 1.056, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.599, D_sup_loss: 0.150, D_sup_acc: 96.64 Train acc: 96.248 Test acc: 96.590 \n",
      "step: 3017 | Train: G_Loss: 1.080, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.650, D_sup_loss: 0.146, D_sup_acc: 96.63 Train acc: 96.192 Test acc: 96.530 \n",
      "step: 3018 | Train: G_Loss: 1.100, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.653, D_sup_loss: 0.150, D_sup_acc: 96.57 Train acc: 96.165 Test acc: 96.490 \n",
      "step: 3019 | Train: G_Loss: 1.161, D_unsup_loss_real: 0.686, D_unsup_loss_fake: 0.625, D_sup_loss: 0.153, D_sup_acc: 96.53 Train acc: 96.098 Test acc: 96.460 \n",
      "step: 3020 | Train: G_Loss: 1.106, D_unsup_loss_real: 0.655, D_unsup_loss_fake: 0.707, D_sup_loss: 0.154, D_sup_acc: 96.50 Train acc: 96.093 Test acc: 96.510 \n",
      "step: 3021 | Train: G_Loss: 1.107, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.590, D_sup_loss: 0.148, D_sup_acc: 96.55 Train acc: 96.252 Test acc: 96.690 \n",
      "step: 3022 | Train: G_Loss: 1.048, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.603, D_sup_loss: 0.145, D_sup_acc: 96.73 Train acc: 96.187 Test acc: 96.670 \n",
      "step: 3023 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.546, D_sup_loss: 0.148, D_sup_acc: 96.71 Train acc: 96.262 Test acc: 96.720 \n",
      "step: 3024 | Train: G_Loss: 1.008, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.618, D_sup_loss: 0.141, D_sup_acc: 96.76 Train acc: 96.235 Test acc: 96.690 \n",
      "step: 3025 | Train: G_Loss: 1.117, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.595, D_sup_loss: 0.145, D_sup_acc: 96.73 Train acc: 96.247 Test acc: 96.680 \n",
      "step: 3026 | Train: G_Loss: 1.152, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.622, D_sup_loss: 0.144, D_sup_acc: 96.72 Train acc: 96.283 Test acc: 96.700 \n",
      "step: 3027 | Train: G_Loss: 1.224, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.623, D_sup_loss: 0.141, D_sup_acc: 96.74 Train acc: 96.300 Test acc: 96.630 \n",
      "step: 3028 | Train: G_Loss: 0.988, D_unsup_loss_real: 0.546, D_unsup_loss_fake: 0.640, D_sup_loss: 0.143, D_sup_acc: 96.67 Train acc: 96.243 Test acc: 96.580 \n",
      "step: 3029 | Train: G_Loss: 1.009, D_unsup_loss_real: 0.530, D_unsup_loss_fake: 0.655, D_sup_loss: 0.145, D_sup_acc: 96.62 Train acc: 96.280 Test acc: 96.630 \n",
      "step: 3030 | Train: G_Loss: 1.059, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.573, D_sup_loss: 0.146, D_sup_acc: 96.67 Train acc: 96.247 Test acc: 96.630 \n",
      "step: 3031 | Train: G_Loss: 1.114, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.680, D_sup_loss: 0.148, D_sup_acc: 96.67 Train acc: 96.333 Test acc: 96.620 \n",
      "step: 3032 | Train: G_Loss: 1.028, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.544, D_sup_loss: 0.146, D_sup_acc: 96.66 Train acc: 96.102 Test acc: 96.450 \n",
      "step: 3033 | Train: G_Loss: 1.021, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.580, D_sup_loss: 0.153, D_sup_acc: 96.49 Train acc: 96.295 Test acc: 96.500 \n",
      "step: 3034 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.652, D_sup_loss: 0.147, D_sup_acc: 96.54 Train acc: 96.285 Test acc: 96.600 \n",
      "step: 3035 | Train: G_Loss: 1.152, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.647, D_sup_loss: 0.144, D_sup_acc: 96.64 Train acc: 96.292 Test acc: 96.570 \n",
      "step: 3036 | Train: G_Loss: 1.061, D_unsup_loss_real: 0.676, D_unsup_loss_fake: 0.559, D_sup_loss: 0.146, D_sup_acc: 96.61 Train acc: 96.270 Test acc: 96.690 \n",
      "step: 3037 | Train: G_Loss: 1.024, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.612, D_sup_loss: 0.145, D_sup_acc: 96.73 Train acc: 96.257 Test acc: 96.620 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3038 | Train: G_Loss: 1.119, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.591, D_sup_loss: 0.143, D_sup_acc: 96.66 Train acc: 96.298 Test acc: 96.630 \n",
      "step: 3039 | Train: G_Loss: 1.137, D_unsup_loss_real: 0.537, D_unsup_loss_fake: 0.640, D_sup_loss: 0.139, D_sup_acc: 96.67 Train acc: 96.338 Test acc: 96.670 \n",
      "step: 3040 | Train: G_Loss: 1.038, D_unsup_loss_real: 0.669, D_unsup_loss_fake: 0.592, D_sup_loss: 0.143, D_sup_acc: 96.71 Train acc: 96.292 Test acc: 96.640 \n",
      "step: 3041 | Train: G_Loss: 1.086, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.589, D_sup_loss: 0.148, D_sup_acc: 96.68 Train acc: 96.245 Test acc: 96.590 \n",
      "step: 3042 | Train: G_Loss: 1.126, D_unsup_loss_real: 0.670, D_unsup_loss_fake: 0.635, D_sup_loss: 0.147, D_sup_acc: 96.63 Train acc: 96.270 Test acc: 96.580 \n",
      "step: 3043 | Train: G_Loss: 1.111, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.645, D_sup_loss: 0.144, D_sup_acc: 96.62 Train acc: 96.330 Test acc: 96.600 \n",
      "step: 3044 | Train: G_Loss: 0.977, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.627, D_sup_loss: 0.142, D_sup_acc: 96.64 Train acc: 96.327 Test acc: 96.680 \n",
      "step: 3045 | Train: G_Loss: 1.073, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.618, D_sup_loss: 0.143, D_sup_acc: 96.72 Train acc: 96.372 Test acc: 96.700 \n",
      "step: 3046 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.672, D_sup_loss: 0.143, D_sup_acc: 96.74 Train acc: 96.370 Test acc: 96.680 \n",
      "step: 3047 | Train: G_Loss: 1.109, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.674, D_sup_loss: 0.142, D_sup_acc: 96.72 Train acc: 96.280 Test acc: 96.700 \n",
      "step: 3048 | Train: G_Loss: 1.045, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.581, D_sup_loss: 0.145, D_sup_acc: 96.74 Train acc: 96.355 Test acc: 96.710 \n",
      "step: 3049 | Train: G_Loss: 1.099, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.640, D_sup_loss: 0.144, D_sup_acc: 96.75 Train acc: 96.225 Test acc: 96.600 \n",
      "step: 3050 | Train: G_Loss: 1.089, D_unsup_loss_real: 0.655, D_unsup_loss_fake: 0.521, D_sup_loss: 0.147, D_sup_acc: 96.64 Train acc: 96.192 Test acc: 96.570 \n",
      "step: 3051 | Train: G_Loss: 1.202, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.659, D_sup_loss: 0.150, D_sup_acc: 96.61 Train acc: 96.237 Test acc: 96.580 \n",
      "step: 3052 | Train: G_Loss: 1.254, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.584, D_sup_loss: 0.149, D_sup_acc: 96.62 Train acc: 96.283 Test acc: 96.600 \n",
      "step: 3053 | Train: G_Loss: 1.176, D_unsup_loss_real: 0.660, D_unsup_loss_fake: 0.528, D_sup_loss: 0.148, D_sup_acc: 96.64 Train acc: 96.318 Test acc: 96.570 \n",
      "step: 3054 | Train: G_Loss: 1.149, D_unsup_loss_real: 0.665, D_unsup_loss_fake: 0.701, D_sup_loss: 0.143, D_sup_acc: 96.61 Train acc: 96.010 Test acc: 96.360 \n",
      "step: 3055 | Train: G_Loss: 1.053, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.636, D_sup_loss: 0.157, D_sup_acc: 96.41 Train acc: 96.038 Test acc: 96.410 \n",
      "step: 3056 | Train: G_Loss: 1.101, D_unsup_loss_real: 0.639, D_unsup_loss_fake: 0.643, D_sup_loss: 0.154, D_sup_acc: 96.46 Train acc: 96.153 Test acc: 96.640 \n",
      "step: 3057 | Train: G_Loss: 1.133, D_unsup_loss_real: 0.644, D_unsup_loss_fake: 0.631, D_sup_loss: 0.152, D_sup_acc: 96.68 Train acc: 96.180 Test acc: 96.620 \n",
      "step: 3058 | Train: G_Loss: 1.060, D_unsup_loss_real: 0.507, D_unsup_loss_fake: 0.627, D_sup_loss: 0.148, D_sup_acc: 96.66 Train acc: 96.222 Test acc: 96.680 \n",
      "step: 3059 | Train: G_Loss: 1.103, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.673, D_sup_loss: 0.147, D_sup_acc: 96.72 Train acc: 96.233 Test acc: 96.670 \n",
      "step: 3060 | Train: G_Loss: 1.109, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.619, D_sup_loss: 0.145, D_sup_acc: 96.71 Train acc: 96.192 Test acc: 96.640 \n",
      "step: 3061 | Train: G_Loss: 1.189, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.563, D_sup_loss: 0.142, D_sup_acc: 96.68 Train acc: 96.065 Test acc: 96.570 \n",
      "step: 3062 | Train: G_Loss: 1.075, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.678, D_sup_loss: 0.146, D_sup_acc: 96.61 Train acc: 95.995 Test acc: 96.430 \n",
      "step: 3063 | Train: G_Loss: 1.143, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.621, D_sup_loss: 0.151, D_sup_acc: 96.48 Train acc: 96.177 Test acc: 96.700 \n",
      "step: 3064 | Train: G_Loss: 1.220, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.553, D_sup_loss: 0.147, D_sup_acc: 96.74 Train acc: 96.233 Test acc: 96.770 \n",
      "step: 3065 | Train: G_Loss: 1.173, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.511, D_sup_loss: 0.141, D_sup_acc: 96.81 Train acc: 96.175 Test acc: 96.740 \n",
      "step: 3066 | Train: G_Loss: 1.096, D_unsup_loss_real: 0.648, D_unsup_loss_fake: 0.698, D_sup_loss: 0.140, D_sup_acc: 96.78 Train acc: 96.242 Test acc: 96.520 \n",
      "step: 3067 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.654, D_sup_loss: 0.140, D_sup_acc: 96.56 Train acc: 96.212 Test acc: 96.740 \n",
      "step: 3068 | Train: G_Loss: 1.082, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.618, D_sup_loss: 0.144, D_sup_acc: 96.78 Train acc: 96.052 Test acc: 96.600 \n",
      "step: 3069 | Train: G_Loss: 1.058, D_unsup_loss_real: 0.691, D_unsup_loss_fake: 0.634, D_sup_loss: 0.154, D_sup_acc: 96.64 Train acc: 96.245 Test acc: 96.710 \n",
      "step: 3070 | Train: G_Loss: 1.066, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.529, D_sup_loss: 0.143, D_sup_acc: 96.75 Train acc: 96.190 Test acc: 96.820 \n",
      "step: 3071 | Train: G_Loss: 1.045, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.595, D_sup_loss: 0.146, D_sup_acc: 96.86 Train acc: 96.143 Test acc: 96.740 \n",
      "step: 3072 | Train: G_Loss: 1.106, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.596, D_sup_loss: 0.153, D_sup_acc: 96.78 Train acc: 96.342 Test acc: 96.780 \n",
      "step: 3073 | Train: G_Loss: 1.083, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.573, D_sup_loss: 0.141, D_sup_acc: 96.82 Train acc: 96.230 Test acc: 96.570 \n",
      "step: 3074 | Train: G_Loss: 1.104, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.608, D_sup_loss: 0.146, D_sup_acc: 96.61 Train acc: 96.300 Test acc: 96.730 \n",
      "step: 3075 | Train: G_Loss: 1.084, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.691, D_sup_loss: 0.147, D_sup_acc: 96.77 Train acc: 96.342 Test acc: 96.780 \n",
      "step: 3076 | Train: G_Loss: 0.989, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.571, D_sup_loss: 0.145, D_sup_acc: 96.82 Train acc: 96.383 Test acc: 96.870 \n",
      "step: 3077 | Train: G_Loss: 1.097, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.708, D_sup_loss: 0.141, D_sup_acc: 96.91 Train acc: 96.370 Test acc: 96.930 \n",
      "step: 3078 | Train: G_Loss: 1.046, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.638, D_sup_loss: 0.141, D_sup_acc: 96.97 Train acc: 96.358 Test acc: 96.890 \n",
      "step: 3079 | Train: G_Loss: 1.079, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.708, D_sup_loss: 0.143, D_sup_acc: 96.93 Train acc: 96.305 Test acc: 96.890 \n",
      "step: 3080 | Train: G_Loss: 1.171, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.640, D_sup_loss: 0.146, D_sup_acc: 96.93 Train acc: 96.315 Test acc: 96.850 \n",
      "step: 3081 | Train: G_Loss: 1.217, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.613, D_sup_loss: 0.145, D_sup_acc: 96.89 Train acc: 96.345 Test acc: 96.820 \n",
      "step: 3082 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.689, D_unsup_loss_fake: 0.617, D_sup_loss: 0.143, D_sup_acc: 96.86 Train acc: 96.280 Test acc: 96.640 \n",
      "step: 3083 | Train: G_Loss: 1.296, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.666, D_sup_loss: 0.148, D_sup_acc: 96.68 Train acc: 96.197 Test acc: 96.660 \n",
      "step: 3084 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.659, D_unsup_loss_fake: 0.569, D_sup_loss: 0.154, D_sup_acc: 96.70 Train acc: 96.303 Test acc: 96.880 \n",
      "step: 3085 | Train: G_Loss: 1.070, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.633, D_sup_loss: 0.146, D_sup_acc: 96.92 Train acc: 96.247 Test acc: 96.640 \n",
      "step: 3086 | Train: G_Loss: 1.099, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.592, D_sup_loss: 0.148, D_sup_acc: 96.68 Train acc: 96.163 Test acc: 96.650 \n",
      "step: 3087 | Train: G_Loss: 1.135, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.610, D_sup_loss: 0.154, D_sup_acc: 96.69 Train acc: 96.237 Test acc: 96.660 \n",
      "step: 3088 | Train: G_Loss: 1.059, D_unsup_loss_real: 0.649, D_unsup_loss_fake: 0.578, D_sup_loss: 0.145, D_sup_acc: 96.70 Train acc: 96.260 Test acc: 96.650 \n",
      "step: 3089 | Train: G_Loss: 0.989, D_unsup_loss_real: 0.665, D_unsup_loss_fake: 0.651, D_sup_loss: 0.148, D_sup_acc: 96.69 Train acc: 96.277 Test acc: 96.780 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3090 | Train: G_Loss: 1.094, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.630, D_sup_loss: 0.148, D_sup_acc: 96.82 Train acc: 96.195 Test acc: 96.710 \n",
      "step: 3091 | Train: G_Loss: 0.985, D_unsup_loss_real: 0.644, D_unsup_loss_fake: 0.638, D_sup_loss: 0.149, D_sup_acc: 96.75 Train acc: 96.073 Test acc: 96.440 \n",
      "step: 3092 | Train: G_Loss: 1.065, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.561, D_sup_loss: 0.156, D_sup_acc: 96.48 Train acc: 95.995 Test acc: 96.340 \n",
      "step: 3093 | Train: G_Loss: 1.059, D_unsup_loss_real: 0.663, D_unsup_loss_fake: 0.636, D_sup_loss: 0.156, D_sup_acc: 96.39 Train acc: 96.212 Test acc: 96.610 \n",
      "step: 3094 | Train: G_Loss: 1.004, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.681, D_sup_loss: 0.149, D_sup_acc: 96.65 Train acc: 96.142 Test acc: 96.500 \n",
      "step: 3095 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.653, D_sup_loss: 0.151, D_sup_acc: 96.54 Train acc: 96.117 Test acc: 96.370 \n",
      "step: 3096 | Train: G_Loss: 1.135, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.678, D_sup_loss: 0.158, D_sup_acc: 96.42 Train acc: 96.163 Test acc: 96.580 \n",
      "step: 3097 | Train: G_Loss: 1.172, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.591, D_sup_loss: 0.154, D_sup_acc: 96.62 Train acc: 96.285 Test acc: 96.700 \n",
      "step: 3098 | Train: G_Loss: 1.126, D_unsup_loss_real: 0.662, D_unsup_loss_fake: 0.595, D_sup_loss: 0.148, D_sup_acc: 96.74 Train acc: 96.238 Test acc: 96.670 \n",
      "step: 3099 | Train: G_Loss: 1.073, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.541, D_sup_loss: 0.150, D_sup_acc: 96.71 Train acc: 96.307 Test acc: 96.690 \n",
      "step: 3100 | Train: G_Loss: 1.052, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.598, D_sup_loss: 0.144, D_sup_acc: 96.73 Train acc: 96.362 Test acc: 96.850 \n",
      "Train Classifier Accuracy: 96.362%\n",
      "\n",
      "Test Classifier Accuracy: 96.850%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_3100.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_3100.h5\n",
      "step: 3101 | Train: G_Loss: 1.032, D_unsup_loss_real: 0.656, D_unsup_loss_fake: 0.595, D_sup_loss: 0.141, D_sup_acc: 96.89 Train acc: 96.202 Test acc: 96.750 \n",
      "step: 3102 | Train: G_Loss: 1.081, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.685, D_sup_loss: 0.147, D_sup_acc: 96.79 Train acc: 96.283 Test acc: 96.790 \n",
      "step: 3103 | Train: G_Loss: 1.013, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.627, D_sup_loss: 0.143, D_sup_acc: 96.83 Train acc: 96.318 Test acc: 96.740 \n",
      "step: 3104 | Train: G_Loss: 1.094, D_unsup_loss_real: 0.664, D_unsup_loss_fake: 0.630, D_sup_loss: 0.145, D_sup_acc: 96.78 Train acc: 96.385 Test acc: 96.800 \n",
      "step: 3105 | Train: G_Loss: 1.060, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.660, D_sup_loss: 0.144, D_sup_acc: 96.84 Train acc: 96.263 Test acc: 96.640 \n",
      "step: 3106 | Train: G_Loss: 1.092, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.621, D_sup_loss: 0.150, D_sup_acc: 96.68 Train acc: 96.267 Test acc: 96.780 \n",
      "step: 3107 | Train: G_Loss: 1.135, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.547, D_sup_loss: 0.149, D_sup_acc: 96.82 Train acc: 96.203 Test acc: 96.460 \n",
      "step: 3108 | Train: G_Loss: 1.205, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.635, D_sup_loss: 0.151, D_sup_acc: 96.50 Train acc: 96.285 Test acc: 96.680 \n",
      "step: 3109 | Train: G_Loss: 1.137, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.636, D_sup_loss: 0.149, D_sup_acc: 96.72 Train acc: 96.288 Test acc: 96.820 \n",
      "step: 3110 | Train: G_Loss: 1.075, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.592, D_sup_loss: 0.148, D_sup_acc: 96.86 Train acc: 96.243 Test acc: 96.660 \n",
      "step: 3111 | Train: G_Loss: 1.013, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.579, D_sup_loss: 0.152, D_sup_acc: 96.70 Train acc: 96.290 Test acc: 96.680 \n",
      "step: 3112 | Train: G_Loss: 1.057, D_unsup_loss_real: 0.649, D_unsup_loss_fake: 0.627, D_sup_loss: 0.148, D_sup_acc: 96.72 Train acc: 96.308 Test acc: 96.560 \n",
      "step: 3113 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.609, D_sup_loss: 0.144, D_sup_acc: 96.60 Train acc: 96.277 Test acc: 96.720 \n",
      "step: 3114 | Train: G_Loss: 1.161, D_unsup_loss_real: 0.669, D_unsup_loss_fake: 0.639, D_sup_loss: 0.145, D_sup_acc: 96.76 Train acc: 96.165 Test acc: 96.660 \n",
      "step: 3115 | Train: G_Loss: 1.060, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.619, D_sup_loss: 0.150, D_sup_acc: 96.70 Train acc: 96.117 Test acc: 96.500 \n",
      "step: 3116 | Train: G_Loss: 1.078, D_unsup_loss_real: 0.677, D_unsup_loss_fake: 0.659, D_sup_loss: 0.150, D_sup_acc: 96.54 Train acc: 96.102 Test acc: 96.510 \n",
      "step: 3117 | Train: G_Loss: 1.030, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.659, D_sup_loss: 0.155, D_sup_acc: 96.55 Train acc: 96.158 Test acc: 96.570 \n",
      "step: 3118 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.576, D_sup_loss: 0.150, D_sup_acc: 96.61 Train acc: 96.338 Test acc: 96.760 \n",
      "step: 3119 | Train: G_Loss: 1.057, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.577, D_sup_loss: 0.144, D_sup_acc: 96.80 Train acc: 96.320 Test acc: 96.740 \n",
      "step: 3120 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.623, D_sup_loss: 0.143, D_sup_acc: 96.78 Train acc: 96.302 Test acc: 96.690 \n",
      "step: 3121 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.553, D_sup_loss: 0.145, D_sup_acc: 96.73 Train acc: 96.262 Test acc: 96.560 \n",
      "step: 3122 | Train: G_Loss: 1.046, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.618, D_sup_loss: 0.144, D_sup_acc: 96.60 Train acc: 96.162 Test acc: 96.550 \n",
      "step: 3123 | Train: G_Loss: 1.044, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.616, D_sup_loss: 0.148, D_sup_acc: 96.59 Train acc: 96.213 Test acc: 96.690 \n",
      "step: 3124 | Train: G_Loss: 1.073, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.637, D_sup_loss: 0.146, D_sup_acc: 96.73 Train acc: 96.278 Test acc: 96.660 \n",
      "step: 3125 | Train: G_Loss: 1.119, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.637, D_sup_loss: 0.146, D_sup_acc: 96.70 Train acc: 96.228 Test acc: 96.580 \n",
      "step: 3126 | Train: G_Loss: 1.112, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.615, D_sup_loss: 0.150, D_sup_acc: 96.62 Train acc: 96.223 Test acc: 96.600 \n",
      "step: 3127 | Train: G_Loss: 1.077, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.612, D_sup_loss: 0.150, D_sup_acc: 96.64 Train acc: 96.158 Test acc: 96.560 \n",
      "step: 3128 | Train: G_Loss: 1.127, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.580, D_sup_loss: 0.152, D_sup_acc: 96.60 Train acc: 96.158 Test acc: 96.560 \n",
      "step: 3129 | Train: G_Loss: 1.115, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.599, D_sup_loss: 0.150, D_sup_acc: 96.60 Train acc: 96.208 Test acc: 96.670 \n",
      "step: 3130 | Train: G_Loss: 1.113, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.604, D_sup_loss: 0.148, D_sup_acc: 96.71 Train acc: 96.225 Test acc: 96.650 \n",
      "step: 3131 | Train: G_Loss: 1.077, D_unsup_loss_real: 0.544, D_unsup_loss_fake: 0.629, D_sup_loss: 0.145, D_sup_acc: 96.69 Train acc: 96.208 Test acc: 96.650 \n",
      "step: 3132 | Train: G_Loss: 1.153, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.607, D_sup_loss: 0.148, D_sup_acc: 96.69 Train acc: 96.223 Test acc: 96.610 \n",
      "step: 3133 | Train: G_Loss: 1.070, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.643, D_sup_loss: 0.143, D_sup_acc: 96.65 Train acc: 96.197 Test acc: 96.580 \n",
      "step: 3134 | Train: G_Loss: 1.092, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.702, D_sup_loss: 0.145, D_sup_acc: 96.62 Train acc: 96.178 Test acc: 96.620 \n",
      "step: 3135 | Train: G_Loss: 1.160, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.676, D_sup_loss: 0.148, D_sup_acc: 96.66 Train acc: 96.033 Test acc: 96.380 \n",
      "step: 3136 | Train: G_Loss: 0.955, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.742, D_sup_loss: 0.160, D_sup_acc: 96.43 Train acc: 95.980 Test acc: 96.510 \n",
      "step: 3137 | Train: G_Loss: 1.195, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.662, D_sup_loss: 0.166, D_sup_acc: 96.55 Train acc: 96.210 Test acc: 96.560 \n",
      "step: 3138 | Train: G_Loss: 1.172, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.538, D_sup_loss: 0.153, D_sup_acc: 96.60 Train acc: 96.252 Test acc: 96.610 \n",
      "step: 3139 | Train: G_Loss: 1.136, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.583, D_sup_loss: 0.149, D_sup_acc: 96.65 Train acc: 96.287 Test acc: 96.730 \n",
      "step: 3140 | Train: G_Loss: 1.195, D_unsup_loss_real: 0.676, D_unsup_loss_fake: 0.589, D_sup_loss: 0.144, D_sup_acc: 96.77 Train acc: 96.302 Test acc: 96.680 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3141 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.676, D_unsup_loss_fake: 0.641, D_sup_loss: 0.144, D_sup_acc: 96.72 Train acc: 96.222 Test acc: 96.600 \n",
      "step: 3142 | Train: G_Loss: 1.132, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.589, D_sup_loss: 0.149, D_sup_acc: 96.64 Train acc: 96.247 Test acc: 96.620 \n",
      "step: 3143 | Train: G_Loss: 1.084, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.568, D_sup_loss: 0.147, D_sup_acc: 96.66 Train acc: 96.162 Test acc: 96.650 \n",
      "step: 3144 | Train: G_Loss: 1.056, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.731, D_sup_loss: 0.151, D_sup_acc: 96.69 Train acc: 96.302 Test acc: 96.840 \n",
      "step: 3145 | Train: G_Loss: 1.089, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.639, D_sup_loss: 0.143, D_sup_acc: 96.88 Train acc: 96.313 Test acc: 96.780 \n",
      "step: 3146 | Train: G_Loss: 1.010, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.589, D_sup_loss: 0.147, D_sup_acc: 96.82 Train acc: 96.263 Test acc: 96.640 \n",
      "step: 3147 | Train: G_Loss: 1.122, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.625, D_sup_loss: 0.150, D_sup_acc: 96.68 Train acc: 96.217 Test acc: 96.600 \n",
      "step: 3148 | Train: G_Loss: 1.125, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.678, D_sup_loss: 0.151, D_sup_acc: 96.64 Train acc: 96.333 Test acc: 96.790 \n",
      "step: 3149 | Train: G_Loss: 1.074, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.665, D_sup_loss: 0.145, D_sup_acc: 96.83 Train acc: 96.398 Test acc: 96.740 \n",
      "step: 3150 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.675, D_sup_loss: 0.148, D_sup_acc: 96.78 Train acc: 96.352 Test acc: 96.840 \n",
      "step: 3151 | Train: G_Loss: 1.161, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.530, D_sup_loss: 0.149, D_sup_acc: 96.88 Train acc: 96.350 Test acc: 96.830 \n",
      "step: 3152 | Train: G_Loss: 1.076, D_unsup_loss_real: 0.660, D_unsup_loss_fake: 0.612, D_sup_loss: 0.144, D_sup_acc: 96.87 Train acc: 96.372 Test acc: 96.890 \n",
      "step: 3153 | Train: G_Loss: 1.037, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.640, D_sup_loss: 0.140, D_sup_acc: 96.93 Train acc: 96.262 Test acc: 96.760 \n",
      "step: 3154 | Train: G_Loss: 1.058, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.552, D_sup_loss: 0.142, D_sup_acc: 96.80 Train acc: 96.243 Test acc: 96.740 \n",
      "step: 3155 | Train: G_Loss: 1.153, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.601, D_sup_loss: 0.146, D_sup_acc: 96.78 Train acc: 96.238 Test acc: 96.640 \n",
      "step: 3156 | Train: G_Loss: 1.084, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.605, D_sup_loss: 0.146, D_sup_acc: 96.68 Train acc: 96.268 Test acc: 96.590 \n",
      "step: 3157 | Train: G_Loss: 1.081, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.627, D_sup_loss: 0.145, D_sup_acc: 96.63 Train acc: 96.190 Test acc: 96.640 \n",
      "step: 3158 | Train: G_Loss: 1.084, D_unsup_loss_real: 0.639, D_unsup_loss_fake: 0.633, D_sup_loss: 0.152, D_sup_acc: 96.68 Train acc: 96.083 Test acc: 96.540 \n",
      "step: 3159 | Train: G_Loss: 1.201, D_unsup_loss_real: 0.545, D_unsup_loss_fake: 0.524, D_sup_loss: 0.160, D_sup_acc: 96.58 Train acc: 96.253 Test acc: 96.710 \n",
      "step: 3160 | Train: G_Loss: 1.160, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.566, D_sup_loss: 0.148, D_sup_acc: 96.75 Train acc: 96.178 Test acc: 96.630 \n",
      "step: 3161 | Train: G_Loss: 1.128, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.599, D_sup_loss: 0.149, D_sup_acc: 96.67 Train acc: 96.288 Test acc: 96.690 \n",
      "step: 3162 | Train: G_Loss: 1.086, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.629, D_sup_loss: 0.143, D_sup_acc: 96.73 Train acc: 96.323 Test acc: 96.710 \n",
      "step: 3163 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.552, D_sup_loss: 0.144, D_sup_acc: 96.75 Train acc: 96.347 Test acc: 96.810 \n",
      "step: 3164 | Train: G_Loss: 1.076, D_unsup_loss_real: 0.538, D_unsup_loss_fake: 0.609, D_sup_loss: 0.142, D_sup_acc: 96.85 Train acc: 96.268 Test acc: 96.730 \n",
      "step: 3165 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.652, D_sup_loss: 0.143, D_sup_acc: 96.77 Train acc: 96.117 Test acc: 96.510 \n",
      "step: 3166 | Train: G_Loss: 1.113, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.593, D_sup_loss: 0.149, D_sup_acc: 96.55 Train acc: 96.132 Test acc: 96.580 \n",
      "step: 3167 | Train: G_Loss: 1.080, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.544, D_sup_loss: 0.153, D_sup_acc: 96.62 Train acc: 96.068 Test acc: 96.590 \n",
      "step: 3168 | Train: G_Loss: 1.159, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.647, D_sup_loss: 0.156, D_sup_acc: 96.63 Train acc: 96.033 Test acc: 96.540 \n",
      "step: 3169 | Train: G_Loss: 1.099, D_unsup_loss_real: 0.698, D_unsup_loss_fake: 0.552, D_sup_loss: 0.159, D_sup_acc: 96.58 Train acc: 96.342 Test acc: 96.720 \n",
      "step: 3170 | Train: G_Loss: 1.063, D_unsup_loss_real: 0.677, D_unsup_loss_fake: 0.598, D_sup_loss: 0.144, D_sup_acc: 96.76 Train acc: 96.298 Test acc: 96.700 \n",
      "step: 3171 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.544, D_unsup_loss_fake: 0.626, D_sup_loss: 0.144, D_sup_acc: 96.74 Train acc: 96.168 Test acc: 96.440 \n",
      "step: 3172 | Train: G_Loss: 1.039, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.610, D_sup_loss: 0.148, D_sup_acc: 96.48 Train acc: 96.250 Test acc: 96.400 \n",
      "step: 3173 | Train: G_Loss: 1.027, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.687, D_sup_loss: 0.149, D_sup_acc: 96.45 Train acc: 96.257 Test acc: 96.660 \n",
      "step: 3174 | Train: G_Loss: 1.013, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.643, D_sup_loss: 0.147, D_sup_acc: 96.70 Train acc: 96.338 Test acc: 96.860 \n",
      "step: 3175 | Train: G_Loss: 1.071, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.755, D_sup_loss: 0.144, D_sup_acc: 96.90 Train acc: 96.300 Test acc: 96.780 \n",
      "step: 3176 | Train: G_Loss: 1.185, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.629, D_sup_loss: 0.147, D_sup_acc: 96.82 Train acc: 96.253 Test acc: 96.860 \n",
      "step: 3177 | Train: G_Loss: 1.161, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.573, D_sup_loss: 0.146, D_sup_acc: 96.90 Train acc: 96.382 Test acc: 96.890 \n",
      "step: 3178 | Train: G_Loss: 1.152, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.610, D_sup_loss: 0.147, D_sup_acc: 96.93 Train acc: 96.387 Test acc: 96.880 \n",
      "step: 3179 | Train: G_Loss: 1.211, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.607, D_sup_loss: 0.142, D_sup_acc: 96.92 Train acc: 96.450 Test acc: 96.870 \n",
      "step: 3180 | Train: G_Loss: 1.092, D_unsup_loss_real: 0.538, D_unsup_loss_fake: 0.651, D_sup_loss: 0.138, D_sup_acc: 96.91 Train acc: 96.397 Test acc: 96.880 \n",
      "step: 3181 | Train: G_Loss: 1.195, D_unsup_loss_real: 0.657, D_unsup_loss_fake: 0.584, D_sup_loss: 0.144, D_sup_acc: 96.92 Train acc: 96.208 Test acc: 96.660 \n",
      "step: 3182 | Train: G_Loss: 1.129, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.592, D_sup_loss: 0.153, D_sup_acc: 96.70 Train acc: 96.248 Test acc: 96.760 \n",
      "step: 3183 | Train: G_Loss: 1.170, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.629, D_sup_loss: 0.148, D_sup_acc: 96.80 Train acc: 96.252 Test acc: 96.730 \n",
      "step: 3184 | Train: G_Loss: 1.163, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.657, D_sup_loss: 0.147, D_sup_acc: 96.77 Train acc: 96.510 Test acc: 96.900 \n",
      "step: 3185 | Train: G_Loss: 1.114, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.660, D_sup_loss: 0.140, D_sup_acc: 96.94 Train acc: 96.510 Test acc: 96.940 \n",
      "step: 3186 | Train: G_Loss: 1.113, D_unsup_loss_real: 0.658, D_unsup_loss_fake: 0.649, D_sup_loss: 0.140, D_sup_acc: 96.98 Train acc: 96.335 Test acc: 96.810 \n",
      "step: 3187 | Train: G_Loss: 1.079, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.549, D_sup_loss: 0.149, D_sup_acc: 96.85 Train acc: 96.318 Test acc: 96.840 \n",
      "step: 3188 | Train: G_Loss: 1.000, D_unsup_loss_real: 0.679, D_unsup_loss_fake: 0.570, D_sup_loss: 0.150, D_sup_acc: 96.88 Train acc: 96.383 Test acc: 96.830 \n",
      "step: 3189 | Train: G_Loss: 1.048, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.638, D_sup_loss: 0.145, D_sup_acc: 96.87 Train acc: 96.450 Test acc: 96.900 \n",
      "step: 3190 | Train: G_Loss: 1.024, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.656, D_sup_loss: 0.139, D_sup_acc: 96.94 Train acc: 96.392 Test acc: 96.860 \n",
      "step: 3191 | Train: G_Loss: 1.097, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.616, D_sup_loss: 0.141, D_sup_acc: 96.90 Train acc: 96.368 Test acc: 96.790 \n",
      "step: 3192 | Train: G_Loss: 1.150, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.609, D_sup_loss: 0.141, D_sup_acc: 96.83 Train acc: 96.333 Test acc: 96.790 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3193 | Train: G_Loss: 1.063, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.609, D_sup_loss: 0.144, D_sup_acc: 96.83 Train acc: 96.370 Test acc: 96.860 \n",
      "step: 3194 | Train: G_Loss: 1.147, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.638, D_sup_loss: 0.141, D_sup_acc: 96.90 Train acc: 96.375 Test acc: 96.700 \n",
      "step: 3195 | Train: G_Loss: 1.036, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.616, D_sup_loss: 0.141, D_sup_acc: 96.74 Train acc: 96.337 Test acc: 96.720 \n",
      "step: 3196 | Train: G_Loss: 1.116, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.597, D_sup_loss: 0.146, D_sup_acc: 96.76 Train acc: 96.412 Test acc: 96.840 \n",
      "step: 3197 | Train: G_Loss: 1.039, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.629, D_sup_loss: 0.142, D_sup_acc: 96.88 Train acc: 96.428 Test acc: 96.830 \n",
      "step: 3198 | Train: G_Loss: 1.129, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.620, D_sup_loss: 0.143, D_sup_acc: 96.87 Train acc: 96.470 Test acc: 96.900 \n",
      "step: 3199 | Train: G_Loss: 1.046, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.590, D_sup_loss: 0.139, D_sup_acc: 96.94 Train acc: 96.288 Test acc: 96.780 \n",
      "step: 3200 | Train: G_Loss: 1.112, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.611, D_sup_loss: 0.145, D_sup_acc: 96.82 Train acc: 96.378 Test acc: 96.860 \n",
      "Train Classifier Accuracy: 96.378%\n",
      "\n",
      "Test Classifier Accuracy: 96.860%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_3200.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_3200.h5\n",
      "step: 3201 | Train: G_Loss: 1.080, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.625, D_sup_loss: 0.140, D_sup_acc: 96.90 Train acc: 96.405 Test acc: 96.850 \n",
      "step: 3202 | Train: G_Loss: 1.091, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.645, D_sup_loss: 0.138, D_sup_acc: 96.89 Train acc: 96.275 Test acc: 96.550 \n",
      "step: 3203 | Train: G_Loss: 1.158, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.648, D_sup_loss: 0.142, D_sup_acc: 96.59 Train acc: 96.358 Test acc: 96.750 \n",
      "step: 3204 | Train: G_Loss: 1.129, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.565, D_sup_loss: 0.143, D_sup_acc: 96.79 Train acc: 96.342 Test acc: 96.760 \n",
      "step: 3205 | Train: G_Loss: 1.117, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.630, D_sup_loss: 0.147, D_sup_acc: 96.80 Train acc: 96.315 Test acc: 96.870 \n",
      "step: 3206 | Train: G_Loss: 1.072, D_unsup_loss_real: 0.661, D_unsup_loss_fake: 0.607, D_sup_loss: 0.147, D_sup_acc: 96.91 Train acc: 96.532 Test acc: 96.900 \n",
      "step: 3207 | Train: G_Loss: 1.081, D_unsup_loss_real: 0.680, D_unsup_loss_fake: 0.662, D_sup_loss: 0.140, D_sup_acc: 96.94 Train acc: 96.363 Test acc: 96.910 \n",
      "step: 3208 | Train: G_Loss: 1.027, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.585, D_sup_loss: 0.147, D_sup_acc: 96.95 Train acc: 96.455 Test acc: 96.890 \n",
      "step: 3209 | Train: G_Loss: 1.126, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.623, D_sup_loss: 0.142, D_sup_acc: 96.93 Train acc: 96.550 Test acc: 96.900 \n",
      "step: 3210 | Train: G_Loss: 1.146, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.587, D_sup_loss: 0.139, D_sup_acc: 96.94 Train acc: 96.268 Test acc: 96.690 \n",
      "step: 3211 | Train: G_Loss: 1.133, D_unsup_loss_real: 0.668, D_unsup_loss_fake: 0.707, D_sup_loss: 0.154, D_sup_acc: 96.73 Train acc: 96.170 Test acc: 96.570 \n",
      "step: 3212 | Train: G_Loss: 1.086, D_unsup_loss_real: 0.676, D_unsup_loss_fake: 0.571, D_sup_loss: 0.160, D_sup_acc: 96.61 Train acc: 96.407 Test acc: 96.840 \n",
      "step: 3213 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.592, D_sup_loss: 0.150, D_sup_acc: 96.88 Train acc: 96.490 Test acc: 96.870 \n",
      "step: 3214 | Train: G_Loss: 1.088, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.566, D_sup_loss: 0.146, D_sup_acc: 96.91 Train acc: 96.412 Test acc: 96.780 \n",
      "step: 3215 | Train: G_Loss: 1.019, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.578, D_sup_loss: 0.148, D_sup_acc: 96.82 Train acc: 96.447 Test acc: 96.670 \n",
      "step: 3216 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.572, D_sup_loss: 0.147, D_sup_acc: 96.71 Train acc: 96.345 Test acc: 96.740 \n",
      "step: 3217 | Train: G_Loss: 1.180, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.645, D_sup_loss: 0.151, D_sup_acc: 96.78 Train acc: 96.335 Test acc: 96.680 \n",
      "step: 3218 | Train: G_Loss: 1.122, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.609, D_sup_loss: 0.150, D_sup_acc: 96.72 Train acc: 96.243 Test acc: 96.640 \n",
      "step: 3219 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.559, D_sup_loss: 0.152, D_sup_acc: 96.68 Train acc: 96.428 Test acc: 96.820 \n",
      "step: 3220 | Train: G_Loss: 1.099, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.681, D_sup_loss: 0.144, D_sup_acc: 96.86 Train acc: 96.340 Test acc: 96.780 \n",
      "step: 3221 | Train: G_Loss: 1.086, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.618, D_sup_loss: 0.143, D_sup_acc: 96.82 Train acc: 96.387 Test acc: 96.680 \n",
      "step: 3222 | Train: G_Loss: 1.160, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.701, D_sup_loss: 0.142, D_sup_acc: 96.72 Train acc: 96.435 Test acc: 96.710 \n",
      "step: 3223 | Train: G_Loss: 1.057, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.652, D_sup_loss: 0.140, D_sup_acc: 96.75 Train acc: 96.322 Test acc: 96.690 \n",
      "step: 3224 | Train: G_Loss: 1.198, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.606, D_sup_loss: 0.146, D_sup_acc: 96.73 Train acc: 96.270 Test acc: 96.630 \n",
      "step: 3225 | Train: G_Loss: 1.025, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.652, D_sup_loss: 0.149, D_sup_acc: 96.67 Train acc: 96.337 Test acc: 96.630 \n",
      "step: 3226 | Train: G_Loss: 1.105, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.642, D_sup_loss: 0.147, D_sup_acc: 96.67 Train acc: 96.158 Test acc: 96.510 \n",
      "step: 3227 | Train: G_Loss: 1.055, D_unsup_loss_real: 0.677, D_unsup_loss_fake: 0.548, D_sup_loss: 0.160, D_sup_acc: 96.55 Train acc: 96.390 Test acc: 96.660 \n",
      "step: 3228 | Train: G_Loss: 1.066, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.642, D_sup_loss: 0.144, D_sup_acc: 96.70 Train acc: 96.190 Test acc: 96.540 \n",
      "step: 3229 | Train: G_Loss: 1.140, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.639, D_sup_loss: 0.154, D_sup_acc: 96.58 Train acc: 96.258 Test acc: 96.640 \n",
      "step: 3230 | Train: G_Loss: 1.087, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.577, D_sup_loss: 0.154, D_sup_acc: 96.68 Train acc: 96.332 Test acc: 96.530 \n",
      "step: 3231 | Train: G_Loss: 1.054, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.619, D_sup_loss: 0.148, D_sup_acc: 96.57 Train acc: 96.118 Test acc: 96.470 \n",
      "step: 3232 | Train: G_Loss: 1.067, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.622, D_sup_loss: 0.159, D_sup_acc: 96.51 Train acc: 96.295 Test acc: 96.630 \n",
      "step: 3233 | Train: G_Loss: 1.055, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.648, D_sup_loss: 0.153, D_sup_acc: 96.67 Train acc: 96.288 Test acc: 96.630 \n",
      "step: 3234 | Train: G_Loss: 1.064, D_unsup_loss_real: 0.649, D_unsup_loss_fake: 0.667, D_sup_loss: 0.154, D_sup_acc: 96.67 Train acc: 96.190 Test acc: 96.630 \n",
      "step: 3235 | Train: G_Loss: 1.108, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.670, D_sup_loss: 0.156, D_sup_acc: 96.67 Train acc: 96.255 Test acc: 96.650 \n",
      "step: 3236 | Train: G_Loss: 1.127, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.635, D_sup_loss: 0.152, D_sup_acc: 96.69 Train acc: 96.312 Test acc: 96.660 \n",
      "step: 3237 | Train: G_Loss: 1.062, D_unsup_loss_real: 0.635, D_unsup_loss_fake: 0.596, D_sup_loss: 0.150, D_sup_acc: 96.70 Train acc: 96.343 Test acc: 96.840 \n",
      "step: 3238 | Train: G_Loss: 1.111, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.566, D_sup_loss: 0.145, D_sup_acc: 96.88 Train acc: 96.228 Test acc: 96.600 \n",
      "step: 3239 | Train: G_Loss: 1.100, D_unsup_loss_real: 0.642, D_unsup_loss_fake: 0.613, D_sup_loss: 0.149, D_sup_acc: 96.64 Train acc: 96.358 Test acc: 96.790 \n",
      "step: 3240 | Train: G_Loss: 1.128, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.516, D_sup_loss: 0.148, D_sup_acc: 96.83 Train acc: 96.412 Test acc: 96.580 \n",
      "step: 3241 | Train: G_Loss: 1.058, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.618, D_sup_loss: 0.142, D_sup_acc: 96.62 Train acc: 96.357 Test acc: 96.650 \n",
      "step: 3242 | Train: G_Loss: 1.099, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.641, D_sup_loss: 0.147, D_sup_acc: 96.69 Train acc: 96.237 Test acc: 96.730 \n",
      "step: 3243 | Train: G_Loss: 1.087, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.652, D_sup_loss: 0.152, D_sup_acc: 96.77 Train acc: 96.302 Test acc: 96.770 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3244 | Train: G_Loss: 1.143, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.645, D_sup_loss: 0.146, D_sup_acc: 96.81 Train acc: 96.290 Test acc: 96.780 \n",
      "step: 3245 | Train: G_Loss: 1.057, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.570, D_sup_loss: 0.149, D_sup_acc: 96.82 Train acc: 96.255 Test acc: 96.850 \n",
      "step: 3246 | Train: G_Loss: 1.043, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.579, D_sup_loss: 0.153, D_sup_acc: 96.89 Train acc: 96.410 Test acc: 96.810 \n",
      "step: 3247 | Train: G_Loss: 1.058, D_unsup_loss_real: 0.531, D_unsup_loss_fake: 0.623, D_sup_loss: 0.145, D_sup_acc: 96.85 Train acc: 96.310 Test acc: 96.760 \n",
      "step: 3248 | Train: G_Loss: 1.100, D_unsup_loss_real: 0.666, D_unsup_loss_fake: 0.593, D_sup_loss: 0.149, D_sup_acc: 96.80 Train acc: 96.403 Test acc: 96.920 \n",
      "step: 3249 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.628, D_unsup_loss_fake: 0.674, D_sup_loss: 0.143, D_sup_acc: 96.96 Train acc: 96.477 Test acc: 96.810 \n",
      "step: 3250 | Train: G_Loss: 1.067, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.645, D_sup_loss: 0.139, D_sup_acc: 96.85 Train acc: 96.437 Test acc: 96.890 \n",
      "step: 3251 | Train: G_Loss: 1.092, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.627, D_sup_loss: 0.143, D_sup_acc: 96.93 Train acc: 96.450 Test acc: 96.820 \n",
      "step: 3252 | Train: G_Loss: 1.051, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.587, D_sup_loss: 0.142, D_sup_acc: 96.86 Train acc: 96.208 Test acc: 96.640 \n",
      "step: 3253 | Train: G_Loss: 1.041, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.581, D_sup_loss: 0.154, D_sup_acc: 96.68 Train acc: 96.383 Test acc: 96.770 \n",
      "step: 3254 | Train: G_Loss: 1.190, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.622, D_sup_loss: 0.148, D_sup_acc: 96.81 Train acc: 96.367 Test acc: 96.620 \n",
      "step: 3255 | Train: G_Loss: 1.066, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.561, D_sup_loss: 0.150, D_sup_acc: 96.66 Train acc: 96.543 Test acc: 96.800 \n",
      "step: 3256 | Train: G_Loss: 1.127, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.668, D_sup_loss: 0.141, D_sup_acc: 96.84 Train acc: 96.473 Test acc: 96.830 \n",
      "step: 3257 | Train: G_Loss: 1.100, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.563, D_sup_loss: 0.142, D_sup_acc: 96.87 Train acc: 96.525 Test acc: 96.860 \n",
      "step: 3258 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.538, D_unsup_loss_fake: 0.592, D_sup_loss: 0.139, D_sup_acc: 96.90 Train acc: 96.537 Test acc: 96.780 \n",
      "step: 3259 | Train: G_Loss: 1.059, D_unsup_loss_real: 0.691, D_unsup_loss_fake: 0.556, D_sup_loss: 0.139, D_sup_acc: 96.82 Train acc: 96.462 Test acc: 96.860 \n",
      "step: 3260 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.584, D_sup_loss: 0.139, D_sup_acc: 96.90 Train acc: 96.370 Test acc: 96.750 \n",
      "step: 3261 | Train: G_Loss: 1.123, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.619, D_sup_loss: 0.143, D_sup_acc: 96.79 Train acc: 96.432 Test acc: 96.800 \n",
      "step: 3262 | Train: G_Loss: 1.128, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.603, D_sup_loss: 0.141, D_sup_acc: 96.84 Train acc: 96.532 Test acc: 96.800 \n",
      "step: 3263 | Train: G_Loss: 1.157, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.628, D_sup_loss: 0.138, D_sup_acc: 96.84 Train acc: 96.527 Test acc: 96.800 \n",
      "step: 3264 | Train: G_Loss: 0.975, D_unsup_loss_real: 0.550, D_unsup_loss_fake: 0.642, D_sup_loss: 0.138, D_sup_acc: 96.84 Train acc: 96.300 Test acc: 96.690 \n",
      "step: 3265 | Train: G_Loss: 1.183, D_unsup_loss_real: 0.463, D_unsup_loss_fake: 0.608, D_sup_loss: 0.149, D_sup_acc: 96.73 Train acc: 96.357 Test acc: 96.800 \n",
      "step: 3266 | Train: G_Loss: 1.043, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.602, D_sup_loss: 0.145, D_sup_acc: 96.84 Train acc: 96.355 Test acc: 96.730 \n",
      "step: 3267 | Train: G_Loss: 1.025, D_unsup_loss_real: 0.685, D_unsup_loss_fake: 0.620, D_sup_loss: 0.147, D_sup_acc: 96.77 Train acc: 96.257 Test acc: 96.640 \n",
      "step: 3268 | Train: G_Loss: 1.106, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.574, D_sup_loss: 0.153, D_sup_acc: 96.68 Train acc: 96.473 Test acc: 96.780 \n",
      "step: 3269 | Train: G_Loss: 1.065, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.609, D_sup_loss: 0.142, D_sup_acc: 96.82 Train acc: 96.482 Test acc: 96.840 \n",
      "step: 3270 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.550, D_unsup_loss_fake: 0.658, D_sup_loss: 0.139, D_sup_acc: 96.88 Train acc: 96.440 Test acc: 96.770 \n",
      "step: 3271 | Train: G_Loss: 1.017, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.520, D_sup_loss: 0.143, D_sup_acc: 96.81 Train acc: 96.382 Test acc: 96.700 \n",
      "step: 3272 | Train: G_Loss: 1.079, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.654, D_sup_loss: 0.142, D_sup_acc: 96.74 Train acc: 96.457 Test acc: 96.820 \n",
      "step: 3273 | Train: G_Loss: 0.950, D_unsup_loss_real: 0.650, D_unsup_loss_fake: 0.589, D_sup_loss: 0.140, D_sup_acc: 96.86 Train acc: 96.402 Test acc: 96.670 \n",
      "step: 3274 | Train: G_Loss: 1.157, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.622, D_sup_loss: 0.145, D_sup_acc: 96.71 Train acc: 96.537 Test acc: 96.820 \n",
      "step: 3275 | Train: G_Loss: 1.154, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.594, D_sup_loss: 0.141, D_sup_acc: 96.86 Train acc: 96.560 Test acc: 96.810 \n",
      "step: 3276 | Train: G_Loss: 1.073, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.571, D_sup_loss: 0.139, D_sup_acc: 96.85 Train acc: 96.527 Test acc: 96.840 \n",
      "step: 3277 | Train: G_Loss: 1.041, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.604, D_sup_loss: 0.140, D_sup_acc: 96.88 Train acc: 96.508 Test acc: 96.940 \n",
      "step: 3278 | Train: G_Loss: 1.152, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.707, D_sup_loss: 0.140, D_sup_acc: 96.98 Train acc: 96.652 Test acc: 96.930 \n",
      "step: 3279 | Train: G_Loss: 1.159, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.678, D_sup_loss: 0.136, D_sup_acc: 96.97 Train acc: 96.547 Test acc: 96.810 \n",
      "step: 3280 | Train: G_Loss: 1.075, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.557, D_sup_loss: 0.139, D_sup_acc: 96.85 Train acc: 96.523 Test acc: 96.770 \n",
      "step: 3281 | Train: G_Loss: 1.133, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.583, D_sup_loss: 0.136, D_sup_acc: 96.81 Train acc: 96.400 Test acc: 96.710 \n",
      "step: 3282 | Train: G_Loss: 1.085, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.589, D_sup_loss: 0.141, D_sup_acc: 96.75 Train acc: 96.472 Test acc: 96.780 \n",
      "step: 3283 | Train: G_Loss: 1.112, D_unsup_loss_real: 0.697, D_unsup_loss_fake: 0.574, D_sup_loss: 0.137, D_sup_acc: 96.82 Train acc: 96.412 Test acc: 96.720 \n",
      "step: 3284 | Train: G_Loss: 1.080, D_unsup_loss_real: 0.697, D_unsup_loss_fake: 0.597, D_sup_loss: 0.142, D_sup_acc: 96.76 Train acc: 96.407 Test acc: 96.720 \n",
      "step: 3285 | Train: G_Loss: 1.092, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.629, D_sup_loss: 0.144, D_sup_acc: 96.76 Train acc: 96.410 Test acc: 96.700 \n",
      "step: 3286 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.616, D_sup_loss: 0.145, D_sup_acc: 96.74 Train acc: 96.452 Test acc: 96.780 \n",
      "step: 3287 | Train: G_Loss: 1.046, D_unsup_loss_real: 0.635, D_unsup_loss_fake: 0.609, D_sup_loss: 0.144, D_sup_acc: 96.82 Train acc: 96.460 Test acc: 96.860 \n",
      "step: 3288 | Train: G_Loss: 1.197, D_unsup_loss_real: 0.628, D_unsup_loss_fake: 0.574, D_sup_loss: 0.141, D_sup_acc: 96.90 Train acc: 96.433 Test acc: 96.810 \n",
      "step: 3289 | Train: G_Loss: 1.085, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.571, D_sup_loss: 0.142, D_sup_acc: 96.85 Train acc: 96.360 Test acc: 96.780 \n",
      "step: 3290 | Train: G_Loss: 1.203, D_unsup_loss_real: 0.469, D_unsup_loss_fake: 0.684, D_sup_loss: 0.144, D_sup_acc: 96.82 Train acc: 96.138 Test acc: 96.500 \n",
      "step: 3291 | Train: G_Loss: 1.129, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.576, D_sup_loss: 0.153, D_sup_acc: 96.54 Train acc: 96.352 Test acc: 96.580 \n",
      "step: 3292 | Train: G_Loss: 1.106, D_unsup_loss_real: 0.658, D_unsup_loss_fake: 0.565, D_sup_loss: 0.144, D_sup_acc: 96.62 Train acc: 96.355 Test acc: 96.600 \n",
      "step: 3293 | Train: G_Loss: 1.045, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.601, D_sup_loss: 0.145, D_sup_acc: 96.64 Train acc: 96.388 Test acc: 96.710 \n",
      "step: 3294 | Train: G_Loss: 1.118, D_unsup_loss_real: 0.644, D_unsup_loss_fake: 0.604, D_sup_loss: 0.144, D_sup_acc: 96.75 Train acc: 96.302 Test acc: 96.730 \n",
      "step: 3295 | Train: G_Loss: 1.088, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.664, D_sup_loss: 0.147, D_sup_acc: 96.77 Train acc: 96.367 Test acc: 96.730 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3296 | Train: G_Loss: 1.061, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.605, D_sup_loss: 0.146, D_sup_acc: 96.77 Train acc: 96.327 Test acc: 96.650 \n",
      "step: 3297 | Train: G_Loss: 1.172, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.688, D_sup_loss: 0.146, D_sup_acc: 96.69 Train acc: 96.270 Test acc: 96.770 \n",
      "step: 3298 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.664, D_sup_loss: 0.147, D_sup_acc: 96.81 Train acc: 96.183 Test acc: 96.670 \n",
      "step: 3299 | Train: G_Loss: 1.116, D_unsup_loss_real: 0.695, D_unsup_loss_fake: 0.598, D_sup_loss: 0.154, D_sup_acc: 96.71 Train acc: 96.272 Test acc: 96.760 \n",
      "step: 3300 | Train: G_Loss: 1.051, D_unsup_loss_real: 0.679, D_unsup_loss_fake: 0.611, D_sup_loss: 0.145, D_sup_acc: 96.80 Train acc: 96.350 Test acc: 96.750 \n",
      "Train Classifier Accuracy: 96.350%\n",
      "\n",
      "Test Classifier Accuracy: 96.750%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_3300.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_3300.h5\n",
      "step: 3301 | Train: G_Loss: 1.032, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.686, D_sup_loss: 0.147, D_sup_acc: 96.79 Train acc: 96.345 Test acc: 96.830 \n",
      "step: 3302 | Train: G_Loss: 0.994, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.614, D_sup_loss: 0.148, D_sup_acc: 96.87 Train acc: 96.455 Test acc: 96.790 \n",
      "step: 3303 | Train: G_Loss: 1.071, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.664, D_sup_loss: 0.141, D_sup_acc: 96.83 Train acc: 96.342 Test acc: 96.680 \n",
      "step: 3304 | Train: G_Loss: 1.032, D_unsup_loss_real: 0.639, D_unsup_loss_fake: 0.597, D_sup_loss: 0.148, D_sup_acc: 96.72 Train acc: 96.382 Test acc: 96.730 \n",
      "step: 3305 | Train: G_Loss: 1.157, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.665, D_sup_loss: 0.143, D_sup_acc: 96.77 Train acc: 96.460 Test acc: 96.980 \n",
      "step: 3306 | Train: G_Loss: 1.081, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.588, D_sup_loss: 0.140, D_sup_acc: 97.02 Train acc: 96.405 Test acc: 96.870 \n",
      "step: 3307 | Train: G_Loss: 1.046, D_unsup_loss_real: 0.657, D_unsup_loss_fake: 0.669, D_sup_loss: 0.137, D_sup_acc: 96.91 Train acc: 96.327 Test acc: 96.940 \n",
      "step: 3308 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.667, D_unsup_loss_fake: 0.588, D_sup_loss: 0.141, D_sup_acc: 96.98 Train acc: 96.278 Test acc: 96.820 \n",
      "step: 3309 | Train: G_Loss: 1.104, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.673, D_sup_loss: 0.144, D_sup_acc: 96.86 Train acc: 96.205 Test acc: 96.660 \n",
      "step: 3310 | Train: G_Loss: 1.172, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.627, D_sup_loss: 0.148, D_sup_acc: 96.70 Train acc: 96.090 Test acc: 96.560 \n",
      "step: 3311 | Train: G_Loss: 1.028, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.530, D_sup_loss: 0.153, D_sup_acc: 96.60 Train acc: 96.358 Test acc: 96.880 \n",
      "step: 3312 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.628, D_sup_loss: 0.141, D_sup_acc: 96.92 Train acc: 96.280 Test acc: 96.770 \n",
      "step: 3313 | Train: G_Loss: 1.088, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.683, D_sup_loss: 0.145, D_sup_acc: 96.81 Train acc: 96.237 Test acc: 96.720 \n",
      "step: 3314 | Train: G_Loss: 1.103, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.521, D_sup_loss: 0.146, D_sup_acc: 96.76 Train acc: 96.323 Test acc: 96.750 \n",
      "step: 3315 | Train: G_Loss: 1.048, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.602, D_sup_loss: 0.141, D_sup_acc: 96.79 Train acc: 96.368 Test acc: 96.780 \n",
      "step: 3316 | Train: G_Loss: 1.082, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.601, D_sup_loss: 0.139, D_sup_acc: 96.82 Train acc: 96.182 Test acc: 96.590 \n",
      "step: 3317 | Train: G_Loss: 1.082, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.531, D_sup_loss: 0.150, D_sup_acc: 96.63 Train acc: 96.232 Test acc: 96.630 \n",
      "step: 3318 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.625, D_sup_loss: 0.150, D_sup_acc: 96.67 Train acc: 96.362 Test acc: 96.710 \n",
      "step: 3319 | Train: G_Loss: 1.063, D_unsup_loss_real: 0.647, D_unsup_loss_fake: 0.607, D_sup_loss: 0.144, D_sup_acc: 96.75 Train acc: 96.355 Test acc: 96.690 \n",
      "step: 3320 | Train: G_Loss: 1.068, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.608, D_sup_loss: 0.142, D_sup_acc: 96.73 Train acc: 96.360 Test acc: 96.680 \n",
      "step: 3321 | Train: G_Loss: 1.077, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.588, D_sup_loss: 0.139, D_sup_acc: 96.72 Train acc: 96.337 Test acc: 96.690 \n",
      "step: 3322 | Train: G_Loss: 1.045, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.665, D_sup_loss: 0.139, D_sup_acc: 96.73 Train acc: 96.248 Test acc: 96.590 \n",
      "step: 3323 | Train: G_Loss: 1.154, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.672, D_sup_loss: 0.147, D_sup_acc: 96.63 Train acc: 96.303 Test acc: 96.560 \n",
      "step: 3324 | Train: G_Loss: 1.023, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.621, D_sup_loss: 0.141, D_sup_acc: 96.60 Train acc: 96.208 Test acc: 96.640 \n",
      "step: 3325 | Train: G_Loss: 1.196, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.590, D_sup_loss: 0.145, D_sup_acc: 96.68 Train acc: 96.268 Test acc: 96.680 \n",
      "step: 3326 | Train: G_Loss: 1.035, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.625, D_sup_loss: 0.141, D_sup_acc: 96.72 Train acc: 96.268 Test acc: 96.690 \n",
      "step: 3327 | Train: G_Loss: 1.146, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.610, D_sup_loss: 0.144, D_sup_acc: 96.73 Train acc: 96.332 Test acc: 96.690 \n",
      "step: 3328 | Train: G_Loss: 1.020, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.605, D_sup_loss: 0.143, D_sup_acc: 96.73 Train acc: 96.243 Test acc: 96.620 \n",
      "step: 3329 | Train: G_Loss: 1.039, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.602, D_sup_loss: 0.145, D_sup_acc: 96.66 Train acc: 96.348 Test acc: 96.710 \n",
      "step: 3330 | Train: G_Loss: 1.129, D_unsup_loss_real: 0.542, D_unsup_loss_fake: 0.603, D_sup_loss: 0.141, D_sup_acc: 96.75 Train acc: 96.425 Test acc: 96.740 \n",
      "step: 3331 | Train: G_Loss: 1.156, D_unsup_loss_real: 0.639, D_unsup_loss_fake: 0.541, D_sup_loss: 0.138, D_sup_acc: 96.78 Train acc: 96.373 Test acc: 96.700 \n",
      "step: 3332 | Train: G_Loss: 1.072, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.588, D_sup_loss: 0.138, D_sup_acc: 96.74 Train acc: 96.287 Test acc: 96.660 \n",
      "step: 3333 | Train: G_Loss: 1.060, D_unsup_loss_real: 0.524, D_unsup_loss_fake: 0.709, D_sup_loss: 0.141, D_sup_acc: 96.70 Train acc: 96.285 Test acc: 96.700 \n",
      "step: 3334 | Train: G_Loss: 1.065, D_unsup_loss_real: 0.677, D_unsup_loss_fake: 0.658, D_sup_loss: 0.145, D_sup_acc: 96.74 Train acc: 96.223 Test acc: 96.590 \n",
      "step: 3335 | Train: G_Loss: 1.069, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.626, D_sup_loss: 0.150, D_sup_acc: 96.63 Train acc: 96.305 Test acc: 96.640 \n",
      "step: 3336 | Train: G_Loss: 1.005, D_unsup_loss_real: 0.671, D_unsup_loss_fake: 0.582, D_sup_loss: 0.147, D_sup_acc: 96.68 Train acc: 96.435 Test acc: 96.730 \n",
      "step: 3337 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.608, D_sup_loss: 0.141, D_sup_acc: 96.77 Train acc: 96.460 Test acc: 96.780 \n",
      "step: 3338 | Train: G_Loss: 1.080, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.638, D_sup_loss: 0.138, D_sup_acc: 96.82 Train acc: 96.463 Test acc: 96.670 \n",
      "step: 3339 | Train: G_Loss: 1.063, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.545, D_sup_loss: 0.139, D_sup_acc: 96.71 Train acc: 96.428 Test acc: 96.800 \n",
      "step: 3340 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.562, D_sup_loss: 0.139, D_sup_acc: 96.84 Train acc: 96.472 Test acc: 96.790 \n",
      "step: 3341 | Train: G_Loss: 1.089, D_unsup_loss_real: 0.646, D_unsup_loss_fake: 0.628, D_sup_loss: 0.140, D_sup_acc: 96.83 Train acc: 96.525 Test acc: 96.720 \n",
      "step: 3342 | Train: G_Loss: 1.071, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.692, D_sup_loss: 0.137, D_sup_acc: 96.76 Train acc: 96.238 Test acc: 96.700 \n",
      "step: 3343 | Train: G_Loss: 1.081, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.625, D_sup_loss: 0.151, D_sup_acc: 96.74 Train acc: 96.188 Test acc: 96.720 \n",
      "step: 3344 | Train: G_Loss: 1.077, D_unsup_loss_real: 0.660, D_unsup_loss_fake: 0.673, D_sup_loss: 0.155, D_sup_acc: 96.76 Train acc: 96.315 Test acc: 96.660 \n",
      "step: 3345 | Train: G_Loss: 1.031, D_unsup_loss_real: 0.514, D_unsup_loss_fake: 0.614, D_sup_loss: 0.154, D_sup_acc: 96.70 Train acc: 96.485 Test acc: 96.700 \n",
      "step: 3346 | Train: G_Loss: 1.049, D_unsup_loss_real: 0.675, D_unsup_loss_fake: 0.566, D_sup_loss: 0.143, D_sup_acc: 96.74 Train acc: 96.438 Test acc: 96.710 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3347 | Train: G_Loss: 1.072, D_unsup_loss_real: 0.542, D_unsup_loss_fake: 0.665, D_sup_loss: 0.148, D_sup_acc: 96.75 Train acc: 96.343 Test acc: 96.710 \n",
      "step: 3348 | Train: G_Loss: 1.039, D_unsup_loss_real: 0.667, D_unsup_loss_fake: 0.592, D_sup_loss: 0.153, D_sup_acc: 96.75 Train acc: 96.335 Test acc: 96.660 \n",
      "step: 3349 | Train: G_Loss: 1.085, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.603, D_sup_loss: 0.151, D_sup_acc: 96.70 Train acc: 96.390 Test acc: 96.730 \n",
      "step: 3350 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.610, D_sup_loss: 0.148, D_sup_acc: 96.77 Train acc: 96.317 Test acc: 96.620 \n",
      "step: 3351 | Train: G_Loss: 1.168, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.648, D_sup_loss: 0.151, D_sup_acc: 96.66 Train acc: 96.477 Test acc: 96.810 \n",
      "step: 3352 | Train: G_Loss: 1.101, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.694, D_sup_loss: 0.145, D_sup_acc: 96.85 Train acc: 96.445 Test acc: 96.760 \n",
      "step: 3353 | Train: G_Loss: 1.103, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.599, D_sup_loss: 0.147, D_sup_acc: 96.80 Train acc: 96.458 Test acc: 96.740 \n",
      "step: 3354 | Train: G_Loss: 1.077, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.606, D_sup_loss: 0.149, D_sup_acc: 96.78 Train acc: 96.650 Test acc: 96.850 \n",
      "step: 3355 | Train: G_Loss: 1.045, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.577, D_sup_loss: 0.136, D_sup_acc: 96.89 Train acc: 96.560 Test acc: 96.910 \n",
      "step: 3356 | Train: G_Loss: 1.122, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.598, D_sup_loss: 0.138, D_sup_acc: 96.95 Train acc: 96.557 Test acc: 96.910 \n",
      "step: 3357 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.645, D_sup_loss: 0.138, D_sup_acc: 96.95 Train acc: 96.288 Test acc: 96.620 \n",
      "step: 3358 | Train: G_Loss: 1.067, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.602, D_sup_loss: 0.153, D_sup_acc: 96.66 Train acc: 96.347 Test acc: 96.740 \n",
      "step: 3359 | Train: G_Loss: 1.161, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.588, D_sup_loss: 0.151, D_sup_acc: 96.78 Train acc: 96.355 Test acc: 96.650 \n",
      "step: 3360 | Train: G_Loss: 1.143, D_unsup_loss_real: 0.639, D_unsup_loss_fake: 0.659, D_sup_loss: 0.150, D_sup_acc: 96.69 Train acc: 96.395 Test acc: 96.810 \n",
      "step: 3361 | Train: G_Loss: 1.184, D_unsup_loss_real: 0.545, D_unsup_loss_fake: 0.610, D_sup_loss: 0.144, D_sup_acc: 96.85 Train acc: 96.498 Test acc: 96.800 \n",
      "step: 3362 | Train: G_Loss: 1.099, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.586, D_sup_loss: 0.138, D_sup_acc: 96.84 Train acc: 96.360 Test acc: 96.830 \n",
      "step: 3363 | Train: G_Loss: 1.108, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.551, D_sup_loss: 0.146, D_sup_acc: 96.87 Train acc: 96.318 Test acc: 96.750 \n",
      "step: 3364 | Train: G_Loss: 1.156, D_unsup_loss_real: 0.644, D_unsup_loss_fake: 0.561, D_sup_loss: 0.145, D_sup_acc: 96.79 Train acc: 96.358 Test acc: 96.840 \n",
      "step: 3365 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.634, D_sup_loss: 0.145, D_sup_acc: 96.88 Train acc: 96.390 Test acc: 96.780 \n",
      "step: 3366 | Train: G_Loss: 1.097, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.582, D_sup_loss: 0.140, D_sup_acc: 96.82 Train acc: 96.337 Test acc: 96.740 \n",
      "step: 3367 | Train: G_Loss: 1.206, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.633, D_sup_loss: 0.143, D_sup_acc: 96.78 Train acc: 96.235 Test acc: 96.690 \n",
      "step: 3368 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.657, D_unsup_loss_fake: 0.565, D_sup_loss: 0.148, D_sup_acc: 96.73 Train acc: 96.320 Test acc: 96.620 \n",
      "step: 3369 | Train: G_Loss: 1.081, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.564, D_sup_loss: 0.146, D_sup_acc: 96.66 Train acc: 96.427 Test acc: 96.670 \n",
      "step: 3370 | Train: G_Loss: 1.211, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.607, D_sup_loss: 0.142, D_sup_acc: 96.71 Train acc: 96.330 Test acc: 96.720 \n",
      "step: 3371 | Train: G_Loss: 1.086, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.595, D_sup_loss: 0.145, D_sup_acc: 96.76 Train acc: 96.287 Test acc: 96.670 \n",
      "step: 3372 | Train: G_Loss: 1.029, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.562, D_sup_loss: 0.146, D_sup_acc: 96.71 Train acc: 96.292 Test acc: 96.730 \n",
      "step: 3373 | Train: G_Loss: 1.100, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.595, D_sup_loss: 0.145, D_sup_acc: 96.77 Train acc: 96.280 Test acc: 96.690 \n",
      "step: 3374 | Train: G_Loss: 1.089, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.549, D_sup_loss: 0.147, D_sup_acc: 96.73 Train acc: 96.325 Test acc: 96.630 \n",
      "step: 3375 | Train: G_Loss: 1.157, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.576, D_sup_loss: 0.146, D_sup_acc: 96.67 Train acc: 96.445 Test acc: 96.760 \n",
      "step: 3376 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.714, D_sup_loss: 0.143, D_sup_acc: 96.80 Train acc: 96.428 Test acc: 96.810 \n",
      "step: 3377 | Train: G_Loss: 1.020, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.633, D_sup_loss: 0.145, D_sup_acc: 96.85 Train acc: 96.378 Test acc: 96.720 \n",
      "step: 3378 | Train: G_Loss: 1.105, D_unsup_loss_real: 0.661, D_unsup_loss_fake: 0.628, D_sup_loss: 0.146, D_sup_acc: 96.76 Train acc: 96.455 Test acc: 96.720 \n",
      "step: 3379 | Train: G_Loss: 1.058, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.634, D_sup_loss: 0.144, D_sup_acc: 96.76 Train acc: 96.485 Test acc: 96.710 \n",
      "step: 3380 | Train: G_Loss: 1.113, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.701, D_sup_loss: 0.142, D_sup_acc: 96.75 Train acc: 96.450 Test acc: 96.770 \n",
      "step: 3381 | Train: G_Loss: 1.115, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.546, D_sup_loss: 0.145, D_sup_acc: 96.81 Train acc: 96.437 Test acc: 96.630 \n",
      "step: 3382 | Train: G_Loss: 1.184, D_unsup_loss_real: 0.678, D_unsup_loss_fake: 0.581, D_sup_loss: 0.144, D_sup_acc: 96.67 Train acc: 96.467 Test acc: 96.690 \n",
      "step: 3383 | Train: G_Loss: 1.089, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.572, D_sup_loss: 0.142, D_sup_acc: 96.73 Train acc: 96.483 Test acc: 96.730 \n",
      "step: 3384 | Train: G_Loss: 1.087, D_unsup_loss_real: 0.662, D_unsup_loss_fake: 0.597, D_sup_loss: 0.141, D_sup_acc: 96.77 Train acc: 96.403 Test acc: 96.600 \n",
      "step: 3385 | Train: G_Loss: 1.034, D_unsup_loss_real: 0.685, D_unsup_loss_fake: 0.584, D_sup_loss: 0.145, D_sup_acc: 96.64 Train acc: 96.182 Test acc: 96.500 \n",
      "step: 3386 | Train: G_Loss: 1.066, D_unsup_loss_real: 0.528, D_unsup_loss_fake: 0.596, D_sup_loss: 0.154, D_sup_acc: 96.54 Train acc: 96.443 Test acc: 96.670 \n",
      "step: 3387 | Train: G_Loss: 1.052, D_unsup_loss_real: 0.684, D_unsup_loss_fake: 0.623, D_sup_loss: 0.140, D_sup_acc: 96.71 Train acc: 96.132 Test acc: 96.320 \n",
      "step: 3388 | Train: G_Loss: 1.066, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.602, D_sup_loss: 0.158, D_sup_acc: 96.37 Train acc: 96.303 Test acc: 96.610 \n",
      "step: 3389 | Train: G_Loss: 1.086, D_unsup_loss_real: 0.544, D_unsup_loss_fake: 0.530, D_sup_loss: 0.150, D_sup_acc: 96.65 Train acc: 96.405 Test acc: 96.680 \n",
      "step: 3390 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.670, D_sup_loss: 0.144, D_sup_acc: 96.72 Train acc: 96.485 Test acc: 96.830 \n",
      "step: 3391 | Train: G_Loss: 1.110, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.683, D_sup_loss: 0.140, D_sup_acc: 96.87 Train acc: 96.367 Test acc: 96.660 \n",
      "step: 3392 | Train: G_Loss: 1.149, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.522, D_sup_loss: 0.153, D_sup_acc: 96.70 Train acc: 96.445 Test acc: 96.760 \n",
      "step: 3393 | Train: G_Loss: 1.105, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.623, D_sup_loss: 0.142, D_sup_acc: 96.80 Train acc: 96.462 Test acc: 96.780 \n",
      "step: 3394 | Train: G_Loss: 1.112, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.651, D_sup_loss: 0.139, D_sup_acc: 96.82 Train acc: 96.407 Test acc: 96.800 \n",
      "step: 3395 | Train: G_Loss: 1.208, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.551, D_sup_loss: 0.141, D_sup_acc: 96.84 Train acc: 96.465 Test acc: 96.790 \n",
      "step: 3396 | Train: G_Loss: 1.084, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.644, D_sup_loss: 0.139, D_sup_acc: 96.83 Train acc: 96.333 Test acc: 96.640 \n",
      "step: 3397 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.588, D_sup_loss: 0.145, D_sup_acc: 96.68 Train acc: 96.385 Test acc: 96.720 \n",
      "step: 3398 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.628, D_unsup_loss_fake: 0.606, D_sup_loss: 0.142, D_sup_acc: 96.76 Train acc: 96.448 Test acc: 96.770 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3399 | Train: G_Loss: 1.183, D_unsup_loss_real: 0.671, D_unsup_loss_fake: 0.592, D_sup_loss: 0.141, D_sup_acc: 96.81 Train acc: 96.290 Test acc: 96.700 \n",
      "step: 3400 | Train: G_Loss: 1.064, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.580, D_sup_loss: 0.148, D_sup_acc: 96.74 Train acc: 96.132 Test acc: 96.510 \n",
      "Train Classifier Accuracy: 96.132%\n",
      "\n",
      "Test Classifier Accuracy: 96.510%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_3400.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_3400.h5\n",
      "step: 3401 | Train: G_Loss: 1.079, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.614, D_sup_loss: 0.159, D_sup_acc: 96.55 Train acc: 96.353 Test acc: 96.690 \n",
      "step: 3402 | Train: G_Loss: 1.054, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.555, D_sup_loss: 0.146, D_sup_acc: 96.73 Train acc: 96.398 Test acc: 96.730 \n",
      "step: 3403 | Train: G_Loss: 1.122, D_unsup_loss_real: 0.642, D_unsup_loss_fake: 0.578, D_sup_loss: 0.146, D_sup_acc: 96.77 Train acc: 96.473 Test acc: 96.800 \n",
      "step: 3404 | Train: G_Loss: 1.130, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.667, D_sup_loss: 0.141, D_sup_acc: 96.84 Train acc: 96.282 Test acc: 96.700 \n",
      "step: 3405 | Train: G_Loss: 1.083, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.552, D_sup_loss: 0.147, D_sup_acc: 96.74 Train acc: 96.373 Test acc: 96.780 \n",
      "step: 3406 | Train: G_Loss: 1.105, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.606, D_sup_loss: 0.142, D_sup_acc: 96.82 Train acc: 96.373 Test acc: 96.750 \n",
      "step: 3407 | Train: G_Loss: 1.073, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.581, D_sup_loss: 0.140, D_sup_acc: 96.79 Train acc: 96.325 Test acc: 96.650 \n",
      "step: 3408 | Train: G_Loss: 1.059, D_unsup_loss_real: 0.635, D_unsup_loss_fake: 0.609, D_sup_loss: 0.143, D_sup_acc: 96.69 Train acc: 96.197 Test acc: 96.640 \n",
      "step: 3409 | Train: G_Loss: 1.086, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.631, D_sup_loss: 0.149, D_sup_acc: 96.68 Train acc: 96.235 Test acc: 96.610 \n",
      "step: 3410 | Train: G_Loss: 1.089, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.591, D_sup_loss: 0.151, D_sup_acc: 96.65 Train acc: 96.398 Test acc: 96.730 \n",
      "step: 3411 | Train: G_Loss: 1.116, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.620, D_sup_loss: 0.141, D_sup_acc: 96.77 Train acc: 96.188 Test acc: 96.470 \n",
      "step: 3412 | Train: G_Loss: 1.238, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.602, D_sup_loss: 0.152, D_sup_acc: 96.51 Train acc: 96.362 Test acc: 96.610 \n",
      "step: 3413 | Train: G_Loss: 1.199, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.585, D_sup_loss: 0.145, D_sup_acc: 96.65 Train acc: 96.435 Test acc: 96.720 \n",
      "step: 3414 | Train: G_Loss: 1.064, D_unsup_loss_real: 0.642, D_unsup_loss_fake: 0.594, D_sup_loss: 0.143, D_sup_acc: 96.76 Train acc: 96.318 Test acc: 96.690 \n",
      "step: 3415 | Train: G_Loss: 1.054, D_unsup_loss_real: 0.635, D_unsup_loss_fake: 0.638, D_sup_loss: 0.145, D_sup_acc: 96.73 Train acc: 96.345 Test acc: 96.650 \n",
      "step: 3416 | Train: G_Loss: 1.035, D_unsup_loss_real: 0.535, D_unsup_loss_fake: 0.581, D_sup_loss: 0.144, D_sup_acc: 96.69 Train acc: 96.308 Test acc: 96.620 \n",
      "step: 3417 | Train: G_Loss: 1.024, D_unsup_loss_real: 0.668, D_unsup_loss_fake: 0.606, D_sup_loss: 0.142, D_sup_acc: 96.66 Train acc: 96.028 Test acc: 96.460 \n",
      "step: 3418 | Train: G_Loss: 1.052, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.633, D_sup_loss: 0.155, D_sup_acc: 96.50 Train acc: 96.218 Test acc: 96.620 \n",
      "step: 3419 | Train: G_Loss: 1.014, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.638, D_sup_loss: 0.147, D_sup_acc: 96.66 Train acc: 96.330 Test acc: 96.860 \n",
      "step: 3420 | Train: G_Loss: 1.061, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.648, D_sup_loss: 0.138, D_sup_acc: 96.90 Train acc: 96.298 Test acc: 96.830 \n",
      "step: 3421 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.599, D_sup_loss: 0.142, D_sup_acc: 96.87 Train acc: 96.452 Test acc: 96.800 \n",
      "step: 3422 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.635, D_unsup_loss_fake: 0.572, D_sup_loss: 0.140, D_sup_acc: 96.84 Train acc: 96.280 Test acc: 96.690 \n",
      "step: 3423 | Train: G_Loss: 1.051, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.600, D_sup_loss: 0.150, D_sup_acc: 96.73 Train acc: 96.328 Test acc: 96.760 \n",
      "step: 3424 | Train: G_Loss: 1.103, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.607, D_sup_loss: 0.147, D_sup_acc: 96.80 Train acc: 96.403 Test acc: 96.830 \n",
      "step: 3425 | Train: G_Loss: 1.064, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.680, D_sup_loss: 0.142, D_sup_acc: 96.87 Train acc: 96.370 Test acc: 96.740 \n",
      "step: 3426 | Train: G_Loss: 1.061, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.567, D_sup_loss: 0.144, D_sup_acc: 96.78 Train acc: 96.332 Test acc: 96.660 \n",
      "step: 3427 | Train: G_Loss: 1.105, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.558, D_sup_loss: 0.146, D_sup_acc: 96.70 Train acc: 96.443 Test acc: 96.750 \n",
      "step: 3428 | Train: G_Loss: 1.061, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.538, D_sup_loss: 0.140, D_sup_acc: 96.79 Train acc: 96.467 Test acc: 96.760 \n",
      "step: 3429 | Train: G_Loss: 1.127, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.625, D_sup_loss: 0.135, D_sup_acc: 96.80 Train acc: 96.270 Test acc: 96.540 \n",
      "step: 3430 | Train: G_Loss: 1.031, D_unsup_loss_real: 0.644, D_unsup_loss_fake: 0.661, D_sup_loss: 0.142, D_sup_acc: 96.58 Train acc: 96.272 Test acc: 96.620 \n",
      "step: 3431 | Train: G_Loss: 1.076, D_unsup_loss_real: 0.515, D_unsup_loss_fake: 0.694, D_sup_loss: 0.145, D_sup_acc: 96.66 Train acc: 96.353 Test acc: 96.680 \n",
      "step: 3432 | Train: G_Loss: 1.059, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.647, D_sup_loss: 0.142, D_sup_acc: 96.72 Train acc: 96.363 Test acc: 96.730 \n",
      "step: 3433 | Train: G_Loss: 1.157, D_unsup_loss_real: 0.667, D_unsup_loss_fake: 0.598, D_sup_loss: 0.148, D_sup_acc: 96.77 Train acc: 96.222 Test acc: 96.620 \n",
      "step: 3434 | Train: G_Loss: 1.077, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.629, D_sup_loss: 0.149, D_sup_acc: 96.66 Train acc: 96.402 Test acc: 96.690 \n",
      "step: 3435 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.546, D_unsup_loss_fake: 0.575, D_sup_loss: 0.141, D_sup_acc: 96.73 Train acc: 96.433 Test acc: 96.770 \n",
      "step: 3436 | Train: G_Loss: 1.153, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.565, D_sup_loss: 0.139, D_sup_acc: 96.81 Train acc: 96.420 Test acc: 96.680 \n",
      "step: 3437 | Train: G_Loss: 1.118, D_unsup_loss_real: 0.536, D_unsup_loss_fake: 0.648, D_sup_loss: 0.139, D_sup_acc: 96.72 Train acc: 96.300 Test acc: 96.520 \n",
      "step: 3438 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.692, D_sup_loss: 0.143, D_sup_acc: 96.56 Train acc: 96.062 Test acc: 96.410 \n",
      "step: 3439 | Train: G_Loss: 1.051, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.594, D_sup_loss: 0.159, D_sup_acc: 96.46 Train acc: 96.088 Test acc: 96.450 \n",
      "step: 3440 | Train: G_Loss: 1.170, D_unsup_loss_real: 0.660, D_unsup_loss_fake: 0.630, D_sup_loss: 0.158, D_sup_acc: 96.49 Train acc: 96.205 Test acc: 96.700 \n",
      "step: 3441 | Train: G_Loss: 1.166, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.593, D_sup_loss: 0.149, D_sup_acc: 96.74 Train acc: 96.065 Test acc: 96.430 \n",
      "step: 3442 | Train: G_Loss: 1.180, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.642, D_sup_loss: 0.158, D_sup_acc: 96.48 Train acc: 96.315 Test acc: 96.610 \n",
      "step: 3443 | Train: G_Loss: 1.117, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.534, D_sup_loss: 0.146, D_sup_acc: 96.65 Train acc: 96.470 Test acc: 96.770 \n",
      "step: 3444 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.575, D_sup_loss: 0.136, D_sup_acc: 96.81 Train acc: 96.538 Test acc: 96.830 \n",
      "step: 3445 | Train: G_Loss: 1.204, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.571, D_sup_loss: 0.134, D_sup_acc: 96.87 Train acc: 96.583 Test acc: 96.860 \n",
      "step: 3446 | Train: G_Loss: 1.122, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.592, D_sup_loss: 0.133, D_sup_acc: 96.90 Train acc: 96.497 Test acc: 96.890 \n",
      "step: 3447 | Train: G_Loss: 1.174, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.600, D_sup_loss: 0.135, D_sup_acc: 96.93 Train acc: 96.498 Test acc: 96.820 \n",
      "step: 3448 | Train: G_Loss: 1.120, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.675, D_sup_loss: 0.139, D_sup_acc: 96.86 Train acc: 96.335 Test acc: 96.770 \n",
      "step: 3449 | Train: G_Loss: 1.164, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.588, D_sup_loss: 0.147, D_sup_acc: 96.81 Train acc: 96.317 Test acc: 96.820 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3450 | Train: G_Loss: 1.096, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.563, D_sup_loss: 0.147, D_sup_acc: 96.86 Train acc: 96.273 Test acc: 96.710 \n",
      "step: 3451 | Train: G_Loss: 1.140, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.525, D_sup_loss: 0.150, D_sup_acc: 96.75 Train acc: 96.513 Test acc: 96.820 \n",
      "step: 3452 | Train: G_Loss: 1.218, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.611, D_sup_loss: 0.134, D_sup_acc: 96.86 Train acc: 96.425 Test acc: 96.820 \n",
      "step: 3453 | Train: G_Loss: 1.192, D_unsup_loss_real: 0.544, D_unsup_loss_fake: 0.610, D_sup_loss: 0.135, D_sup_acc: 96.86 Train acc: 96.515 Test acc: 96.870 \n",
      "step: 3454 | Train: G_Loss: 1.123, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.651, D_sup_loss: 0.132, D_sup_acc: 96.91 Train acc: 96.317 Test acc: 96.750 \n",
      "step: 3455 | Train: G_Loss: 1.103, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.555, D_sup_loss: 0.139, D_sup_acc: 96.79 Train acc: 96.340 Test acc: 96.920 \n",
      "step: 3456 | Train: G_Loss: 1.119, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.616, D_sup_loss: 0.138, D_sup_acc: 96.96 Train acc: 96.463 Test acc: 96.930 \n",
      "step: 3457 | Train: G_Loss: 1.204, D_unsup_loss_real: 0.628, D_unsup_loss_fake: 0.688, D_sup_loss: 0.134, D_sup_acc: 96.97 Train acc: 96.408 Test acc: 96.870 \n",
      "step: 3458 | Train: G_Loss: 1.234, D_unsup_loss_real: 0.628, D_unsup_loss_fake: 0.631, D_sup_loss: 0.140, D_sup_acc: 96.91 Train acc: 96.400 Test acc: 96.920 \n",
      "step: 3459 | Train: G_Loss: 1.088, D_unsup_loss_real: 0.656, D_unsup_loss_fake: 0.643, D_sup_loss: 0.140, D_sup_acc: 96.96 Train acc: 96.455 Test acc: 96.880 \n",
      "step: 3460 | Train: G_Loss: 1.113, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.594, D_sup_loss: 0.138, D_sup_acc: 96.92 Train acc: 96.495 Test acc: 96.980 \n",
      "step: 3461 | Train: G_Loss: 1.133, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.581, D_sup_loss: 0.137, D_sup_acc: 97.02 Train acc: 96.497 Test acc: 96.940 \n",
      "step: 3462 | Train: G_Loss: 1.155, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.602, D_sup_loss: 0.139, D_sup_acc: 96.98 Train acc: 96.450 Test acc: 96.810 \n",
      "step: 3463 | Train: G_Loss: 1.161, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.639, D_sup_loss: 0.139, D_sup_acc: 96.85 Train acc: 96.493 Test acc: 96.900 \n",
      "step: 3464 | Train: G_Loss: 1.220, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.521, D_sup_loss: 0.136, D_sup_acc: 96.94 Train acc: 96.463 Test acc: 96.750 \n",
      "step: 3465 | Train: G_Loss: 1.072, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.646, D_sup_loss: 0.136, D_sup_acc: 96.79 Train acc: 96.378 Test acc: 96.720 \n",
      "step: 3466 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.551, D_sup_loss: 0.146, D_sup_acc: 96.76 Train acc: 96.503 Test acc: 96.850 \n",
      "step: 3467 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.694, D_unsup_loss_fake: 0.648, D_sup_loss: 0.138, D_sup_acc: 96.89 Train acc: 96.302 Test acc: 96.650 \n",
      "step: 3468 | Train: G_Loss: 1.175, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.530, D_sup_loss: 0.144, D_sup_acc: 96.69 Train acc: 96.487 Test acc: 96.930 \n",
      "step: 3469 | Train: G_Loss: 1.117, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.594, D_sup_loss: 0.136, D_sup_acc: 96.97 Train acc: 96.413 Test acc: 96.730 \n",
      "step: 3470 | Train: G_Loss: 1.152, D_unsup_loss_real: 0.670, D_unsup_loss_fake: 0.628, D_sup_loss: 0.137, D_sup_acc: 96.77 Train acc: 96.257 Test acc: 96.710 \n",
      "step: 3471 | Train: G_Loss: 1.087, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.644, D_sup_loss: 0.147, D_sup_acc: 96.75 Train acc: 96.180 Test acc: 96.730 \n",
      "step: 3472 | Train: G_Loss: 1.027, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.606, D_sup_loss: 0.147, D_sup_acc: 96.77 Train acc: 96.337 Test acc: 96.810 \n",
      "step: 3473 | Train: G_Loss: 1.039, D_unsup_loss_real: 0.652, D_unsup_loss_fake: 0.592, D_sup_loss: 0.141, D_sup_acc: 96.85 Train acc: 96.242 Test acc: 96.820 \n",
      "step: 3474 | Train: G_Loss: 1.092, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.557, D_sup_loss: 0.146, D_sup_acc: 96.86 Train acc: 96.343 Test acc: 96.800 \n",
      "step: 3475 | Train: G_Loss: 1.044, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.643, D_sup_loss: 0.139, D_sup_acc: 96.84 Train acc: 96.273 Test acc: 96.810 \n",
      "step: 3476 | Train: G_Loss: 1.122, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.621, D_sup_loss: 0.146, D_sup_acc: 96.85 Train acc: 96.168 Test acc: 96.700 \n",
      "step: 3477 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.588, D_sup_loss: 0.151, D_sup_acc: 96.74 Train acc: 96.388 Test acc: 96.810 \n",
      "step: 3478 | Train: G_Loss: 1.109, D_unsup_loss_real: 0.534, D_unsup_loss_fake: 0.611, D_sup_loss: 0.141, D_sup_acc: 96.85 Train acc: 96.463 Test acc: 96.850 \n",
      "step: 3479 | Train: G_Loss: 1.079, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.688, D_sup_loss: 0.136, D_sup_acc: 96.89 Train acc: 96.383 Test acc: 96.800 \n",
      "step: 3480 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.644, D_unsup_loss_fake: 0.554, D_sup_loss: 0.142, D_sup_acc: 96.84 Train acc: 96.338 Test acc: 96.750 \n",
      "step: 3481 | Train: G_Loss: 1.173, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.604, D_sup_loss: 0.144, D_sup_acc: 96.79 Train acc: 96.217 Test acc: 96.710 \n",
      "step: 3482 | Train: G_Loss: 0.975, D_unsup_loss_real: 0.663, D_unsup_loss_fake: 0.618, D_sup_loss: 0.148, D_sup_acc: 96.75 Train acc: 96.440 Test acc: 96.970 \n",
      "step: 3483 | Train: G_Loss: 1.120, D_unsup_loss_real: 0.524, D_unsup_loss_fake: 0.635, D_sup_loss: 0.136, D_sup_acc: 97.01 Train acc: 96.492 Test acc: 97.000 \n",
      "step: 3484 | Train: G_Loss: 1.062, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.654, D_sup_loss: 0.133, D_sup_acc: 97.04 Train acc: 96.260 Test acc: 96.700 \n",
      "step: 3485 | Train: G_Loss: 1.137, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.604, D_sup_loss: 0.147, D_sup_acc: 96.74 Train acc: 96.245 Test acc: 96.730 \n",
      "step: 3486 | Train: G_Loss: 1.155, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.597, D_sup_loss: 0.147, D_sup_acc: 96.77 Train acc: 96.292 Test acc: 96.870 \n",
      "step: 3487 | Train: G_Loss: 1.099, D_unsup_loss_real: 0.639, D_unsup_loss_fake: 0.653, D_sup_loss: 0.142, D_sup_acc: 96.91 Train acc: 96.175 Test acc: 96.740 \n",
      "step: 3488 | Train: G_Loss: 1.118, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.572, D_sup_loss: 0.151, D_sup_acc: 96.78 Train acc: 96.423 Test acc: 96.880 \n",
      "step: 3489 | Train: G_Loss: 1.005, D_unsup_loss_real: 0.679, D_unsup_loss_fake: 0.651, D_sup_loss: 0.139, D_sup_acc: 96.92 Train acc: 96.147 Test acc: 96.700 \n",
      "step: 3490 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.536, D_unsup_loss_fake: 0.562, D_sup_loss: 0.153, D_sup_acc: 96.74 Train acc: 96.258 Test acc: 96.720 \n",
      "step: 3491 | Train: G_Loss: 1.056, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.568, D_sup_loss: 0.144, D_sup_acc: 96.76 Train acc: 96.337 Test acc: 96.810 \n",
      "step: 3492 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.591, D_sup_loss: 0.142, D_sup_acc: 96.85 Train acc: 96.437 Test acc: 96.840 \n",
      "step: 3493 | Train: G_Loss: 1.045, D_unsup_loss_real: 0.642, D_unsup_loss_fake: 0.668, D_sup_loss: 0.141, D_sup_acc: 96.88 Train acc: 96.128 Test acc: 96.660 \n",
      "step: 3494 | Train: G_Loss: 1.087, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.649, D_sup_loss: 0.152, D_sup_acc: 96.70 Train acc: 96.358 Test acc: 96.830 \n",
      "step: 3495 | Train: G_Loss: 1.130, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.578, D_sup_loss: 0.143, D_sup_acc: 96.87 Train acc: 96.218 Test acc: 96.770 \n",
      "step: 3496 | Train: G_Loss: 1.108, D_unsup_loss_real: 0.660, D_unsup_loss_fake: 0.585, D_sup_loss: 0.151, D_sup_acc: 96.81 Train acc: 96.497 Test acc: 96.760 \n",
      "step: 3497 | Train: G_Loss: 1.046, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.596, D_sup_loss: 0.137, D_sup_acc: 96.80 Train acc: 96.383 Test acc: 96.700 \n",
      "step: 3498 | Train: G_Loss: 1.049, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.629, D_sup_loss: 0.141, D_sup_acc: 96.74 Train acc: 96.395 Test acc: 96.740 \n",
      "step: 3499 | Train: G_Loss: 1.072, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.671, D_sup_loss: 0.143, D_sup_acc: 96.78 Train acc: 96.515 Test acc: 96.780 \n",
      "step: 3500 | Train: G_Loss: 1.119, D_unsup_loss_real: 0.628, D_unsup_loss_fake: 0.554, D_sup_loss: 0.141, D_sup_acc: 96.82 Train acc: 96.470 Test acc: 96.810 \n",
      "Train Classifier Accuracy: 96.470%\n",
      "\n",
      "Test Classifier Accuracy: 96.810%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_3500.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_3500.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3501 | Train: G_Loss: 1.084, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.596, D_sup_loss: 0.139, D_sup_acc: 96.85 Train acc: 96.507 Test acc: 96.870 \n",
      "step: 3502 | Train: G_Loss: 1.146, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.572, D_sup_loss: 0.138, D_sup_acc: 96.91 Train acc: 96.512 Test acc: 96.800 \n",
      "step: 3503 | Train: G_Loss: 1.077, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.647, D_sup_loss: 0.139, D_sup_acc: 96.84 Train acc: 96.590 Test acc: 96.880 \n",
      "step: 3504 | Train: G_Loss: 1.073, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.568, D_sup_loss: 0.139, D_sup_acc: 96.92 Train acc: 96.493 Test acc: 96.700 \n",
      "step: 3505 | Train: G_Loss: 1.119, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.620, D_sup_loss: 0.141, D_sup_acc: 96.74 Train acc: 96.433 Test acc: 96.820 \n",
      "step: 3506 | Train: G_Loss: 1.125, D_unsup_loss_real: 0.663, D_unsup_loss_fake: 0.683, D_sup_loss: 0.146, D_sup_acc: 96.86 Train acc: 96.415 Test acc: 96.810 \n",
      "step: 3507 | Train: G_Loss: 1.149, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.562, D_sup_loss: 0.149, D_sup_acc: 96.85 Train acc: 96.318 Test acc: 96.820 \n",
      "step: 3508 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.535, D_unsup_loss_fake: 0.543, D_sup_loss: 0.152, D_sup_acc: 96.86 Train acc: 96.430 Test acc: 96.820 \n",
      "step: 3509 | Train: G_Loss: 1.006, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.560, D_sup_loss: 0.144, D_sup_acc: 96.86 Train acc: 96.457 Test acc: 96.760 \n",
      "step: 3510 | Train: G_Loss: 1.150, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.614, D_sup_loss: 0.144, D_sup_acc: 96.80 Train acc: 96.483 Test acc: 96.760 \n",
      "step: 3511 | Train: G_Loss: 1.065, D_unsup_loss_real: 0.671, D_unsup_loss_fake: 0.651, D_sup_loss: 0.142, D_sup_acc: 96.80 Train acc: 96.543 Test acc: 96.900 \n",
      "step: 3512 | Train: G_Loss: 1.062, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.599, D_sup_loss: 0.141, D_sup_acc: 96.94 Train acc: 96.568 Test acc: 96.950 \n",
      "step: 3513 | Train: G_Loss: 1.117, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.605, D_sup_loss: 0.141, D_sup_acc: 96.99 Train acc: 96.597 Test acc: 96.980 \n",
      "step: 3514 | Train: G_Loss: 1.118, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.641, D_sup_loss: 0.139, D_sup_acc: 97.02 Train acc: 96.203 Test acc: 96.700 \n",
      "step: 3515 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.522, D_sup_loss: 0.153, D_sup_acc: 96.74 Train acc: 96.498 Test acc: 96.870 \n",
      "step: 3516 | Train: G_Loss: 1.064, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.542, D_sup_loss: 0.140, D_sup_acc: 96.91 Train acc: 96.530 Test acc: 96.890 \n",
      "step: 3517 | Train: G_Loss: 1.136, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.576, D_sup_loss: 0.137, D_sup_acc: 96.93 Train acc: 96.502 Test acc: 96.820 \n",
      "step: 3518 | Train: G_Loss: 1.021, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.589, D_sup_loss: 0.135, D_sup_acc: 96.86 Train acc: 96.527 Test acc: 96.920 \n",
      "step: 3519 | Train: G_Loss: 1.057, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.621, D_sup_loss: 0.136, D_sup_acc: 96.96 Train acc: 96.343 Test acc: 96.700 \n",
      "step: 3520 | Train: G_Loss: 1.125, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.580, D_sup_loss: 0.143, D_sup_acc: 96.74 Train acc: 96.332 Test acc: 96.900 \n",
      "step: 3521 | Train: G_Loss: 1.129, D_unsup_loss_real: 0.661, D_unsup_loss_fake: 0.590, D_sup_loss: 0.143, D_sup_acc: 96.94 Train acc: 96.482 Test acc: 96.830 \n",
      "step: 3522 | Train: G_Loss: 1.079, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.642, D_sup_loss: 0.137, D_sup_acc: 96.87 Train acc: 96.490 Test acc: 96.860 \n",
      "step: 3523 | Train: G_Loss: 1.079, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.598, D_sup_loss: 0.137, D_sup_acc: 96.90 Train acc: 96.242 Test acc: 96.780 \n",
      "step: 3524 | Train: G_Loss: 1.175, D_unsup_loss_real: 0.648, D_unsup_loss_fake: 0.625, D_sup_loss: 0.146, D_sup_acc: 96.82 Train acc: 96.453 Test acc: 96.890 \n",
      "step: 3525 | Train: G_Loss: 1.045, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.606, D_sup_loss: 0.138, D_sup_acc: 96.93 Train acc: 96.352 Test acc: 96.820 \n",
      "step: 3526 | Train: G_Loss: 1.061, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.628, D_sup_loss: 0.143, D_sup_acc: 96.86 Train acc: 96.590 Test acc: 96.890 \n",
      "step: 3527 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.683, D_sup_loss: 0.136, D_sup_acc: 96.93 Train acc: 96.415 Test acc: 96.790 \n",
      "step: 3528 | Train: G_Loss: 1.092, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.609, D_sup_loss: 0.143, D_sup_acc: 96.83 Train acc: 96.465 Test acc: 96.920 \n",
      "step: 3529 | Train: G_Loss: 1.118, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.629, D_sup_loss: 0.141, D_sup_acc: 96.96 Train acc: 96.440 Test acc: 96.840 \n",
      "step: 3530 | Train: G_Loss: 1.087, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.624, D_sup_loss: 0.139, D_sup_acc: 96.88 Train acc: 96.340 Test acc: 96.860 \n",
      "step: 3531 | Train: G_Loss: 1.016, D_unsup_loss_real: 0.654, D_unsup_loss_fake: 0.571, D_sup_loss: 0.144, D_sup_acc: 96.90 Train acc: 96.460 Test acc: 96.880 \n",
      "step: 3532 | Train: G_Loss: 1.088, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.576, D_sup_loss: 0.141, D_sup_acc: 96.92 Train acc: 96.412 Test acc: 96.800 \n",
      "step: 3533 | Train: G_Loss: 1.225, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.612, D_sup_loss: 0.143, D_sup_acc: 96.84 Train acc: 96.418 Test acc: 96.820 \n",
      "step: 3534 | Train: G_Loss: 1.026, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.592, D_sup_loss: 0.140, D_sup_acc: 96.86 Train acc: 96.357 Test acc: 96.750 \n",
      "step: 3535 | Train: G_Loss: 1.079, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.653, D_sup_loss: 0.141, D_sup_acc: 96.79 Train acc: 96.367 Test acc: 96.760 \n",
      "step: 3536 | Train: G_Loss: 1.155, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.633, D_sup_loss: 0.144, D_sup_acc: 96.80 Train acc: 96.338 Test acc: 96.770 \n",
      "step: 3537 | Train: G_Loss: 1.162, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.581, D_sup_loss: 0.147, D_sup_acc: 96.81 Train acc: 96.517 Test acc: 96.920 \n",
      "step: 3538 | Train: G_Loss: 1.149, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.627, D_sup_loss: 0.141, D_sup_acc: 96.96 Train acc: 96.548 Test acc: 96.920 \n",
      "step: 3539 | Train: G_Loss: 1.128, D_unsup_loss_real: 0.656, D_unsup_loss_fake: 0.533, D_sup_loss: 0.134, D_sup_acc: 96.96 Train acc: 96.585 Test acc: 96.940 \n",
      "step: 3540 | Train: G_Loss: 1.158, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.600, D_sup_loss: 0.131, D_sup_acc: 96.98 Train acc: 96.518 Test acc: 96.890 \n",
      "step: 3541 | Train: G_Loss: 1.056, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.550, D_sup_loss: 0.135, D_sup_acc: 96.93 Train acc: 96.543 Test acc: 96.900 \n",
      "step: 3542 | Train: G_Loss: 1.130, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.702, D_sup_loss: 0.136, D_sup_acc: 96.94 Train acc: 96.465 Test acc: 96.820 \n",
      "step: 3543 | Train: G_Loss: 1.189, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.668, D_sup_loss: 0.138, D_sup_acc: 96.86 Train acc: 96.382 Test acc: 96.790 \n",
      "step: 3544 | Train: G_Loss: 1.161, D_unsup_loss_real: 0.631, D_unsup_loss_fake: 0.562, D_sup_loss: 0.140, D_sup_acc: 96.83 Train acc: 96.287 Test acc: 96.740 \n",
      "step: 3545 | Train: G_Loss: 1.079, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.627, D_sup_loss: 0.147, D_sup_acc: 96.78 Train acc: 96.407 Test acc: 96.820 \n",
      "step: 3546 | Train: G_Loss: 1.077, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.580, D_sup_loss: 0.141, D_sup_acc: 96.86 Train acc: 96.437 Test acc: 96.830 \n",
      "step: 3547 | Train: G_Loss: 1.109, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.605, D_sup_loss: 0.139, D_sup_acc: 96.87 Train acc: 96.428 Test acc: 96.730 \n",
      "step: 3548 | Train: G_Loss: 1.016, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.594, D_sup_loss: 0.140, D_sup_acc: 96.77 Train acc: 96.392 Test acc: 96.770 \n",
      "step: 3549 | Train: G_Loss: 1.065, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.601, D_sup_loss: 0.140, D_sup_acc: 96.81 Train acc: 96.612 Test acc: 96.940 \n",
      "step: 3550 | Train: G_Loss: 1.061, D_unsup_loss_real: 0.546, D_unsup_loss_fake: 0.561, D_sup_loss: 0.131, D_sup_acc: 96.98 Train acc: 96.542 Test acc: 97.000 \n",
      "step: 3551 | Train: G_Loss: 1.177, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.524, D_sup_loss: 0.133, D_sup_acc: 97.04 Train acc: 96.603 Test acc: 96.970 \n",
      "step: 3552 | Train: G_Loss: 1.157, D_unsup_loss_real: 0.530, D_unsup_loss_fake: 0.614, D_sup_loss: 0.128, D_sup_acc: 97.01 Train acc: 96.597 Test acc: 96.970 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3553 | Train: G_Loss: 0.987, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.565, D_sup_loss: 0.131, D_sup_acc: 97.01 Train acc: 96.648 Test acc: 96.930 \n",
      "step: 3554 | Train: G_Loss: 1.155, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.683, D_sup_loss: 0.130, D_sup_acc: 96.97 Train acc: 96.578 Test acc: 96.910 \n",
      "step: 3555 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.532, D_sup_loss: 0.133, D_sup_acc: 96.95 Train acc: 96.580 Test acc: 96.840 \n",
      "step: 3556 | Train: G_Loss: 1.130, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.564, D_sup_loss: 0.134, D_sup_acc: 96.88 Train acc: 96.602 Test acc: 96.880 \n",
      "step: 3557 | Train: G_Loss: 1.141, D_unsup_loss_real: 0.541, D_unsup_loss_fake: 0.606, D_sup_loss: 0.134, D_sup_acc: 96.92 Train acc: 96.673 Test acc: 96.950 \n",
      "step: 3558 | Train: G_Loss: 1.143, D_unsup_loss_real: 0.525, D_unsup_loss_fake: 0.601, D_sup_loss: 0.130, D_sup_acc: 96.99 Train acc: 96.653 Test acc: 96.920 \n",
      "step: 3559 | Train: G_Loss: 1.201, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.565, D_sup_loss: 0.128, D_sup_acc: 96.96 Train acc: 96.695 Test acc: 96.910 \n",
      "step: 3560 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.646, D_sup_loss: 0.127, D_sup_acc: 96.95 Train acc: 96.513 Test acc: 96.930 \n",
      "step: 3561 | Train: G_Loss: 1.127, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.631, D_sup_loss: 0.134, D_sup_acc: 96.97 Train acc: 96.473 Test acc: 96.950 \n",
      "step: 3562 | Train: G_Loss: 1.192, D_unsup_loss_real: 0.546, D_unsup_loss_fake: 0.666, D_sup_loss: 0.135, D_sup_acc: 96.99 Train acc: 96.605 Test acc: 96.990 \n",
      "step: 3563 | Train: G_Loss: 1.169, D_unsup_loss_real: 0.663, D_unsup_loss_fake: 0.579, D_sup_loss: 0.128, D_sup_acc: 97.03 Train acc: 96.575 Test acc: 96.980 \n",
      "step: 3564 | Train: G_Loss: 1.107, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.579, D_sup_loss: 0.133, D_sup_acc: 97.02 Train acc: 96.562 Test acc: 96.920 \n",
      "step: 3565 | Train: G_Loss: 1.088, D_unsup_loss_real: 0.665, D_unsup_loss_fake: 0.567, D_sup_loss: 0.133, D_sup_acc: 96.96 Train acc: 96.490 Test acc: 96.850 \n",
      "step: 3566 | Train: G_Loss: 1.185, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.532, D_sup_loss: 0.137, D_sup_acc: 96.89 Train acc: 96.513 Test acc: 96.890 \n",
      "step: 3567 | Train: G_Loss: 1.129, D_unsup_loss_real: 0.653, D_unsup_loss_fake: 0.625, D_sup_loss: 0.134, D_sup_acc: 96.93 Train acc: 96.580 Test acc: 96.950 \n",
      "step: 3568 | Train: G_Loss: 1.089, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.656, D_sup_loss: 0.135, D_sup_acc: 96.99 Train acc: 96.558 Test acc: 96.980 \n",
      "step: 3569 | Train: G_Loss: 1.042, D_unsup_loss_real: 0.659, D_unsup_loss_fake: 0.540, D_sup_loss: 0.136, D_sup_acc: 97.02 Train acc: 96.593 Test acc: 96.870 \n",
      "step: 3570 | Train: G_Loss: 1.096, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.786, D_sup_loss: 0.136, D_sup_acc: 96.91 Train acc: 96.583 Test acc: 96.940 \n",
      "step: 3571 | Train: G_Loss: 1.063, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.601, D_sup_loss: 0.137, D_sup_acc: 96.98 Train acc: 96.558 Test acc: 96.950 \n",
      "step: 3572 | Train: G_Loss: 1.167, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.599, D_sup_loss: 0.137, D_sup_acc: 96.99 Train acc: 96.408 Test acc: 96.720 \n",
      "step: 3573 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.617, D_sup_loss: 0.147, D_sup_acc: 96.76 Train acc: 96.503 Test acc: 96.780 \n",
      "step: 3574 | Train: G_Loss: 1.077, D_unsup_loss_real: 0.663, D_unsup_loss_fake: 0.578, D_sup_loss: 0.143, D_sup_acc: 96.82 Train acc: 96.622 Test acc: 96.910 \n",
      "step: 3575 | Train: G_Loss: 1.019, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.620, D_sup_loss: 0.136, D_sup_acc: 96.95 Train acc: 96.607 Test acc: 96.940 \n",
      "step: 3576 | Train: G_Loss: 1.193, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.650, D_sup_loss: 0.139, D_sup_acc: 96.98 Train acc: 96.527 Test acc: 96.870 \n",
      "step: 3577 | Train: G_Loss: 1.087, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.633, D_sup_loss: 0.141, D_sup_acc: 96.91 Train acc: 96.597 Test acc: 96.960 \n",
      "step: 3578 | Train: G_Loss: 1.041, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.551, D_sup_loss: 0.138, D_sup_acc: 97.00 Train acc: 96.545 Test acc: 96.810 \n",
      "step: 3579 | Train: G_Loss: 1.063, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.640, D_sup_loss: 0.140, D_sup_acc: 96.85 Train acc: 96.537 Test acc: 96.910 \n",
      "step: 3580 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.457, D_unsup_loss_fake: 0.580, D_sup_loss: 0.138, D_sup_acc: 96.95 Train acc: 96.595 Test acc: 96.910 \n",
      "step: 3581 | Train: G_Loss: 1.033, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.508, D_sup_loss: 0.132, D_sup_acc: 96.95 Train acc: 96.632 Test acc: 96.990 \n",
      "step: 3582 | Train: G_Loss: 1.133, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.632, D_sup_loss: 0.130, D_sup_acc: 97.03 Train acc: 96.482 Test acc: 96.920 \n",
      "step: 3583 | Train: G_Loss: 1.110, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.686, D_sup_loss: 0.135, D_sup_acc: 96.96 Train acc: 96.377 Test acc: 96.810 \n",
      "step: 3584 | Train: G_Loss: 1.109, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.612, D_sup_loss: 0.142, D_sup_acc: 96.85 Train acc: 96.455 Test acc: 96.890 \n",
      "step: 3585 | Train: G_Loss: 1.224, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.644, D_sup_loss: 0.141, D_sup_acc: 96.93 Train acc: 96.382 Test acc: 96.900 \n",
      "step: 3586 | Train: G_Loss: 1.185, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.640, D_sup_loss: 0.142, D_sup_acc: 96.94 Train acc: 96.583 Test acc: 96.870 \n",
      "step: 3587 | Train: G_Loss: 1.080, D_unsup_loss_real: 0.642, D_unsup_loss_fake: 0.591, D_sup_loss: 0.134, D_sup_acc: 96.91 Train acc: 96.517 Test acc: 96.940 \n",
      "step: 3588 | Train: G_Loss: 1.107, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.602, D_sup_loss: 0.134, D_sup_acc: 96.98 Train acc: 96.628 Test acc: 96.940 \n",
      "step: 3589 | Train: G_Loss: 1.153, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.592, D_sup_loss: 0.129, D_sup_acc: 96.98 Train acc: 96.462 Test acc: 96.890 \n",
      "step: 3590 | Train: G_Loss: 1.034, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.652, D_sup_loss: 0.136, D_sup_acc: 96.93 Train acc: 96.257 Test acc: 96.820 \n",
      "step: 3591 | Train: G_Loss: 1.193, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.662, D_sup_loss: 0.147, D_sup_acc: 96.86 Train acc: 96.635 Test acc: 96.890 \n",
      "step: 3592 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.619, D_sup_loss: 0.132, D_sup_acc: 96.93 Train acc: 96.257 Test acc: 96.740 \n",
      "step: 3593 | Train: G_Loss: 1.109, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.612, D_sup_loss: 0.149, D_sup_acc: 96.78 Train acc: 96.423 Test acc: 96.750 \n",
      "step: 3594 | Train: G_Loss: 1.090, D_unsup_loss_real: 0.664, D_unsup_loss_fake: 0.577, D_sup_loss: 0.140, D_sup_acc: 96.79 Train acc: 96.395 Test acc: 96.810 \n",
      "step: 3595 | Train: G_Loss: 1.066, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.570, D_sup_loss: 0.142, D_sup_acc: 96.85 Train acc: 96.448 Test acc: 96.790 \n",
      "step: 3596 | Train: G_Loss: 1.162, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.629, D_sup_loss: 0.140, D_sup_acc: 96.83 Train acc: 96.347 Test acc: 96.780 \n",
      "step: 3597 | Train: G_Loss: 1.184, D_unsup_loss_real: 0.631, D_unsup_loss_fake: 0.571, D_sup_loss: 0.144, D_sup_acc: 96.82 Train acc: 96.493 Test acc: 96.940 \n",
      "step: 3598 | Train: G_Loss: 1.129, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.622, D_sup_loss: 0.135, D_sup_acc: 96.98 Train acc: 96.360 Test acc: 96.820 \n",
      "step: 3599 | Train: G_Loss: 1.071, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.562, D_sup_loss: 0.140, D_sup_acc: 96.86 Train acc: 96.430 Test acc: 96.900 \n",
      "step: 3600 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.659, D_sup_loss: 0.135, D_sup_acc: 96.94 Train acc: 96.427 Test acc: 96.880 \n",
      "Train Classifier Accuracy: 96.427%\n",
      "\n",
      "Test Classifier Accuracy: 96.880%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_3600.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_3600.h5\n",
      "step: 3601 | Train: G_Loss: 1.169, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.546, D_sup_loss: 0.135, D_sup_acc: 96.92 Train acc: 96.138 Test acc: 96.680 \n",
      "step: 3602 | Train: G_Loss: 1.011, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.594, D_sup_loss: 0.147, D_sup_acc: 96.72 Train acc: 96.225 Test acc: 96.730 \n",
      "step: 3603 | Train: G_Loss: 1.080, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.607, D_sup_loss: 0.143, D_sup_acc: 96.77 Train acc: 96.488 Test acc: 96.940 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3604 | Train: G_Loss: 1.125, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.544, D_sup_loss: 0.135, D_sup_acc: 96.98 Train acc: 96.623 Test acc: 97.040 \n",
      "step: 3605 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.525, D_unsup_loss_fake: 0.609, D_sup_loss: 0.129, D_sup_acc: 97.08 Train acc: 96.592 Test acc: 97.030 \n",
      "step: 3606 | Train: G_Loss: 1.164, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.651, D_sup_loss: 0.129, D_sup_acc: 97.07 Train acc: 96.563 Test acc: 96.910 \n",
      "step: 3607 | Train: G_Loss: 1.133, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.611, D_sup_loss: 0.130, D_sup_acc: 96.95 Train acc: 96.537 Test acc: 96.910 \n",
      "step: 3608 | Train: G_Loss: 1.208, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.577, D_sup_loss: 0.134, D_sup_acc: 96.95 Train acc: 96.487 Test acc: 96.800 \n",
      "step: 3609 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.548, D_sup_loss: 0.136, D_sup_acc: 96.84 Train acc: 96.490 Test acc: 96.900 \n",
      "step: 3610 | Train: G_Loss: 1.140, D_unsup_loss_real: 0.642, D_unsup_loss_fake: 0.644, D_sup_loss: 0.136, D_sup_acc: 96.94 Train acc: 96.468 Test acc: 96.780 \n",
      "step: 3611 | Train: G_Loss: 1.203, D_unsup_loss_real: 0.663, D_unsup_loss_fake: 0.645, D_sup_loss: 0.141, D_sup_acc: 96.82 Train acc: 96.517 Test acc: 96.860 \n",
      "step: 3612 | Train: G_Loss: 1.094, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.575, D_sup_loss: 0.141, D_sup_acc: 96.90 Train acc: 96.450 Test acc: 96.710 \n",
      "step: 3613 | Train: G_Loss: 1.187, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.594, D_sup_loss: 0.145, D_sup_acc: 96.75 Train acc: 96.607 Test acc: 96.750 \n",
      "step: 3614 | Train: G_Loss: 1.203, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.648, D_sup_loss: 0.139, D_sup_acc: 96.79 Train acc: 96.558 Test acc: 96.710 \n",
      "step: 3615 | Train: G_Loss: 1.127, D_unsup_loss_real: 0.639, D_unsup_loss_fake: 0.606, D_sup_loss: 0.140, D_sup_acc: 96.75 Train acc: 96.568 Test acc: 96.760 \n",
      "step: 3616 | Train: G_Loss: 1.128, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.607, D_sup_loss: 0.141, D_sup_acc: 96.80 Train acc: 96.472 Test acc: 96.690 \n",
      "step: 3617 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.620, D_sup_loss: 0.145, D_sup_acc: 96.73 Train acc: 96.495 Test acc: 96.730 \n",
      "step: 3618 | Train: G_Loss: 1.164, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.582, D_sup_loss: 0.145, D_sup_acc: 96.77 Train acc: 96.373 Test acc: 96.620 \n",
      "step: 3619 | Train: G_Loss: 1.127, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.567, D_sup_loss: 0.148, D_sup_acc: 96.66 Train acc: 96.527 Test acc: 96.750 \n",
      "step: 3620 | Train: G_Loss: 1.053, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.607, D_sup_loss: 0.140, D_sup_acc: 96.79 Train acc: 96.555 Test acc: 96.820 \n",
      "step: 3621 | Train: G_Loss: 1.089, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.609, D_sup_loss: 0.139, D_sup_acc: 96.86 Train acc: 96.320 Test acc: 96.760 \n",
      "step: 3622 | Train: G_Loss: 1.078, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.663, D_sup_loss: 0.147, D_sup_acc: 96.80 Train acc: 96.612 Test acc: 96.790 \n",
      "step: 3623 | Train: G_Loss: 1.221, D_unsup_loss_real: 0.535, D_unsup_loss_fake: 0.591, D_sup_loss: 0.134, D_sup_acc: 96.83 Train acc: 96.647 Test acc: 96.950 \n",
      "step: 3624 | Train: G_Loss: 1.106, D_unsup_loss_real: 0.628, D_unsup_loss_fake: 0.529, D_sup_loss: 0.132, D_sup_acc: 96.99 Train acc: 96.727 Test acc: 96.970 \n",
      "step: 3625 | Train: G_Loss: 1.136, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.617, D_sup_loss: 0.129, D_sup_acc: 97.01 Train acc: 96.520 Test acc: 96.890 \n",
      "step: 3626 | Train: G_Loss: 1.140, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.647, D_sup_loss: 0.136, D_sup_acc: 96.93 Train acc: 96.437 Test acc: 96.820 \n",
      "step: 3627 | Train: G_Loss: 1.079, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.663, D_sup_loss: 0.138, D_sup_acc: 96.86 Train acc: 96.628 Test acc: 97.020 \n",
      "step: 3628 | Train: G_Loss: 1.173, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.548, D_sup_loss: 0.131, D_sup_acc: 97.06 Train acc: 96.577 Test acc: 96.910 \n",
      "step: 3629 | Train: G_Loss: 1.210, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.556, D_sup_loss: 0.133, D_sup_acc: 96.95 Train acc: 96.498 Test acc: 96.840 \n",
      "step: 3630 | Train: G_Loss: 1.178, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.594, D_sup_loss: 0.135, D_sup_acc: 96.88 Train acc: 96.535 Test acc: 96.840 \n",
      "step: 3631 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.663, D_unsup_loss_fake: 0.604, D_sup_loss: 0.136, D_sup_acc: 96.88 Train acc: 96.535 Test acc: 96.890 \n",
      "step: 3632 | Train: G_Loss: 1.106, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.649, D_sup_loss: 0.137, D_sup_acc: 96.93 Train acc: 96.600 Test acc: 96.980 \n",
      "step: 3633 | Train: G_Loss: 1.132, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.523, D_sup_loss: 0.135, D_sup_acc: 97.02 Train acc: 96.593 Test acc: 96.910 \n",
      "step: 3634 | Train: G_Loss: 1.222, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.683, D_sup_loss: 0.132, D_sup_acc: 96.95 Train acc: 96.665 Test acc: 96.990 \n",
      "step: 3635 | Train: G_Loss: 1.025, D_unsup_loss_real: 0.678, D_unsup_loss_fake: 0.554, D_sup_loss: 0.132, D_sup_acc: 97.03 Train acc: 96.580 Test acc: 96.860 \n",
      "step: 3636 | Train: G_Loss: 1.039, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.680, D_sup_loss: 0.134, D_sup_acc: 96.90 Train acc: 96.392 Test acc: 96.770 \n",
      "step: 3637 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.652, D_sup_loss: 0.145, D_sup_acc: 96.81 Train acc: 96.635 Test acc: 96.910 \n",
      "step: 3638 | Train: G_Loss: 1.191, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.546, D_sup_loss: 0.135, D_sup_acc: 96.95 Train acc: 96.388 Test acc: 96.810 \n",
      "step: 3639 | Train: G_Loss: 1.183, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.623, D_sup_loss: 0.145, D_sup_acc: 96.85 Train acc: 96.703 Test acc: 96.940 \n",
      "step: 3640 | Train: G_Loss: 1.195, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.589, D_sup_loss: 0.128, D_sup_acc: 96.98 Train acc: 96.738 Test acc: 97.030 \n",
      "step: 3641 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.601, D_sup_loss: 0.130, D_sup_acc: 97.07 Train acc: 96.738 Test acc: 97.020 \n",
      "step: 3642 | Train: G_Loss: 1.168, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.600, D_sup_loss: 0.131, D_sup_acc: 97.06 Train acc: 96.663 Test acc: 96.890 \n",
      "step: 3643 | Train: G_Loss: 1.136, D_unsup_loss_real: 0.651, D_unsup_loss_fake: 0.648, D_sup_loss: 0.136, D_sup_acc: 96.93 Train acc: 96.593 Test acc: 96.830 \n",
      "step: 3644 | Train: G_Loss: 1.106, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.623, D_sup_loss: 0.136, D_sup_acc: 96.87 Train acc: 96.507 Test acc: 96.840 \n",
      "step: 3645 | Train: G_Loss: 1.108, D_unsup_loss_real: 0.541, D_unsup_loss_fake: 0.550, D_sup_loss: 0.139, D_sup_acc: 96.88 Train acc: 96.607 Test acc: 96.910 \n",
      "step: 3646 | Train: G_Loss: 1.168, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.619, D_sup_loss: 0.133, D_sup_acc: 96.95 Train acc: 96.453 Test acc: 96.690 \n",
      "step: 3647 | Train: G_Loss: 1.133, D_unsup_loss_real: 0.695, D_unsup_loss_fake: 0.595, D_sup_loss: 0.140, D_sup_acc: 96.73 Train acc: 96.480 Test acc: 96.770 \n",
      "step: 3648 | Train: G_Loss: 1.096, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.599, D_sup_loss: 0.141, D_sup_acc: 96.81 Train acc: 96.380 Test acc: 96.820 \n",
      "step: 3649 | Train: G_Loss: 1.116, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.644, D_sup_loss: 0.146, D_sup_acc: 96.86 Train acc: 96.083 Test acc: 96.650 \n",
      "step: 3650 | Train: G_Loss: 1.083, D_unsup_loss_real: 0.698, D_unsup_loss_fake: 0.637, D_sup_loss: 0.157, D_sup_acc: 96.69 Train acc: 96.357 Test acc: 96.770 \n",
      "step: 3651 | Train: G_Loss: 1.174, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.589, D_sup_loss: 0.150, D_sup_acc: 96.81 Train acc: 96.465 Test acc: 96.800 \n",
      "step: 3652 | Train: G_Loss: 1.159, D_unsup_loss_real: 0.668, D_unsup_loss_fake: 0.610, D_sup_loss: 0.141, D_sup_acc: 96.84 Train acc: 96.567 Test acc: 96.900 \n",
      "step: 3653 | Train: G_Loss: 1.105, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.540, D_sup_loss: 0.139, D_sup_acc: 96.94 Train acc: 96.470 Test acc: 96.790 \n",
      "step: 3654 | Train: G_Loss: 1.159, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.563, D_sup_loss: 0.140, D_sup_acc: 96.83 Train acc: 96.480 Test acc: 96.850 \n",
      "step: 3655 | Train: G_Loss: 1.094, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.620, D_sup_loss: 0.140, D_sup_acc: 96.89 Train acc: 96.493 Test acc: 96.880 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3656 | Train: G_Loss: 1.132, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.623, D_sup_loss: 0.140, D_sup_acc: 96.92 Train acc: 96.497 Test acc: 96.880 \n",
      "step: 3657 | Train: G_Loss: 1.172, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.624, D_sup_loss: 0.139, D_sup_acc: 96.92 Train acc: 96.448 Test acc: 96.830 \n",
      "step: 3658 | Train: G_Loss: 1.164, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.619, D_sup_loss: 0.140, D_sup_acc: 96.87 Train acc: 96.367 Test acc: 96.710 \n",
      "step: 3659 | Train: G_Loss: 1.125, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.557, D_sup_loss: 0.145, D_sup_acc: 96.75 Train acc: 96.043 Test acc: 96.510 \n",
      "step: 3660 | Train: G_Loss: 1.202, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.621, D_sup_loss: 0.162, D_sup_acc: 96.55 Train acc: 96.362 Test acc: 96.790 \n",
      "step: 3661 | Train: G_Loss: 1.169, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.636, D_sup_loss: 0.146, D_sup_acc: 96.83 Train acc: 96.433 Test acc: 96.730 \n",
      "step: 3662 | Train: G_Loss: 1.136, D_unsup_loss_real: 0.661, D_unsup_loss_fake: 0.649, D_sup_loss: 0.142, D_sup_acc: 96.77 Train acc: 96.335 Test acc: 96.700 \n",
      "step: 3663 | Train: G_Loss: 1.082, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.638, D_sup_loss: 0.149, D_sup_acc: 96.74 Train acc: 96.390 Test acc: 96.780 \n",
      "step: 3664 | Train: G_Loss: 1.116, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.624, D_sup_loss: 0.145, D_sup_acc: 96.82 Train acc: 96.330 Test acc: 96.770 \n",
      "step: 3665 | Train: G_Loss: 1.083, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.520, D_sup_loss: 0.148, D_sup_acc: 96.81 Train acc: 96.482 Test acc: 96.760 \n",
      "step: 3666 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.651, D_unsup_loss_fake: 0.630, D_sup_loss: 0.145, D_sup_acc: 96.80 Train acc: 96.458 Test acc: 96.820 \n",
      "step: 3667 | Train: G_Loss: 1.146, D_unsup_loss_real: 0.507, D_unsup_loss_fake: 0.724, D_sup_loss: 0.140, D_sup_acc: 96.86 Train acc: 96.332 Test acc: 96.860 \n",
      "step: 3668 | Train: G_Loss: 1.044, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.595, D_sup_loss: 0.146, D_sup_acc: 96.90 Train acc: 96.622 Test acc: 96.840 \n",
      "step: 3669 | Train: G_Loss: 1.087, D_unsup_loss_real: 0.724, D_unsup_loss_fake: 0.616, D_sup_loss: 0.135, D_sup_acc: 96.88 Train acc: 96.667 Test acc: 96.870 \n",
      "step: 3670 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.597, D_sup_loss: 0.139, D_sup_acc: 96.91 Train acc: 96.645 Test acc: 96.810 \n",
      "step: 3671 | Train: G_Loss: 1.063, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.588, D_sup_loss: 0.137, D_sup_acc: 96.85 Train acc: 96.623 Test acc: 96.920 \n",
      "step: 3672 | Train: G_Loss: 1.132, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.611, D_sup_loss: 0.140, D_sup_acc: 96.96 Train acc: 96.708 Test acc: 96.860 \n",
      "step: 3673 | Train: G_Loss: 1.099, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.575, D_sup_loss: 0.133, D_sup_acc: 96.90 Train acc: 96.682 Test acc: 96.830 \n",
      "step: 3674 | Train: G_Loss: 1.039, D_unsup_loss_real: 0.647, D_unsup_loss_fake: 0.689, D_sup_loss: 0.136, D_sup_acc: 96.87 Train acc: 96.380 Test acc: 96.780 \n",
      "step: 3675 | Train: G_Loss: 1.029, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.588, D_sup_loss: 0.150, D_sup_acc: 96.82 Train acc: 96.477 Test acc: 96.740 \n",
      "step: 3676 | Train: G_Loss: 1.075, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.665, D_sup_loss: 0.147, D_sup_acc: 96.78 Train acc: 96.257 Test acc: 96.730 \n",
      "step: 3677 | Train: G_Loss: 1.091, D_unsup_loss_real: 0.667, D_unsup_loss_fake: 0.644, D_sup_loss: 0.155, D_sup_acc: 96.77 Train acc: 96.670 Test acc: 96.890 \n",
      "step: 3678 | Train: G_Loss: 1.034, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.622, D_sup_loss: 0.136, D_sup_acc: 96.93 Train acc: 96.562 Test acc: 96.860 \n",
      "step: 3679 | Train: G_Loss: 1.130, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.692, D_sup_loss: 0.139, D_sup_acc: 96.90 Train acc: 96.485 Test acc: 96.840 \n",
      "step: 3680 | Train: G_Loss: 1.089, D_unsup_loss_real: 0.545, D_unsup_loss_fake: 0.653, D_sup_loss: 0.144, D_sup_acc: 96.88 Train acc: 96.618 Test acc: 96.970 \n",
      "step: 3681 | Train: G_Loss: 1.118, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.613, D_sup_loss: 0.136, D_sup_acc: 97.01 Train acc: 96.472 Test acc: 96.850 \n",
      "step: 3682 | Train: G_Loss: 1.174, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.601, D_sup_loss: 0.145, D_sup_acc: 96.89 Train acc: 96.745 Test acc: 96.850 \n",
      "step: 3683 | Train: G_Loss: 1.126, D_unsup_loss_real: 0.673, D_unsup_loss_fake: 0.553, D_sup_loss: 0.131, D_sup_acc: 96.89 Train acc: 96.518 Test acc: 96.860 \n",
      "step: 3684 | Train: G_Loss: 1.099, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.585, D_sup_loss: 0.140, D_sup_acc: 96.90 Train acc: 96.752 Test acc: 96.930 \n",
      "step: 3685 | Train: G_Loss: 1.111, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.570, D_sup_loss: 0.133, D_sup_acc: 96.97 Train acc: 96.448 Test acc: 96.750 \n",
      "step: 3686 | Train: G_Loss: 1.152, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.628, D_sup_loss: 0.146, D_sup_acc: 96.79 Train acc: 96.208 Test acc: 96.630 \n",
      "step: 3687 | Train: G_Loss: 1.203, D_unsup_loss_real: 0.686, D_unsup_loss_fake: 0.670, D_sup_loss: 0.158, D_sup_acc: 96.67 Train acc: 96.528 Test acc: 96.790 \n",
      "step: 3688 | Train: G_Loss: 1.070, D_unsup_loss_real: 0.687, D_unsup_loss_fake: 0.614, D_sup_loss: 0.144, D_sup_acc: 96.83 Train acc: 96.302 Test acc: 96.640 \n",
      "step: 3689 | Train: G_Loss: 1.103, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.551, D_sup_loss: 0.153, D_sup_acc: 96.68 Train acc: 96.648 Test acc: 96.890 \n",
      "step: 3690 | Train: G_Loss: 1.120, D_unsup_loss_real: 0.648, D_unsup_loss_fake: 0.613, D_sup_loss: 0.136, D_sup_acc: 96.93 Train acc: 96.540 Test acc: 96.970 \n",
      "step: 3691 | Train: G_Loss: 1.074, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.622, D_sup_loss: 0.141, D_sup_acc: 97.01 Train acc: 96.550 Test acc: 96.900 \n",
      "step: 3692 | Train: G_Loss: 1.106, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.646, D_sup_loss: 0.140, D_sup_acc: 96.94 Train acc: 96.348 Test acc: 96.700 \n",
      "step: 3693 | Train: G_Loss: 1.075, D_unsup_loss_real: 0.527, D_unsup_loss_fake: 0.580, D_sup_loss: 0.148, D_sup_acc: 96.74 Train acc: 96.382 Test acc: 96.770 \n",
      "step: 3694 | Train: G_Loss: 1.114, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.651, D_sup_loss: 0.145, D_sup_acc: 96.81 Train acc: 96.292 Test acc: 96.660 \n",
      "step: 3695 | Train: G_Loss: 1.136, D_unsup_loss_real: 0.652, D_unsup_loss_fake: 0.597, D_sup_loss: 0.151, D_sup_acc: 96.70 Train acc: 96.520 Test acc: 96.920 \n",
      "step: 3696 | Train: G_Loss: 1.127, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.644, D_sup_loss: 0.142, D_sup_acc: 96.96 Train acc: 96.562 Test acc: 96.970 \n",
      "step: 3697 | Train: G_Loss: 1.092, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.517, D_sup_loss: 0.141, D_sup_acc: 97.01 Train acc: 96.630 Test acc: 96.960 \n",
      "step: 3698 | Train: G_Loss: 1.088, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.623, D_sup_loss: 0.134, D_sup_acc: 97.00 Train acc: 96.612 Test acc: 96.940 \n",
      "step: 3699 | Train: G_Loss: 1.181, D_unsup_loss_real: 0.521, D_unsup_loss_fake: 0.634, D_sup_loss: 0.138, D_sup_acc: 96.98 Train acc: 96.613 Test acc: 96.850 \n",
      "step: 3700 | Train: G_Loss: 1.106, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.643, D_sup_loss: 0.136, D_sup_acc: 96.89 Train acc: 96.698 Test acc: 96.890 \n",
      "Train Classifier Accuracy: 96.698%\n",
      "\n",
      "Test Classifier Accuracy: 96.890%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_3700.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_3700.h5\n",
      "step: 3701 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.642, D_sup_loss: 0.132, D_sup_acc: 96.93 Train acc: 96.520 Test acc: 96.930 \n",
      "step: 3702 | Train: G_Loss: 1.137, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.597, D_sup_loss: 0.144, D_sup_acc: 96.97 Train acc: 96.633 Test acc: 96.840 \n",
      "step: 3703 | Train: G_Loss: 1.152, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.600, D_sup_loss: 0.138, D_sup_acc: 96.88 Train acc: 96.555 Test acc: 96.760 \n",
      "step: 3704 | Train: G_Loss: 1.112, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.585, D_sup_loss: 0.140, D_sup_acc: 96.80 Train acc: 96.665 Test acc: 96.860 \n",
      "step: 3705 | Train: G_Loss: 1.084, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.534, D_sup_loss: 0.135, D_sup_acc: 96.90 Train acc: 96.302 Test acc: 96.710 \n",
      "step: 3706 | Train: G_Loss: 1.106, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.552, D_sup_loss: 0.153, D_sup_acc: 96.75 Train acc: 96.163 Test acc: 96.550 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3707 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.624, D_sup_loss: 0.160, D_sup_acc: 96.59 Train acc: 96.542 Test acc: 96.810 \n",
      "step: 3708 | Train: G_Loss: 1.075, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.562, D_sup_loss: 0.137, D_sup_acc: 96.85 Train acc: 96.610 Test acc: 96.860 \n",
      "step: 3709 | Train: G_Loss: 0.977, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.667, D_sup_loss: 0.131, D_sup_acc: 96.90 Train acc: 96.605 Test acc: 96.830 \n",
      "step: 3710 | Train: G_Loss: 1.119, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.587, D_sup_loss: 0.133, D_sup_acc: 96.87 Train acc: 96.598 Test acc: 96.900 \n",
      "step: 3711 | Train: G_Loss: 1.105, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.639, D_sup_loss: 0.137, D_sup_acc: 96.94 Train acc: 96.688 Test acc: 97.010 \n",
      "step: 3712 | Train: G_Loss: 1.168, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.621, D_sup_loss: 0.136, D_sup_acc: 97.05 Train acc: 96.600 Test acc: 96.870 \n",
      "step: 3713 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.653, D_sup_loss: 0.136, D_sup_acc: 96.91 Train acc: 96.508 Test acc: 96.880 \n",
      "step: 3714 | Train: G_Loss: 1.179, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.591, D_sup_loss: 0.141, D_sup_acc: 96.92 Train acc: 96.383 Test acc: 96.810 \n",
      "step: 3715 | Train: G_Loss: 1.204, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.632, D_sup_loss: 0.144, D_sup_acc: 96.85 Train acc: 96.358 Test acc: 96.850 \n",
      "step: 3716 | Train: G_Loss: 1.099, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.585, D_sup_loss: 0.145, D_sup_acc: 96.89 Train acc: 96.473 Test acc: 96.800 \n",
      "step: 3717 | Train: G_Loss: 1.143, D_unsup_loss_real: 0.663, D_unsup_loss_fake: 0.601, D_sup_loss: 0.142, D_sup_acc: 96.84 Train acc: 96.657 Test acc: 96.870 \n",
      "step: 3718 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.635, D_unsup_loss_fake: 0.623, D_sup_loss: 0.134, D_sup_acc: 96.91 Train acc: 96.542 Test acc: 96.930 \n",
      "step: 3719 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.617, D_sup_loss: 0.139, D_sup_acc: 96.97 Train acc: 96.627 Test acc: 97.030 \n",
      "step: 3720 | Train: G_Loss: 1.159, D_unsup_loss_real: 0.666, D_unsup_loss_fake: 0.611, D_sup_loss: 0.138, D_sup_acc: 97.07 Train acc: 96.710 Test acc: 96.930 \n",
      "step: 3721 | Train: G_Loss: 1.114, D_unsup_loss_real: 0.646, D_unsup_loss_fake: 0.686, D_sup_loss: 0.134, D_sup_acc: 96.97 Train acc: 96.492 Test acc: 96.990 \n",
      "step: 3722 | Train: G_Loss: 1.081, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.558, D_sup_loss: 0.144, D_sup_acc: 97.03 Train acc: 96.560 Test acc: 96.890 \n",
      "step: 3723 | Train: G_Loss: 1.089, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.569, D_sup_loss: 0.137, D_sup_acc: 96.93 Train acc: 96.413 Test acc: 96.890 \n",
      "step: 3724 | Train: G_Loss: 1.212, D_unsup_loss_real: 0.534, D_unsup_loss_fake: 0.656, D_sup_loss: 0.142, D_sup_acc: 96.93 Train acc: 96.490 Test acc: 96.820 \n",
      "step: 3725 | Train: G_Loss: 1.185, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.638, D_sup_loss: 0.138, D_sup_acc: 96.86 Train acc: 96.383 Test acc: 96.840 \n",
      "step: 3726 | Train: G_Loss: 1.081, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.647, D_sup_loss: 0.148, D_sup_acc: 96.88 Train acc: 96.190 Test acc: 96.710 \n",
      "step: 3727 | Train: G_Loss: 1.156, D_unsup_loss_real: 0.538, D_unsup_loss_fake: 0.617, D_sup_loss: 0.154, D_sup_acc: 96.75 Train acc: 96.253 Test acc: 96.750 \n",
      "step: 3728 | Train: G_Loss: 1.061, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.627, D_sup_loss: 0.150, D_sup_acc: 96.79 Train acc: 96.480 Test acc: 96.920 \n",
      "step: 3729 | Train: G_Loss: 1.162, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.605, D_sup_loss: 0.143, D_sup_acc: 96.96 Train acc: 96.673 Test acc: 96.920 \n",
      "step: 3730 | Train: G_Loss: 1.041, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.592, D_sup_loss: 0.136, D_sup_acc: 96.96 Train acc: 96.545 Test acc: 96.910 \n",
      "step: 3731 | Train: G_Loss: 1.032, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.571, D_sup_loss: 0.144, D_sup_acc: 96.95 Train acc: 96.590 Test acc: 96.930 \n",
      "step: 3732 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.544, D_unsup_loss_fake: 0.643, D_sup_loss: 0.140, D_sup_acc: 96.97 Train acc: 96.622 Test acc: 97.020 \n",
      "step: 3733 | Train: G_Loss: 1.104, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.638, D_sup_loss: 0.136, D_sup_acc: 97.06 Train acc: 96.497 Test acc: 96.940 \n",
      "step: 3734 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.591, D_sup_loss: 0.141, D_sup_acc: 96.98 Train acc: 96.507 Test acc: 96.950 \n",
      "step: 3735 | Train: G_Loss: 1.060, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.626, D_sup_loss: 0.142, D_sup_acc: 96.99 Train acc: 96.502 Test acc: 96.880 \n",
      "step: 3736 | Train: G_Loss: 1.073, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.600, D_sup_loss: 0.144, D_sup_acc: 96.92 Train acc: 96.608 Test acc: 96.960 \n",
      "step: 3737 | Train: G_Loss: 1.003, D_unsup_loss_real: 0.651, D_unsup_loss_fake: 0.613, D_sup_loss: 0.139, D_sup_acc: 97.00 Train acc: 96.458 Test acc: 96.900 \n",
      "step: 3738 | Train: G_Loss: 1.105, D_unsup_loss_real: 0.649, D_unsup_loss_fake: 0.570, D_sup_loss: 0.144, D_sup_acc: 96.94 Train acc: 96.605 Test acc: 97.000 \n",
      "step: 3739 | Train: G_Loss: 1.114, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.598, D_sup_loss: 0.138, D_sup_acc: 97.04 Train acc: 96.557 Test acc: 97.010 \n",
      "step: 3740 | Train: G_Loss: 1.126, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.632, D_sup_loss: 0.140, D_sup_acc: 97.05 Train acc: 96.610 Test acc: 96.960 \n",
      "step: 3741 | Train: G_Loss: 1.092, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.612, D_sup_loss: 0.139, D_sup_acc: 97.00 Train acc: 96.590 Test acc: 96.900 \n",
      "step: 3742 | Train: G_Loss: 1.071, D_unsup_loss_real: 0.542, D_unsup_loss_fake: 0.658, D_sup_loss: 0.142, D_sup_acc: 96.94 Train acc: 96.697 Test acc: 97.010 \n",
      "step: 3743 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.624, D_sup_loss: 0.138, D_sup_acc: 97.05 Train acc: 96.542 Test acc: 96.840 \n",
      "step: 3744 | Train: G_Loss: 1.099, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.555, D_sup_loss: 0.145, D_sup_acc: 96.88 Train acc: 96.687 Test acc: 96.940 \n",
      "step: 3745 | Train: G_Loss: 1.111, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.609, D_sup_loss: 0.134, D_sup_acc: 96.98 Train acc: 96.720 Test acc: 96.880 \n",
      "step: 3746 | Train: G_Loss: 1.118, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.611, D_sup_loss: 0.134, D_sup_acc: 96.92 Train acc: 96.677 Test acc: 96.990 \n",
      "step: 3747 | Train: G_Loss: 1.112, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.584, D_sup_loss: 0.135, D_sup_acc: 97.03 Train acc: 96.502 Test acc: 96.890 \n",
      "step: 3748 | Train: G_Loss: 1.027, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.542, D_sup_loss: 0.142, D_sup_acc: 96.93 Train acc: 96.483 Test acc: 96.890 \n",
      "step: 3749 | Train: G_Loss: 1.057, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.639, D_sup_loss: 0.140, D_sup_acc: 96.93 Train acc: 96.477 Test acc: 96.900 \n",
      "step: 3750 | Train: G_Loss: 1.096, D_unsup_loss_real: 0.707, D_unsup_loss_fake: 0.570, D_sup_loss: 0.142, D_sup_acc: 96.94 Train acc: 96.522 Test acc: 96.870 \n",
      "step: 3751 | Train: G_Loss: 1.086, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.631, D_sup_loss: 0.139, D_sup_acc: 96.91 Train acc: 96.520 Test acc: 96.860 \n",
      "step: 3752 | Train: G_Loss: 1.147, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.630, D_sup_loss: 0.142, D_sup_acc: 96.90 Train acc: 96.355 Test acc: 96.790 \n",
      "step: 3753 | Train: G_Loss: 1.195, D_unsup_loss_real: 0.679, D_unsup_loss_fake: 0.569, D_sup_loss: 0.152, D_sup_acc: 96.83 Train acc: 96.465 Test acc: 96.780 \n",
      "step: 3754 | Train: G_Loss: 1.021, D_unsup_loss_real: 0.659, D_unsup_loss_fake: 0.564, D_sup_loss: 0.141, D_sup_acc: 96.82 Train acc: 96.232 Test acc: 96.650 \n",
      "step: 3755 | Train: G_Loss: 1.088, D_unsup_loss_real: 0.671, D_unsup_loss_fake: 0.611, D_sup_loss: 0.152, D_sup_acc: 96.69 Train acc: 96.653 Test acc: 97.000 \n",
      "step: 3756 | Train: G_Loss: 1.107, D_unsup_loss_real: 0.520, D_unsup_loss_fake: 0.625, D_sup_loss: 0.131, D_sup_acc: 97.04 Train acc: 96.598 Test acc: 96.990 \n",
      "step: 3757 | Train: G_Loss: 1.065, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.650, D_sup_loss: 0.132, D_sup_acc: 97.03 Train acc: 96.527 Test acc: 96.910 \n",
      "step: 3758 | Train: G_Loss: 1.073, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.639, D_sup_loss: 0.138, D_sup_acc: 96.95 Train acc: 96.453 Test acc: 96.790 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3759 | Train: G_Loss: 1.083, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.589, D_sup_loss: 0.142, D_sup_acc: 96.83 Train acc: 96.272 Test acc: 96.640 \n",
      "step: 3760 | Train: G_Loss: 1.073, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.658, D_sup_loss: 0.153, D_sup_acc: 96.68 Train acc: 96.387 Test acc: 96.860 \n",
      "step: 3761 | Train: G_Loss: 1.081, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.701, D_sup_loss: 0.146, D_sup_acc: 96.90 Train acc: 96.262 Test acc: 96.740 \n",
      "step: 3762 | Train: G_Loss: 1.086, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.525, D_sup_loss: 0.154, D_sup_acc: 96.78 Train acc: 96.512 Test acc: 96.850 \n",
      "step: 3763 | Train: G_Loss: 1.182, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.679, D_sup_loss: 0.139, D_sup_acc: 96.89 Train acc: 96.590 Test acc: 96.860 \n",
      "step: 3764 | Train: G_Loss: 1.270, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.585, D_sup_loss: 0.138, D_sup_acc: 96.90 Train acc: 96.528 Test acc: 96.820 \n",
      "step: 3765 | Train: G_Loss: 1.155, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.553, D_sup_loss: 0.139, D_sup_acc: 96.86 Train acc: 96.597 Test acc: 96.860 \n",
      "step: 3766 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.647, D_unsup_loss_fake: 0.600, D_sup_loss: 0.137, D_sup_acc: 96.90 Train acc: 96.560 Test acc: 96.880 \n",
      "step: 3767 | Train: G_Loss: 1.048, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.557, D_sup_loss: 0.140, D_sup_acc: 96.92 Train acc: 96.498 Test acc: 96.890 \n",
      "step: 3768 | Train: G_Loss: 1.147, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.588, D_sup_loss: 0.140, D_sup_acc: 96.93 Train acc: 96.393 Test acc: 96.910 \n",
      "step: 3769 | Train: G_Loss: 1.195, D_unsup_loss_real: 0.650, D_unsup_loss_fake: 0.613, D_sup_loss: 0.141, D_sup_acc: 96.95 Train acc: 96.317 Test acc: 96.780 \n",
      "step: 3770 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.645, D_sup_loss: 0.148, D_sup_acc: 96.82 Train acc: 96.197 Test acc: 96.630 \n",
      "step: 3771 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.651, D_sup_loss: 0.154, D_sup_acc: 96.67 Train acc: 96.548 Test acc: 96.950 \n",
      "step: 3772 | Train: G_Loss: 0.998, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.559, D_sup_loss: 0.139, D_sup_acc: 96.99 Train acc: 96.500 Test acc: 96.990 \n",
      "step: 3773 | Train: G_Loss: 1.105, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.649, D_sup_loss: 0.141, D_sup_acc: 97.03 Train acc: 96.668 Test acc: 96.960 \n",
      "step: 3774 | Train: G_Loss: 1.062, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.628, D_sup_loss: 0.136, D_sup_acc: 97.00 Train acc: 96.468 Test acc: 96.920 \n",
      "step: 3775 | Train: G_Loss: 1.065, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.656, D_sup_loss: 0.143, D_sup_acc: 96.96 Train acc: 96.695 Test acc: 96.960 \n",
      "step: 3776 | Train: G_Loss: 1.149, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.604, D_sup_loss: 0.134, D_sup_acc: 97.00 Train acc: 96.707 Test acc: 96.980 \n",
      "step: 3777 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.576, D_sup_loss: 0.133, D_sup_acc: 97.02 Train acc: 96.648 Test acc: 97.040 \n",
      "step: 3778 | Train: G_Loss: 1.174, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.676, D_sup_loss: 0.133, D_sup_acc: 97.08 Train acc: 96.462 Test acc: 96.970 \n",
      "step: 3779 | Train: G_Loss: 1.113, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.570, D_sup_loss: 0.140, D_sup_acc: 97.01 Train acc: 96.653 Test acc: 96.990 \n",
      "step: 3780 | Train: G_Loss: 1.075, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.590, D_sup_loss: 0.133, D_sup_acc: 97.03 Train acc: 96.615 Test acc: 96.990 \n",
      "step: 3781 | Train: G_Loss: 1.190, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.610, D_sup_loss: 0.135, D_sup_acc: 97.03 Train acc: 96.420 Test acc: 96.810 \n",
      "step: 3782 | Train: G_Loss: 1.116, D_unsup_loss_real: 0.671, D_unsup_loss_fake: 0.588, D_sup_loss: 0.141, D_sup_acc: 96.85 Train acc: 96.612 Test acc: 96.960 \n",
      "step: 3783 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.629, D_sup_loss: 0.132, D_sup_acc: 97.00 Train acc: 96.333 Test acc: 96.750 \n",
      "step: 3784 | Train: G_Loss: 1.065, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.651, D_sup_loss: 0.145, D_sup_acc: 96.79 Train acc: 96.495 Test acc: 96.930 \n",
      "step: 3785 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.628, D_sup_loss: 0.136, D_sup_acc: 96.97 Train acc: 96.292 Test acc: 96.790 \n",
      "step: 3786 | Train: G_Loss: 1.066, D_unsup_loss_real: 0.676, D_unsup_loss_fake: 0.674, D_sup_loss: 0.146, D_sup_acc: 96.83 Train acc: 96.558 Test acc: 96.910 \n",
      "step: 3787 | Train: G_Loss: 1.075, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.636, D_sup_loss: 0.134, D_sup_acc: 96.95 Train acc: 96.322 Test acc: 96.810 \n",
      "step: 3788 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.605, D_sup_loss: 0.144, D_sup_acc: 96.85 Train acc: 96.505 Test acc: 96.880 \n",
      "step: 3789 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.566, D_sup_loss: 0.137, D_sup_acc: 96.92 Train acc: 96.338 Test acc: 96.760 \n",
      "step: 3790 | Train: G_Loss: 1.151, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.522, D_sup_loss: 0.144, D_sup_acc: 96.80 Train acc: 96.447 Test acc: 96.840 \n",
      "step: 3791 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.547, D_sup_loss: 0.139, D_sup_acc: 96.88 Train acc: 96.653 Test acc: 96.990 \n",
      "step: 3792 | Train: G_Loss: 1.036, D_unsup_loss_real: 0.657, D_unsup_loss_fake: 0.630, D_sup_loss: 0.131, D_sup_acc: 97.03 Train acc: 96.230 Test acc: 96.660 \n",
      "step: 3793 | Train: G_Loss: 1.111, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.692, D_sup_loss: 0.146, D_sup_acc: 96.70 Train acc: 96.330 Test acc: 96.710 \n",
      "step: 3794 | Train: G_Loss: 1.173, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.654, D_sup_loss: 0.143, D_sup_acc: 96.75 Train acc: 96.532 Test acc: 96.870 \n",
      "step: 3795 | Train: G_Loss: 1.178, D_unsup_loss_real: 0.527, D_unsup_loss_fake: 0.544, D_sup_loss: 0.134, D_sup_acc: 96.91 Train acc: 96.577 Test acc: 96.880 \n",
      "step: 3796 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.558, D_sup_loss: 0.135, D_sup_acc: 96.92 Train acc: 96.578 Test acc: 96.850 \n",
      "step: 3797 | Train: G_Loss: 1.139, D_unsup_loss_real: 0.656, D_unsup_loss_fake: 0.653, D_sup_loss: 0.133, D_sup_acc: 96.89 Train acc: 96.202 Test acc: 96.630 \n",
      "step: 3798 | Train: G_Loss: 1.044, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.576, D_sup_loss: 0.150, D_sup_acc: 96.67 Train acc: 96.348 Test acc: 96.750 \n",
      "step: 3799 | Train: G_Loss: 1.116, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.644, D_sup_loss: 0.145, D_sup_acc: 96.79 Train acc: 96.378 Test acc: 96.650 \n",
      "step: 3800 | Train: G_Loss: 1.132, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.579, D_sup_loss: 0.145, D_sup_acc: 96.69 Train acc: 96.298 Test acc: 96.790 \n",
      "Train Classifier Accuracy: 96.298%\n",
      "\n",
      "Test Classifier Accuracy: 96.790%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_3800.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_3800.h5\n",
      "step: 3801 | Train: G_Loss: 1.062, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.563, D_sup_loss: 0.146, D_sup_acc: 96.83 Train acc: 96.550 Test acc: 96.840 \n",
      "step: 3802 | Train: G_Loss: 1.205, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.603, D_sup_loss: 0.137, D_sup_acc: 96.88 Train acc: 96.248 Test acc: 96.650 \n",
      "step: 3803 | Train: G_Loss: 1.113, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.578, D_sup_loss: 0.150, D_sup_acc: 96.69 Train acc: 96.198 Test acc: 96.600 \n",
      "step: 3804 | Train: G_Loss: 1.107, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.567, D_sup_loss: 0.149, D_sup_acc: 96.64 Train acc: 96.243 Test acc: 96.660 \n",
      "step: 3805 | Train: G_Loss: 1.127, D_unsup_loss_real: 0.655, D_unsup_loss_fake: 0.615, D_sup_loss: 0.144, D_sup_acc: 96.70 Train acc: 96.148 Test acc: 96.640 \n",
      "step: 3806 | Train: G_Loss: 1.030, D_unsup_loss_real: 0.734, D_unsup_loss_fake: 0.626, D_sup_loss: 0.149, D_sup_acc: 96.68 Train acc: 96.605 Test acc: 96.930 \n",
      "step: 3807 | Train: G_Loss: 1.163, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.623, D_sup_loss: 0.133, D_sup_acc: 96.97 Train acc: 96.555 Test acc: 96.780 \n",
      "step: 3808 | Train: G_Loss: 1.167, D_unsup_loss_real: 0.467, D_unsup_loss_fake: 0.623, D_sup_loss: 0.134, D_sup_acc: 96.82 Train acc: 96.622 Test acc: 96.880 \n",
      "step: 3809 | Train: G_Loss: 1.088, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.580, D_sup_loss: 0.132, D_sup_acc: 96.92 Train acc: 96.483 Test acc: 96.800 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3810 | Train: G_Loss: 1.140, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.670, D_sup_loss: 0.138, D_sup_acc: 96.84 Train acc: 96.237 Test acc: 96.690 \n",
      "step: 3811 | Train: G_Loss: 1.072, D_unsup_loss_real: 0.664, D_unsup_loss_fake: 0.634, D_sup_loss: 0.152, D_sup_acc: 96.73 Train acc: 96.337 Test acc: 96.810 \n",
      "step: 3812 | Train: G_Loss: 1.192, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.557, D_sup_loss: 0.149, D_sup_acc: 96.85 Train acc: 96.417 Test acc: 96.830 \n",
      "step: 3813 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.604, D_sup_loss: 0.145, D_sup_acc: 96.87 Train acc: 96.330 Test acc: 96.840 \n",
      "step: 3814 | Train: G_Loss: 1.104, D_unsup_loss_real: 0.664, D_unsup_loss_fake: 0.549, D_sup_loss: 0.148, D_sup_acc: 96.88 Train acc: 96.537 Test acc: 96.970 \n",
      "step: 3815 | Train: G_Loss: 1.122, D_unsup_loss_real: 0.650, D_unsup_loss_fake: 0.618, D_sup_loss: 0.139, D_sup_acc: 97.01 Train acc: 96.585 Test acc: 97.020 \n",
      "step: 3816 | Train: G_Loss: 1.092, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.562, D_sup_loss: 0.136, D_sup_acc: 97.06 Train acc: 96.577 Test acc: 96.820 \n",
      "step: 3817 | Train: G_Loss: 1.076, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.563, D_sup_loss: 0.135, D_sup_acc: 96.86 Train acc: 96.458 Test acc: 96.890 \n",
      "step: 3818 | Train: G_Loss: 1.082, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.661, D_sup_loss: 0.139, D_sup_acc: 96.93 Train acc: 96.552 Test acc: 96.930 \n",
      "step: 3819 | Train: G_Loss: 1.112, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.692, D_sup_loss: 0.134, D_sup_acc: 96.97 Train acc: 96.593 Test acc: 96.970 \n",
      "step: 3820 | Train: G_Loss: 1.061, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.586, D_sup_loss: 0.130, D_sup_acc: 97.01 Train acc: 96.448 Test acc: 97.030 \n",
      "step: 3821 | Train: G_Loss: 1.078, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.634, D_sup_loss: 0.137, D_sup_acc: 97.07 Train acc: 96.527 Test acc: 97.020 \n",
      "step: 3822 | Train: G_Loss: 1.100, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.568, D_sup_loss: 0.137, D_sup_acc: 97.06 Train acc: 96.287 Test acc: 96.900 \n",
      "step: 3823 | Train: G_Loss: 1.118, D_unsup_loss_real: 0.651, D_unsup_loss_fake: 0.588, D_sup_loss: 0.145, D_sup_acc: 96.94 Train acc: 96.262 Test acc: 96.870 \n",
      "step: 3824 | Train: G_Loss: 1.101, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.647, D_sup_loss: 0.146, D_sup_acc: 96.91 Train acc: 96.333 Test acc: 96.880 \n",
      "step: 3825 | Train: G_Loss: 1.164, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.664, D_sup_loss: 0.143, D_sup_acc: 96.92 Train acc: 96.398 Test acc: 96.950 \n",
      "step: 3826 | Train: G_Loss: 1.097, D_unsup_loss_real: 0.674, D_unsup_loss_fake: 0.655, D_sup_loss: 0.143, D_sup_acc: 96.99 Train acc: 96.495 Test acc: 97.020 \n",
      "step: 3827 | Train: G_Loss: 1.177, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.652, D_sup_loss: 0.139, D_sup_acc: 97.06 Train acc: 96.585 Test acc: 96.860 \n",
      "step: 3828 | Train: G_Loss: 1.067, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.678, D_sup_loss: 0.137, D_sup_acc: 96.90 Train acc: 96.488 Test acc: 96.970 \n",
      "step: 3829 | Train: G_Loss: 1.117, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.607, D_sup_loss: 0.143, D_sup_acc: 97.01 Train acc: 96.610 Test acc: 97.030 \n",
      "step: 3830 | Train: G_Loss: 1.127, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.597, D_sup_loss: 0.137, D_sup_acc: 97.07 Train acc: 96.500 Test acc: 96.950 \n",
      "step: 3831 | Train: G_Loss: 1.065, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.529, D_sup_loss: 0.139, D_sup_acc: 96.99 Train acc: 96.633 Test acc: 96.960 \n",
      "step: 3832 | Train: G_Loss: 1.012, D_unsup_loss_real: 0.655, D_unsup_loss_fake: 0.595, D_sup_loss: 0.132, D_sup_acc: 97.00 Train acc: 96.357 Test acc: 96.850 \n",
      "step: 3833 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.606, D_sup_loss: 0.144, D_sup_acc: 96.89 Train acc: 96.483 Test acc: 96.890 \n",
      "step: 3834 | Train: G_Loss: 1.097, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.701, D_sup_loss: 0.139, D_sup_acc: 96.93 Train acc: 96.597 Test acc: 96.960 \n",
      "step: 3835 | Train: G_Loss: 1.076, D_unsup_loss_real: 0.520, D_unsup_loss_fake: 0.594, D_sup_loss: 0.137, D_sup_acc: 97.00 Train acc: 96.457 Test acc: 96.870 \n",
      "step: 3836 | Train: G_Loss: 1.116, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.627, D_sup_loss: 0.142, D_sup_acc: 96.91 Train acc: 96.573 Test acc: 96.930 \n",
      "step: 3837 | Train: G_Loss: 1.146, D_unsup_loss_real: 0.642, D_unsup_loss_fake: 0.707, D_sup_loss: 0.138, D_sup_acc: 96.97 Train acc: 96.448 Test acc: 97.000 \n",
      "step: 3838 | Train: G_Loss: 1.117, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.550, D_sup_loss: 0.143, D_sup_acc: 97.04 Train acc: 96.447 Test acc: 96.970 \n",
      "step: 3839 | Train: G_Loss: 1.158, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.693, D_sup_loss: 0.143, D_sup_acc: 97.01 Train acc: 96.528 Test acc: 96.970 \n",
      "step: 3840 | Train: G_Loss: 1.096, D_unsup_loss_real: 0.648, D_unsup_loss_fake: 0.595, D_sup_loss: 0.140, D_sup_acc: 97.01 Train acc: 96.735 Test acc: 97.080 \n",
      "step: 3841 | Train: G_Loss: 1.108, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.579, D_sup_loss: 0.132, D_sup_acc: 97.12 Train acc: 96.840 Test acc: 97.070 \n",
      "step: 3842 | Train: G_Loss: 1.120, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.590, D_sup_loss: 0.128, D_sup_acc: 97.11 Train acc: 96.753 Test acc: 97.030 \n",
      "step: 3843 | Train: G_Loss: 1.222, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.669, D_sup_loss: 0.131, D_sup_acc: 97.07 Train acc: 96.748 Test acc: 97.120 \n",
      "step: 3844 | Train: G_Loss: 1.275, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.542, D_sup_loss: 0.131, D_sup_acc: 97.16 Train acc: 96.730 Test acc: 97.100 \n",
      "step: 3845 | Train: G_Loss: 1.191, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.648, D_sup_loss: 0.131, D_sup_acc: 97.14 Train acc: 96.558 Test acc: 97.040 \n",
      "step: 3846 | Train: G_Loss: 1.077, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.625, D_sup_loss: 0.140, D_sup_acc: 97.08 Train acc: 96.515 Test acc: 96.990 \n",
      "step: 3847 | Train: G_Loss: 1.101, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.598, D_sup_loss: 0.143, D_sup_acc: 97.03 Train acc: 96.353 Test acc: 96.890 \n",
      "step: 3848 | Train: G_Loss: 1.189, D_unsup_loss_real: 0.688, D_unsup_loss_fake: 0.557, D_sup_loss: 0.145, D_sup_acc: 96.93 Train acc: 96.613 Test acc: 97.010 \n",
      "step: 3849 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.650, D_sup_loss: 0.135, D_sup_acc: 97.05 Train acc: 96.590 Test acc: 96.920 \n",
      "step: 3850 | Train: G_Loss: 0.948, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.594, D_sup_loss: 0.136, D_sup_acc: 96.96 Train acc: 96.537 Test acc: 96.920 \n",
      "step: 3851 | Train: G_Loss: 1.216, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.606, D_sup_loss: 0.138, D_sup_acc: 96.96 Train acc: 96.727 Test acc: 97.000 \n",
      "step: 3852 | Train: G_Loss: 1.111, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.656, D_sup_loss: 0.130, D_sup_acc: 97.04 Train acc: 96.478 Test acc: 96.900 \n",
      "step: 3853 | Train: G_Loss: 1.173, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.587, D_sup_loss: 0.140, D_sup_acc: 96.94 Train acc: 96.692 Test acc: 97.000 \n",
      "step: 3854 | Train: G_Loss: 1.081, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.574, D_sup_loss: 0.131, D_sup_acc: 97.04 Train acc: 96.638 Test acc: 96.970 \n",
      "step: 3855 | Train: G_Loss: 1.111, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.648, D_sup_loss: 0.130, D_sup_acc: 97.01 Train acc: 96.385 Test acc: 96.720 \n",
      "step: 3856 | Train: G_Loss: 1.109, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.614, D_sup_loss: 0.143, D_sup_acc: 96.76 Train acc: 96.335 Test acc: 96.770 \n",
      "step: 3857 | Train: G_Loss: 1.064, D_unsup_loss_real: 0.682, D_unsup_loss_fake: 0.588, D_sup_loss: 0.146, D_sup_acc: 96.81 Train acc: 96.362 Test acc: 96.690 \n",
      "step: 3858 | Train: G_Loss: 1.077, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.593, D_sup_loss: 0.146, D_sup_acc: 96.73 Train acc: 96.597 Test acc: 96.900 \n",
      "step: 3859 | Train: G_Loss: 1.240, D_unsup_loss_real: 0.550, D_unsup_loss_fake: 0.557, D_sup_loss: 0.134, D_sup_acc: 96.94 Train acc: 96.578 Test acc: 96.850 \n",
      "step: 3860 | Train: G_Loss: 1.154, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.627, D_sup_loss: 0.134, D_sup_acc: 96.89 Train acc: 96.540 Test acc: 96.890 \n",
      "step: 3861 | Train: G_Loss: 1.110, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.584, D_sup_loss: 0.136, D_sup_acc: 96.93 Train acc: 96.533 Test acc: 96.790 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3862 | Train: G_Loss: 1.141, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.581, D_sup_loss: 0.137, D_sup_acc: 96.83 Train acc: 96.502 Test acc: 96.770 \n",
      "step: 3863 | Train: G_Loss: 1.126, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.611, D_sup_loss: 0.138, D_sup_acc: 96.81 Train acc: 96.642 Test acc: 96.860 \n",
      "step: 3864 | Train: G_Loss: 1.080, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.611, D_sup_loss: 0.132, D_sup_acc: 96.90 Train acc: 96.445 Test acc: 96.870 \n",
      "step: 3865 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.494, D_unsup_loss_fake: 0.686, D_sup_loss: 0.141, D_sup_acc: 96.91 Train acc: 96.395 Test acc: 96.910 \n",
      "step: 3866 | Train: G_Loss: 1.069, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.614, D_sup_loss: 0.143, D_sup_acc: 96.95 Train acc: 96.638 Test acc: 96.900 \n",
      "step: 3867 | Train: G_Loss: 1.160, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.554, D_sup_loss: 0.135, D_sup_acc: 96.94 Train acc: 96.510 Test acc: 96.880 \n",
      "step: 3868 | Train: G_Loss: 1.201, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.553, D_sup_loss: 0.139, D_sup_acc: 96.92 Train acc: 96.545 Test acc: 96.900 \n",
      "step: 3869 | Train: G_Loss: 1.174, D_unsup_loss_real: 0.639, D_unsup_loss_fake: 0.646, D_sup_loss: 0.136, D_sup_acc: 96.94 Train acc: 96.553 Test acc: 96.800 \n",
      "step: 3870 | Train: G_Loss: 1.074, D_unsup_loss_real: 0.668, D_unsup_loss_fake: 0.527, D_sup_loss: 0.137, D_sup_acc: 96.84 Train acc: 96.482 Test acc: 96.790 \n",
      "step: 3871 | Train: G_Loss: 1.090, D_unsup_loss_real: 0.661, D_unsup_loss_fake: 0.639, D_sup_loss: 0.139, D_sup_acc: 96.83 Train acc: 96.592 Test acc: 96.930 \n",
      "step: 3872 | Train: G_Loss: 1.044, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.636, D_sup_loss: 0.134, D_sup_acc: 96.97 Train acc: 96.353 Test acc: 96.810 \n",
      "step: 3873 | Train: G_Loss: 1.035, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.627, D_sup_loss: 0.145, D_sup_acc: 96.85 Train acc: 96.497 Test acc: 96.830 \n",
      "step: 3874 | Train: G_Loss: 1.069, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.663, D_sup_loss: 0.142, D_sup_acc: 96.87 Train acc: 96.647 Test acc: 96.890 \n",
      "step: 3875 | Train: G_Loss: 1.073, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.588, D_sup_loss: 0.138, D_sup_acc: 96.93 Train acc: 96.557 Test acc: 96.910 \n",
      "step: 3876 | Train: G_Loss: 1.170, D_unsup_loss_real: 0.652, D_unsup_loss_fake: 0.602, D_sup_loss: 0.139, D_sup_acc: 96.95 Train acc: 96.330 Test acc: 96.650 \n",
      "step: 3877 | Train: G_Loss: 1.013, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.604, D_sup_loss: 0.153, D_sup_acc: 96.69 Train acc: 96.322 Test acc: 96.770 \n",
      "step: 3878 | Train: G_Loss: 1.186, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.565, D_sup_loss: 0.152, D_sup_acc: 96.81 Train acc: 96.385 Test acc: 96.830 \n",
      "step: 3879 | Train: G_Loss: 1.161, D_unsup_loss_real: 0.662, D_unsup_loss_fake: 0.596, D_sup_loss: 0.146, D_sup_acc: 96.87 Train acc: 96.555 Test acc: 96.890 \n",
      "step: 3880 | Train: G_Loss: 1.156, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.549, D_sup_loss: 0.137, D_sup_acc: 96.93 Train acc: 96.637 Test acc: 97.020 \n",
      "step: 3881 | Train: G_Loss: 1.096, D_unsup_loss_real: 0.685, D_unsup_loss_fake: 0.614, D_sup_loss: 0.133, D_sup_acc: 97.06 Train acc: 96.627 Test acc: 97.040 \n",
      "step: 3882 | Train: G_Loss: 1.061, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.623, D_sup_loss: 0.131, D_sup_acc: 97.08 Train acc: 96.708 Test acc: 97.010 \n",
      "step: 3883 | Train: G_Loss: 1.078, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.646, D_sup_loss: 0.131, D_sup_acc: 97.05 Train acc: 96.678 Test acc: 97.060 \n",
      "step: 3884 | Train: G_Loss: 1.055, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.571, D_sup_loss: 0.134, D_sup_acc: 97.10 Train acc: 96.633 Test acc: 97.070 \n",
      "step: 3885 | Train: G_Loss: 1.052, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.587, D_sup_loss: 0.135, D_sup_acc: 97.11 Train acc: 96.692 Test acc: 97.080 \n",
      "step: 3886 | Train: G_Loss: 1.091, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.611, D_sup_loss: 0.132, D_sup_acc: 97.12 Train acc: 96.777 Test acc: 97.050 \n",
      "step: 3887 | Train: G_Loss: 1.066, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.609, D_sup_loss: 0.128, D_sup_acc: 97.09 Train acc: 96.615 Test acc: 97.010 \n",
      "step: 3888 | Train: G_Loss: 1.077, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.631, D_sup_loss: 0.137, D_sup_acc: 97.05 Train acc: 96.708 Test acc: 97.040 \n",
      "step: 3889 | Train: G_Loss: 1.141, D_unsup_loss_real: 0.508, D_unsup_loss_fake: 0.640, D_sup_loss: 0.136, D_sup_acc: 97.08 Train acc: 96.677 Test acc: 97.130 \n",
      "step: 3890 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.682, D_sup_loss: 0.136, D_sup_acc: 97.17 Train acc: 96.673 Test acc: 97.080 \n",
      "step: 3891 | Train: G_Loss: 1.112, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.639, D_sup_loss: 0.138, D_sup_acc: 97.12 Train acc: 96.690 Test acc: 97.080 \n",
      "step: 3892 | Train: G_Loss: 1.050, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.620, D_sup_loss: 0.138, D_sup_acc: 97.12 Train acc: 96.670 Test acc: 97.010 \n",
      "step: 3893 | Train: G_Loss: 1.104, D_unsup_loss_real: 0.680, D_unsup_loss_fake: 0.597, D_sup_loss: 0.141, D_sup_acc: 97.05 Train acc: 96.593 Test acc: 96.870 \n",
      "step: 3894 | Train: G_Loss: 1.108, D_unsup_loss_real: 0.742, D_unsup_loss_fake: 0.675, D_sup_loss: 0.147, D_sup_acc: 96.91 Train acc: 96.628 Test acc: 97.080 \n",
      "step: 3895 | Train: G_Loss: 1.078, D_unsup_loss_real: 0.687, D_unsup_loss_fake: 0.667, D_sup_loss: 0.144, D_sup_acc: 97.12 Train acc: 96.758 Test acc: 97.030 \n",
      "step: 3896 | Train: G_Loss: 1.023, D_unsup_loss_real: 0.653, D_unsup_loss_fake: 0.650, D_sup_loss: 0.137, D_sup_acc: 97.07 Train acc: 96.765 Test acc: 97.060 \n",
      "step: 3897 | Train: G_Loss: 1.112, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.660, D_sup_loss: 0.137, D_sup_acc: 97.10 Train acc: 96.757 Test acc: 96.990 \n",
      "step: 3898 | Train: G_Loss: 1.125, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.564, D_sup_loss: 0.136, D_sup_acc: 97.03 Train acc: 96.788 Test acc: 96.940 \n",
      "step: 3899 | Train: G_Loss: 1.076, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.607, D_sup_loss: 0.132, D_sup_acc: 96.98 Train acc: 96.767 Test acc: 96.970 \n",
      "step: 3900 | Train: G_Loss: 1.059, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.585, D_sup_loss: 0.134, D_sup_acc: 97.01 Train acc: 96.762 Test acc: 96.910 \n",
      "Train Classifier Accuracy: 96.762%\n",
      "\n",
      "Test Classifier Accuracy: 96.910%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_3900.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_3900.h5\n",
      "step: 3901 | Train: G_Loss: 1.063, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.705, D_sup_loss: 0.134, D_sup_acc: 96.95 Train acc: 96.778 Test acc: 97.120 \n",
      "step: 3902 | Train: G_Loss: 0.987, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.565, D_sup_loss: 0.134, D_sup_acc: 97.16 Train acc: 96.758 Test acc: 97.020 \n",
      "step: 3903 | Train: G_Loss: 1.141, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.652, D_sup_loss: 0.134, D_sup_acc: 97.06 Train acc: 96.745 Test acc: 96.990 \n",
      "step: 3904 | Train: G_Loss: 1.140, D_unsup_loss_real: 0.656, D_unsup_loss_fake: 0.677, D_sup_loss: 0.139, D_sup_acc: 97.03 Train acc: 96.782 Test acc: 96.950 \n",
      "step: 3905 | Train: G_Loss: 1.179, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.591, D_sup_loss: 0.138, D_sup_acc: 96.99 Train acc: 96.522 Test acc: 96.700 \n",
      "step: 3906 | Train: G_Loss: 1.027, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.628, D_sup_loss: 0.151, D_sup_acc: 96.74 Train acc: 96.620 Test acc: 96.790 \n",
      "step: 3907 | Train: G_Loss: 1.118, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.582, D_sup_loss: 0.146, D_sup_acc: 96.83 Train acc: 96.770 Test acc: 97.040 \n",
      "step: 3908 | Train: G_Loss: 1.022, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.576, D_sup_loss: 0.138, D_sup_acc: 97.08 Train acc: 96.730 Test acc: 96.980 \n",
      "step: 3909 | Train: G_Loss: 1.112, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.572, D_sup_loss: 0.138, D_sup_acc: 97.02 Train acc: 96.725 Test acc: 96.850 \n",
      "step: 3910 | Train: G_Loss: 1.094, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.602, D_sup_loss: 0.134, D_sup_acc: 96.89 Train acc: 96.685 Test acc: 96.910 \n",
      "step: 3911 | Train: G_Loss: 1.040, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.641, D_sup_loss: 0.140, D_sup_acc: 96.95 Train acc: 96.693 Test acc: 96.950 \n",
      "step: 3912 | Train: G_Loss: 0.977, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.576, D_sup_loss: 0.138, D_sup_acc: 96.99 Train acc: 96.605 Test acc: 96.960 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3913 | Train: G_Loss: 1.027, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.629, D_sup_loss: 0.141, D_sup_acc: 97.00 Train acc: 96.558 Test acc: 96.870 \n",
      "step: 3914 | Train: G_Loss: 1.036, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.622, D_sup_loss: 0.143, D_sup_acc: 96.91 Train acc: 96.673 Test acc: 97.000 \n",
      "step: 3915 | Train: G_Loss: 0.990, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.553, D_sup_loss: 0.139, D_sup_acc: 97.04 Train acc: 96.628 Test acc: 96.940 \n",
      "step: 3916 | Train: G_Loss: 1.060, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.601, D_sup_loss: 0.137, D_sup_acc: 96.98 Train acc: 96.733 Test acc: 97.010 \n",
      "step: 3917 | Train: G_Loss: 1.059, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.629, D_sup_loss: 0.133, D_sup_acc: 97.05 Train acc: 96.575 Test acc: 96.940 \n",
      "step: 3918 | Train: G_Loss: 1.039, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.625, D_sup_loss: 0.143, D_sup_acc: 96.98 Train acc: 96.658 Test acc: 96.890 \n",
      "step: 3919 | Train: G_Loss: 1.045, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.654, D_sup_loss: 0.136, D_sup_acc: 96.93 Train acc: 96.722 Test acc: 96.930 \n",
      "step: 3920 | Train: G_Loss: 1.180, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.667, D_sup_loss: 0.134, D_sup_acc: 96.97 Train acc: 96.733 Test acc: 96.940 \n",
      "step: 3921 | Train: G_Loss: 1.063, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.651, D_sup_loss: 0.136, D_sup_acc: 96.98 Train acc: 96.628 Test acc: 96.990 \n",
      "step: 3922 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.632, D_sup_loss: 0.144, D_sup_acc: 97.03 Train acc: 96.738 Test acc: 96.960 \n",
      "step: 3923 | Train: G_Loss: 1.108, D_unsup_loss_real: 0.684, D_unsup_loss_fake: 0.608, D_sup_loss: 0.140, D_sup_acc: 97.00 Train acc: 96.702 Test acc: 97.010 \n",
      "step: 3924 | Train: G_Loss: 1.070, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.654, D_sup_loss: 0.139, D_sup_acc: 97.05 Train acc: 96.670 Test acc: 96.990 \n",
      "step: 3925 | Train: G_Loss: 1.158, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.563, D_sup_loss: 0.143, D_sup_acc: 97.03 Train acc: 96.670 Test acc: 96.960 \n",
      "step: 3926 | Train: G_Loss: 1.106, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.532, D_sup_loss: 0.142, D_sup_acc: 97.00 Train acc: 96.802 Test acc: 96.980 \n",
      "step: 3927 | Train: G_Loss: 1.080, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.600, D_sup_loss: 0.137, D_sup_acc: 97.02 Train acc: 96.750 Test acc: 96.970 \n",
      "step: 3928 | Train: G_Loss: 1.056, D_unsup_loss_real: 0.651, D_unsup_loss_fake: 0.620, D_sup_loss: 0.139, D_sup_acc: 97.01 Train acc: 96.783 Test acc: 97.050 \n",
      "step: 3929 | Train: G_Loss: 1.054, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.579, D_sup_loss: 0.137, D_sup_acc: 97.09 Train acc: 96.767 Test acc: 96.990 \n",
      "step: 3930 | Train: G_Loss: 1.058, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.612, D_sup_loss: 0.136, D_sup_acc: 97.03 Train acc: 96.727 Test acc: 96.910 \n",
      "step: 3931 | Train: G_Loss: 1.129, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.570, D_sup_loss: 0.138, D_sup_acc: 96.95 Train acc: 96.708 Test acc: 96.810 \n",
      "step: 3932 | Train: G_Loss: 1.060, D_unsup_loss_real: 0.652, D_unsup_loss_fake: 0.644, D_sup_loss: 0.138, D_sup_acc: 96.85 Train acc: 96.832 Test acc: 97.010 \n",
      "step: 3933 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.612, D_sup_loss: 0.134, D_sup_acc: 97.05 Train acc: 96.695 Test acc: 97.030 \n",
      "step: 3934 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.564, D_sup_loss: 0.141, D_sup_acc: 97.07 Train acc: 96.775 Test acc: 97.090 \n",
      "step: 3935 | Train: G_Loss: 1.082, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.637, D_sup_loss: 0.130, D_sup_acc: 97.13 Train acc: 96.650 Test acc: 97.010 \n",
      "step: 3936 | Train: G_Loss: 1.038, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.608, D_sup_loss: 0.138, D_sup_acc: 97.05 Train acc: 96.610 Test acc: 97.000 \n",
      "step: 3937 | Train: G_Loss: 1.130, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.586, D_sup_loss: 0.138, D_sup_acc: 97.04 Train acc: 96.690 Test acc: 97.100 \n",
      "step: 3938 | Train: G_Loss: 1.005, D_unsup_loss_real: 0.541, D_unsup_loss_fake: 0.610, D_sup_loss: 0.135, D_sup_acc: 97.14 Train acc: 96.710 Test acc: 97.100 \n",
      "step: 3939 | Train: G_Loss: 1.073, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.592, D_sup_loss: 0.136, D_sup_acc: 97.14 Train acc: 96.735 Test acc: 97.110 \n",
      "step: 3940 | Train: G_Loss: 1.149, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.580, D_sup_loss: 0.135, D_sup_acc: 97.15 Train acc: 96.720 Test acc: 97.130 \n",
      "step: 3941 | Train: G_Loss: 1.165, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.613, D_sup_loss: 0.136, D_sup_acc: 97.17 Train acc: 96.642 Test acc: 97.000 \n",
      "step: 3942 | Train: G_Loss: 1.164, D_unsup_loss_real: 0.550, D_unsup_loss_fake: 0.544, D_sup_loss: 0.140, D_sup_acc: 97.04 Train acc: 96.735 Test acc: 97.170 \n",
      "step: 3943 | Train: G_Loss: 1.203, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.660, D_sup_loss: 0.135, D_sup_acc: 97.21 Train acc: 96.383 Test acc: 96.690 \n",
      "step: 3944 | Train: G_Loss: 1.182, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.610, D_sup_loss: 0.148, D_sup_acc: 96.73 Train acc: 96.763 Test acc: 97.150 \n",
      "step: 3945 | Train: G_Loss: 1.059, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.600, D_sup_loss: 0.132, D_sup_acc: 97.19 Train acc: 96.670 Test acc: 96.960 \n",
      "step: 3946 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.572, D_sup_loss: 0.138, D_sup_acc: 97.00 Train acc: 96.675 Test acc: 97.070 \n",
      "step: 3947 | Train: G_Loss: 1.183, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.578, D_sup_loss: 0.138, D_sup_acc: 97.11 Train acc: 96.823 Test acc: 97.130 \n",
      "step: 3948 | Train: G_Loss: 1.092, D_unsup_loss_real: 0.628, D_unsup_loss_fake: 0.617, D_sup_loss: 0.129, D_sup_acc: 97.17 Train acc: 96.755 Test acc: 97.080 \n",
      "step: 3949 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.551, D_sup_loss: 0.133, D_sup_acc: 97.12 Train acc: 96.895 Test acc: 97.160 \n",
      "step: 3950 | Train: G_Loss: 1.119, D_unsup_loss_real: 0.537, D_unsup_loss_fake: 0.564, D_sup_loss: 0.127, D_sup_acc: 97.20 Train acc: 96.900 Test acc: 97.090 \n",
      "step: 3951 | Train: G_Loss: 1.152, D_unsup_loss_real: 0.685, D_unsup_loss_fake: 0.582, D_sup_loss: 0.127, D_sup_acc: 97.13 Train acc: 96.757 Test acc: 96.960 \n",
      "step: 3952 | Train: G_Loss: 1.187, D_unsup_loss_real: 0.660, D_unsup_loss_fake: 0.563, D_sup_loss: 0.136, D_sup_acc: 97.00 Train acc: 96.737 Test acc: 96.980 \n",
      "step: 3953 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.599, D_sup_loss: 0.138, D_sup_acc: 97.02 Train acc: 96.667 Test acc: 96.920 \n",
      "step: 3954 | Train: G_Loss: 1.163, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.562, D_sup_loss: 0.142, D_sup_acc: 96.96 Train acc: 96.817 Test acc: 96.980 \n",
      "step: 3955 | Train: G_Loss: 1.037, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.593, D_sup_loss: 0.131, D_sup_acc: 97.02 Train acc: 96.755 Test acc: 97.030 \n",
      "step: 3956 | Train: G_Loss: 1.015, D_unsup_loss_real: 0.658, D_unsup_loss_fake: 0.619, D_sup_loss: 0.136, D_sup_acc: 97.07 Train acc: 96.467 Test acc: 96.890 \n",
      "step: 3957 | Train: G_Loss: 1.039, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.587, D_sup_loss: 0.147, D_sup_acc: 96.93 Train acc: 96.655 Test acc: 96.990 \n",
      "step: 3958 | Train: G_Loss: 1.099, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.584, D_sup_loss: 0.138, D_sup_acc: 97.03 Train acc: 96.733 Test acc: 96.980 \n",
      "step: 3959 | Train: G_Loss: 1.120, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.596, D_sup_loss: 0.134, D_sup_acc: 97.02 Train acc: 96.725 Test acc: 96.910 \n",
      "step: 3960 | Train: G_Loss: 1.007, D_unsup_loss_real: 0.642, D_unsup_loss_fake: 0.611, D_sup_loss: 0.134, D_sup_acc: 96.95 Train acc: 96.780 Test acc: 97.080 \n",
      "step: 3961 | Train: G_Loss: 1.061, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.640, D_sup_loss: 0.133, D_sup_acc: 97.12 Train acc: 96.562 Test acc: 96.900 \n",
      "step: 3962 | Train: G_Loss: 1.137, D_unsup_loss_real: 0.524, D_unsup_loss_fake: 0.600, D_sup_loss: 0.142, D_sup_acc: 96.94 Train acc: 96.663 Test acc: 96.980 \n",
      "step: 3963 | Train: G_Loss: 1.260, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.607, D_sup_loss: 0.137, D_sup_acc: 97.02 Train acc: 96.672 Test acc: 96.980 \n",
      "step: 3964 | Train: G_Loss: 1.207, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.575, D_sup_loss: 0.136, D_sup_acc: 97.02 Train acc: 96.810 Test acc: 97.110 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3965 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.636, D_sup_loss: 0.130, D_sup_acc: 97.15 Train acc: 96.805 Test acc: 97.100 \n",
      "step: 3966 | Train: G_Loss: 1.141, D_unsup_loss_real: 0.646, D_unsup_loss_fake: 0.658, D_sup_loss: 0.127, D_sup_acc: 97.14 Train acc: 96.557 Test acc: 96.890 \n",
      "step: 3967 | Train: G_Loss: 1.063, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.581, D_sup_loss: 0.141, D_sup_acc: 96.93 Train acc: 96.390 Test acc: 96.650 \n",
      "step: 3968 | Train: G_Loss: 1.052, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.633, D_sup_loss: 0.151, D_sup_acc: 96.69 Train acc: 96.642 Test acc: 96.980 \n",
      "step: 3969 | Train: G_Loss: 1.136, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.612, D_sup_loss: 0.136, D_sup_acc: 97.02 Train acc: 96.732 Test acc: 96.950 \n",
      "step: 3970 | Train: G_Loss: 1.122, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.602, D_sup_loss: 0.132, D_sup_acc: 96.99 Train acc: 96.732 Test acc: 96.970 \n",
      "step: 3971 | Train: G_Loss: 1.002, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.623, D_sup_loss: 0.131, D_sup_acc: 97.01 Train acc: 96.673 Test acc: 96.990 \n",
      "step: 3972 | Train: G_Loss: 1.132, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.661, D_sup_loss: 0.134, D_sup_acc: 97.03 Train acc: 96.392 Test acc: 96.800 \n",
      "step: 3973 | Train: G_Loss: 1.035, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.623, D_sup_loss: 0.149, D_sup_acc: 96.84 Train acc: 96.818 Test acc: 97.150 \n",
      "step: 3974 | Train: G_Loss: 1.107, D_unsup_loss_real: 0.544, D_unsup_loss_fake: 0.647, D_sup_loss: 0.130, D_sup_acc: 97.19 Train acc: 96.735 Test acc: 97.030 \n",
      "step: 3975 | Train: G_Loss: 1.238, D_unsup_loss_real: 0.512, D_unsup_loss_fake: 0.577, D_sup_loss: 0.132, D_sup_acc: 97.07 Train acc: 96.742 Test acc: 97.060 \n",
      "step: 3976 | Train: G_Loss: 1.149, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.553, D_sup_loss: 0.131, D_sup_acc: 97.10 Train acc: 96.782 Test acc: 97.060 \n",
      "step: 3977 | Train: G_Loss: 1.180, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.601, D_sup_loss: 0.130, D_sup_acc: 97.10 Train acc: 96.492 Test acc: 96.620 \n",
      "step: 3978 | Train: G_Loss: 1.084, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.563, D_sup_loss: 0.143, D_sup_acc: 96.66 Train acc: 96.650 Test acc: 97.020 \n",
      "step: 3979 | Train: G_Loss: 1.076, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.633, D_sup_loss: 0.135, D_sup_acc: 97.06 Train acc: 96.797 Test acc: 97.000 \n",
      "step: 3980 | Train: G_Loss: 1.092, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.615, D_sup_loss: 0.130, D_sup_acc: 97.04 Train acc: 96.787 Test acc: 96.980 \n",
      "step: 3981 | Train: G_Loss: 1.181, D_unsup_loss_real: 0.711, D_unsup_loss_fake: 0.567, D_sup_loss: 0.130, D_sup_acc: 97.02 Train acc: 96.788 Test acc: 97.040 \n",
      "step: 3982 | Train: G_Loss: 1.063, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.554, D_sup_loss: 0.132, D_sup_acc: 97.08 Train acc: 96.802 Test acc: 97.060 \n",
      "step: 3983 | Train: G_Loss: 1.103, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.624, D_sup_loss: 0.129, D_sup_acc: 97.10 Train acc: 96.527 Test acc: 96.830 \n",
      "step: 3984 | Train: G_Loss: 1.048, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.619, D_sup_loss: 0.143, D_sup_acc: 96.87 Train acc: 96.542 Test acc: 96.830 \n",
      "step: 3985 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.559, D_sup_loss: 0.143, D_sup_acc: 96.87 Train acc: 96.543 Test acc: 96.840 \n",
      "step: 3986 | Train: G_Loss: 1.125, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.627, D_sup_loss: 0.139, D_sup_acc: 96.88 Train acc: 96.717 Test acc: 96.960 \n",
      "step: 3987 | Train: G_Loss: 1.147, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.655, D_sup_loss: 0.133, D_sup_acc: 97.00 Train acc: 96.655 Test acc: 96.870 \n",
      "step: 3988 | Train: G_Loss: 1.105, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.653, D_sup_loss: 0.137, D_sup_acc: 96.91 Train acc: 96.730 Test acc: 96.890 \n",
      "step: 3989 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.599, D_sup_loss: 0.135, D_sup_acc: 96.93 Train acc: 96.577 Test acc: 96.750 \n",
      "step: 3990 | Train: G_Loss: 1.070, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.579, D_sup_loss: 0.143, D_sup_acc: 96.79 Train acc: 96.703 Test acc: 96.920 \n",
      "step: 3991 | Train: G_Loss: 1.082, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.580, D_sup_loss: 0.135, D_sup_acc: 96.96 Train acc: 96.720 Test acc: 96.940 \n",
      "step: 3992 | Train: G_Loss: 1.082, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.628, D_sup_loss: 0.136, D_sup_acc: 96.98 Train acc: 96.663 Test acc: 96.830 \n",
      "step: 3993 | Train: G_Loss: 1.159, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.605, D_sup_loss: 0.142, D_sup_acc: 96.87 Train acc: 96.765 Test acc: 96.910 \n",
      "step: 3994 | Train: G_Loss: 1.067, D_unsup_loss_real: 0.654, D_unsup_loss_fake: 0.546, D_sup_loss: 0.135, D_sup_acc: 96.95 Train acc: 96.440 Test acc: 96.680 \n",
      "step: 3995 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.687, D_unsup_loss_fake: 0.647, D_sup_loss: 0.151, D_sup_acc: 96.72 Train acc: 96.750 Test acc: 96.930 \n",
      "step: 3996 | Train: G_Loss: 1.100, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.694, D_sup_loss: 0.134, D_sup_acc: 96.97 Train acc: 96.683 Test acc: 96.900 \n",
      "step: 3997 | Train: G_Loss: 0.993, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.583, D_sup_loss: 0.137, D_sup_acc: 96.94 Train acc: 96.720 Test acc: 96.870 \n",
      "step: 3998 | Train: G_Loss: 0.993, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.635, D_sup_loss: 0.136, D_sup_acc: 96.91 Train acc: 96.695 Test acc: 96.870 \n",
      "step: 3999 | Train: G_Loss: 1.158, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.616, D_sup_loss: 0.136, D_sup_acc: 96.91 Train acc: 96.523 Test acc: 96.800 \n",
      "step: 4000 | Train: G_Loss: 1.130, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.617, D_sup_loss: 0.144, D_sup_acc: 96.84 Train acc: 96.253 Test acc: 96.650 \n",
      "Train Classifier Accuracy: 96.253%\n",
      "\n",
      "Test Classifier Accuracy: 96.650%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_4000.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_4000.h5\n",
      "step: 4001 | Train: G_Loss: 1.202, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.499, D_sup_loss: 0.155, D_sup_acc: 96.69 Train acc: 96.705 Test acc: 96.850 \n",
      "step: 4002 | Train: G_Loss: 1.018, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.628, D_sup_loss: 0.133, D_sup_acc: 96.89 Train acc: 96.527 Test acc: 96.760 \n",
      "step: 4003 | Train: G_Loss: 1.129, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.639, D_sup_loss: 0.143, D_sup_acc: 96.80 Train acc: 96.590 Test acc: 96.930 \n",
      "step: 4004 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.563, D_sup_loss: 0.138, D_sup_acc: 96.97 Train acc: 96.615 Test acc: 96.840 \n",
      "step: 4005 | Train: G_Loss: 1.074, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.596, D_sup_loss: 0.133, D_sup_acc: 96.88 Train acc: 96.600 Test acc: 96.810 \n",
      "step: 4006 | Train: G_Loss: 1.111, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.632, D_sup_loss: 0.135, D_sup_acc: 96.85 Train acc: 96.480 Test acc: 96.720 \n",
      "step: 4007 | Train: G_Loss: 1.177, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.589, D_sup_loss: 0.141, D_sup_acc: 96.76 Train acc: 96.442 Test acc: 96.700 \n",
      "step: 4008 | Train: G_Loss: 1.108, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.598, D_sup_loss: 0.141, D_sup_acc: 96.74 Train acc: 96.568 Test acc: 96.750 \n",
      "step: 4009 | Train: G_Loss: 1.075, D_unsup_loss_real: 0.666, D_unsup_loss_fake: 0.694, D_sup_loss: 0.135, D_sup_acc: 96.79 Train acc: 96.575 Test acc: 96.830 \n",
      "step: 4010 | Train: G_Loss: 1.065, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.643, D_sup_loss: 0.137, D_sup_acc: 96.87 Train acc: 96.570 Test acc: 96.750 \n",
      "step: 4011 | Train: G_Loss: 1.208, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.595, D_sup_loss: 0.139, D_sup_acc: 96.79 Train acc: 96.618 Test acc: 96.810 \n",
      "step: 4012 | Train: G_Loss: 1.072, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.629, D_sup_loss: 0.138, D_sup_acc: 96.85 Train acc: 96.532 Test acc: 96.860 \n",
      "step: 4013 | Train: G_Loss: 1.127, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.585, D_sup_loss: 0.140, D_sup_acc: 96.90 Train acc: 96.455 Test acc: 96.870 \n",
      "step: 4014 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.615, D_sup_loss: 0.143, D_sup_acc: 96.91 Train acc: 96.478 Test acc: 96.780 \n",
      "step: 4015 | Train: G_Loss: 1.154, D_unsup_loss_real: 0.533, D_unsup_loss_fake: 0.601, D_sup_loss: 0.143, D_sup_acc: 96.82 Train acc: 96.468 Test acc: 96.800 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4016 | Train: G_Loss: 1.032, D_unsup_loss_real: 0.685, D_unsup_loss_fake: 0.693, D_sup_loss: 0.139, D_sup_acc: 96.84 Train acc: 96.302 Test acc: 96.700 \n",
      "step: 4017 | Train: G_Loss: 1.162, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.605, D_sup_loss: 0.149, D_sup_acc: 96.74 Train acc: 96.507 Test acc: 96.910 \n",
      "step: 4018 | Train: G_Loss: 1.099, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.638, D_sup_loss: 0.140, D_sup_acc: 96.95 Train acc: 96.597 Test acc: 96.870 \n",
      "step: 4019 | Train: G_Loss: 1.150, D_unsup_loss_real: 0.535, D_unsup_loss_fake: 0.633, D_sup_loss: 0.137, D_sup_acc: 96.91 Train acc: 96.552 Test acc: 96.920 \n",
      "step: 4020 | Train: G_Loss: 1.123, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.570, D_sup_loss: 0.141, D_sup_acc: 96.96 Train acc: 96.710 Test acc: 96.870 \n",
      "step: 4021 | Train: G_Loss: 1.070, D_unsup_loss_real: 0.676, D_unsup_loss_fake: 0.621, D_sup_loss: 0.131, D_sup_acc: 96.91 Train acc: 96.672 Test acc: 96.970 \n",
      "step: 4022 | Train: G_Loss: 1.166, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.630, D_sup_loss: 0.136, D_sup_acc: 97.01 Train acc: 96.578 Test acc: 96.900 \n",
      "step: 4023 | Train: G_Loss: 1.176, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.614, D_sup_loss: 0.142, D_sup_acc: 96.94 Train acc: 96.575 Test acc: 96.900 \n",
      "step: 4024 | Train: G_Loss: 1.106, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.597, D_sup_loss: 0.140, D_sup_acc: 96.94 Train acc: 96.650 Test acc: 96.830 \n",
      "step: 4025 | Train: G_Loss: 1.178, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.679, D_sup_loss: 0.136, D_sup_acc: 96.87 Train acc: 96.500 Test acc: 96.870 \n",
      "step: 4026 | Train: G_Loss: 1.101, D_unsup_loss_real: 0.662, D_unsup_loss_fake: 0.602, D_sup_loss: 0.142, D_sup_acc: 96.91 Train acc: 96.513 Test acc: 96.710 \n",
      "step: 4027 | Train: G_Loss: 1.173, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.565, D_sup_loss: 0.141, D_sup_acc: 96.75 Train acc: 96.625 Test acc: 96.730 \n",
      "step: 4028 | Train: G_Loss: 1.030, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.656, D_sup_loss: 0.138, D_sup_acc: 96.77 Train acc: 96.430 Test acc: 96.670 \n",
      "step: 4029 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.591, D_sup_loss: 0.147, D_sup_acc: 96.71 Train acc: 96.208 Test acc: 96.500 \n",
      "step: 4030 | Train: G_Loss: 1.150, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.615, D_sup_loss: 0.157, D_sup_acc: 96.54 Train acc: 96.492 Test acc: 96.850 \n",
      "step: 4031 | Train: G_Loss: 1.118, D_unsup_loss_real: 0.654, D_unsup_loss_fake: 0.675, D_sup_loss: 0.144, D_sup_acc: 96.89 Train acc: 96.088 Test acc: 96.470 \n",
      "step: 4032 | Train: G_Loss: 1.076, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.547, D_sup_loss: 0.166, D_sup_acc: 96.51 Train acc: 96.268 Test acc: 96.640 \n",
      "step: 4033 | Train: G_Loss: 1.123, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.596, D_sup_loss: 0.156, D_sup_acc: 96.68 Train acc: 96.537 Test acc: 96.830 \n",
      "step: 4034 | Train: G_Loss: 1.200, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.604, D_sup_loss: 0.144, D_sup_acc: 96.87 Train acc: 96.718 Test acc: 97.030 \n",
      "step: 4035 | Train: G_Loss: 1.237, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.571, D_sup_loss: 0.137, D_sup_acc: 97.07 Train acc: 96.672 Test acc: 96.960 \n",
      "step: 4036 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.568, D_sup_loss: 0.135, D_sup_acc: 97.00 Train acc: 96.390 Test acc: 96.790 \n",
      "step: 4037 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.574, D_sup_loss: 0.147, D_sup_acc: 96.83 Train acc: 96.467 Test acc: 96.800 \n",
      "step: 4038 | Train: G_Loss: 1.136, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.537, D_sup_loss: 0.144, D_sup_acc: 96.84 Train acc: 96.650 Test acc: 96.910 \n",
      "step: 4039 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.644, D_sup_loss: 0.133, D_sup_acc: 96.95 Train acc: 96.698 Test acc: 96.850 \n",
      "step: 4040 | Train: G_Loss: 1.187, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.572, D_sup_loss: 0.135, D_sup_acc: 96.89 Train acc: 96.763 Test acc: 97.000 \n",
      "step: 4041 | Train: G_Loss: 1.119, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.560, D_sup_loss: 0.132, D_sup_acc: 97.04 Train acc: 96.730 Test acc: 97.010 \n",
      "step: 4042 | Train: G_Loss: 1.071, D_unsup_loss_real: 0.662, D_unsup_loss_fake: 0.579, D_sup_loss: 0.132, D_sup_acc: 97.05 Train acc: 96.735 Test acc: 97.020 \n",
      "step: 4043 | Train: G_Loss: 1.078, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.645, D_sup_loss: 0.134, D_sup_acc: 97.06 Train acc: 96.553 Test acc: 96.740 \n",
      "step: 4044 | Train: G_Loss: 1.114, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.602, D_sup_loss: 0.145, D_sup_acc: 96.78 Train acc: 96.580 Test acc: 96.750 \n",
      "step: 4045 | Train: G_Loss: 1.099, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.598, D_sup_loss: 0.143, D_sup_acc: 96.79 Train acc: 96.598 Test acc: 96.860 \n",
      "step: 4046 | Train: G_Loss: 1.146, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.594, D_sup_loss: 0.142, D_sup_acc: 96.90 Train acc: 96.382 Test acc: 96.670 \n",
      "step: 4047 | Train: G_Loss: 1.115, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.579, D_sup_loss: 0.153, D_sup_acc: 96.71 Train acc: 96.662 Test acc: 97.050 \n",
      "step: 4048 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.586, D_sup_loss: 0.138, D_sup_acc: 97.09 Train acc: 96.618 Test acc: 96.930 \n",
      "step: 4049 | Train: G_Loss: 1.132, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.639, D_sup_loss: 0.137, D_sup_acc: 96.97 Train acc: 96.487 Test acc: 96.810 \n",
      "step: 4050 | Train: G_Loss: 1.195, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.603, D_sup_loss: 0.147, D_sup_acc: 96.85 Train acc: 96.345 Test acc: 96.590 \n",
      "step: 4051 | Train: G_Loss: 1.115, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.554, D_sup_loss: 0.155, D_sup_acc: 96.63 Train acc: 96.588 Test acc: 96.770 \n",
      "step: 4052 | Train: G_Loss: 1.214, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.524, D_sup_loss: 0.142, D_sup_acc: 96.81 Train acc: 96.670 Test acc: 96.750 \n",
      "step: 4053 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.603, D_sup_loss: 0.137, D_sup_acc: 96.79 Train acc: 96.567 Test acc: 96.760 \n",
      "step: 4054 | Train: G_Loss: 1.053, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.588, D_sup_loss: 0.142, D_sup_acc: 96.80 Train acc: 96.507 Test acc: 96.730 \n",
      "step: 4055 | Train: G_Loss: 1.109, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.551, D_sup_loss: 0.146, D_sup_acc: 96.77 Train acc: 96.700 Test acc: 96.780 \n",
      "step: 4056 | Train: G_Loss: 1.069, D_unsup_loss_real: 0.662, D_unsup_loss_fake: 0.585, D_sup_loss: 0.134, D_sup_acc: 96.82 Train acc: 96.733 Test acc: 96.870 \n",
      "step: 4057 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.569, D_sup_loss: 0.132, D_sup_acc: 96.91 Train acc: 96.675 Test acc: 96.840 \n",
      "step: 4058 | Train: G_Loss: 1.058, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.608, D_sup_loss: 0.135, D_sup_acc: 96.88 Train acc: 96.562 Test acc: 96.780 \n",
      "step: 4059 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.573, D_sup_loss: 0.138, D_sup_acc: 96.82 Train acc: 96.692 Test acc: 96.920 \n",
      "step: 4060 | Train: G_Loss: 1.100, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.622, D_sup_loss: 0.130, D_sup_acc: 96.96 Train acc: 96.690 Test acc: 96.910 \n",
      "step: 4061 | Train: G_Loss: 1.243, D_unsup_loss_real: 0.521, D_unsup_loss_fake: 0.725, D_sup_loss: 0.130, D_sup_acc: 96.95 Train acc: 96.505 Test acc: 96.810 \n",
      "step: 4062 | Train: G_Loss: 1.113, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.585, D_sup_loss: 0.138, D_sup_acc: 96.85 Train acc: 96.585 Test acc: 96.850 \n",
      "step: 4063 | Train: G_Loss: 1.126, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.576, D_sup_loss: 0.137, D_sup_acc: 96.89 Train acc: 96.662 Test acc: 96.840 \n",
      "step: 4064 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.531, D_unsup_loss_fake: 0.576, D_sup_loss: 0.133, D_sup_acc: 96.88 Train acc: 96.588 Test acc: 96.840 \n",
      "step: 4065 | Train: G_Loss: 1.115, D_unsup_loss_real: 0.664, D_unsup_loss_fake: 0.574, D_sup_loss: 0.134, D_sup_acc: 96.88 Train acc: 96.492 Test acc: 96.750 \n",
      "step: 4066 | Train: G_Loss: 1.159, D_unsup_loss_real: 0.646, D_unsup_loss_fake: 0.595, D_sup_loss: 0.139, D_sup_acc: 96.79 Train acc: 96.562 Test acc: 96.690 \n",
      "step: 4067 | Train: G_Loss: 1.128, D_unsup_loss_real: 0.668, D_unsup_loss_fake: 0.625, D_sup_loss: 0.136, D_sup_acc: 96.73 Train acc: 96.458 Test acc: 96.770 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4068 | Train: G_Loss: 1.128, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.620, D_sup_loss: 0.144, D_sup_acc: 96.81 Train acc: 96.647 Test acc: 96.920 \n",
      "step: 4069 | Train: G_Loss: 1.183, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.615, D_sup_loss: 0.132, D_sup_acc: 96.96 Train acc: 96.498 Test acc: 96.830 \n",
      "step: 4070 | Train: G_Loss: 1.067, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.656, D_sup_loss: 0.140, D_sup_acc: 96.87 Train acc: 96.525 Test acc: 96.800 \n",
      "step: 4071 | Train: G_Loss: 1.136, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.634, D_sup_loss: 0.139, D_sup_acc: 96.84 Train acc: 96.652 Test acc: 96.940 \n",
      "step: 4072 | Train: G_Loss: 1.083, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.622, D_sup_loss: 0.138, D_sup_acc: 96.98 Train acc: 96.722 Test acc: 96.910 \n",
      "step: 4073 | Train: G_Loss: 1.071, D_unsup_loss_real: 0.658, D_unsup_loss_fake: 0.603, D_sup_loss: 0.130, D_sup_acc: 96.95 Train acc: 96.578 Test acc: 96.770 \n",
      "step: 4074 | Train: G_Loss: 1.077, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.620, D_sup_loss: 0.142, D_sup_acc: 96.81 Train acc: 96.578 Test acc: 96.830 \n",
      "step: 4075 | Train: G_Loss: 1.153, D_unsup_loss_real: 0.628, D_unsup_loss_fake: 0.630, D_sup_loss: 0.141, D_sup_acc: 96.87 Train acc: 96.410 Test acc: 96.670 \n",
      "step: 4076 | Train: G_Loss: 1.073, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.559, D_sup_loss: 0.150, D_sup_acc: 96.71 Train acc: 96.495 Test acc: 96.750 \n",
      "step: 4077 | Train: G_Loss: 1.063, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.636, D_sup_loss: 0.142, D_sup_acc: 96.79 Train acc: 96.345 Test acc: 96.680 \n",
      "step: 4078 | Train: G_Loss: 1.193, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.631, D_sup_loss: 0.148, D_sup_acc: 96.72 Train acc: 96.248 Test acc: 96.630 \n",
      "step: 4079 | Train: G_Loss: 1.201, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.577, D_sup_loss: 0.153, D_sup_acc: 96.67 Train acc: 96.675 Test acc: 96.850 \n",
      "step: 4080 | Train: G_Loss: 1.181, D_unsup_loss_real: 0.523, D_unsup_loss_fake: 0.600, D_sup_loss: 0.134, D_sup_acc: 96.89 Train acc: 96.538 Test acc: 96.830 \n",
      "step: 4081 | Train: G_Loss: 1.214, D_unsup_loss_real: 0.673, D_unsup_loss_fake: 0.664, D_sup_loss: 0.142, D_sup_acc: 96.87 Train acc: 96.555 Test acc: 96.860 \n",
      "step: 4082 | Train: G_Loss: 1.031, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.589, D_sup_loss: 0.143, D_sup_acc: 96.90 Train acc: 96.497 Test acc: 96.790 \n",
      "step: 4083 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.507, D_sup_loss: 0.144, D_sup_acc: 96.83 Train acc: 96.573 Test acc: 96.840 \n",
      "step: 4084 | Train: G_Loss: 1.113, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.630, D_sup_loss: 0.136, D_sup_acc: 96.88 Train acc: 96.385 Test acc: 96.800 \n",
      "step: 4085 | Train: G_Loss: 1.137, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.567, D_sup_loss: 0.142, D_sup_acc: 96.84 Train acc: 96.290 Test acc: 96.690 \n",
      "step: 4086 | Train: G_Loss: 1.181, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.516, D_sup_loss: 0.145, D_sup_acc: 96.73 Train acc: 96.567 Test acc: 96.920 \n",
      "step: 4087 | Train: G_Loss: 1.158, D_unsup_loss_real: 0.665, D_unsup_loss_fake: 0.628, D_sup_loss: 0.130, D_sup_acc: 96.96 Train acc: 96.523 Test acc: 96.990 \n",
      "step: 4088 | Train: G_Loss: 1.226, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.612, D_sup_loss: 0.134, D_sup_acc: 97.03 Train acc: 96.507 Test acc: 96.920 \n",
      "step: 4089 | Train: G_Loss: 1.270, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.583, D_sup_loss: 0.136, D_sup_acc: 96.96 Train acc: 96.485 Test acc: 96.820 \n",
      "step: 4090 | Train: G_Loss: 1.211, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.623, D_sup_loss: 0.139, D_sup_acc: 96.86 Train acc: 96.572 Test acc: 96.900 \n",
      "step: 4091 | Train: G_Loss: 1.123, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.565, D_sup_loss: 0.136, D_sup_acc: 96.94 Train acc: 96.678 Test acc: 97.060 \n",
      "step: 4092 | Train: G_Loss: 1.210, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.596, D_sup_loss: 0.131, D_sup_acc: 97.10 Train acc: 96.662 Test acc: 97.040 \n",
      "step: 4093 | Train: G_Loss: 1.062, D_unsup_loss_real: 0.646, D_unsup_loss_fake: 0.576, D_sup_loss: 0.133, D_sup_acc: 97.08 Train acc: 96.753 Test acc: 97.000 \n",
      "step: 4094 | Train: G_Loss: 1.206, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.592, D_sup_loss: 0.127, D_sup_acc: 97.04 Train acc: 96.810 Test acc: 96.970 \n",
      "step: 4095 | Train: G_Loss: 1.067, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.639, D_sup_loss: 0.125, D_sup_acc: 97.01 Train acc: 96.740 Test acc: 97.090 \n",
      "step: 4096 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.632, D_sup_loss: 0.129, D_sup_acc: 97.13 Train acc: 96.563 Test acc: 97.080 \n",
      "step: 4097 | Train: G_Loss: 1.075, D_unsup_loss_real: 0.648, D_unsup_loss_fake: 0.572, D_sup_loss: 0.138, D_sup_acc: 97.12 Train acc: 96.738 Test acc: 97.080 \n",
      "step: 4098 | Train: G_Loss: 1.185, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.640, D_sup_loss: 0.131, D_sup_acc: 97.12 Train acc: 96.718 Test acc: 97.020 \n",
      "step: 4099 | Train: G_Loss: 1.171, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.657, D_sup_loss: 0.134, D_sup_acc: 97.06 Train acc: 96.728 Test acc: 97.010 \n",
      "step: 4100 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.654, D_unsup_loss_fake: 0.592, D_sup_loss: 0.134, D_sup_acc: 97.05 Train acc: 96.562 Test acc: 96.850 \n",
      "Train Classifier Accuracy: 96.562%\n",
      "\n",
      "Test Classifier Accuracy: 96.850%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_4100.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_4100.h5\n",
      "step: 4101 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.592, D_sup_loss: 0.143, D_sup_acc: 96.89 Train acc: 96.603 Test acc: 96.920 \n",
      "step: 4102 | Train: G_Loss: 1.150, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.658, D_sup_loss: 0.137, D_sup_acc: 96.96 Train acc: 96.538 Test acc: 96.860 \n",
      "step: 4103 | Train: G_Loss: 1.071, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.562, D_sup_loss: 0.141, D_sup_acc: 96.90 Train acc: 96.718 Test acc: 96.960 \n",
      "step: 4104 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.515, D_sup_loss: 0.131, D_sup_acc: 97.00 Train acc: 96.595 Test acc: 96.960 \n",
      "step: 4105 | Train: G_Loss: 1.150, D_unsup_loss_real: 0.631, D_unsup_loss_fake: 0.579, D_sup_loss: 0.137, D_sup_acc: 97.00 Train acc: 96.668 Test acc: 96.940 \n",
      "step: 4106 | Train: G_Loss: 1.137, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.628, D_sup_loss: 0.134, D_sup_acc: 96.98 Train acc: 96.543 Test acc: 96.920 \n",
      "step: 4107 | Train: G_Loss: 1.083, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.609, D_sup_loss: 0.138, D_sup_acc: 96.96 Train acc: 96.602 Test acc: 96.950 \n",
      "step: 4108 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.597, D_sup_loss: 0.136, D_sup_acc: 96.99 Train acc: 96.603 Test acc: 96.860 \n",
      "step: 4109 | Train: G_Loss: 1.085, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.587, D_sup_loss: 0.134, D_sup_acc: 96.90 Train acc: 96.483 Test acc: 96.930 \n",
      "step: 4110 | Train: G_Loss: 1.179, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.608, D_sup_loss: 0.143, D_sup_acc: 96.97 Train acc: 96.633 Test acc: 96.860 \n",
      "step: 4111 | Train: G_Loss: 1.177, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.622, D_sup_loss: 0.137, D_sup_acc: 96.90 Train acc: 96.607 Test acc: 96.830 \n",
      "step: 4112 | Train: G_Loss: 1.112, D_unsup_loss_real: 0.659, D_unsup_loss_fake: 0.619, D_sup_loss: 0.138, D_sup_acc: 96.87 Train acc: 96.640 Test acc: 96.820 \n",
      "step: 4113 | Train: G_Loss: 1.118, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.557, D_sup_loss: 0.137, D_sup_acc: 96.86 Train acc: 96.600 Test acc: 96.810 \n",
      "step: 4114 | Train: G_Loss: 1.186, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.607, D_sup_loss: 0.139, D_sup_acc: 96.85 Train acc: 96.547 Test acc: 96.800 \n",
      "step: 4115 | Train: G_Loss: 1.127, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.611, D_sup_loss: 0.141, D_sup_acc: 96.84 Train acc: 96.577 Test acc: 96.800 \n",
      "step: 4116 | Train: G_Loss: 1.040, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.547, D_sup_loss: 0.141, D_sup_acc: 96.84 Train acc: 96.595 Test acc: 96.880 \n",
      "step: 4117 | Train: G_Loss: 1.120, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.696, D_sup_loss: 0.137, D_sup_acc: 96.92 Train acc: 96.480 Test acc: 96.740 \n",
      "step: 4118 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.572, D_sup_loss: 0.144, D_sup_acc: 96.78 Train acc: 96.598 Test acc: 96.920 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4119 | Train: G_Loss: 1.043, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.571, D_sup_loss: 0.135, D_sup_acc: 96.96 Train acc: 96.710 Test acc: 96.970 \n",
      "step: 4120 | Train: G_Loss: 1.054, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.588, D_sup_loss: 0.131, D_sup_acc: 97.01 Train acc: 96.718 Test acc: 96.850 \n",
      "step: 4121 | Train: G_Loss: 1.094, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.625, D_sup_loss: 0.131, D_sup_acc: 96.89 Train acc: 96.497 Test acc: 96.770 \n",
      "step: 4122 | Train: G_Loss: 1.166, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.600, D_sup_loss: 0.144, D_sup_acc: 96.81 Train acc: 96.590 Test acc: 96.920 \n",
      "step: 4123 | Train: G_Loss: 1.212, D_unsup_loss_real: 0.528, D_unsup_loss_fake: 0.634, D_sup_loss: 0.136, D_sup_acc: 96.96 Train acc: 96.773 Test acc: 97.040 \n",
      "step: 4124 | Train: G_Loss: 1.109, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.537, D_sup_loss: 0.126, D_sup_acc: 97.08 Train acc: 96.790 Test acc: 97.120 \n",
      "step: 4125 | Train: G_Loss: 1.154, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.550, D_sup_loss: 0.128, D_sup_acc: 97.16 Train acc: 96.788 Test acc: 96.950 \n",
      "step: 4126 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.640, D_sup_loss: 0.125, D_sup_acc: 96.99 Train acc: 96.693 Test acc: 96.950 \n",
      "step: 4127 | Train: G_Loss: 1.083, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.651, D_sup_loss: 0.135, D_sup_acc: 96.99 Train acc: 96.783 Test acc: 96.970 \n",
      "step: 4128 | Train: G_Loss: 1.048, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.557, D_sup_loss: 0.131, D_sup_acc: 97.01 Train acc: 96.733 Test acc: 97.020 \n",
      "step: 4129 | Train: G_Loss: 1.182, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.626, D_sup_loss: 0.132, D_sup_acc: 97.06 Train acc: 96.790 Test acc: 96.960 \n",
      "step: 4130 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.555, D_sup_loss: 0.129, D_sup_acc: 97.00 Train acc: 96.770 Test acc: 96.980 \n",
      "step: 4131 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.554, D_sup_loss: 0.132, D_sup_acc: 97.02 Train acc: 96.747 Test acc: 97.020 \n",
      "step: 4132 | Train: G_Loss: 1.116, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.623, D_sup_loss: 0.132, D_sup_acc: 97.06 Train acc: 96.728 Test acc: 96.930 \n",
      "step: 4133 | Train: G_Loss: 1.099, D_unsup_loss_real: 0.661, D_unsup_loss_fake: 0.654, D_sup_loss: 0.137, D_sup_acc: 96.97 Train acc: 96.722 Test acc: 96.940 \n",
      "step: 4134 | Train: G_Loss: 0.982, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.674, D_sup_loss: 0.134, D_sup_acc: 96.98 Train acc: 96.660 Test acc: 96.790 \n",
      "step: 4135 | Train: G_Loss: 1.087, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.633, D_sup_loss: 0.142, D_sup_acc: 96.83 Train acc: 96.758 Test acc: 96.930 \n",
      "step: 4136 | Train: G_Loss: 1.193, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.568, D_sup_loss: 0.136, D_sup_acc: 96.97 Train acc: 96.562 Test acc: 96.810 \n",
      "step: 4137 | Train: G_Loss: 1.094, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.621, D_sup_loss: 0.144, D_sup_acc: 96.85 Train acc: 96.608 Test acc: 96.920 \n",
      "step: 4138 | Train: G_Loss: 1.170, D_unsup_loss_real: 0.537, D_unsup_loss_fake: 0.560, D_sup_loss: 0.141, D_sup_acc: 96.96 Train acc: 96.448 Test acc: 96.820 \n",
      "step: 4139 | Train: G_Loss: 1.147, D_unsup_loss_real: 0.665, D_unsup_loss_fake: 0.692, D_sup_loss: 0.144, D_sup_acc: 96.86 Train acc: 96.578 Test acc: 96.770 \n",
      "step: 4140 | Train: G_Loss: 1.154, D_unsup_loss_real: 0.684, D_unsup_loss_fake: 0.557, D_sup_loss: 0.143, D_sup_acc: 96.81 Train acc: 96.418 Test acc: 96.770 \n",
      "step: 4141 | Train: G_Loss: 1.082, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.624, D_sup_loss: 0.149, D_sup_acc: 96.81 Train acc: 96.422 Test acc: 96.750 \n",
      "step: 4142 | Train: G_Loss: 1.110, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.571, D_sup_loss: 0.150, D_sup_acc: 96.79 Train acc: 96.530 Test acc: 96.780 \n",
      "step: 4143 | Train: G_Loss: 1.085, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.563, D_sup_loss: 0.147, D_sup_acc: 96.82 Train acc: 96.627 Test acc: 96.880 \n",
      "step: 4144 | Train: G_Loss: 1.107, D_unsup_loss_real: 0.494, D_unsup_loss_fake: 0.613, D_sup_loss: 0.143, D_sup_acc: 96.92 Train acc: 96.592 Test acc: 96.910 \n",
      "step: 4145 | Train: G_Loss: 1.139, D_unsup_loss_real: 0.538, D_unsup_loss_fake: 0.662, D_sup_loss: 0.139, D_sup_acc: 96.95 Train acc: 96.588 Test acc: 96.960 \n",
      "step: 4146 | Train: G_Loss: 1.272, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.628, D_sup_loss: 0.137, D_sup_acc: 97.00 Train acc: 96.663 Test acc: 96.970 \n",
      "step: 4147 | Train: G_Loss: 1.133, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.599, D_sup_loss: 0.135, D_sup_acc: 97.01 Train acc: 96.547 Test acc: 96.870 \n",
      "step: 4148 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.661, D_unsup_loss_fake: 0.581, D_sup_loss: 0.141, D_sup_acc: 96.91 Train acc: 96.282 Test acc: 96.700 \n",
      "step: 4149 | Train: G_Loss: 1.056, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.601, D_sup_loss: 0.152, D_sup_acc: 96.74 Train acc: 96.295 Test acc: 96.770 \n",
      "step: 4150 | Train: G_Loss: 1.182, D_unsup_loss_real: 0.646, D_unsup_loss_fake: 0.685, D_sup_loss: 0.151, D_sup_acc: 96.81 Train acc: 96.673 Test acc: 97.010 \n",
      "step: 4151 | Train: G_Loss: 1.120, D_unsup_loss_real: 0.550, D_unsup_loss_fake: 0.644, D_sup_loss: 0.132, D_sup_acc: 97.05 Train acc: 96.558 Test acc: 96.950 \n",
      "step: 4152 | Train: G_Loss: 1.074, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.505, D_sup_loss: 0.142, D_sup_acc: 96.99 Train acc: 96.677 Test acc: 96.950 \n",
      "step: 4153 | Train: G_Loss: 1.113, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.603, D_sup_loss: 0.132, D_sup_acc: 96.99 Train acc: 96.622 Test acc: 96.920 \n",
      "step: 4154 | Train: G_Loss: 1.167, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.555, D_sup_loss: 0.140, D_sup_acc: 96.96 Train acc: 96.497 Test acc: 96.900 \n",
      "step: 4155 | Train: G_Loss: 1.181, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.559, D_sup_loss: 0.146, D_sup_acc: 96.94 Train acc: 96.435 Test acc: 96.800 \n",
      "step: 4156 | Train: G_Loss: 1.128, D_unsup_loss_real: 0.648, D_unsup_loss_fake: 0.565, D_sup_loss: 0.145, D_sup_acc: 96.84 Train acc: 96.702 Test acc: 97.010 \n",
      "step: 4157 | Train: G_Loss: 1.110, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.592, D_sup_loss: 0.134, D_sup_acc: 97.05 Train acc: 96.727 Test acc: 97.010 \n",
      "step: 4158 | Train: G_Loss: 1.106, D_unsup_loss_real: 0.698, D_unsup_loss_fake: 0.648, D_sup_loss: 0.130, D_sup_acc: 97.05 Train acc: 96.700 Test acc: 97.090 \n",
      "step: 4159 | Train: G_Loss: 1.162, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.601, D_sup_loss: 0.133, D_sup_acc: 97.13 Train acc: 96.690 Test acc: 97.130 \n",
      "step: 4160 | Train: G_Loss: 0.999, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.727, D_sup_loss: 0.134, D_sup_acc: 97.17 Train acc: 96.603 Test acc: 97.030 \n",
      "step: 4161 | Train: G_Loss: 1.056, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.597, D_sup_loss: 0.138, D_sup_acc: 97.07 Train acc: 96.588 Test acc: 97.050 \n",
      "step: 4162 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.534, D_sup_loss: 0.141, D_sup_acc: 97.09 Train acc: 96.805 Test acc: 96.850 \n",
      "step: 4163 | Train: G_Loss: 1.070, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.654, D_sup_loss: 0.128, D_sup_acc: 96.89 Train acc: 96.730 Test acc: 97.030 \n",
      "step: 4164 | Train: G_Loss: 1.153, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.537, D_sup_loss: 0.134, D_sup_acc: 97.07 Train acc: 96.678 Test acc: 97.150 \n",
      "step: 4165 | Train: G_Loss: 1.189, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.610, D_sup_loss: 0.132, D_sup_acc: 97.19 Train acc: 96.673 Test acc: 97.060 \n",
      "step: 4166 | Train: G_Loss: 1.162, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.616, D_sup_loss: 0.134, D_sup_acc: 97.10 Train acc: 96.618 Test acc: 97.070 \n",
      "step: 4167 | Train: G_Loss: 1.119, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.536, D_sup_loss: 0.136, D_sup_acc: 97.11 Train acc: 96.740 Test acc: 97.100 \n",
      "step: 4168 | Train: G_Loss: 1.057, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.562, D_sup_loss: 0.130, D_sup_acc: 97.14 Train acc: 96.747 Test acc: 97.030 \n",
      "step: 4169 | Train: G_Loss: 1.003, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.629, D_sup_loss: 0.131, D_sup_acc: 97.07 Train acc: 96.772 Test acc: 97.010 \n",
      "step: 4170 | Train: G_Loss: 1.100, D_unsup_loss_real: 0.498, D_unsup_loss_fake: 0.587, D_sup_loss: 0.131, D_sup_acc: 97.05 Train acc: 96.743 Test acc: 97.010 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4171 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.657, D_unsup_loss_fake: 0.610, D_sup_loss: 0.132, D_sup_acc: 97.05 Train acc: 96.583 Test acc: 97.070 \n",
      "step: 4172 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.619, D_sup_loss: 0.139, D_sup_acc: 97.11 Train acc: 96.498 Test acc: 96.990 \n",
      "step: 4173 | Train: G_Loss: 1.085, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.607, D_sup_loss: 0.142, D_sup_acc: 97.03 Train acc: 96.623 Test acc: 97.070 \n",
      "step: 4174 | Train: G_Loss: 1.097, D_unsup_loss_real: 0.631, D_unsup_loss_fake: 0.614, D_sup_loss: 0.136, D_sup_acc: 97.11 Train acc: 96.723 Test acc: 97.140 \n",
      "step: 4175 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.536, D_sup_loss: 0.131, D_sup_acc: 97.18 Train acc: 96.710 Test acc: 97.130 \n",
      "step: 4176 | Train: G_Loss: 1.149, D_unsup_loss_real: 0.646, D_unsup_loss_fake: 0.562, D_sup_loss: 0.132, D_sup_acc: 97.17 Train acc: 96.797 Test acc: 96.980 \n",
      "step: 4177 | Train: G_Loss: 1.058, D_unsup_loss_real: 0.709, D_unsup_loss_fake: 0.600, D_sup_loss: 0.129, D_sup_acc: 97.02 Train acc: 96.403 Test acc: 96.890 \n",
      "step: 4178 | Train: G_Loss: 1.125, D_unsup_loss_real: 0.526, D_unsup_loss_fake: 0.553, D_sup_loss: 0.148, D_sup_acc: 96.93 Train acc: 96.513 Test acc: 96.920 \n",
      "step: 4179 | Train: G_Loss: 1.112, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.606, D_sup_loss: 0.142, D_sup_acc: 96.96 Train acc: 96.547 Test acc: 96.920 \n",
      "step: 4180 | Train: G_Loss: 1.091, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.616, D_sup_loss: 0.138, D_sup_acc: 96.96 Train acc: 96.595 Test acc: 97.050 \n",
      "step: 4181 | Train: G_Loss: 1.091, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.627, D_sup_loss: 0.134, D_sup_acc: 97.09 Train acc: 96.620 Test acc: 97.100 \n",
      "step: 4182 | Train: G_Loss: 1.066, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.620, D_sup_loss: 0.132, D_sup_acc: 97.14 Train acc: 96.633 Test acc: 97.150 \n",
      "step: 4183 | Train: G_Loss: 1.105, D_unsup_loss_real: 0.663, D_unsup_loss_fake: 0.625, D_sup_loss: 0.132, D_sup_acc: 97.19 Train acc: 96.592 Test acc: 97.120 \n",
      "step: 4184 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.647, D_sup_loss: 0.134, D_sup_acc: 97.16 Train acc: 96.545 Test acc: 97.040 \n",
      "step: 4185 | Train: G_Loss: 1.084, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.637, D_sup_loss: 0.138, D_sup_acc: 97.08 Train acc: 96.600 Test acc: 97.060 \n",
      "step: 4186 | Train: G_Loss: 1.108, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.547, D_sup_loss: 0.138, D_sup_acc: 97.10 Train acc: 96.720 Test acc: 97.090 \n",
      "step: 4187 | Train: G_Loss: 1.104, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.548, D_sup_loss: 0.133, D_sup_acc: 97.13 Train acc: 96.660 Test acc: 97.040 \n",
      "step: 4188 | Train: G_Loss: 1.092, D_unsup_loss_real: 0.544, D_unsup_loss_fake: 0.544, D_sup_loss: 0.135, D_sup_acc: 97.08 Train acc: 96.635 Test acc: 97.100 \n",
      "step: 4189 | Train: G_Loss: 1.119, D_unsup_loss_real: 0.639, D_unsup_loss_fake: 0.662, D_sup_loss: 0.137, D_sup_acc: 97.14 Train acc: 96.660 Test acc: 97.050 \n",
      "step: 4190 | Train: G_Loss: 1.151, D_unsup_loss_real: 0.650, D_unsup_loss_fake: 0.621, D_sup_loss: 0.135, D_sup_acc: 97.09 Train acc: 96.530 Test acc: 96.940 \n",
      "step: 4191 | Train: G_Loss: 1.180, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.657, D_sup_loss: 0.142, D_sup_acc: 96.98 Train acc: 96.718 Test acc: 97.070 \n",
      "step: 4192 | Train: G_Loss: 1.078, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.654, D_sup_loss: 0.137, D_sup_acc: 97.11 Train acc: 96.572 Test acc: 97.030 \n",
      "step: 4193 | Train: G_Loss: 1.025, D_unsup_loss_real: 0.689, D_unsup_loss_fake: 0.568, D_sup_loss: 0.144, D_sup_acc: 97.07 Train acc: 96.607 Test acc: 97.060 \n",
      "step: 4194 | Train: G_Loss: 1.084, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.596, D_sup_loss: 0.142, D_sup_acc: 97.10 Train acc: 96.392 Test acc: 96.830 \n",
      "step: 4195 | Train: G_Loss: 1.068, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.606, D_sup_loss: 0.151, D_sup_acc: 96.87 Train acc: 96.477 Test acc: 96.960 \n",
      "step: 4196 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.687, D_sup_loss: 0.147, D_sup_acc: 97.00 Train acc: 96.707 Test acc: 97.130 \n",
      "step: 4197 | Train: G_Loss: 1.079, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.610, D_sup_loss: 0.136, D_sup_acc: 97.17 Train acc: 96.510 Test acc: 96.980 \n",
      "step: 4198 | Train: G_Loss: 1.072, D_unsup_loss_real: 0.537, D_unsup_loss_fake: 0.582, D_sup_loss: 0.146, D_sup_acc: 97.02 Train acc: 96.485 Test acc: 96.840 \n",
      "step: 4199 | Train: G_Loss: 1.175, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.581, D_sup_loss: 0.147, D_sup_acc: 96.88 Train acc: 96.652 Test acc: 97.000 \n",
      "step: 4200 | Train: G_Loss: 1.101, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.552, D_sup_loss: 0.137, D_sup_acc: 97.04 Train acc: 96.687 Test acc: 96.980 \n",
      "Train Classifier Accuracy: 96.687%\n",
      "\n",
      "Test Classifier Accuracy: 96.980%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_4200.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_4200.h5\n",
      "step: 4201 | Train: G_Loss: 1.114, D_unsup_loss_real: 0.546, D_unsup_loss_fake: 0.610, D_sup_loss: 0.135, D_sup_acc: 97.02 Train acc: 96.680 Test acc: 96.940 \n",
      "step: 4202 | Train: G_Loss: 1.199, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.650, D_sup_loss: 0.134, D_sup_acc: 96.98 Train acc: 96.605 Test acc: 96.870 \n",
      "step: 4203 | Train: G_Loss: 1.038, D_unsup_loss_real: 0.652, D_unsup_loss_fake: 0.587, D_sup_loss: 0.138, D_sup_acc: 96.91 Train acc: 96.425 Test acc: 96.840 \n",
      "step: 4204 | Train: G_Loss: 1.162, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.605, D_sup_loss: 0.148, D_sup_acc: 96.88 Train acc: 96.465 Test acc: 96.830 \n",
      "step: 4205 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.644, D_unsup_loss_fake: 0.608, D_sup_loss: 0.145, D_sup_acc: 96.87 Train acc: 96.515 Test acc: 96.780 \n",
      "step: 4206 | Train: G_Loss: 1.066, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.596, D_sup_loss: 0.142, D_sup_acc: 96.82 Train acc: 96.352 Test acc: 96.810 \n",
      "step: 4207 | Train: G_Loss: 1.109, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.592, D_sup_loss: 0.147, D_sup_acc: 96.85 Train acc: 96.573 Test acc: 97.050 \n",
      "step: 4208 | Train: G_Loss: 1.080, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.579, D_sup_loss: 0.139, D_sup_acc: 97.09 Train acc: 96.528 Test acc: 97.110 \n",
      "step: 4209 | Train: G_Loss: 1.105, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.644, D_sup_loss: 0.138, D_sup_acc: 97.15 Train acc: 96.567 Test acc: 96.990 \n",
      "step: 4210 | Train: G_Loss: 1.087, D_unsup_loss_real: 0.684, D_unsup_loss_fake: 0.666, D_sup_loss: 0.142, D_sup_acc: 97.03 Train acc: 96.465 Test acc: 96.810 \n",
      "step: 4211 | Train: G_Loss: 1.030, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.582, D_sup_loss: 0.147, D_sup_acc: 96.85 Train acc: 96.402 Test acc: 96.760 \n",
      "step: 4212 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.537, D_sup_loss: 0.147, D_sup_acc: 96.80 Train acc: 96.513 Test acc: 96.930 \n",
      "step: 4213 | Train: G_Loss: 1.075, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.625, D_sup_loss: 0.139, D_sup_acc: 96.97 Train acc: 96.542 Test acc: 97.070 \n",
      "step: 4214 | Train: G_Loss: 1.214, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.619, D_sup_loss: 0.138, D_sup_acc: 97.11 Train acc: 96.567 Test acc: 97.010 \n",
      "step: 4215 | Train: G_Loss: 1.127, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.560, D_sup_loss: 0.134, D_sup_acc: 97.05 Train acc: 96.618 Test acc: 97.230 \n",
      "step: 4216 | Train: G_Loss: 1.112, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.613, D_sup_loss: 0.133, D_sup_acc: 97.27 Train acc: 96.627 Test acc: 97.110 \n",
      "step: 4217 | Train: G_Loss: 1.061, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.605, D_sup_loss: 0.133, D_sup_acc: 97.15 Train acc: 96.495 Test acc: 97.010 \n",
      "step: 4218 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.626, D_sup_loss: 0.140, D_sup_acc: 97.05 Train acc: 96.532 Test acc: 96.990 \n",
      "step: 4219 | Train: G_Loss: 1.047, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.544, D_sup_loss: 0.140, D_sup_acc: 97.03 Train acc: 96.577 Test acc: 97.100 \n",
      "step: 4220 | Train: G_Loss: 1.067, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.634, D_sup_loss: 0.138, D_sup_acc: 97.14 Train acc: 96.623 Test acc: 97.140 \n",
      "step: 4221 | Train: G_Loss: 1.202, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.641, D_sup_loss: 0.133, D_sup_acc: 97.18 Train acc: 96.683 Test acc: 97.140 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4222 | Train: G_Loss: 1.088, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.610, D_sup_loss: 0.131, D_sup_acc: 97.18 Train acc: 96.600 Test acc: 97.000 \n",
      "step: 4223 | Train: G_Loss: 1.010, D_unsup_loss_real: 0.654, D_unsup_loss_fake: 0.572, D_sup_loss: 0.135, D_sup_acc: 97.04 Train acc: 96.615 Test acc: 97.020 \n",
      "step: 4224 | Train: G_Loss: 1.137, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.663, D_sup_loss: 0.135, D_sup_acc: 97.06 Train acc: 96.600 Test acc: 97.040 \n",
      "step: 4225 | Train: G_Loss: 1.116, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.585, D_sup_loss: 0.139, D_sup_acc: 97.08 Train acc: 96.427 Test acc: 96.840 \n",
      "step: 4226 | Train: G_Loss: 1.069, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.616, D_sup_loss: 0.150, D_sup_acc: 96.88 Train acc: 96.595 Test acc: 96.930 \n",
      "step: 4227 | Train: G_Loss: 1.116, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.572, D_sup_loss: 0.139, D_sup_acc: 96.97 Train acc: 96.542 Test acc: 96.960 \n",
      "step: 4228 | Train: G_Loss: 1.101, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.605, D_sup_loss: 0.139, D_sup_acc: 97.00 Train acc: 96.408 Test acc: 96.920 \n",
      "step: 4229 | Train: G_Loss: 1.092, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.598, D_sup_loss: 0.150, D_sup_acc: 96.96 Train acc: 96.567 Test acc: 96.850 \n",
      "step: 4230 | Train: G_Loss: 1.105, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.637, D_sup_loss: 0.137, D_sup_acc: 96.89 Train acc: 96.510 Test acc: 96.990 \n",
      "step: 4231 | Train: G_Loss: 1.161, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.586, D_sup_loss: 0.143, D_sup_acc: 97.03 Train acc: 96.510 Test acc: 96.880 \n",
      "step: 4232 | Train: G_Loss: 1.105, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.545, D_sup_loss: 0.145, D_sup_acc: 96.92 Train acc: 96.618 Test acc: 96.900 \n",
      "step: 4233 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.611, D_sup_loss: 0.137, D_sup_acc: 96.94 Train acc: 96.577 Test acc: 97.000 \n",
      "step: 4234 | Train: G_Loss: 0.990, D_unsup_loss_real: 0.495, D_unsup_loss_fake: 0.551, D_sup_loss: 0.134, D_sup_acc: 97.04 Train acc: 96.565 Test acc: 97.000 \n",
      "step: 4235 | Train: G_Loss: 1.079, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.641, D_sup_loss: 0.135, D_sup_acc: 97.04 Train acc: 96.397 Test acc: 96.970 \n",
      "step: 4236 | Train: G_Loss: 1.008, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.573, D_sup_loss: 0.146, D_sup_acc: 97.01 Train acc: 96.508 Test acc: 96.980 \n",
      "step: 4237 | Train: G_Loss: 1.023, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.577, D_sup_loss: 0.138, D_sup_acc: 97.02 Train acc: 96.652 Test acc: 97.040 \n",
      "step: 4238 | Train: G_Loss: 1.164, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.673, D_sup_loss: 0.131, D_sup_acc: 97.08 Train acc: 96.530 Test acc: 96.920 \n",
      "step: 4239 | Train: G_Loss: 0.979, D_unsup_loss_real: 0.652, D_unsup_loss_fake: 0.557, D_sup_loss: 0.137, D_sup_acc: 96.96 Train acc: 96.498 Test acc: 96.900 \n",
      "step: 4240 | Train: G_Loss: 0.973, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.666, D_sup_loss: 0.140, D_sup_acc: 96.94 Train acc: 96.448 Test acc: 96.890 \n",
      "step: 4241 | Train: G_Loss: 1.153, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.616, D_sup_loss: 0.142, D_sup_acc: 96.93 Train acc: 96.503 Test acc: 96.940 \n",
      "step: 4242 | Train: G_Loss: 1.129, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.588, D_sup_loss: 0.138, D_sup_acc: 96.98 Train acc: 96.633 Test acc: 97.040 \n",
      "step: 4243 | Train: G_Loss: 1.263, D_unsup_loss_real: 0.542, D_unsup_loss_fake: 0.641, D_sup_loss: 0.131, D_sup_acc: 97.08 Train acc: 96.422 Test acc: 96.840 \n",
      "step: 4244 | Train: G_Loss: 1.242, D_unsup_loss_real: 0.656, D_unsup_loss_fake: 0.698, D_sup_loss: 0.143, D_sup_acc: 96.88 Train acc: 96.545 Test acc: 96.880 \n",
      "step: 4245 | Train: G_Loss: 1.229, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.557, D_sup_loss: 0.139, D_sup_acc: 96.92 Train acc: 96.622 Test acc: 96.960 \n",
      "step: 4246 | Train: G_Loss: 1.072, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.567, D_sup_loss: 0.135, D_sup_acc: 97.00 Train acc: 96.515 Test acc: 97.000 \n",
      "step: 4247 | Train: G_Loss: 1.297, D_unsup_loss_real: 0.518, D_unsup_loss_fake: 0.588, D_sup_loss: 0.138, D_sup_acc: 97.04 Train acc: 96.632 Test acc: 96.960 \n",
      "step: 4248 | Train: G_Loss: 1.130, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.552, D_sup_loss: 0.134, D_sup_acc: 97.00 Train acc: 96.572 Test acc: 96.970 \n",
      "step: 4249 | Train: G_Loss: 1.178, D_unsup_loss_real: 0.696, D_unsup_loss_fake: 0.615, D_sup_loss: 0.134, D_sup_acc: 97.01 Train acc: 96.495 Test acc: 96.830 \n",
      "step: 4250 | Train: G_Loss: 1.049, D_unsup_loss_real: 0.664, D_unsup_loss_fake: 0.607, D_sup_loss: 0.139, D_sup_acc: 96.87 Train acc: 96.405 Test acc: 96.920 \n",
      "step: 4251 | Train: G_Loss: 1.107, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.556, D_sup_loss: 0.143, D_sup_acc: 96.96 Train acc: 96.580 Test acc: 96.960 \n",
      "step: 4252 | Train: G_Loss: 1.078, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.625, D_sup_loss: 0.133, D_sup_acc: 97.00 Train acc: 96.405 Test acc: 96.770 \n",
      "step: 4253 | Train: G_Loss: 1.036, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.609, D_sup_loss: 0.142, D_sup_acc: 96.81 Train acc: 96.483 Test acc: 96.840 \n",
      "step: 4254 | Train: G_Loss: 1.086, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.643, D_sup_loss: 0.138, D_sup_acc: 96.88 Train acc: 96.580 Test acc: 96.980 \n",
      "step: 4255 | Train: G_Loss: 1.079, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.621, D_sup_loss: 0.135, D_sup_acc: 97.02 Train acc: 96.540 Test acc: 97.000 \n",
      "step: 4256 | Train: G_Loss: 1.058, D_unsup_loss_real: 0.535, D_unsup_loss_fake: 0.623, D_sup_loss: 0.136, D_sup_acc: 97.04 Train acc: 96.587 Test acc: 97.090 \n",
      "step: 4257 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.612, D_sup_loss: 0.133, D_sup_acc: 97.13 Train acc: 96.488 Test acc: 96.890 \n",
      "step: 4258 | Train: G_Loss: 1.155, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.568, D_sup_loss: 0.138, D_sup_acc: 96.93 Train acc: 96.652 Test acc: 97.070 \n",
      "step: 4259 | Train: G_Loss: 1.057, D_unsup_loss_real: 0.695, D_unsup_loss_fake: 0.624, D_sup_loss: 0.130, D_sup_acc: 97.11 Train acc: 96.590 Test acc: 97.050 \n",
      "step: 4260 | Train: G_Loss: 1.028, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.624, D_sup_loss: 0.135, D_sup_acc: 97.09 Train acc: 96.485 Test acc: 97.050 \n",
      "step: 4261 | Train: G_Loss: 1.173, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.604, D_sup_loss: 0.140, D_sup_acc: 97.09 Train acc: 96.663 Test acc: 97.070 \n",
      "step: 4262 | Train: G_Loss: 1.220, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.595, D_sup_loss: 0.128, D_sup_acc: 97.11 Train acc: 96.680 Test acc: 97.100 \n",
      "step: 4263 | Train: G_Loss: 1.050, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.635, D_sup_loss: 0.127, D_sup_acc: 97.14 Train acc: 96.615 Test acc: 97.060 \n",
      "step: 4264 | Train: G_Loss: 1.044, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.664, D_sup_loss: 0.132, D_sup_acc: 97.10 Train acc: 96.550 Test acc: 96.960 \n",
      "step: 4265 | Train: G_Loss: 1.047, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.573, D_sup_loss: 0.138, D_sup_acc: 97.00 Train acc: 96.547 Test acc: 97.040 \n",
      "step: 4266 | Train: G_Loss: 1.114, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.574, D_sup_loss: 0.134, D_sup_acc: 97.08 Train acc: 96.728 Test acc: 97.190 \n",
      "step: 4267 | Train: G_Loss: 1.240, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.598, D_sup_loss: 0.127, D_sup_acc: 97.23 Train acc: 96.608 Test acc: 97.000 \n",
      "step: 4268 | Train: G_Loss: 1.172, D_unsup_loss_real: 0.642, D_unsup_loss_fake: 0.622, D_sup_loss: 0.133, D_sup_acc: 97.04 Train acc: 96.495 Test acc: 96.840 \n",
      "step: 4269 | Train: G_Loss: 1.075, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.614, D_sup_loss: 0.141, D_sup_acc: 96.88 Train acc: 96.452 Test acc: 96.880 \n",
      "step: 4270 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.583, D_sup_loss: 0.141, D_sup_acc: 96.92 Train acc: 96.490 Test acc: 96.870 \n",
      "step: 4271 | Train: G_Loss: 1.075, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.608, D_sup_loss: 0.138, D_sup_acc: 96.91 Train acc: 96.525 Test acc: 97.030 \n",
      "step: 4272 | Train: G_Loss: 1.034, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.628, D_sup_loss: 0.132, D_sup_acc: 97.07 Train acc: 96.493 Test acc: 97.040 \n",
      "step: 4273 | Train: G_Loss: 1.137, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.574, D_sup_loss: 0.136, D_sup_acc: 97.08 Train acc: 96.465 Test acc: 97.010 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4274 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.545, D_unsup_loss_fake: 0.703, D_sup_loss: 0.135, D_sup_acc: 97.05 Train acc: 96.452 Test acc: 96.940 \n",
      "step: 4275 | Train: G_Loss: 1.106, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.546, D_sup_loss: 0.141, D_sup_acc: 96.98 Train acc: 96.670 Test acc: 97.030 \n",
      "step: 4276 | Train: G_Loss: 1.017, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.637, D_sup_loss: 0.130, D_sup_acc: 97.07 Train acc: 96.498 Test acc: 96.830 \n",
      "step: 4277 | Train: G_Loss: 1.031, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.624, D_sup_loss: 0.141, D_sup_acc: 96.87 Train acc: 96.417 Test acc: 96.900 \n",
      "step: 4278 | Train: G_Loss: 1.084, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.600, D_sup_loss: 0.142, D_sup_acc: 96.94 Train acc: 96.675 Test acc: 97.000 \n",
      "step: 4279 | Train: G_Loss: 1.174, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.562, D_sup_loss: 0.131, D_sup_acc: 97.04 Train acc: 96.682 Test acc: 97.040 \n",
      "step: 4280 | Train: G_Loss: 1.126, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.543, D_sup_loss: 0.127, D_sup_acc: 97.08 Train acc: 96.793 Test acc: 96.990 \n",
      "step: 4281 | Train: G_Loss: 1.248, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.643, D_sup_loss: 0.125, D_sup_acc: 97.03 Train acc: 96.635 Test acc: 96.930 \n",
      "step: 4282 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.537, D_unsup_loss_fake: 0.633, D_sup_loss: 0.134, D_sup_acc: 96.97 Train acc: 96.605 Test acc: 96.930 \n",
      "step: 4283 | Train: G_Loss: 1.096, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.585, D_sup_loss: 0.134, D_sup_acc: 96.97 Train acc: 96.662 Test acc: 97.000 \n",
      "step: 4284 | Train: G_Loss: 1.099, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.608, D_sup_loss: 0.132, D_sup_acc: 97.04 Train acc: 96.733 Test acc: 97.070 \n",
      "step: 4285 | Train: G_Loss: 1.091, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.612, D_sup_loss: 0.129, D_sup_acc: 97.11 Train acc: 96.612 Test acc: 97.060 \n",
      "step: 4286 | Train: G_Loss: 1.137, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.613, D_sup_loss: 0.135, D_sup_acc: 97.10 Train acc: 96.708 Test acc: 97.180 \n",
      "step: 4287 | Train: G_Loss: 1.203, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.644, D_sup_loss: 0.131, D_sup_acc: 97.22 Train acc: 96.810 Test acc: 97.100 \n",
      "step: 4288 | Train: G_Loss: 1.080, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.587, D_sup_loss: 0.125, D_sup_acc: 97.14 Train acc: 96.740 Test acc: 97.100 \n",
      "step: 4289 | Train: G_Loss: 1.253, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.642, D_sup_loss: 0.130, D_sup_acc: 97.14 Train acc: 96.758 Test acc: 97.110 \n",
      "step: 4290 | Train: G_Loss: 1.177, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.640, D_sup_loss: 0.126, D_sup_acc: 97.15 Train acc: 96.757 Test acc: 97.190 \n",
      "step: 4291 | Train: G_Loss: 1.050, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.534, D_sup_loss: 0.126, D_sup_acc: 97.23 Train acc: 96.708 Test acc: 97.110 \n",
      "step: 4292 | Train: G_Loss: 1.024, D_unsup_loss_real: 0.517, D_unsup_loss_fake: 0.535, D_sup_loss: 0.130, D_sup_acc: 97.15 Train acc: 96.717 Test acc: 96.990 \n",
      "step: 4293 | Train: G_Loss: 1.059, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.614, D_sup_loss: 0.131, D_sup_acc: 97.03 Train acc: 96.688 Test acc: 96.900 \n",
      "step: 4294 | Train: G_Loss: 1.120, D_unsup_loss_real: 0.648, D_unsup_loss_fake: 0.563, D_sup_loss: 0.135, D_sup_acc: 96.94 Train acc: 96.740 Test acc: 97.030 \n",
      "step: 4295 | Train: G_Loss: 1.141, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.562, D_sup_loss: 0.129, D_sup_acc: 97.07 Train acc: 96.697 Test acc: 97.000 \n",
      "step: 4296 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.538, D_unsup_loss_fake: 0.637, D_sup_loss: 0.133, D_sup_acc: 97.04 Train acc: 96.628 Test acc: 96.910 \n",
      "step: 4297 | Train: G_Loss: 1.070, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.562, D_sup_loss: 0.136, D_sup_acc: 96.95 Train acc: 96.700 Test acc: 97.000 \n",
      "step: 4298 | Train: G_Loss: 1.152, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.585, D_sup_loss: 0.133, D_sup_acc: 97.04 Train acc: 96.630 Test acc: 97.030 \n",
      "step: 4299 | Train: G_Loss: 1.170, D_unsup_loss_real: 0.544, D_unsup_loss_fake: 0.624, D_sup_loss: 0.135, D_sup_acc: 97.07 Train acc: 96.330 Test acc: 96.680 \n",
      "step: 4300 | Train: G_Loss: 1.071, D_unsup_loss_real: 0.674, D_unsup_loss_fake: 0.510, D_sup_loss: 0.149, D_sup_acc: 96.72 Train acc: 96.630 Test acc: 96.950 \n",
      "Train Classifier Accuracy: 96.630%\n",
      "\n",
      "Test Classifier Accuracy: 96.950%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_4300.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_4300.h5\n",
      "step: 4301 | Train: G_Loss: 1.127, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.660, D_sup_loss: 0.138, D_sup_acc: 96.99 Train acc: 96.573 Test acc: 96.840 \n",
      "step: 4302 | Train: G_Loss: 1.163, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.599, D_sup_loss: 0.140, D_sup_acc: 96.88 Train acc: 96.720 Test acc: 96.980 \n",
      "step: 4303 | Train: G_Loss: 1.126, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.546, D_sup_loss: 0.129, D_sup_acc: 97.02 Train acc: 96.650 Test acc: 96.800 \n",
      "step: 4304 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.538, D_unsup_loss_fake: 0.637, D_sup_loss: 0.134, D_sup_acc: 96.84 Train acc: 96.593 Test acc: 96.800 \n",
      "step: 4305 | Train: G_Loss: 1.137, D_unsup_loss_real: 0.668, D_unsup_loss_fake: 0.628, D_sup_loss: 0.136, D_sup_acc: 96.84 Train acc: 96.708 Test acc: 96.880 \n",
      "step: 4306 | Train: G_Loss: 1.110, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.611, D_sup_loss: 0.131, D_sup_acc: 96.92 Train acc: 96.725 Test acc: 96.820 \n",
      "step: 4307 | Train: G_Loss: 1.039, D_unsup_loss_real: 0.538, D_unsup_loss_fake: 0.678, D_sup_loss: 0.129, D_sup_acc: 96.86 Train acc: 96.448 Test acc: 96.660 \n",
      "step: 4308 | Train: G_Loss: 1.180, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.571, D_sup_loss: 0.144, D_sup_acc: 96.70 Train acc: 96.657 Test acc: 96.800 \n",
      "step: 4309 | Train: G_Loss: 1.140, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.653, D_sup_loss: 0.134, D_sup_acc: 96.84 Train acc: 96.728 Test acc: 96.820 \n",
      "step: 4310 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.668, D_unsup_loss_fake: 0.546, D_sup_loss: 0.132, D_sup_acc: 96.86 Train acc: 96.717 Test acc: 96.920 \n",
      "step: 4311 | Train: G_Loss: 1.194, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.635, D_sup_loss: 0.135, D_sup_acc: 96.96 Train acc: 96.713 Test acc: 96.880 \n",
      "step: 4312 | Train: G_Loss: 1.078, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.651, D_sup_loss: 0.131, D_sup_acc: 96.92 Train acc: 96.728 Test acc: 96.890 \n",
      "step: 4313 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.579, D_sup_loss: 0.132, D_sup_acc: 96.93 Train acc: 96.682 Test acc: 96.930 \n",
      "step: 4314 | Train: G_Loss: 1.129, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.578, D_sup_loss: 0.132, D_sup_acc: 96.97 Train acc: 96.748 Test acc: 96.740 \n",
      "step: 4315 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.550, D_unsup_loss_fake: 0.666, D_sup_loss: 0.125, D_sup_acc: 96.78 Train acc: 96.537 Test acc: 96.880 \n",
      "step: 4316 | Train: G_Loss: 1.114, D_unsup_loss_real: 0.639, D_unsup_loss_fake: 0.533, D_sup_loss: 0.135, D_sup_acc: 96.92 Train acc: 96.528 Test acc: 96.660 \n",
      "step: 4317 | Train: G_Loss: 1.046, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.642, D_sup_loss: 0.137, D_sup_acc: 96.70 Train acc: 96.577 Test acc: 96.850 \n",
      "step: 4318 | Train: G_Loss: 1.103, D_unsup_loss_real: 0.662, D_unsup_loss_fake: 0.576, D_sup_loss: 0.130, D_sup_acc: 96.89 Train acc: 96.390 Test acc: 96.720 \n",
      "step: 4319 | Train: G_Loss: 1.147, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.575, D_sup_loss: 0.140, D_sup_acc: 96.76 Train acc: 96.560 Test acc: 96.740 \n",
      "step: 4320 | Train: G_Loss: 1.152, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.578, D_sup_loss: 0.135, D_sup_acc: 96.78 Train acc: 96.585 Test acc: 96.790 \n",
      "step: 4321 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.669, D_unsup_loss_fake: 0.582, D_sup_loss: 0.136, D_sup_acc: 96.83 Train acc: 96.625 Test acc: 96.850 \n",
      "step: 4322 | Train: G_Loss: 1.181, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.608, D_sup_loss: 0.134, D_sup_acc: 96.89 Train acc: 96.632 Test acc: 96.860 \n",
      "step: 4323 | Train: G_Loss: 1.057, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.610, D_sup_loss: 0.136, D_sup_acc: 96.90 Train acc: 96.610 Test acc: 96.860 \n",
      "step: 4324 | Train: G_Loss: 1.116, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.605, D_sup_loss: 0.140, D_sup_acc: 96.90 Train acc: 96.520 Test acc: 96.680 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4325 | Train: G_Loss: 1.166, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.581, D_sup_loss: 0.142, D_sup_acc: 96.72 Train acc: 96.723 Test acc: 96.910 \n",
      "step: 4326 | Train: G_Loss: 1.174, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.608, D_sup_loss: 0.130, D_sup_acc: 96.95 Train acc: 96.477 Test acc: 96.760 \n",
      "step: 4327 | Train: G_Loss: 1.127, D_unsup_loss_real: 0.677, D_unsup_loss_fake: 0.615, D_sup_loss: 0.146, D_sup_acc: 96.80 Train acc: 96.608 Test acc: 96.870 \n",
      "step: 4328 | Train: G_Loss: 1.080, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.607, D_sup_loss: 0.140, D_sup_acc: 96.91 Train acc: 96.618 Test acc: 96.870 \n",
      "step: 4329 | Train: G_Loss: 1.141, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.550, D_sup_loss: 0.138, D_sup_acc: 96.91 Train acc: 96.583 Test acc: 96.860 \n",
      "step: 4330 | Train: G_Loss: 1.170, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.641, D_sup_loss: 0.137, D_sup_acc: 96.90 Train acc: 96.573 Test acc: 96.830 \n",
      "step: 4331 | Train: G_Loss: 1.128, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.557, D_sup_loss: 0.137, D_sup_acc: 96.87 Train acc: 96.447 Test acc: 96.770 \n",
      "step: 4332 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.644, D_sup_loss: 0.142, D_sup_acc: 96.81 Train acc: 96.635 Test acc: 96.860 \n",
      "step: 4333 | Train: G_Loss: 1.062, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.583, D_sup_loss: 0.136, D_sup_acc: 96.90 Train acc: 96.612 Test acc: 96.870 \n",
      "step: 4334 | Train: G_Loss: 1.087, D_unsup_loss_real: 0.663, D_unsup_loss_fake: 0.593, D_sup_loss: 0.141, D_sup_acc: 96.91 Train acc: 96.650 Test acc: 96.900 \n",
      "step: 4335 | Train: G_Loss: 1.105, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.576, D_sup_loss: 0.136, D_sup_acc: 96.94 Train acc: 96.725 Test acc: 97.040 \n",
      "step: 4336 | Train: G_Loss: 1.164, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.628, D_sup_loss: 0.130, D_sup_acc: 97.08 Train acc: 96.622 Test acc: 96.900 \n",
      "step: 4337 | Train: G_Loss: 1.183, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.590, D_sup_loss: 0.136, D_sup_acc: 96.94 Train acc: 96.788 Test acc: 96.950 \n",
      "step: 4338 | Train: G_Loss: 1.090, D_unsup_loss_real: 0.648, D_unsup_loss_fake: 0.601, D_sup_loss: 0.127, D_sup_acc: 96.99 Train acc: 96.683 Test acc: 96.950 \n",
      "step: 4339 | Train: G_Loss: 1.157, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.655, D_sup_loss: 0.132, D_sup_acc: 96.99 Train acc: 96.713 Test acc: 96.990 \n",
      "step: 4340 | Train: G_Loss: 1.273, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.623, D_sup_loss: 0.131, D_sup_acc: 97.03 Train acc: 96.715 Test acc: 97.050 \n",
      "step: 4341 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.636, D_sup_loss: 0.133, D_sup_acc: 97.09 Train acc: 96.743 Test acc: 97.040 \n",
      "step: 4342 | Train: G_Loss: 1.292, D_unsup_loss_real: 0.534, D_unsup_loss_fake: 0.584, D_sup_loss: 0.133, D_sup_acc: 97.08 Train acc: 96.820 Test acc: 97.060 \n",
      "step: 4343 | Train: G_Loss: 1.109, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.695, D_sup_loss: 0.129, D_sup_acc: 97.10 Train acc: 96.557 Test acc: 96.860 \n",
      "step: 4344 | Train: G_Loss: 1.176, D_unsup_loss_real: 0.524, D_unsup_loss_fake: 0.597, D_sup_loss: 0.145, D_sup_acc: 96.90 Train acc: 96.813 Test acc: 97.050 \n",
      "step: 4345 | Train: G_Loss: 1.082, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.506, D_sup_loss: 0.131, D_sup_acc: 97.09 Train acc: 96.870 Test acc: 97.060 \n",
      "step: 4346 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.594, D_sup_loss: 0.124, D_sup_acc: 97.10 Train acc: 96.848 Test acc: 97.040 \n",
      "step: 4347 | Train: G_Loss: 1.246, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.602, D_sup_loss: 0.126, D_sup_acc: 97.08 Train acc: 96.863 Test acc: 97.100 \n",
      "step: 4348 | Train: G_Loss: 1.127, D_unsup_loss_real: 0.668, D_unsup_loss_fake: 0.592, D_sup_loss: 0.127, D_sup_acc: 97.14 Train acc: 96.820 Test acc: 96.960 \n",
      "step: 4349 | Train: G_Loss: 1.149, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.645, D_sup_loss: 0.129, D_sup_acc: 97.00 Train acc: 96.533 Test acc: 96.810 \n",
      "step: 4350 | Train: G_Loss: 1.087, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.624, D_sup_loss: 0.138, D_sup_acc: 96.85 Train acc: 96.293 Test acc: 96.570 \n",
      "step: 4351 | Train: G_Loss: 1.133, D_unsup_loss_real: 0.672, D_unsup_loss_fake: 0.654, D_sup_loss: 0.159, D_sup_acc: 96.61 Train acc: 96.385 Test acc: 96.840 \n",
      "step: 4352 | Train: G_Loss: 1.060, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.620, D_sup_loss: 0.151, D_sup_acc: 96.88 Train acc: 96.665 Test acc: 97.070 \n",
      "step: 4353 | Train: G_Loss: 1.118, D_unsup_loss_real: 0.534, D_unsup_loss_fake: 0.599, D_sup_loss: 0.138, D_sup_acc: 97.11 Train acc: 96.567 Test acc: 96.900 \n",
      "step: 4354 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.635, D_unsup_loss_fake: 0.678, D_sup_loss: 0.141, D_sup_acc: 96.94 Train acc: 96.668 Test acc: 97.030 \n",
      "step: 4355 | Train: G_Loss: 1.175, D_unsup_loss_real: 0.684, D_unsup_loss_fake: 0.579, D_sup_loss: 0.136, D_sup_acc: 97.07 Train acc: 96.695 Test acc: 97.080 \n",
      "step: 4356 | Train: G_Loss: 1.127, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.628, D_sup_loss: 0.139, D_sup_acc: 97.12 Train acc: 96.772 Test acc: 97.040 \n",
      "step: 4357 | Train: G_Loss: 1.068, D_unsup_loss_real: 0.655, D_unsup_loss_fake: 0.595, D_sup_loss: 0.134, D_sup_acc: 97.08 Train acc: 96.803 Test acc: 96.990 \n",
      "step: 4358 | Train: G_Loss: 1.109, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.553, D_sup_loss: 0.133, D_sup_acc: 97.03 Train acc: 96.793 Test acc: 97.050 \n",
      "step: 4359 | Train: G_Loss: 1.048, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.614, D_sup_loss: 0.132, D_sup_acc: 97.09 Train acc: 96.768 Test acc: 96.970 \n",
      "step: 4360 | Train: G_Loss: 1.153, D_unsup_loss_real: 0.527, D_unsup_loss_fake: 0.645, D_sup_loss: 0.130, D_sup_acc: 97.01 Train acc: 96.803 Test acc: 97.060 \n",
      "step: 4361 | Train: G_Loss: 1.188, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.526, D_sup_loss: 0.131, D_sup_acc: 97.10 Train acc: 96.788 Test acc: 96.900 \n",
      "step: 4362 | Train: G_Loss: 1.133, D_unsup_loss_real: 0.635, D_unsup_loss_fake: 0.553, D_sup_loss: 0.127, D_sup_acc: 96.94 Train acc: 96.730 Test acc: 96.980 \n",
      "step: 4363 | Train: G_Loss: 1.113, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.628, D_sup_loss: 0.134, D_sup_acc: 97.02 Train acc: 96.690 Test acc: 97.000 \n",
      "step: 4364 | Train: G_Loss: 1.126, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.583, D_sup_loss: 0.136, D_sup_acc: 97.04 Train acc: 96.780 Test acc: 97.040 \n",
      "step: 4365 | Train: G_Loss: 1.128, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.606, D_sup_loss: 0.128, D_sup_acc: 97.08 Train acc: 96.617 Test acc: 97.020 \n",
      "step: 4366 | Train: G_Loss: 1.088, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.584, D_sup_loss: 0.138, D_sup_acc: 97.06 Train acc: 96.705 Test acc: 96.940 \n",
      "step: 4367 | Train: G_Loss: 1.140, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.610, D_sup_loss: 0.130, D_sup_acc: 96.98 Train acc: 96.762 Test acc: 97.080 \n",
      "step: 4368 | Train: G_Loss: 1.162, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.621, D_sup_loss: 0.130, D_sup_acc: 97.12 Train acc: 96.837 Test acc: 97.050 \n",
      "step: 4369 | Train: G_Loss: 1.078, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.568, D_sup_loss: 0.130, D_sup_acc: 97.09 Train acc: 96.827 Test acc: 97.080 \n",
      "step: 4370 | Train: G_Loss: 1.100, D_unsup_loss_real: 0.528, D_unsup_loss_fake: 0.603, D_sup_loss: 0.128, D_sup_acc: 97.12 Train acc: 96.867 Test acc: 97.140 \n",
      "step: 4371 | Train: G_Loss: 1.152, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.568, D_sup_loss: 0.129, D_sup_acc: 97.18 Train acc: 96.772 Test acc: 97.060 \n",
      "step: 4372 | Train: G_Loss: 1.125, D_unsup_loss_real: 0.670, D_unsup_loss_fake: 0.593, D_sup_loss: 0.132, D_sup_acc: 97.10 Train acc: 96.812 Test acc: 96.950 \n",
      "step: 4373 | Train: G_Loss: 1.096, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.611, D_sup_loss: 0.129, D_sup_acc: 96.99 Train acc: 96.595 Test acc: 96.940 \n",
      "step: 4374 | Train: G_Loss: 1.186, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.566, D_sup_loss: 0.145, D_sup_acc: 96.98 Train acc: 96.572 Test acc: 96.930 \n",
      "step: 4375 | Train: G_Loss: 1.079, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.577, D_sup_loss: 0.142, D_sup_acc: 96.97 Train acc: 96.698 Test acc: 97.020 \n",
      "step: 4376 | Train: G_Loss: 1.103, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.648, D_sup_loss: 0.137, D_sup_acc: 97.06 Train acc: 96.778 Test acc: 96.920 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4377 | Train: G_Loss: 1.127, D_unsup_loss_real: 0.664, D_unsup_loss_fake: 0.544, D_sup_loss: 0.130, D_sup_acc: 96.96 Train acc: 96.748 Test acc: 97.070 \n",
      "step: 4378 | Train: G_Loss: 1.085, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.624, D_sup_loss: 0.136, D_sup_acc: 97.11 Train acc: 96.415 Test acc: 96.890 \n",
      "step: 4379 | Train: G_Loss: 1.103, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.603, D_sup_loss: 0.155, D_sup_acc: 96.93 Train acc: 96.712 Test acc: 97.120 \n",
      "step: 4380 | Train: G_Loss: 1.079, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.535, D_sup_loss: 0.140, D_sup_acc: 97.16 Train acc: 96.617 Test acc: 97.020 \n",
      "step: 4381 | Train: G_Loss: 1.046, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.652, D_sup_loss: 0.144, D_sup_acc: 97.06 Train acc: 96.703 Test acc: 97.110 \n",
      "step: 4382 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.650, D_unsup_loss_fake: 0.662, D_sup_loss: 0.139, D_sup_acc: 97.15 Train acc: 96.682 Test acc: 97.010 \n",
      "step: 4383 | Train: G_Loss: 1.153, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.649, D_sup_loss: 0.140, D_sup_acc: 97.05 Train acc: 96.595 Test acc: 96.940 \n",
      "step: 4384 | Train: G_Loss: 1.127, D_unsup_loss_real: 0.647, D_unsup_loss_fake: 0.531, D_sup_loss: 0.150, D_sup_acc: 96.98 Train acc: 96.528 Test acc: 96.930 \n",
      "step: 4385 | Train: G_Loss: 1.043, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.639, D_sup_loss: 0.148, D_sup_acc: 96.97 Train acc: 96.727 Test acc: 97.020 \n",
      "step: 4386 | Train: G_Loss: 1.168, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.591, D_sup_loss: 0.139, D_sup_acc: 97.06 Train acc: 96.750 Test acc: 97.130 \n",
      "step: 4387 | Train: G_Loss: 1.141, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.629, D_sup_loss: 0.133, D_sup_acc: 97.17 Train acc: 96.607 Test acc: 96.960 \n",
      "step: 4388 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.660, D_unsup_loss_fake: 0.590, D_sup_loss: 0.144, D_sup_acc: 97.00 Train acc: 96.783 Test acc: 97.110 \n",
      "step: 4389 | Train: G_Loss: 1.179, D_unsup_loss_real: 0.535, D_unsup_loss_fake: 0.582, D_sup_loss: 0.130, D_sup_acc: 97.15 Train acc: 96.818 Test acc: 97.110 \n",
      "step: 4390 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.525, D_unsup_loss_fake: 0.624, D_sup_loss: 0.131, D_sup_acc: 97.15 Train acc: 96.777 Test acc: 97.010 \n",
      "step: 4391 | Train: G_Loss: 1.115, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.645, D_sup_loss: 0.131, D_sup_acc: 97.05 Train acc: 96.578 Test acc: 96.930 \n",
      "step: 4392 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.577, D_sup_loss: 0.147, D_sup_acc: 96.97 Train acc: 96.740 Test acc: 97.000 \n",
      "step: 4393 | Train: G_Loss: 1.034, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.586, D_sup_loss: 0.134, D_sup_acc: 97.04 Train acc: 96.758 Test acc: 97.020 \n",
      "step: 4394 | Train: G_Loss: 1.244, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.597, D_sup_loss: 0.135, D_sup_acc: 97.06 Train acc: 96.738 Test acc: 96.970 \n",
      "step: 4395 | Train: G_Loss: 1.202, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.607, D_sup_loss: 0.133, D_sup_acc: 97.01 Train acc: 96.793 Test acc: 97.050 \n",
      "step: 4396 | Train: G_Loss: 1.166, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.579, D_sup_loss: 0.128, D_sup_acc: 97.09 Train acc: 96.835 Test acc: 97.110 \n",
      "step: 4397 | Train: G_Loss: 1.146, D_unsup_loss_real: 0.674, D_unsup_loss_fake: 0.583, D_sup_loss: 0.129, D_sup_acc: 97.15 Train acc: 96.703 Test acc: 97.000 \n",
      "step: 4398 | Train: G_Loss: 1.128, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.517, D_sup_loss: 0.134, D_sup_acc: 97.04 Train acc: 96.823 Test acc: 97.110 \n",
      "step: 4399 | Train: G_Loss: 1.075, D_unsup_loss_real: 0.529, D_unsup_loss_fake: 0.572, D_sup_loss: 0.127, D_sup_acc: 97.15 Train acc: 96.882 Test acc: 97.090 \n",
      "step: 4400 | Train: G_Loss: 1.078, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.523, D_sup_loss: 0.122, D_sup_acc: 97.13 Train acc: 96.740 Test acc: 97.060 \n",
      "Train Classifier Accuracy: 96.740%\n",
      "\n",
      "Test Classifier Accuracy: 97.060%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_4400.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_4400.h5\n",
      "step: 4401 | Train: G_Loss: 0.936, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.628, D_sup_loss: 0.130, D_sup_acc: 97.10 Train acc: 96.803 Test acc: 97.150 \n",
      "step: 4402 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.533, D_unsup_loss_fake: 0.620, D_sup_loss: 0.127, D_sup_acc: 97.19 Train acc: 96.848 Test acc: 97.140 \n",
      "step: 4403 | Train: G_Loss: 1.119, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.624, D_sup_loss: 0.127, D_sup_acc: 97.18 Train acc: 96.678 Test acc: 97.080 \n",
      "step: 4404 | Train: G_Loss: 1.172, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.559, D_sup_loss: 0.135, D_sup_acc: 97.12 Train acc: 96.847 Test acc: 97.050 \n",
      "step: 4405 | Train: G_Loss: 1.039, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.632, D_sup_loss: 0.124, D_sup_acc: 97.09 Train acc: 96.747 Test acc: 97.000 \n",
      "step: 4406 | Train: G_Loss: 1.023, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.650, D_sup_loss: 0.133, D_sup_acc: 97.04 Train acc: 96.715 Test acc: 97.040 \n",
      "step: 4407 | Train: G_Loss: 1.046, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.586, D_sup_loss: 0.131, D_sup_acc: 97.08 Train acc: 96.787 Test acc: 97.150 \n",
      "step: 4408 | Train: G_Loss: 1.136, D_unsup_loss_real: 0.546, D_unsup_loss_fake: 0.644, D_sup_loss: 0.128, D_sup_acc: 97.19 Train acc: 96.798 Test acc: 97.140 \n",
      "step: 4409 | Train: G_Loss: 1.206, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.566, D_sup_loss: 0.127, D_sup_acc: 97.18 Train acc: 96.820 Test acc: 97.070 \n",
      "step: 4410 | Train: G_Loss: 1.120, D_unsup_loss_real: 0.657, D_unsup_loss_fake: 0.596, D_sup_loss: 0.129, D_sup_acc: 97.11 Train acc: 96.747 Test acc: 96.980 \n",
      "step: 4411 | Train: G_Loss: 1.105, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.631, D_sup_loss: 0.133, D_sup_acc: 97.02 Train acc: 96.803 Test acc: 97.010 \n",
      "step: 4412 | Train: G_Loss: 1.053, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.620, D_sup_loss: 0.131, D_sup_acc: 97.05 Train acc: 96.785 Test acc: 96.960 \n",
      "step: 4413 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.515, D_unsup_loss_fake: 0.619, D_sup_loss: 0.133, D_sup_acc: 97.00 Train acc: 96.758 Test acc: 96.970 \n",
      "step: 4414 | Train: G_Loss: 1.223, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.556, D_sup_loss: 0.134, D_sup_acc: 97.01 Train acc: 96.837 Test acc: 97.080 \n",
      "step: 4415 | Train: G_Loss: 1.180, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.609, D_sup_loss: 0.124, D_sup_acc: 97.12 Train acc: 96.895 Test acc: 97.060 \n",
      "step: 4416 | Train: G_Loss: 1.060, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.573, D_sup_loss: 0.128, D_sup_acc: 97.10 Train acc: 96.875 Test acc: 97.030 \n",
      "step: 4417 | Train: G_Loss: 1.123, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.592, D_sup_loss: 0.130, D_sup_acc: 97.07 Train acc: 96.865 Test acc: 97.060 \n",
      "step: 4418 | Train: G_Loss: 1.150, D_unsup_loss_real: 0.537, D_unsup_loss_fake: 0.649, D_sup_loss: 0.130, D_sup_acc: 97.10 Train acc: 96.763 Test acc: 97.020 \n",
      "step: 4419 | Train: G_Loss: 1.096, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.584, D_sup_loss: 0.132, D_sup_acc: 97.06 Train acc: 96.763 Test acc: 96.970 \n",
      "step: 4420 | Train: G_Loss: 1.125, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.634, D_sup_loss: 0.130, D_sup_acc: 97.01 Train acc: 96.817 Test acc: 97.060 \n",
      "step: 4421 | Train: G_Loss: 1.123, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.564, D_sup_loss: 0.126, D_sup_acc: 97.10 Train acc: 96.815 Test acc: 96.980 \n",
      "step: 4422 | Train: G_Loss: 1.133, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.573, D_sup_loss: 0.129, D_sup_acc: 97.02 Train acc: 96.848 Test acc: 97.070 \n",
      "step: 4423 | Train: G_Loss: 1.135, D_unsup_loss_real: 0.647, D_unsup_loss_fake: 0.648, D_sup_loss: 0.127, D_sup_acc: 97.11 Train acc: 96.708 Test acc: 96.890 \n",
      "step: 4424 | Train: G_Loss: 1.141, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.610, D_sup_loss: 0.139, D_sup_acc: 96.93 Train acc: 96.662 Test acc: 97.020 \n",
      "step: 4425 | Train: G_Loss: 1.080, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.648, D_sup_loss: 0.138, D_sup_acc: 97.06 Train acc: 96.300 Test acc: 96.640 \n",
      "step: 4426 | Train: G_Loss: 1.118, D_unsup_loss_real: 0.655, D_unsup_loss_fake: 0.574, D_sup_loss: 0.158, D_sup_acc: 96.68 Train acc: 96.520 Test acc: 96.810 \n",
      "step: 4427 | Train: G_Loss: 1.157, D_unsup_loss_real: 0.542, D_unsup_loss_fake: 0.626, D_sup_loss: 0.144, D_sup_acc: 96.85 Train acc: 96.833 Test acc: 97.050 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4428 | Train: G_Loss: 1.181, D_unsup_loss_real: 0.523, D_unsup_loss_fake: 0.607, D_sup_loss: 0.126, D_sup_acc: 97.09 Train acc: 96.862 Test acc: 97.000 \n",
      "step: 4429 | Train: G_Loss: 1.064, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.662, D_sup_loss: 0.123, D_sup_acc: 97.04 Train acc: 96.480 Test acc: 96.740 \n",
      "step: 4430 | Train: G_Loss: 1.059, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.619, D_sup_loss: 0.142, D_sup_acc: 96.78 Train acc: 96.787 Test acc: 97.030 \n",
      "step: 4431 | Train: G_Loss: 1.126, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.610, D_sup_loss: 0.127, D_sup_acc: 97.07 Train acc: 96.695 Test acc: 96.900 \n",
      "step: 4432 | Train: G_Loss: 1.213, D_unsup_loss_real: 0.546, D_unsup_loss_fake: 0.580, D_sup_loss: 0.132, D_sup_acc: 96.94 Train acc: 96.605 Test acc: 96.910 \n",
      "step: 4433 | Train: G_Loss: 1.220, D_unsup_loss_real: 0.544, D_unsup_loss_fake: 0.638, D_sup_loss: 0.135, D_sup_acc: 96.95 Train acc: 96.712 Test acc: 96.960 \n",
      "step: 4434 | Train: G_Loss: 1.151, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.589, D_sup_loss: 0.129, D_sup_acc: 97.00 Train acc: 96.722 Test acc: 96.930 \n",
      "step: 4435 | Train: G_Loss: 1.107, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.631, D_sup_loss: 0.134, D_sup_acc: 96.97 Train acc: 96.723 Test acc: 97.060 \n",
      "step: 4436 | Train: G_Loss: 1.277, D_unsup_loss_real: 0.642, D_unsup_loss_fake: 0.580, D_sup_loss: 0.135, D_sup_acc: 97.10 Train acc: 96.738 Test acc: 97.040 \n",
      "step: 4437 | Train: G_Loss: 1.115, D_unsup_loss_real: 0.654, D_unsup_loss_fake: 0.581, D_sup_loss: 0.132, D_sup_acc: 97.08 Train acc: 96.652 Test acc: 96.980 \n",
      "step: 4438 | Train: G_Loss: 1.154, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.593, D_sup_loss: 0.137, D_sup_acc: 97.02 Train acc: 96.725 Test acc: 97.020 \n",
      "step: 4439 | Train: G_Loss: 1.141, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.576, D_sup_loss: 0.134, D_sup_acc: 97.06 Train acc: 96.770 Test acc: 97.090 \n",
      "step: 4440 | Train: G_Loss: 1.017, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.558, D_sup_loss: 0.129, D_sup_acc: 97.13 Train acc: 96.707 Test acc: 96.970 \n",
      "step: 4441 | Train: G_Loss: 1.002, D_unsup_loss_real: 0.628, D_unsup_loss_fake: 0.579, D_sup_loss: 0.132, D_sup_acc: 97.01 Train acc: 96.650 Test acc: 97.000 \n",
      "step: 4442 | Train: G_Loss: 1.015, D_unsup_loss_real: 0.657, D_unsup_loss_fake: 0.665, D_sup_loss: 0.136, D_sup_acc: 97.04 Train acc: 96.703 Test acc: 97.000 \n",
      "step: 4443 | Train: G_Loss: 1.086, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.605, D_sup_loss: 0.134, D_sup_acc: 97.04 Train acc: 96.760 Test acc: 96.930 \n",
      "step: 4444 | Train: G_Loss: 1.141, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.606, D_sup_loss: 0.132, D_sup_acc: 96.97 Train acc: 96.745 Test acc: 97.020 \n",
      "step: 4445 | Train: G_Loss: 1.130, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.608, D_sup_loss: 0.136, D_sup_acc: 97.06 Train acc: 96.738 Test acc: 97.010 \n",
      "step: 4446 | Train: G_Loss: 1.182, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.628, D_sup_loss: 0.135, D_sup_acc: 97.05 Train acc: 96.673 Test acc: 96.910 \n",
      "step: 4447 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.571, D_sup_loss: 0.140, D_sup_acc: 96.95 Train acc: 96.740 Test acc: 96.980 \n",
      "step: 4448 | Train: G_Loss: 1.106, D_unsup_loss_real: 0.533, D_unsup_loss_fake: 0.649, D_sup_loss: 0.134, D_sup_acc: 97.02 Train acc: 96.762 Test acc: 97.020 \n",
      "step: 4449 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.593, D_sup_loss: 0.136, D_sup_acc: 97.06 Train acc: 96.863 Test acc: 97.020 \n",
      "step: 4450 | Train: G_Loss: 1.009, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.642, D_sup_loss: 0.132, D_sup_acc: 97.06 Train acc: 96.835 Test acc: 97.040 \n",
      "step: 4451 | Train: G_Loss: 1.220, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.635, D_sup_loss: 0.131, D_sup_acc: 97.08 Train acc: 96.833 Test acc: 96.950 \n",
      "step: 4452 | Train: G_Loss: 1.111, D_unsup_loss_real: 0.665, D_unsup_loss_fake: 0.585, D_sup_loss: 0.129, D_sup_acc: 96.99 Train acc: 96.683 Test acc: 96.990 \n",
      "step: 4453 | Train: G_Loss: 1.113, D_unsup_loss_real: 0.528, D_unsup_loss_fake: 0.535, D_sup_loss: 0.144, D_sup_acc: 97.03 Train acc: 96.860 Test acc: 97.140 \n",
      "step: 4454 | Train: G_Loss: 1.100, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.589, D_sup_loss: 0.132, D_sup_acc: 97.18 Train acc: 96.883 Test acc: 97.110 \n",
      "step: 4455 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.591, D_sup_loss: 0.127, D_sup_acc: 97.15 Train acc: 96.865 Test acc: 97.170 \n",
      "step: 4456 | Train: G_Loss: 1.149, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.585, D_sup_loss: 0.131, D_sup_acc: 97.21 Train acc: 96.882 Test acc: 97.080 \n",
      "step: 4457 | Train: G_Loss: 1.122, D_unsup_loss_real: 0.647, D_unsup_loss_fake: 0.590, D_sup_loss: 0.125, D_sup_acc: 97.12 Train acc: 96.858 Test acc: 97.080 \n",
      "step: 4458 | Train: G_Loss: 1.031, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.597, D_sup_loss: 0.127, D_sup_acc: 97.12 Train acc: 96.765 Test acc: 97.070 \n",
      "step: 4459 | Train: G_Loss: 1.170, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.577, D_sup_loss: 0.135, D_sup_acc: 97.11 Train acc: 96.820 Test acc: 97.080 \n",
      "step: 4460 | Train: G_Loss: 1.170, D_unsup_loss_real: 0.647, D_unsup_loss_fake: 0.525, D_sup_loss: 0.133, D_sup_acc: 97.12 Train acc: 96.867 Test acc: 97.090 \n",
      "step: 4461 | Train: G_Loss: 1.114, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.634, D_sup_loss: 0.125, D_sup_acc: 97.13 Train acc: 96.790 Test acc: 96.950 \n",
      "step: 4462 | Train: G_Loss: 1.039, D_unsup_loss_real: 0.663, D_unsup_loss_fake: 0.657, D_sup_loss: 0.134, D_sup_acc: 96.99 Train acc: 96.803 Test acc: 97.070 \n",
      "step: 4463 | Train: G_Loss: 1.100, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.552, D_sup_loss: 0.132, D_sup_acc: 97.11 Train acc: 96.767 Test acc: 97.010 \n",
      "step: 4464 | Train: G_Loss: 1.058, D_unsup_loss_real: 0.646, D_unsup_loss_fake: 0.586, D_sup_loss: 0.135, D_sup_acc: 97.05 Train acc: 96.708 Test acc: 97.010 \n",
      "step: 4465 | Train: G_Loss: 1.125, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.608, D_sup_loss: 0.140, D_sup_acc: 97.05 Train acc: 96.797 Test acc: 97.090 \n",
      "step: 4466 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.516, D_sup_loss: 0.134, D_sup_acc: 97.13 Train acc: 96.858 Test acc: 97.060 \n",
      "step: 4467 | Train: G_Loss: 1.104, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.568, D_sup_loss: 0.125, D_sup_acc: 97.10 Train acc: 96.858 Test acc: 97.130 \n",
      "step: 4468 | Train: G_Loss: 0.951, D_unsup_loss_real: 0.631, D_unsup_loss_fake: 0.556, D_sup_loss: 0.127, D_sup_acc: 97.17 Train acc: 96.735 Test acc: 97.080 \n",
      "step: 4469 | Train: G_Loss: 0.976, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.640, D_sup_loss: 0.135, D_sup_acc: 97.12 Train acc: 96.770 Test acc: 97.110 \n",
      "step: 4470 | Train: G_Loss: 1.027, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.681, D_sup_loss: 0.138, D_sup_acc: 97.15 Train acc: 96.605 Test acc: 96.990 \n",
      "step: 4471 | Train: G_Loss: 1.158, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.606, D_sup_loss: 0.145, D_sup_acc: 97.03 Train acc: 96.483 Test acc: 96.910 \n",
      "step: 4472 | Train: G_Loss: 1.214, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.595, D_sup_loss: 0.153, D_sup_acc: 96.95 Train acc: 96.843 Test acc: 97.170 \n",
      "step: 4473 | Train: G_Loss: 1.076, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.592, D_sup_loss: 0.131, D_sup_acc: 97.21 Train acc: 96.735 Test acc: 97.060 \n",
      "step: 4474 | Train: G_Loss: 1.114, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.606, D_sup_loss: 0.139, D_sup_acc: 97.10 Train acc: 96.650 Test acc: 96.990 \n",
      "step: 4475 | Train: G_Loss: 1.072, D_unsup_loss_real: 0.550, D_unsup_loss_fake: 0.591, D_sup_loss: 0.142, D_sup_acc: 97.03 Train acc: 96.802 Test acc: 97.020 \n",
      "step: 4476 | Train: G_Loss: 1.154, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.616, D_sup_loss: 0.127, D_sup_acc: 97.06 Train acc: 96.788 Test acc: 97.030 \n",
      "step: 4477 | Train: G_Loss: 1.198, D_unsup_loss_real: 0.671, D_unsup_loss_fake: 0.637, D_sup_loss: 0.133, D_sup_acc: 97.07 Train acc: 96.813 Test acc: 96.970 \n",
      "step: 4478 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.550, D_sup_loss: 0.130, D_sup_acc: 97.01 Train acc: 96.758 Test acc: 97.020 \n",
      "step: 4479 | Train: G_Loss: 1.071, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.592, D_sup_loss: 0.133, D_sup_acc: 97.06 Train acc: 96.812 Test acc: 96.980 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4480 | Train: G_Loss: 1.073, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.633, D_sup_loss: 0.129, D_sup_acc: 97.02 Train acc: 96.790 Test acc: 97.090 \n",
      "step: 4481 | Train: G_Loss: 1.068, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.573, D_sup_loss: 0.131, D_sup_acc: 97.13 Train acc: 96.747 Test acc: 97.060 \n",
      "step: 4482 | Train: G_Loss: 1.132, D_unsup_loss_real: 0.527, D_unsup_loss_fake: 0.667, D_sup_loss: 0.135, D_sup_acc: 97.10 Train acc: 96.683 Test acc: 97.050 \n",
      "step: 4483 | Train: G_Loss: 1.161, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.555, D_sup_loss: 0.136, D_sup_acc: 97.09 Train acc: 96.773 Test acc: 96.990 \n",
      "step: 4484 | Train: G_Loss: 1.171, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.575, D_sup_loss: 0.130, D_sup_acc: 97.03 Train acc: 96.555 Test acc: 96.850 \n",
      "step: 4485 | Train: G_Loss: 1.064, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.650, D_sup_loss: 0.146, D_sup_acc: 96.89 Train acc: 96.520 Test acc: 96.910 \n",
      "step: 4486 | Train: G_Loss: 1.115, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.621, D_sup_loss: 0.149, D_sup_acc: 96.95 Train acc: 96.500 Test acc: 96.830 \n",
      "step: 4487 | Train: G_Loss: 1.191, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.605, D_sup_loss: 0.148, D_sup_acc: 96.87 Train acc: 96.682 Test acc: 97.030 \n",
      "step: 4488 | Train: G_Loss: 1.056, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.553, D_sup_loss: 0.137, D_sup_acc: 97.07 Train acc: 96.890 Test acc: 97.080 \n",
      "step: 4489 | Train: G_Loss: 1.053, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.579, D_sup_loss: 0.126, D_sup_acc: 97.12 Train acc: 96.855 Test acc: 97.100 \n",
      "step: 4490 | Train: G_Loss: 1.099, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.614, D_sup_loss: 0.126, D_sup_acc: 97.14 Train acc: 96.748 Test acc: 97.000 \n",
      "step: 4491 | Train: G_Loss: 1.028, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.615, D_sup_loss: 0.133, D_sup_acc: 97.04 Train acc: 96.688 Test acc: 97.070 \n",
      "step: 4492 | Train: G_Loss: 1.084, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.637, D_sup_loss: 0.134, D_sup_acc: 97.11 Train acc: 96.718 Test acc: 97.090 \n",
      "step: 4493 | Train: G_Loss: 1.255, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.594, D_sup_loss: 0.131, D_sup_acc: 97.13 Train acc: 96.670 Test acc: 97.060 \n",
      "step: 4494 | Train: G_Loss: 1.199, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.567, D_sup_loss: 0.132, D_sup_acc: 97.10 Train acc: 96.603 Test acc: 96.990 \n",
      "step: 4495 | Train: G_Loss: 1.254, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.555, D_sup_loss: 0.139, D_sup_acc: 97.03 Train acc: 96.580 Test acc: 96.980 \n",
      "step: 4496 | Train: G_Loss: 1.214, D_unsup_loss_real: 0.646, D_unsup_loss_fake: 0.557, D_sup_loss: 0.140, D_sup_acc: 97.02 Train acc: 96.645 Test acc: 97.020 \n",
      "step: 4497 | Train: G_Loss: 1.166, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.554, D_sup_loss: 0.137, D_sup_acc: 97.06 Train acc: 96.757 Test acc: 97.060 \n",
      "step: 4498 | Train: G_Loss: 1.170, D_unsup_loss_real: 0.517, D_unsup_loss_fake: 0.616, D_sup_loss: 0.132, D_sup_acc: 97.10 Train acc: 96.862 Test acc: 97.110 \n",
      "step: 4499 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.547, D_sup_loss: 0.127, D_sup_acc: 97.15 Train acc: 96.675 Test acc: 97.010 \n",
      "step: 4500 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.566, D_sup_loss: 0.137, D_sup_acc: 97.05 Train acc: 96.528 Test acc: 96.800 \n",
      "Train Classifier Accuracy: 96.528%\n",
      "\n",
      "Test Classifier Accuracy: 96.800%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_4500.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_4500.h5\n",
      "step: 4501 | Train: G_Loss: 1.118, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.571, D_sup_loss: 0.144, D_sup_acc: 96.84 Train acc: 96.650 Test acc: 96.980 \n",
      "step: 4502 | Train: G_Loss: 1.074, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.641, D_sup_loss: 0.138, D_sup_acc: 97.02 Train acc: 96.297 Test acc: 96.700 \n",
      "step: 4503 | Train: G_Loss: 1.155, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.643, D_sup_loss: 0.156, D_sup_acc: 96.74 Train acc: 96.752 Test acc: 97.170 \n",
      "step: 4504 | Train: G_Loss: 1.109, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.522, D_sup_loss: 0.132, D_sup_acc: 97.21 Train acc: 96.802 Test acc: 97.240 \n",
      "step: 4505 | Train: G_Loss: 1.177, D_unsup_loss_real: 0.507, D_unsup_loss_fake: 0.621, D_sup_loss: 0.128, D_sup_acc: 97.27 Train acc: 96.762 Test acc: 97.230 \n",
      "step: 4506 | Train: G_Loss: 1.163, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.651, D_sup_loss: 0.127, D_sup_acc: 97.27 Train acc: 96.730 Test acc: 97.140 \n",
      "step: 4507 | Train: G_Loss: 1.180, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.574, D_sup_loss: 0.132, D_sup_acc: 97.18 Train acc: 96.513 Test acc: 96.830 \n",
      "step: 4508 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.650, D_unsup_loss_fake: 0.640, D_sup_loss: 0.145, D_sup_acc: 96.87 Train acc: 96.732 Test acc: 97.090 \n",
      "step: 4509 | Train: G_Loss: 1.147, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.621, D_sup_loss: 0.134, D_sup_acc: 97.13 Train acc: 96.763 Test acc: 97.200 \n",
      "step: 4510 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.570, D_sup_loss: 0.132, D_sup_acc: 97.24 Train acc: 96.810 Test acc: 97.120 \n",
      "step: 4511 | Train: G_Loss: 1.164, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.573, D_sup_loss: 0.131, D_sup_acc: 97.16 Train acc: 96.793 Test acc: 97.220 \n",
      "step: 4512 | Train: G_Loss: 1.040, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.615, D_sup_loss: 0.129, D_sup_acc: 97.26 Train acc: 96.857 Test acc: 97.160 \n",
      "step: 4513 | Train: G_Loss: 1.128, D_unsup_loss_real: 0.538, D_unsup_loss_fake: 0.575, D_sup_loss: 0.126, D_sup_acc: 97.20 Train acc: 96.857 Test acc: 97.090 \n",
      "step: 4514 | Train: G_Loss: 1.101, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.612, D_sup_loss: 0.124, D_sup_acc: 97.13 Train acc: 96.700 Test acc: 97.150 \n",
      "step: 4515 | Train: G_Loss: 1.199, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.639, D_sup_loss: 0.132, D_sup_acc: 97.19 Train acc: 96.788 Test acc: 97.120 \n",
      "step: 4516 | Train: G_Loss: 1.046, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.571, D_sup_loss: 0.131, D_sup_acc: 97.16 Train acc: 96.727 Test acc: 97.070 \n",
      "step: 4517 | Train: G_Loss: 1.214, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.582, D_sup_loss: 0.136, D_sup_acc: 97.11 Train acc: 96.700 Test acc: 96.970 \n",
      "step: 4518 | Train: G_Loss: 1.294, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.543, D_sup_loss: 0.139, D_sup_acc: 97.01 Train acc: 96.785 Test acc: 97.100 \n",
      "step: 4519 | Train: G_Loss: 1.115, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.652, D_sup_loss: 0.132, D_sup_acc: 97.14 Train acc: 96.682 Test acc: 97.110 \n",
      "step: 4520 | Train: G_Loss: 1.147, D_unsup_loss_real: 0.651, D_unsup_loss_fake: 0.565, D_sup_loss: 0.137, D_sup_acc: 97.15 Train acc: 96.753 Test acc: 97.110 \n",
      "step: 4521 | Train: G_Loss: 1.115, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.571, D_sup_loss: 0.133, D_sup_acc: 97.15 Train acc: 96.830 Test acc: 97.110 \n",
      "step: 4522 | Train: G_Loss: 1.150, D_unsup_loss_real: 0.516, D_unsup_loss_fake: 0.575, D_sup_loss: 0.130, D_sup_acc: 97.15 Train acc: 96.892 Test acc: 97.140 \n",
      "step: 4523 | Train: G_Loss: 1.207, D_unsup_loss_real: 0.544, D_unsup_loss_fake: 0.607, D_sup_loss: 0.124, D_sup_acc: 97.18 Train acc: 96.827 Test acc: 97.060 \n",
      "step: 4524 | Train: G_Loss: 1.175, D_unsup_loss_real: 0.526, D_unsup_loss_fake: 0.555, D_sup_loss: 0.128, D_sup_acc: 97.10 Train acc: 96.658 Test acc: 96.970 \n",
      "step: 4525 | Train: G_Loss: 1.163, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.593, D_sup_loss: 0.136, D_sup_acc: 97.01 Train acc: 96.745 Test acc: 97.070 \n",
      "step: 4526 | Train: G_Loss: 1.132, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.525, D_sup_loss: 0.131, D_sup_acc: 97.11 Train acc: 96.790 Test acc: 97.070 \n",
      "step: 4527 | Train: G_Loss: 1.179, D_unsup_loss_real: 0.527, D_unsup_loss_fake: 0.601, D_sup_loss: 0.129, D_sup_acc: 97.11 Train acc: 96.825 Test acc: 97.120 \n",
      "step: 4528 | Train: G_Loss: 1.163, D_unsup_loss_real: 0.655, D_unsup_loss_fake: 0.560, D_sup_loss: 0.127, D_sup_acc: 97.16 Train acc: 96.700 Test acc: 96.970 \n",
      "step: 4529 | Train: G_Loss: 1.100, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.536, D_sup_loss: 0.135, D_sup_acc: 97.01 Train acc: 96.737 Test acc: 97.120 \n",
      "step: 4530 | Train: G_Loss: 1.193, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.627, D_sup_loss: 0.129, D_sup_acc: 97.16 Train acc: 96.727 Test acc: 97.120 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4531 | Train: G_Loss: 1.181, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.648, D_sup_loss: 0.131, D_sup_acc: 97.16 Train acc: 96.697 Test acc: 97.040 \n",
      "step: 4532 | Train: G_Loss: 1.251, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.569, D_sup_loss: 0.133, D_sup_acc: 97.08 Train acc: 96.770 Test acc: 97.090 \n",
      "step: 4533 | Train: G_Loss: 1.239, D_unsup_loss_real: 0.538, D_unsup_loss_fake: 0.667, D_sup_loss: 0.126, D_sup_acc: 97.13 Train acc: 96.682 Test acc: 97.010 \n",
      "step: 4534 | Train: G_Loss: 1.227, D_unsup_loss_real: 0.676, D_unsup_loss_fake: 0.542, D_sup_loss: 0.133, D_sup_acc: 97.05 Train acc: 96.722 Test acc: 97.080 \n",
      "step: 4535 | Train: G_Loss: 1.168, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.520, D_sup_loss: 0.131, D_sup_acc: 97.12 Train acc: 96.717 Test acc: 96.880 \n",
      "step: 4536 | Train: G_Loss: 1.155, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.611, D_sup_loss: 0.128, D_sup_acc: 96.92 Train acc: 96.523 Test acc: 96.950 \n",
      "step: 4537 | Train: G_Loss: 1.151, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.533, D_sup_loss: 0.141, D_sup_acc: 96.99 Train acc: 96.698 Test acc: 97.040 \n",
      "step: 4538 | Train: G_Loss: 1.099, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.562, D_sup_loss: 0.130, D_sup_acc: 97.08 Train acc: 96.803 Test acc: 97.210 \n",
      "step: 4539 | Train: G_Loss: 1.164, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.683, D_sup_loss: 0.125, D_sup_acc: 97.25 Train acc: 96.590 Test acc: 96.990 \n",
      "step: 4540 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.629, D_sup_loss: 0.137, D_sup_acc: 97.03 Train acc: 96.712 Test acc: 97.120 \n",
      "step: 4541 | Train: G_Loss: 1.091, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.579, D_sup_loss: 0.133, D_sup_acc: 97.16 Train acc: 96.733 Test acc: 97.120 \n",
      "step: 4542 | Train: G_Loss: 1.140, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.598, D_sup_loss: 0.131, D_sup_acc: 97.16 Train acc: 96.685 Test acc: 97.070 \n",
      "step: 4543 | Train: G_Loss: 1.106, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.601, D_sup_loss: 0.133, D_sup_acc: 97.11 Train acc: 96.678 Test acc: 96.960 \n",
      "step: 4544 | Train: G_Loss: 1.146, D_unsup_loss_real: 0.538, D_unsup_loss_fake: 0.591, D_sup_loss: 0.135, D_sup_acc: 97.00 Train acc: 96.720 Test acc: 96.990 \n",
      "step: 4545 | Train: G_Loss: 1.099, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.598, D_sup_loss: 0.131, D_sup_acc: 97.03 Train acc: 96.497 Test acc: 96.860 \n",
      "step: 4546 | Train: G_Loss: 1.105, D_unsup_loss_real: 0.671, D_unsup_loss_fake: 0.606, D_sup_loss: 0.139, D_sup_acc: 96.90 Train acc: 96.660 Test acc: 97.120 \n",
      "step: 4547 | Train: G_Loss: 1.190, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.627, D_sup_loss: 0.131, D_sup_acc: 97.16 Train acc: 96.743 Test acc: 97.080 \n",
      "step: 4548 | Train: G_Loss: 1.094, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.607, D_sup_loss: 0.124, D_sup_acc: 97.12 Train acc: 96.707 Test acc: 96.980 \n",
      "step: 4549 | Train: G_Loss: 1.047, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.587, D_sup_loss: 0.131, D_sup_acc: 97.02 Train acc: 96.425 Test acc: 96.760 \n",
      "step: 4550 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.561, D_sup_loss: 0.144, D_sup_acc: 96.80 Train acc: 96.572 Test acc: 96.820 \n",
      "step: 4551 | Train: G_Loss: 1.069, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.660, D_sup_loss: 0.136, D_sup_acc: 96.86 Train acc: 96.425 Test acc: 96.740 \n",
      "step: 4552 | Train: G_Loss: 1.165, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.574, D_sup_loss: 0.148, D_sup_acc: 96.78 Train acc: 96.717 Test acc: 96.950 \n",
      "step: 4553 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.571, D_sup_loss: 0.135, D_sup_acc: 96.99 Train acc: 96.738 Test acc: 97.060 \n",
      "step: 4554 | Train: G_Loss: 1.195, D_unsup_loss_real: 0.532, D_unsup_loss_fake: 0.599, D_sup_loss: 0.133, D_sup_acc: 97.10 Train acc: 96.675 Test acc: 97.040 \n",
      "step: 4555 | Train: G_Loss: 1.163, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.661, D_sup_loss: 0.137, D_sup_acc: 97.08 Train acc: 96.658 Test acc: 96.980 \n",
      "step: 4556 | Train: G_Loss: 1.116, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.616, D_sup_loss: 0.135, D_sup_acc: 97.02 Train acc: 96.607 Test acc: 96.970 \n",
      "step: 4557 | Train: G_Loss: 1.167, D_unsup_loss_real: 0.644, D_unsup_loss_fake: 0.601, D_sup_loss: 0.139, D_sup_acc: 97.01 Train acc: 96.633 Test acc: 96.990 \n",
      "step: 4558 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.668, D_sup_loss: 0.137, D_sup_acc: 97.03 Train acc: 96.858 Test acc: 97.180 \n",
      "step: 4559 | Train: G_Loss: 1.125, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.623, D_sup_loss: 0.124, D_sup_acc: 97.22 Train acc: 96.673 Test acc: 97.010 \n",
      "step: 4560 | Train: G_Loss: 1.104, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.578, D_sup_loss: 0.136, D_sup_acc: 97.05 Train acc: 96.622 Test acc: 96.910 \n",
      "step: 4561 | Train: G_Loss: 1.074, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.503, D_sup_loss: 0.137, D_sup_acc: 96.95 Train acc: 96.732 Test acc: 97.070 \n",
      "step: 4562 | Train: G_Loss: 1.090, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.650, D_sup_loss: 0.133, D_sup_acc: 97.11 Train acc: 96.703 Test acc: 97.060 \n",
      "step: 4563 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.619, D_sup_loss: 0.135, D_sup_acc: 97.10 Train acc: 96.828 Test acc: 97.120 \n",
      "step: 4564 | Train: G_Loss: 1.157, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.616, D_sup_loss: 0.129, D_sup_acc: 97.16 Train acc: 96.790 Test acc: 97.130 \n",
      "step: 4565 | Train: G_Loss: 1.104, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.601, D_sup_loss: 0.132, D_sup_acc: 97.17 Train acc: 96.820 Test acc: 97.050 \n",
      "step: 4566 | Train: G_Loss: 1.191, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.624, D_sup_loss: 0.131, D_sup_acc: 97.09 Train acc: 96.792 Test acc: 96.960 \n",
      "step: 4567 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.565, D_sup_loss: 0.128, D_sup_acc: 97.00 Train acc: 96.757 Test acc: 96.940 \n",
      "step: 4568 | Train: G_Loss: 1.157, D_unsup_loss_real: 0.680, D_unsup_loss_fake: 0.642, D_sup_loss: 0.135, D_sup_acc: 96.98 Train acc: 96.485 Test acc: 96.730 \n",
      "step: 4569 | Train: G_Loss: 1.214, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.636, D_sup_loss: 0.142, D_sup_acc: 96.77 Train acc: 96.702 Test acc: 96.870 \n",
      "step: 4570 | Train: G_Loss: 1.177, D_unsup_loss_real: 0.532, D_unsup_loss_fake: 0.562, D_sup_loss: 0.135, D_sup_acc: 96.91 Train acc: 96.765 Test acc: 96.980 \n",
      "step: 4571 | Train: G_Loss: 1.105, D_unsup_loss_real: 0.685, D_unsup_loss_fake: 0.546, D_sup_loss: 0.130, D_sup_acc: 97.02 Train acc: 96.668 Test acc: 97.040 \n",
      "step: 4572 | Train: G_Loss: 1.056, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.564, D_sup_loss: 0.140, D_sup_acc: 97.08 Train acc: 96.613 Test acc: 96.960 \n",
      "step: 4573 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.544, D_unsup_loss_fake: 0.628, D_sup_loss: 0.143, D_sup_acc: 97.00 Train acc: 96.773 Test acc: 97.090 \n",
      "step: 4574 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.610, D_sup_loss: 0.136, D_sup_acc: 97.13 Train acc: 96.828 Test acc: 97.140 \n",
      "step: 4575 | Train: G_Loss: 1.105, D_unsup_loss_real: 0.538, D_unsup_loss_fake: 0.602, D_sup_loss: 0.132, D_sup_acc: 97.18 Train acc: 96.807 Test acc: 97.110 \n",
      "step: 4576 | Train: G_Loss: 1.114, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.621, D_sup_loss: 0.133, D_sup_acc: 97.15 Train acc: 96.712 Test acc: 96.960 \n",
      "step: 4577 | Train: G_Loss: 1.072, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.592, D_sup_loss: 0.140, D_sup_acc: 97.00 Train acc: 96.842 Test acc: 97.100 \n",
      "step: 4578 | Train: G_Loss: 1.077, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.562, D_sup_loss: 0.133, D_sup_acc: 97.14 Train acc: 96.652 Test acc: 97.040 \n",
      "step: 4579 | Train: G_Loss: 1.069, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.595, D_sup_loss: 0.145, D_sup_acc: 97.08 Train acc: 96.743 Test acc: 97.030 \n",
      "step: 4580 | Train: G_Loss: 1.158, D_unsup_loss_real: 0.631, D_unsup_loss_fake: 0.590, D_sup_loss: 0.139, D_sup_acc: 97.07 Train acc: 96.803 Test acc: 97.150 \n",
      "step: 4581 | Train: G_Loss: 1.099, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.573, D_sup_loss: 0.134, D_sup_acc: 97.19 Train acc: 96.872 Test acc: 97.100 \n",
      "step: 4582 | Train: G_Loss: 1.127, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.625, D_sup_loss: 0.129, D_sup_acc: 97.14 Train acc: 96.753 Test acc: 96.980 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4583 | Train: G_Loss: 1.117, D_unsup_loss_real: 0.686, D_unsup_loss_fake: 0.596, D_sup_loss: 0.135, D_sup_acc: 97.02 Train acc: 96.473 Test acc: 96.910 \n",
      "step: 4584 | Train: G_Loss: 1.078, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.571, D_sup_loss: 0.152, D_sup_acc: 96.95 Train acc: 96.825 Test acc: 97.150 \n",
      "step: 4585 | Train: G_Loss: 1.096, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.653, D_sup_loss: 0.131, D_sup_acc: 97.19 Train acc: 96.915 Test acc: 97.160 \n",
      "step: 4586 | Train: G_Loss: 1.128, D_unsup_loss_real: 0.505, D_unsup_loss_fake: 0.580, D_sup_loss: 0.126, D_sup_acc: 97.20 Train acc: 96.813 Test acc: 97.150 \n",
      "step: 4587 | Train: G_Loss: 1.106, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.611, D_sup_loss: 0.127, D_sup_acc: 97.19 Train acc: 96.593 Test acc: 97.030 \n",
      "step: 4588 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.579, D_sup_loss: 0.140, D_sup_acc: 97.07 Train acc: 96.473 Test acc: 96.860 \n",
      "step: 4589 | Train: G_Loss: 1.198, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.514, D_sup_loss: 0.149, D_sup_acc: 96.90 Train acc: 96.652 Test acc: 97.040 \n",
      "step: 4590 | Train: G_Loss: 1.143, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.588, D_sup_loss: 0.137, D_sup_acc: 97.08 Train acc: 96.578 Test acc: 96.960 \n",
      "step: 4591 | Train: G_Loss: 1.109, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.559, D_sup_loss: 0.141, D_sup_acc: 97.00 Train acc: 96.745 Test acc: 97.070 \n",
      "step: 4592 | Train: G_Loss: 1.126, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.540, D_sup_loss: 0.130, D_sup_acc: 97.11 Train acc: 96.742 Test acc: 97.060 \n",
      "step: 4593 | Train: G_Loss: 1.171, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.652, D_sup_loss: 0.131, D_sup_acc: 97.10 Train acc: 96.827 Test acc: 97.090 \n",
      "step: 4594 | Train: G_Loss: 1.084, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.582, D_sup_loss: 0.128, D_sup_acc: 97.13 Train acc: 96.627 Test acc: 96.950 \n",
      "step: 4595 | Train: G_Loss: 1.177, D_unsup_loss_real: 0.535, D_unsup_loss_fake: 0.584, D_sup_loss: 0.136, D_sup_acc: 96.99 Train acc: 96.657 Test acc: 97.130 \n",
      "step: 4596 | Train: G_Loss: 1.106, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.615, D_sup_loss: 0.135, D_sup_acc: 97.17 Train acc: 96.813 Test acc: 97.160 \n",
      "step: 4597 | Train: G_Loss: 1.091, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.655, D_sup_loss: 0.127, D_sup_acc: 97.20 Train acc: 96.645 Test acc: 97.130 \n",
      "step: 4598 | Train: G_Loss: 1.186, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.621, D_sup_loss: 0.136, D_sup_acc: 97.17 Train acc: 96.522 Test acc: 96.960 \n",
      "step: 4599 | Train: G_Loss: 1.128, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.547, D_sup_loss: 0.143, D_sup_acc: 97.00 Train acc: 96.735 Test acc: 97.130 \n",
      "step: 4600 | Train: G_Loss: 1.153, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.613, D_sup_loss: 0.133, D_sup_acc: 97.17 Train acc: 96.687 Test acc: 97.150 \n",
      "Train Classifier Accuracy: 96.687%\n",
      "\n",
      "Test Classifier Accuracy: 97.150%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_4600.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_4600.h5\n",
      "step: 4601 | Train: G_Loss: 1.202, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.608, D_sup_loss: 0.132, D_sup_acc: 97.19 Train acc: 96.607 Test acc: 97.010 \n",
      "step: 4602 | Train: G_Loss: 1.166, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.517, D_sup_loss: 0.138, D_sup_acc: 97.05 Train acc: 96.667 Test acc: 97.080 \n",
      "step: 4603 | Train: G_Loss: 1.123, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.621, D_sup_loss: 0.135, D_sup_acc: 97.12 Train acc: 96.702 Test acc: 97.170 \n",
      "step: 4604 | Train: G_Loss: 1.040, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.526, D_sup_loss: 0.135, D_sup_acc: 97.21 Train acc: 96.888 Test acc: 97.200 \n",
      "step: 4605 | Train: G_Loss: 1.157, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.581, D_sup_loss: 0.124, D_sup_acc: 97.24 Train acc: 96.773 Test acc: 97.180 \n",
      "step: 4606 | Train: G_Loss: 1.149, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.572, D_sup_loss: 0.130, D_sup_acc: 97.22 Train acc: 96.805 Test acc: 97.090 \n",
      "step: 4607 | Train: G_Loss: 1.078, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.597, D_sup_loss: 0.129, D_sup_acc: 97.13 Train acc: 96.793 Test acc: 97.060 \n",
      "step: 4608 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.593, D_sup_loss: 0.130, D_sup_acc: 97.10 Train acc: 96.667 Test acc: 97.040 \n",
      "step: 4609 | Train: G_Loss: 1.070, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.636, D_sup_loss: 0.132, D_sup_acc: 97.08 Train acc: 96.728 Test acc: 97.110 \n",
      "step: 4610 | Train: G_Loss: 1.025, D_unsup_loss_real: 0.530, D_unsup_loss_fake: 0.671, D_sup_loss: 0.134, D_sup_acc: 97.15 Train acc: 96.697 Test acc: 97.140 \n",
      "step: 4611 | Train: G_Loss: 1.161, D_unsup_loss_real: 0.677, D_unsup_loss_fake: 0.586, D_sup_loss: 0.135, D_sup_acc: 97.18 Train acc: 96.880 Test acc: 97.250 \n",
      "step: 4612 | Train: G_Loss: 1.191, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.611, D_sup_loss: 0.125, D_sup_acc: 97.28 Train acc: 96.795 Test acc: 97.240 \n",
      "step: 4613 | Train: G_Loss: 1.100, D_unsup_loss_real: 0.544, D_unsup_loss_fake: 0.670, D_sup_loss: 0.127, D_sup_acc: 97.27 Train acc: 96.728 Test acc: 97.140 \n",
      "step: 4614 | Train: G_Loss: 1.132, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.616, D_sup_loss: 0.131, D_sup_acc: 97.18 Train acc: 96.698 Test acc: 97.230 \n",
      "step: 4615 | Train: G_Loss: 1.158, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.615, D_sup_loss: 0.135, D_sup_acc: 97.27 Train acc: 96.818 Test acc: 97.240 \n",
      "step: 4616 | Train: G_Loss: 1.128, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.535, D_sup_loss: 0.129, D_sup_acc: 97.27 Train acc: 96.817 Test acc: 97.320 \n",
      "step: 4617 | Train: G_Loss: 1.153, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.605, D_sup_loss: 0.126, D_sup_acc: 97.35 Train acc: 96.592 Test acc: 97.120 \n",
      "step: 4618 | Train: G_Loss: 1.116, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.628, D_sup_loss: 0.140, D_sup_acc: 97.16 Train acc: 96.768 Test acc: 97.050 \n",
      "step: 4619 | Train: G_Loss: 1.118, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.646, D_sup_loss: 0.132, D_sup_acc: 97.09 Train acc: 96.813 Test acc: 97.060 \n",
      "step: 4620 | Train: G_Loss: 1.116, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.603, D_sup_loss: 0.129, D_sup_acc: 97.10 Train acc: 96.633 Test acc: 96.910 \n",
      "step: 4621 | Train: G_Loss: 1.166, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.594, D_sup_loss: 0.137, D_sup_acc: 96.95 Train acc: 96.752 Test acc: 97.000 \n",
      "step: 4622 | Train: G_Loss: 1.154, D_unsup_loss_real: 0.635, D_unsup_loss_fake: 0.604, D_sup_loss: 0.134, D_sup_acc: 97.04 Train acc: 96.645 Test acc: 97.060 \n",
      "step: 4623 | Train: G_Loss: 1.084, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.585, D_sup_loss: 0.140, D_sup_acc: 97.10 Train acc: 96.773 Test acc: 97.140 \n",
      "step: 4624 | Train: G_Loss: 1.140, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.565, D_sup_loss: 0.131, D_sup_acc: 97.18 Train acc: 96.822 Test acc: 97.170 \n",
      "step: 4625 | Train: G_Loss: 1.147, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.583, D_sup_loss: 0.128, D_sup_acc: 97.21 Train acc: 96.808 Test acc: 97.060 \n",
      "step: 4626 | Train: G_Loss: 1.090, D_unsup_loss_real: 0.644, D_unsup_loss_fake: 0.606, D_sup_loss: 0.129, D_sup_acc: 97.10 Train acc: 96.858 Test acc: 97.200 \n",
      "step: 4627 | Train: G_Loss: 1.080, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.631, D_sup_loss: 0.127, D_sup_acc: 97.24 Train acc: 96.860 Test acc: 97.260 \n",
      "step: 4628 | Train: G_Loss: 1.179, D_unsup_loss_real: 0.527, D_unsup_loss_fake: 0.611, D_sup_loss: 0.127, D_sup_acc: 97.29 Train acc: 96.858 Test acc: 97.100 \n",
      "step: 4629 | Train: G_Loss: 1.170, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.639, D_sup_loss: 0.125, D_sup_acc: 97.14 Train acc: 96.698 Test acc: 97.020 \n",
      "step: 4630 | Train: G_Loss: 1.106, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.540, D_sup_loss: 0.134, D_sup_acc: 97.06 Train acc: 96.793 Test acc: 97.230 \n",
      "step: 4631 | Train: G_Loss: 1.091, D_unsup_loss_real: 0.726, D_unsup_loss_fake: 0.672, D_sup_loss: 0.129, D_sup_acc: 97.27 Train acc: 96.773 Test acc: 97.130 \n",
      "step: 4632 | Train: G_Loss: 1.064, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.628, D_sup_loss: 0.132, D_sup_acc: 97.17 Train acc: 96.690 Test acc: 97.180 \n",
      "step: 4633 | Train: G_Loss: 1.114, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.612, D_sup_loss: 0.137, D_sup_acc: 97.22 Train acc: 96.822 Test acc: 97.120 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4634 | Train: G_Loss: 1.073, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.629, D_sup_loss: 0.132, D_sup_acc: 97.16 Train acc: 96.692 Test acc: 97.090 \n",
      "step: 4635 | Train: G_Loss: 1.190, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.602, D_sup_loss: 0.136, D_sup_acc: 97.13 Train acc: 96.628 Test acc: 96.940 \n",
      "step: 4636 | Train: G_Loss: 1.154, D_unsup_loss_real: 0.536, D_unsup_loss_fake: 0.616, D_sup_loss: 0.144, D_sup_acc: 96.98 Train acc: 96.840 Test acc: 97.100 \n",
      "step: 4637 | Train: G_Loss: 1.119, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.570, D_sup_loss: 0.126, D_sup_acc: 97.14 Train acc: 96.782 Test acc: 97.100 \n",
      "step: 4638 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.584, D_sup_loss: 0.133, D_sup_acc: 97.14 Train acc: 96.745 Test acc: 97.110 \n",
      "step: 4639 | Train: G_Loss: 1.179, D_unsup_loss_real: 0.531, D_unsup_loss_fake: 0.672, D_sup_loss: 0.135, D_sup_acc: 97.15 Train acc: 96.600 Test acc: 97.070 \n",
      "step: 4640 | Train: G_Loss: 1.125, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.593, D_sup_loss: 0.142, D_sup_acc: 97.11 Train acc: 96.402 Test acc: 96.750 \n",
      "step: 4641 | Train: G_Loss: 1.107, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.600, D_sup_loss: 0.154, D_sup_acc: 96.79 Train acc: 96.837 Test acc: 97.110 \n",
      "step: 4642 | Train: G_Loss: 1.154, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.556, D_sup_loss: 0.130, D_sup_acc: 97.15 Train acc: 96.832 Test acc: 97.090 \n",
      "step: 4643 | Train: G_Loss: 1.164, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.536, D_sup_loss: 0.130, D_sup_acc: 97.13 Train acc: 96.903 Test acc: 97.230 \n",
      "step: 4644 | Train: G_Loss: 1.162, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.577, D_sup_loss: 0.125, D_sup_acc: 97.27 Train acc: 96.837 Test acc: 97.130 \n",
      "step: 4645 | Train: G_Loss: 1.087, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.615, D_sup_loss: 0.130, D_sup_acc: 97.17 Train acc: 96.853 Test acc: 97.170 \n",
      "step: 4646 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.618, D_sup_loss: 0.127, D_sup_acc: 97.21 Train acc: 96.898 Test acc: 97.130 \n",
      "step: 4647 | Train: G_Loss: 1.109, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.630, D_sup_loss: 0.126, D_sup_acc: 97.17 Train acc: 96.882 Test acc: 97.160 \n",
      "step: 4648 | Train: G_Loss: 1.068, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.615, D_sup_loss: 0.129, D_sup_acc: 97.20 Train acc: 96.657 Test acc: 96.930 \n",
      "step: 4649 | Train: G_Loss: 1.266, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.577, D_sup_loss: 0.142, D_sup_acc: 96.97 Train acc: 96.933 Test acc: 97.200 \n",
      "step: 4650 | Train: G_Loss: 1.129, D_unsup_loss_real: 0.671, D_unsup_loss_fake: 0.638, D_sup_loss: 0.122, D_sup_acc: 97.24 Train acc: 96.833 Test acc: 97.210 \n",
      "step: 4651 | Train: G_Loss: 1.070, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.613, D_sup_loss: 0.131, D_sup_acc: 97.25 Train acc: 96.805 Test acc: 97.210 \n",
      "step: 4652 | Train: G_Loss: 1.176, D_unsup_loss_real: 0.545, D_unsup_loss_fake: 0.628, D_sup_loss: 0.130, D_sup_acc: 97.25 Train acc: 96.745 Test acc: 97.200 \n",
      "step: 4653 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.595, D_sup_loss: 0.131, D_sup_acc: 97.24 Train acc: 96.795 Test acc: 97.210 \n",
      "step: 4654 | Train: G_Loss: 1.164, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.590, D_sup_loss: 0.128, D_sup_acc: 97.25 Train acc: 96.812 Test acc: 97.220 \n",
      "step: 4655 | Train: G_Loss: 1.091, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.554, D_sup_loss: 0.128, D_sup_acc: 97.26 Train acc: 96.827 Test acc: 97.180 \n",
      "step: 4656 | Train: G_Loss: 1.119, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.656, D_sup_loss: 0.126, D_sup_acc: 97.22 Train acc: 96.783 Test acc: 97.170 \n",
      "step: 4657 | Train: G_Loss: 1.152, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.665, D_sup_loss: 0.131, D_sup_acc: 97.21 Train acc: 96.342 Test acc: 96.750 \n",
      "step: 4658 | Train: G_Loss: 1.109, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.598, D_sup_loss: 0.155, D_sup_acc: 96.79 Train acc: 96.793 Test acc: 97.090 \n",
      "step: 4659 | Train: G_Loss: 1.155, D_unsup_loss_real: 0.655, D_unsup_loss_fake: 0.506, D_sup_loss: 0.134, D_sup_acc: 97.13 Train acc: 96.873 Test acc: 97.140 \n",
      "step: 4660 | Train: G_Loss: 1.203, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.656, D_sup_loss: 0.129, D_sup_acc: 97.18 Train acc: 96.705 Test acc: 96.980 \n",
      "step: 4661 | Train: G_Loss: 1.111, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.613, D_sup_loss: 0.138, D_sup_acc: 97.02 Train acc: 96.710 Test acc: 97.070 \n",
      "step: 4662 | Train: G_Loss: 1.200, D_unsup_loss_real: 0.509, D_unsup_loss_fake: 0.528, D_sup_loss: 0.136, D_sup_acc: 97.11 Train acc: 96.845 Test acc: 97.250 \n",
      "step: 4663 | Train: G_Loss: 1.035, D_unsup_loss_real: 0.653, D_unsup_loss_fake: 0.589, D_sup_loss: 0.126, D_sup_acc: 97.28 Train acc: 96.837 Test acc: 97.150 \n",
      "step: 4664 | Train: G_Loss: 1.137, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.662, D_sup_loss: 0.128, D_sup_acc: 97.19 Train acc: 96.623 Test acc: 96.840 \n",
      "step: 4665 | Train: G_Loss: 1.066, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.651, D_sup_loss: 0.139, D_sup_acc: 96.88 Train acc: 96.605 Test acc: 96.910 \n",
      "step: 4666 | Train: G_Loss: 1.182, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.622, D_sup_loss: 0.141, D_sup_acc: 96.95 Train acc: 96.725 Test acc: 97.070 \n",
      "step: 4667 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.542, D_sup_loss: 0.135, D_sup_acc: 97.11 Train acc: 96.820 Test acc: 97.220 \n",
      "step: 4668 | Train: G_Loss: 1.223, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.574, D_sup_loss: 0.127, D_sup_acc: 97.26 Train acc: 96.688 Test acc: 97.010 \n",
      "step: 4669 | Train: G_Loss: 1.067, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.540, D_sup_loss: 0.132, D_sup_acc: 97.05 Train acc: 96.415 Test acc: 96.810 \n",
      "step: 4670 | Train: G_Loss: 1.223, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.504, D_sup_loss: 0.150, D_sup_acc: 96.85 Train acc: 96.812 Test acc: 97.130 \n",
      "step: 4671 | Train: G_Loss: 1.130, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.627, D_sup_loss: 0.128, D_sup_acc: 97.17 Train acc: 96.553 Test acc: 96.920 \n",
      "step: 4672 | Train: G_Loss: 1.176, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.662, D_sup_loss: 0.140, D_sup_acc: 96.96 Train acc: 96.543 Test acc: 97.010 \n",
      "step: 4673 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.691, D_sup_loss: 0.142, D_sup_acc: 97.05 Train acc: 96.652 Test acc: 97.040 \n",
      "step: 4674 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.676, D_unsup_loss_fake: 0.590, D_sup_loss: 0.140, D_sup_acc: 97.08 Train acc: 96.470 Test acc: 96.790 \n",
      "step: 4675 | Train: G_Loss: 1.118, D_unsup_loss_real: 0.690, D_unsup_loss_fake: 0.562, D_sup_loss: 0.153, D_sup_acc: 96.83 Train acc: 96.497 Test acc: 96.780 \n",
      "step: 4676 | Train: G_Loss: 1.172, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.647, D_sup_loss: 0.151, D_sup_acc: 96.82 Train acc: 96.762 Test acc: 97.010 \n",
      "step: 4677 | Train: G_Loss: 1.094, D_unsup_loss_real: 0.651, D_unsup_loss_fake: 0.569, D_sup_loss: 0.140, D_sup_acc: 97.05 Train acc: 96.338 Test acc: 96.770 \n",
      "step: 4678 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.630, D_sup_loss: 0.158, D_sup_acc: 96.81 Train acc: 96.722 Test acc: 97.120 \n",
      "step: 4679 | Train: G_Loss: 1.175, D_unsup_loss_real: 0.654, D_unsup_loss_fake: 0.631, D_sup_loss: 0.137, D_sup_acc: 97.16 Train acc: 96.667 Test acc: 97.140 \n",
      "step: 4680 | Train: G_Loss: 1.074, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.549, D_sup_loss: 0.141, D_sup_acc: 97.18 Train acc: 96.755 Test acc: 97.230 \n",
      "step: 4681 | Train: G_Loss: 1.091, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.586, D_sup_loss: 0.133, D_sup_acc: 97.27 Train acc: 96.802 Test acc: 97.310 \n",
      "step: 4682 | Train: G_Loss: 1.103, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.605, D_sup_loss: 0.131, D_sup_acc: 97.34 Train acc: 96.863 Test acc: 97.200 \n",
      "step: 4683 | Train: G_Loss: 1.172, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.622, D_sup_loss: 0.127, D_sup_acc: 97.24 Train acc: 96.692 Test acc: 97.040 \n",
      "step: 4684 | Train: G_Loss: 1.097, D_unsup_loss_real: 0.635, D_unsup_loss_fake: 0.672, D_sup_loss: 0.136, D_sup_acc: 97.08 Train acc: 96.438 Test acc: 96.830 \n",
      "step: 4685 | Train: G_Loss: 1.191, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.649, D_sup_loss: 0.151, D_sup_acc: 96.87 Train acc: 96.600 Test acc: 96.910 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4686 | Train: G_Loss: 1.125, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.617, D_sup_loss: 0.141, D_sup_acc: 96.95 Train acc: 96.447 Test acc: 96.860 \n",
      "step: 4687 | Train: G_Loss: 1.116, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.599, D_sup_loss: 0.147, D_sup_acc: 96.90 Train acc: 96.613 Test acc: 97.070 \n",
      "step: 4688 | Train: G_Loss: 1.203, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.628, D_sup_loss: 0.140, D_sup_acc: 97.11 Train acc: 96.232 Test acc: 96.710 \n",
      "step: 4689 | Train: G_Loss: 1.013, D_unsup_loss_real: 0.690, D_unsup_loss_fake: 0.653, D_sup_loss: 0.156, D_sup_acc: 96.75 Train acc: 96.467 Test acc: 96.940 \n",
      "step: 4690 | Train: G_Loss: 1.188, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.570, D_sup_loss: 0.147, D_sup_acc: 96.98 Train acc: 96.562 Test acc: 96.930 \n",
      "step: 4691 | Train: G_Loss: 1.246, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.520, D_sup_loss: 0.143, D_sup_acc: 96.97 Train acc: 96.800 Test acc: 97.150 \n",
      "step: 4692 | Train: G_Loss: 1.061, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.557, D_sup_loss: 0.126, D_sup_acc: 97.19 Train acc: 96.545 Test acc: 96.910 \n",
      "step: 4693 | Train: G_Loss: 1.119, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.550, D_sup_loss: 0.141, D_sup_acc: 96.95 Train acc: 96.728 Test acc: 97.060 \n",
      "step: 4694 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.512, D_unsup_loss_fake: 0.601, D_sup_loss: 0.129, D_sup_acc: 97.10 Train acc: 96.857 Test acc: 97.210 \n",
      "step: 4695 | Train: G_Loss: 1.032, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.628, D_sup_loss: 0.125, D_sup_acc: 97.25 Train acc: 96.712 Test acc: 97.100 \n",
      "step: 4696 | Train: G_Loss: 1.166, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.660, D_sup_loss: 0.133, D_sup_acc: 97.14 Train acc: 96.672 Test acc: 97.110 \n",
      "step: 4697 | Train: G_Loss: 1.139, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.597, D_sup_loss: 0.137, D_sup_acc: 97.15 Train acc: 96.765 Test acc: 97.190 \n",
      "step: 4698 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.688, D_unsup_loss_fake: 0.577, D_sup_loss: 0.132, D_sup_acc: 97.23 Train acc: 96.677 Test acc: 97.110 \n",
      "step: 4699 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.575, D_sup_loss: 0.138, D_sup_acc: 97.15 Train acc: 96.787 Test acc: 97.220 \n",
      "step: 4700 | Train: G_Loss: 1.071, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.605, D_sup_loss: 0.128, D_sup_acc: 97.26 Train acc: 96.703 Test acc: 97.090 \n",
      "Train Classifier Accuracy: 96.703%\n",
      "\n",
      "Test Classifier Accuracy: 97.090%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_4700.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_4700.h5\n",
      "step: 4701 | Train: G_Loss: 1.079, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.540, D_sup_loss: 0.137, D_sup_acc: 97.13 Train acc: 96.820 Test acc: 97.160 \n",
      "step: 4702 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.682, D_sup_loss: 0.129, D_sup_acc: 97.20 Train acc: 96.430 Test acc: 96.910 \n",
      "step: 4703 | Train: G_Loss: 1.092, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.645, D_sup_loss: 0.153, D_sup_acc: 96.95 Train acc: 96.560 Test acc: 97.190 \n",
      "step: 4704 | Train: G_Loss: 1.149, D_unsup_loss_real: 0.472, D_unsup_loss_fake: 0.606, D_sup_loss: 0.142, D_sup_acc: 97.23 Train acc: 96.592 Test acc: 97.220 \n",
      "step: 4705 | Train: G_Loss: 1.188, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.584, D_sup_loss: 0.139, D_sup_acc: 97.26 Train acc: 96.693 Test acc: 97.210 \n",
      "step: 4706 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.677, D_sup_loss: 0.131, D_sup_acc: 97.25 Train acc: 96.688 Test acc: 97.070 \n",
      "step: 4707 | Train: G_Loss: 1.103, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.541, D_sup_loss: 0.136, D_sup_acc: 97.11 Train acc: 96.570 Test acc: 97.040 \n",
      "step: 4708 | Train: G_Loss: 1.154, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.624, D_sup_loss: 0.141, D_sup_acc: 97.08 Train acc: 96.668 Test acc: 97.130 \n",
      "step: 4709 | Train: G_Loss: 1.079, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.610, D_sup_loss: 0.137, D_sup_acc: 97.17 Train acc: 96.633 Test acc: 97.080 \n",
      "step: 4710 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.611, D_sup_loss: 0.136, D_sup_acc: 97.12 Train acc: 96.523 Test acc: 97.010 \n",
      "step: 4711 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.580, D_sup_loss: 0.142, D_sup_acc: 97.05 Train acc: 96.665 Test acc: 97.090 \n",
      "step: 4712 | Train: G_Loss: 1.211, D_unsup_loss_real: 0.666, D_unsup_loss_fake: 0.588, D_sup_loss: 0.135, D_sup_acc: 97.13 Train acc: 96.838 Test acc: 97.110 \n",
      "step: 4713 | Train: G_Loss: 1.165, D_unsup_loss_real: 0.664, D_unsup_loss_fake: 0.635, D_sup_loss: 0.126, D_sup_acc: 97.15 Train acc: 96.808 Test acc: 97.310 \n",
      "step: 4714 | Train: G_Loss: 1.219, D_unsup_loss_real: 0.474, D_unsup_loss_fake: 0.627, D_sup_loss: 0.130, D_sup_acc: 97.34 Train acc: 96.893 Test acc: 97.210 \n",
      "step: 4715 | Train: G_Loss: 1.200, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.634, D_sup_loss: 0.126, D_sup_acc: 97.25 Train acc: 96.872 Test acc: 97.250 \n",
      "step: 4716 | Train: G_Loss: 1.114, D_unsup_loss_real: 0.678, D_unsup_loss_fake: 0.643, D_sup_loss: 0.127, D_sup_acc: 97.28 Train acc: 96.652 Test acc: 97.110 \n",
      "step: 4717 | Train: G_Loss: 1.084, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.566, D_sup_loss: 0.140, D_sup_acc: 97.15 Train acc: 96.663 Test acc: 97.160 \n",
      "step: 4718 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.554, D_sup_loss: 0.138, D_sup_acc: 97.20 Train acc: 96.703 Test acc: 97.230 \n",
      "step: 4719 | Train: G_Loss: 1.059, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.599, D_sup_loss: 0.136, D_sup_acc: 97.27 Train acc: 96.780 Test acc: 97.340 \n",
      "step: 4720 | Train: G_Loss: 1.172, D_unsup_loss_real: 0.667, D_unsup_loss_fake: 0.677, D_sup_loss: 0.130, D_sup_acc: 97.37 Train acc: 96.732 Test acc: 97.270 \n",
      "step: 4721 | Train: G_Loss: 1.123, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.642, D_sup_loss: 0.134, D_sup_acc: 97.30 Train acc: 96.667 Test acc: 97.090 \n",
      "step: 4722 | Train: G_Loss: 1.005, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.570, D_sup_loss: 0.135, D_sup_acc: 97.13 Train acc: 96.687 Test acc: 97.170 \n",
      "step: 4723 | Train: G_Loss: 1.084, D_unsup_loss_real: 0.661, D_unsup_loss_fake: 0.606, D_sup_loss: 0.137, D_sup_acc: 97.21 Train acc: 96.630 Test acc: 97.130 \n",
      "step: 4724 | Train: G_Loss: 1.047, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.600, D_sup_loss: 0.140, D_sup_acc: 97.17 Train acc: 96.707 Test acc: 97.190 \n",
      "step: 4725 | Train: G_Loss: 1.132, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.678, D_sup_loss: 0.134, D_sup_acc: 97.23 Train acc: 96.747 Test acc: 97.020 \n",
      "step: 4726 | Train: G_Loss: 1.136, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.645, D_sup_loss: 0.133, D_sup_acc: 97.06 Train acc: 96.652 Test acc: 97.070 \n",
      "step: 4727 | Train: G_Loss: 1.139, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.585, D_sup_loss: 0.140, D_sup_acc: 97.11 Train acc: 96.787 Test acc: 97.200 \n",
      "step: 4728 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.589, D_sup_loss: 0.132, D_sup_acc: 97.24 Train acc: 96.612 Test acc: 97.080 \n",
      "step: 4729 | Train: G_Loss: 1.106, D_unsup_loss_real: 0.528, D_unsup_loss_fake: 0.534, D_sup_loss: 0.139, D_sup_acc: 97.12 Train acc: 96.648 Test acc: 97.140 \n",
      "step: 4730 | Train: G_Loss: 1.117, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.631, D_sup_loss: 0.137, D_sup_acc: 97.18 Train acc: 96.723 Test acc: 97.190 \n",
      "step: 4731 | Train: G_Loss: 1.175, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.617, D_sup_loss: 0.132, D_sup_acc: 97.23 Train acc: 96.650 Test acc: 97.190 \n",
      "step: 4732 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.648, D_unsup_loss_fake: 0.604, D_sup_loss: 0.139, D_sup_acc: 97.23 Train acc: 96.712 Test acc: 97.120 \n",
      "step: 4733 | Train: G_Loss: 1.041, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.642, D_sup_loss: 0.137, D_sup_acc: 97.16 Train acc: 96.673 Test acc: 97.130 \n",
      "step: 4734 | Train: G_Loss: 1.078, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.578, D_sup_loss: 0.139, D_sup_acc: 97.17 Train acc: 96.672 Test acc: 97.090 \n",
      "step: 4735 | Train: G_Loss: 1.133, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.639, D_sup_loss: 0.137, D_sup_acc: 97.13 Train acc: 96.555 Test acc: 96.970 \n",
      "step: 4736 | Train: G_Loss: 1.141, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.588, D_sup_loss: 0.147, D_sup_acc: 97.01 Train acc: 96.703 Test acc: 97.100 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4737 | Train: G_Loss: 1.107, D_unsup_loss_real: 0.659, D_unsup_loss_fake: 0.606, D_sup_loss: 0.139, D_sup_acc: 97.14 Train acc: 96.577 Test acc: 97.050 \n",
      "step: 4738 | Train: G_Loss: 1.061, D_unsup_loss_real: 0.546, D_unsup_loss_fake: 0.563, D_sup_loss: 0.145, D_sup_acc: 97.09 Train acc: 96.522 Test acc: 97.030 \n",
      "step: 4739 | Train: G_Loss: 1.202, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.609, D_sup_loss: 0.147, D_sup_acc: 97.07 Train acc: 96.725 Test acc: 97.200 \n",
      "step: 4740 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.583, D_sup_loss: 0.134, D_sup_acc: 97.24 Train acc: 96.652 Test acc: 97.140 \n",
      "step: 4741 | Train: G_Loss: 1.087, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.600, D_sup_loss: 0.139, D_sup_acc: 97.18 Train acc: 96.740 Test acc: 97.110 \n",
      "step: 4742 | Train: G_Loss: 0.977, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.597, D_sup_loss: 0.132, D_sup_acc: 97.15 Train acc: 96.758 Test acc: 97.140 \n",
      "step: 4743 | Train: G_Loss: 1.080, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.642, D_sup_loss: 0.138, D_sup_acc: 97.18 Train acc: 96.795 Test acc: 97.120 \n",
      "step: 4744 | Train: G_Loss: 1.178, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.607, D_sup_loss: 0.135, D_sup_acc: 97.16 Train acc: 96.817 Test acc: 96.960 \n",
      "step: 4745 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.668, D_sup_loss: 0.132, D_sup_acc: 97.00 Train acc: 96.810 Test acc: 96.960 \n",
      "step: 4746 | Train: G_Loss: 1.079, D_unsup_loss_real: 0.691, D_unsup_loss_fake: 0.669, D_sup_loss: 0.133, D_sup_acc: 97.00 Train acc: 96.735 Test acc: 97.070 \n",
      "step: 4747 | Train: G_Loss: 1.094, D_unsup_loss_real: 0.693, D_unsup_loss_fake: 0.640, D_sup_loss: 0.138, D_sup_acc: 97.11 Train acc: 96.750 Test acc: 97.030 \n",
      "step: 4748 | Train: G_Loss: 1.074, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.611, D_sup_loss: 0.135, D_sup_acc: 97.07 Train acc: 96.617 Test acc: 96.920 \n",
      "step: 4749 | Train: G_Loss: 1.085, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.610, D_sup_loss: 0.144, D_sup_acc: 96.96 Train acc: 96.717 Test acc: 96.970 \n",
      "step: 4750 | Train: G_Loss: 1.105, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.597, D_sup_loss: 0.136, D_sup_acc: 97.01 Train acc: 96.765 Test acc: 97.000 \n",
      "step: 4751 | Train: G_Loss: 1.040, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.708, D_sup_loss: 0.135, D_sup_acc: 97.04 Train acc: 96.787 Test acc: 97.050 \n",
      "step: 4752 | Train: G_Loss: 1.240, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.643, D_sup_loss: 0.130, D_sup_acc: 97.09 Train acc: 96.715 Test acc: 97.030 \n",
      "step: 4753 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.563, D_sup_loss: 0.137, D_sup_acc: 97.07 Train acc: 96.655 Test acc: 96.870 \n",
      "step: 4754 | Train: G_Loss: 1.226, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.577, D_sup_loss: 0.142, D_sup_acc: 96.91 Train acc: 96.513 Test acc: 96.740 \n",
      "step: 4755 | Train: G_Loss: 1.186, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.602, D_sup_loss: 0.147, D_sup_acc: 96.78 Train acc: 96.555 Test acc: 96.860 \n",
      "step: 4756 | Train: G_Loss: 1.152, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.487, D_sup_loss: 0.144, D_sup_acc: 96.90 Train acc: 96.680 Test acc: 97.010 \n",
      "step: 4757 | Train: G_Loss: 1.224, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.526, D_sup_loss: 0.131, D_sup_acc: 97.05 Train acc: 96.718 Test acc: 96.990 \n",
      "step: 4758 | Train: G_Loss: 1.063, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.563, D_sup_loss: 0.128, D_sup_acc: 97.03 Train acc: 96.603 Test acc: 96.990 \n",
      "step: 4759 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.634, D_sup_loss: 0.137, D_sup_acc: 97.03 Train acc: 96.592 Test acc: 97.030 \n",
      "step: 4760 | Train: G_Loss: 1.076, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.545, D_sup_loss: 0.139, D_sup_acc: 97.07 Train acc: 96.395 Test acc: 96.800 \n",
      "step: 4761 | Train: G_Loss: 1.048, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.703, D_sup_loss: 0.152, D_sup_acc: 96.84 Train acc: 96.638 Test acc: 96.970 \n",
      "step: 4762 | Train: G_Loss: 1.063, D_unsup_loss_real: 0.541, D_unsup_loss_fake: 0.613, D_sup_loss: 0.137, D_sup_acc: 97.01 Train acc: 96.663 Test acc: 96.990 \n",
      "step: 4763 | Train: G_Loss: 1.173, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.623, D_sup_loss: 0.136, D_sup_acc: 97.03 Train acc: 96.793 Test acc: 97.080 \n",
      "step: 4764 | Train: G_Loss: 1.139, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.645, D_sup_loss: 0.128, D_sup_acc: 97.12 Train acc: 96.743 Test acc: 97.090 \n",
      "step: 4765 | Train: G_Loss: 1.126, D_unsup_loss_real: 0.529, D_unsup_loss_fake: 0.621, D_sup_loss: 0.131, D_sup_acc: 97.13 Train acc: 96.563 Test acc: 96.920 \n",
      "step: 4766 | Train: G_Loss: 1.220, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.598, D_sup_loss: 0.139, D_sup_acc: 96.96 Train acc: 96.663 Test acc: 97.110 \n",
      "step: 4767 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.545, D_sup_loss: 0.136, D_sup_acc: 97.15 Train acc: 96.737 Test acc: 96.980 \n",
      "step: 4768 | Train: G_Loss: 1.097, D_unsup_loss_real: 0.690, D_unsup_loss_fake: 0.609, D_sup_loss: 0.133, D_sup_acc: 97.02 Train acc: 96.732 Test acc: 96.930 \n",
      "step: 4769 | Train: G_Loss: 1.167, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.623, D_sup_loss: 0.134, D_sup_acc: 96.97 Train acc: 96.745 Test acc: 96.970 \n",
      "step: 4770 | Train: G_Loss: 1.143, D_unsup_loss_real: 0.661, D_unsup_loss_fake: 0.608, D_sup_loss: 0.128, D_sup_acc: 97.01 Train acc: 96.488 Test acc: 96.830 \n",
      "step: 4771 | Train: G_Loss: 1.140, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.600, D_sup_loss: 0.148, D_sup_acc: 96.87 Train acc: 96.553 Test acc: 96.950 \n",
      "step: 4772 | Train: G_Loss: 1.053, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.561, D_sup_loss: 0.142, D_sup_acc: 96.99 Train acc: 96.735 Test acc: 97.020 \n",
      "step: 4773 | Train: G_Loss: 1.192, D_unsup_loss_real: 0.635, D_unsup_loss_fake: 0.555, D_sup_loss: 0.129, D_sup_acc: 97.06 Train acc: 96.713 Test acc: 97.000 \n",
      "step: 4774 | Train: G_Loss: 1.119, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.616, D_sup_loss: 0.130, D_sup_acc: 97.04 Train acc: 96.787 Test acc: 97.020 \n",
      "step: 4775 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.631, D_sup_loss: 0.130, D_sup_acc: 97.06 Train acc: 96.712 Test acc: 96.890 \n",
      "step: 4776 | Train: G_Loss: 1.186, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.643, D_sup_loss: 0.139, D_sup_acc: 96.93 Train acc: 96.707 Test acc: 96.870 \n",
      "step: 4777 | Train: G_Loss: 1.166, D_unsup_loss_real: 0.499, D_unsup_loss_fake: 0.648, D_sup_loss: 0.134, D_sup_acc: 96.91 Train acc: 96.695 Test acc: 96.910 \n",
      "step: 4778 | Train: G_Loss: 1.153, D_unsup_loss_real: 0.546, D_unsup_loss_fake: 0.552, D_sup_loss: 0.135, D_sup_acc: 96.95 Train acc: 96.632 Test acc: 96.960 \n",
      "step: 4779 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.533, D_sup_loss: 0.132, D_sup_acc: 97.00 Train acc: 96.577 Test acc: 96.980 \n",
      "step: 4780 | Train: G_Loss: 1.236, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.630, D_sup_loss: 0.136, D_sup_acc: 97.02 Train acc: 96.778 Test acc: 97.030 \n",
      "step: 4781 | Train: G_Loss: 1.177, D_unsup_loss_real: 0.672, D_unsup_loss_fake: 0.644, D_sup_loss: 0.129, D_sup_acc: 97.07 Train acc: 96.788 Test acc: 97.150 \n",
      "step: 4782 | Train: G_Loss: 1.074, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.595, D_sup_loss: 0.126, D_sup_acc: 97.19 Train acc: 96.685 Test acc: 97.000 \n",
      "step: 4783 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.626, D_sup_loss: 0.131, D_sup_acc: 97.04 Train acc: 96.480 Test acc: 96.880 \n",
      "step: 4784 | Train: G_Loss: 1.099, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.632, D_sup_loss: 0.144, D_sup_acc: 96.92 Train acc: 96.512 Test acc: 97.000 \n",
      "step: 4785 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.660, D_sup_loss: 0.143, D_sup_acc: 97.04 Train acc: 96.585 Test acc: 96.990 \n",
      "step: 4786 | Train: G_Loss: 1.109, D_unsup_loss_real: 0.693, D_unsup_loss_fake: 0.633, D_sup_loss: 0.140, D_sup_acc: 97.03 Train acc: 96.403 Test acc: 96.670 \n",
      "step: 4787 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.598, D_sup_loss: 0.151, D_sup_acc: 96.71 Train acc: 96.613 Test acc: 96.860 \n",
      "step: 4788 | Train: G_Loss: 1.188, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.616, D_sup_loss: 0.139, D_sup_acc: 96.90 Train acc: 96.650 Test acc: 96.980 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4789 | Train: G_Loss: 1.175, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.542, D_sup_loss: 0.134, D_sup_acc: 97.02 Train acc: 96.687 Test acc: 97.030 \n",
      "step: 4790 | Train: G_Loss: 1.060, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.589, D_sup_loss: 0.129, D_sup_acc: 97.07 Train acc: 96.570 Test acc: 96.990 \n",
      "step: 4791 | Train: G_Loss: 1.073, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.586, D_sup_loss: 0.140, D_sup_acc: 97.03 Train acc: 96.747 Test acc: 96.970 \n",
      "step: 4792 | Train: G_Loss: 1.047, D_unsup_loss_real: 0.676, D_unsup_loss_fake: 0.502, D_sup_loss: 0.132, D_sup_acc: 97.01 Train acc: 96.597 Test acc: 96.860 \n",
      "step: 4793 | Train: G_Loss: 1.101, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.547, D_sup_loss: 0.140, D_sup_acc: 96.90 Train acc: 96.842 Test acc: 97.000 \n",
      "step: 4794 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.525, D_unsup_loss_fake: 0.654, D_sup_loss: 0.126, D_sup_acc: 97.04 Train acc: 96.772 Test acc: 96.930 \n",
      "step: 4795 | Train: G_Loss: 1.284, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.631, D_sup_loss: 0.130, D_sup_acc: 96.97 Train acc: 96.860 Test acc: 97.090 \n",
      "step: 4796 | Train: G_Loss: 1.153, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.641, D_sup_loss: 0.126, D_sup_acc: 97.13 Train acc: 96.472 Test acc: 96.820 \n",
      "step: 4797 | Train: G_Loss: 1.192, D_unsup_loss_real: 0.685, D_unsup_loss_fake: 0.578, D_sup_loss: 0.144, D_sup_acc: 96.86 Train acc: 96.952 Test acc: 97.200 \n",
      "step: 4798 | Train: G_Loss: 1.094, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.587, D_sup_loss: 0.122, D_sup_acc: 97.24 Train acc: 96.885 Test acc: 97.090 \n",
      "step: 4799 | Train: G_Loss: 1.130, D_unsup_loss_real: 0.656, D_unsup_loss_fake: 0.617, D_sup_loss: 0.123, D_sup_acc: 97.13 Train acc: 96.813 Test acc: 97.140 \n",
      "step: 4800 | Train: G_Loss: 1.188, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.598, D_sup_loss: 0.128, D_sup_acc: 97.18 Train acc: 96.745 Test acc: 97.000 \n",
      "Train Classifier Accuracy: 96.745%\n",
      "\n",
      "Test Classifier Accuracy: 97.000%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_4800.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_4800.h5\n",
      "step: 4801 | Train: G_Loss: 1.155, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.707, D_sup_loss: 0.130, D_sup_acc: 97.04 Train acc: 96.583 Test acc: 96.810 \n",
      "step: 4802 | Train: G_Loss: 1.266, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.604, D_sup_loss: 0.145, D_sup_acc: 96.85 Train acc: 96.665 Test acc: 96.860 \n",
      "step: 4803 | Train: G_Loss: 1.149, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.529, D_sup_loss: 0.140, D_sup_acc: 96.90 Train acc: 96.733 Test acc: 96.960 \n",
      "step: 4804 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.531, D_sup_loss: 0.135, D_sup_acc: 97.00 Train acc: 96.917 Test acc: 97.120 \n",
      "step: 4805 | Train: G_Loss: 1.187, D_unsup_loss_real: 0.698, D_unsup_loss_fake: 0.682, D_sup_loss: 0.123, D_sup_acc: 97.16 Train acc: 96.758 Test acc: 96.990 \n",
      "step: 4806 | Train: G_Loss: 1.171, D_unsup_loss_real: 0.533, D_unsup_loss_fake: 0.547, D_sup_loss: 0.134, D_sup_acc: 97.03 Train acc: 96.832 Test acc: 97.100 \n",
      "step: 4807 | Train: G_Loss: 1.092, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.666, D_sup_loss: 0.127, D_sup_acc: 97.14 Train acc: 96.643 Test acc: 97.020 \n",
      "step: 4808 | Train: G_Loss: 1.115, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.557, D_sup_loss: 0.141, D_sup_acc: 97.06 Train acc: 96.847 Test acc: 97.130 \n",
      "step: 4809 | Train: G_Loss: 1.194, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.549, D_sup_loss: 0.126, D_sup_acc: 97.17 Train acc: 96.792 Test acc: 97.010 \n",
      "step: 4810 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.536, D_sup_loss: 0.127, D_sup_acc: 97.05 Train acc: 96.660 Test acc: 96.960 \n",
      "step: 4811 | Train: G_Loss: 1.089, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.631, D_sup_loss: 0.136, D_sup_acc: 97.00 Train acc: 96.692 Test acc: 96.970 \n",
      "step: 4812 | Train: G_Loss: 1.061, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.637, D_sup_loss: 0.134, D_sup_acc: 97.01 Train acc: 96.487 Test acc: 96.760 \n",
      "step: 4813 | Train: G_Loss: 1.064, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.558, D_sup_loss: 0.145, D_sup_acc: 96.80 Train acc: 96.450 Test acc: 96.760 \n",
      "step: 4814 | Train: G_Loss: 1.155, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.617, D_sup_loss: 0.145, D_sup_acc: 96.80 Train acc: 96.673 Test acc: 96.900 \n",
      "step: 4815 | Train: G_Loss: 1.143, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.671, D_sup_loss: 0.133, D_sup_acc: 96.94 Train acc: 96.727 Test acc: 96.940 \n",
      "step: 4816 | Train: G_Loss: 1.107, D_unsup_loss_real: 0.653, D_unsup_loss_fake: 0.632, D_sup_loss: 0.131, D_sup_acc: 96.98 Train acc: 96.730 Test acc: 96.950 \n",
      "step: 4817 | Train: G_Loss: 1.178, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.588, D_sup_loss: 0.133, D_sup_acc: 96.99 Train acc: 96.737 Test acc: 96.940 \n",
      "step: 4818 | Train: G_Loss: 1.179, D_unsup_loss_real: 0.546, D_unsup_loss_fake: 0.632, D_sup_loss: 0.133, D_sup_acc: 96.98 Train acc: 96.712 Test acc: 96.920 \n",
      "step: 4819 | Train: G_Loss: 1.195, D_unsup_loss_real: 0.542, D_unsup_loss_fake: 0.588, D_sup_loss: 0.135, D_sup_acc: 96.96 Train acc: 96.722 Test acc: 96.950 \n",
      "step: 4820 | Train: G_Loss: 1.169, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.570, D_sup_loss: 0.134, D_sup_acc: 96.99 Train acc: 96.738 Test acc: 96.890 \n",
      "step: 4821 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.470, D_sup_loss: 0.132, D_sup_acc: 96.93 Train acc: 96.820 Test acc: 97.010 \n",
      "step: 4822 | Train: G_Loss: 1.156, D_unsup_loss_real: 0.531, D_unsup_loss_fake: 0.537, D_sup_loss: 0.128, D_sup_acc: 97.05 Train acc: 96.712 Test acc: 96.870 \n",
      "step: 4823 | Train: G_Loss: 1.126, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.548, D_sup_loss: 0.130, D_sup_acc: 96.91 Train acc: 96.590 Test acc: 96.960 \n",
      "step: 4824 | Train: G_Loss: 1.116, D_unsup_loss_real: 0.644, D_unsup_loss_fake: 0.639, D_sup_loss: 0.138, D_sup_acc: 97.00 Train acc: 96.480 Test acc: 96.770 \n",
      "step: 4825 | Train: G_Loss: 1.112, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.593, D_sup_loss: 0.148, D_sup_acc: 96.81 Train acc: 96.733 Test acc: 96.970 \n",
      "step: 4826 | Train: G_Loss: 1.050, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.645, D_sup_loss: 0.131, D_sup_acc: 97.01 Train acc: 96.497 Test acc: 96.780 \n",
      "step: 4827 | Train: G_Loss: 1.064, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.619, D_sup_loss: 0.145, D_sup_acc: 96.82 Train acc: 96.635 Test acc: 96.830 \n",
      "step: 4828 | Train: G_Loss: 1.106, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.571, D_sup_loss: 0.138, D_sup_acc: 96.87 Train acc: 96.852 Test acc: 96.930 \n",
      "step: 4829 | Train: G_Loss: 1.120, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.636, D_sup_loss: 0.126, D_sup_acc: 96.97 Train acc: 96.882 Test acc: 97.100 \n",
      "step: 4830 | Train: G_Loss: 1.043, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.626, D_sup_loss: 0.128, D_sup_acc: 97.14 Train acc: 96.683 Test acc: 96.980 \n",
      "step: 4831 | Train: G_Loss: 1.156, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.713, D_sup_loss: 0.136, D_sup_acc: 97.02 Train acc: 96.588 Test acc: 96.900 \n",
      "step: 4832 | Train: G_Loss: 1.174, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.583, D_sup_loss: 0.147, D_sup_acc: 96.94 Train acc: 96.865 Test acc: 97.130 \n",
      "step: 4833 | Train: G_Loss: 1.135, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.578, D_sup_loss: 0.131, D_sup_acc: 97.17 Train acc: 96.882 Test acc: 97.070 \n",
      "step: 4834 | Train: G_Loss: 1.298, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.591, D_sup_loss: 0.127, D_sup_acc: 97.11 Train acc: 96.903 Test acc: 97.120 \n",
      "step: 4835 | Train: G_Loss: 1.112, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.613, D_sup_loss: 0.128, D_sup_acc: 97.16 Train acc: 96.547 Test acc: 96.900 \n",
      "step: 4836 | Train: G_Loss: 1.155, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.515, D_sup_loss: 0.148, D_sup_acc: 96.94 Train acc: 96.615 Test acc: 96.910 \n",
      "step: 4837 | Train: G_Loss: 1.228, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.552, D_sup_loss: 0.140, D_sup_acc: 96.95 Train acc: 96.850 Test acc: 97.140 \n",
      "step: 4838 | Train: G_Loss: 1.127, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.556, D_sup_loss: 0.126, D_sup_acc: 97.18 Train acc: 96.917 Test acc: 97.140 \n",
      "step: 4839 | Train: G_Loss: 1.154, D_unsup_loss_real: 0.628, D_unsup_loss_fake: 0.657, D_sup_loss: 0.120, D_sup_acc: 97.18 Train acc: 96.900 Test acc: 97.130 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4840 | Train: G_Loss: 1.105, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.627, D_sup_loss: 0.125, D_sup_acc: 97.17 Train acc: 96.863 Test acc: 97.070 \n",
      "step: 4841 | Train: G_Loss: 1.055, D_unsup_loss_real: 0.647, D_unsup_loss_fake: 0.607, D_sup_loss: 0.128, D_sup_acc: 97.11 Train acc: 96.713 Test acc: 96.930 \n",
      "step: 4842 | Train: G_Loss: 1.063, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.572, D_sup_loss: 0.142, D_sup_acc: 96.97 Train acc: 96.617 Test acc: 96.910 \n",
      "step: 4843 | Train: G_Loss: 1.118, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.658, D_sup_loss: 0.144, D_sup_acc: 96.95 Train acc: 96.767 Test acc: 97.160 \n",
      "step: 4844 | Train: G_Loss: 1.210, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.618, D_sup_loss: 0.133, D_sup_acc: 97.20 Train acc: 96.698 Test acc: 96.970 \n",
      "step: 4845 | Train: G_Loss: 1.216, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.604, D_sup_loss: 0.135, D_sup_acc: 97.01 Train acc: 96.755 Test acc: 97.030 \n",
      "step: 4846 | Train: G_Loss: 1.223, D_unsup_loss_real: 0.513, D_unsup_loss_fake: 0.633, D_sup_loss: 0.131, D_sup_acc: 97.07 Train acc: 96.783 Test acc: 97.020 \n",
      "step: 4847 | Train: G_Loss: 1.129, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.623, D_sup_loss: 0.132, D_sup_acc: 97.06 Train acc: 96.410 Test acc: 96.680 \n",
      "step: 4848 | Train: G_Loss: 1.085, D_unsup_loss_real: 0.661, D_unsup_loss_fake: 0.554, D_sup_loss: 0.154, D_sup_acc: 96.72 Train acc: 96.733 Test acc: 96.930 \n",
      "step: 4849 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.663, D_unsup_loss_fake: 0.600, D_sup_loss: 0.135, D_sup_acc: 96.97 Train acc: 96.773 Test acc: 97.000 \n",
      "step: 4850 | Train: G_Loss: 1.119, D_unsup_loss_real: 0.537, D_unsup_loss_fake: 0.587, D_sup_loss: 0.132, D_sup_acc: 97.04 Train acc: 96.610 Test acc: 96.870 \n",
      "step: 4851 | Train: G_Loss: 1.081, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.544, D_sup_loss: 0.143, D_sup_acc: 96.91 Train acc: 96.817 Test acc: 97.100 \n",
      "step: 4852 | Train: G_Loss: 1.030, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.587, D_sup_loss: 0.128, D_sup_acc: 97.14 Train acc: 96.698 Test acc: 96.980 \n",
      "step: 4853 | Train: G_Loss: 0.970, D_unsup_loss_real: 0.648, D_unsup_loss_fake: 0.639, D_sup_loss: 0.134, D_sup_acc: 97.02 Train acc: 96.455 Test acc: 96.720 \n",
      "step: 4854 | Train: G_Loss: 1.198, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.604, D_sup_loss: 0.147, D_sup_acc: 96.76 Train acc: 96.743 Test acc: 97.060 \n",
      "step: 4855 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.635, D_unsup_loss_fake: 0.587, D_sup_loss: 0.136, D_sup_acc: 97.10 Train acc: 96.727 Test acc: 97.040 \n",
      "step: 4856 | Train: G_Loss: 1.130, D_unsup_loss_real: 0.510, D_unsup_loss_fake: 0.580, D_sup_loss: 0.134, D_sup_acc: 97.08 Train acc: 96.618 Test acc: 96.890 \n",
      "step: 4857 | Train: G_Loss: 1.200, D_unsup_loss_real: 0.545, D_unsup_loss_fake: 0.612, D_sup_loss: 0.138, D_sup_acc: 96.93 Train acc: 96.758 Test acc: 97.180 \n",
      "step: 4858 | Train: G_Loss: 1.186, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.543, D_sup_loss: 0.131, D_sup_acc: 97.22 Train acc: 96.920 Test acc: 97.220 \n",
      "step: 4859 | Train: G_Loss: 1.175, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.579, D_sup_loss: 0.122, D_sup_acc: 97.26 Train acc: 96.887 Test acc: 97.210 \n",
      "step: 4860 | Train: G_Loss: 1.123, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.629, D_sup_loss: 0.123, D_sup_acc: 97.25 Train acc: 96.625 Test acc: 96.970 \n",
      "step: 4861 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.635, D_sup_loss: 0.139, D_sup_acc: 97.01 Train acc: 96.808 Test acc: 97.150 \n",
      "step: 4862 | Train: G_Loss: 1.072, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.558, D_sup_loss: 0.130, D_sup_acc: 97.19 Train acc: 96.810 Test acc: 97.200 \n",
      "step: 4863 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.576, D_sup_loss: 0.128, D_sup_acc: 97.24 Train acc: 96.702 Test acc: 97.070 \n",
      "step: 4864 | Train: G_Loss: 1.114, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.598, D_sup_loss: 0.132, D_sup_acc: 97.11 Train acc: 96.785 Test acc: 97.160 \n",
      "step: 4865 | Train: G_Loss: 1.221, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.562, D_sup_loss: 0.130, D_sup_acc: 97.20 Train acc: 96.837 Test acc: 97.160 \n",
      "step: 4866 | Train: G_Loss: 1.110, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.586, D_sup_loss: 0.124, D_sup_acc: 97.20 Train acc: 96.633 Test acc: 97.030 \n",
      "step: 4867 | Train: G_Loss: 1.137, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.547, D_sup_loss: 0.138, D_sup_acc: 97.07 Train acc: 96.597 Test acc: 96.910 \n",
      "step: 4868 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.606, D_sup_loss: 0.141, D_sup_acc: 96.95 Train acc: 96.585 Test acc: 96.860 \n",
      "step: 4869 | Train: G_Loss: 1.068, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.588, D_sup_loss: 0.138, D_sup_acc: 96.90 Train acc: 96.753 Test acc: 97.100 \n",
      "step: 4870 | Train: G_Loss: 1.237, D_unsup_loss_real: 0.628, D_unsup_loss_fake: 0.650, D_sup_loss: 0.129, D_sup_acc: 97.14 Train acc: 96.817 Test acc: 97.020 \n",
      "step: 4871 | Train: G_Loss: 1.136, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.608, D_sup_loss: 0.122, D_sup_acc: 97.06 Train acc: 96.630 Test acc: 97.000 \n",
      "step: 4872 | Train: G_Loss: 1.224, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.556, D_sup_loss: 0.139, D_sup_acc: 97.04 Train acc: 96.888 Test acc: 97.210 \n",
      "step: 4873 | Train: G_Loss: 1.222, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.645, D_sup_loss: 0.119, D_sup_acc: 97.25 Train acc: 96.860 Test acc: 97.150 \n",
      "step: 4874 | Train: G_Loss: 1.116, D_unsup_loss_real: 0.647, D_unsup_loss_fake: 0.584, D_sup_loss: 0.125, D_sup_acc: 97.19 Train acc: 96.832 Test acc: 97.200 \n",
      "step: 4875 | Train: G_Loss: 1.117, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.540, D_sup_loss: 0.131, D_sup_acc: 97.24 Train acc: 96.750 Test acc: 97.070 \n",
      "step: 4876 | Train: G_Loss: 1.115, D_unsup_loss_real: 0.527, D_unsup_loss_fake: 0.605, D_sup_loss: 0.137, D_sup_acc: 97.11 Train acc: 96.825 Test acc: 97.180 \n",
      "step: 4877 | Train: G_Loss: 1.110, D_unsup_loss_real: 0.534, D_unsup_loss_fake: 0.699, D_sup_loss: 0.131, D_sup_acc: 97.22 Train acc: 96.668 Test acc: 97.080 \n",
      "step: 4878 | Train: G_Loss: 1.007, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.594, D_sup_loss: 0.135, D_sup_acc: 97.12 Train acc: 96.743 Test acc: 97.170 \n",
      "step: 4879 | Train: G_Loss: 1.165, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.558, D_sup_loss: 0.132, D_sup_acc: 97.21 Train acc: 96.755 Test acc: 97.170 \n",
      "step: 4880 | Train: G_Loss: 1.091, D_unsup_loss_real: 0.667, D_unsup_loss_fake: 0.617, D_sup_loss: 0.130, D_sup_acc: 97.21 Train acc: 96.670 Test acc: 97.010 \n",
      "step: 4881 | Train: G_Loss: 1.133, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.587, D_sup_loss: 0.138, D_sup_acc: 97.05 Train acc: 96.812 Test acc: 97.100 \n",
      "step: 4882 | Train: G_Loss: 1.180, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.663, D_sup_loss: 0.128, D_sup_acc: 97.14 Train acc: 96.947 Test acc: 97.120 \n",
      "step: 4883 | Train: G_Loss: 1.021, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.592, D_sup_loss: 0.121, D_sup_acc: 97.16 Train acc: 96.722 Test acc: 96.980 \n",
      "step: 4884 | Train: G_Loss: 1.073, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.578, D_sup_loss: 0.133, D_sup_acc: 97.02 Train acc: 96.582 Test acc: 96.800 \n",
      "step: 4885 | Train: G_Loss: 1.156, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.578, D_sup_loss: 0.138, D_sup_acc: 96.84 Train acc: 96.658 Test acc: 96.940 \n",
      "step: 4886 | Train: G_Loss: 1.053, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.591, D_sup_loss: 0.135, D_sup_acc: 96.98 Train acc: 96.770 Test acc: 97.040 \n",
      "step: 4887 | Train: G_Loss: 1.133, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.515, D_sup_loss: 0.131, D_sup_acc: 97.08 Train acc: 96.812 Test acc: 97.070 \n",
      "step: 4888 | Train: G_Loss: 1.090, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.632, D_sup_loss: 0.127, D_sup_acc: 97.11 Train acc: 96.787 Test acc: 97.090 \n",
      "step: 4889 | Train: G_Loss: 1.104, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.630, D_sup_loss: 0.131, D_sup_acc: 97.13 Train acc: 96.757 Test acc: 96.990 \n",
      "step: 4890 | Train: G_Loss: 1.129, D_unsup_loss_real: 0.535, D_unsup_loss_fake: 0.615, D_sup_loss: 0.132, D_sup_acc: 97.03 Train acc: 96.800 Test acc: 97.070 \n",
      "step: 4891 | Train: G_Loss: 1.268, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.569, D_sup_loss: 0.128, D_sup_acc: 97.11 Train acc: 96.807 Test acc: 97.090 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4892 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.562, D_sup_loss: 0.122, D_sup_acc: 97.13 Train acc: 96.798 Test acc: 97.010 \n",
      "step: 4893 | Train: G_Loss: 1.156, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.561, D_sup_loss: 0.126, D_sup_acc: 97.05 Train acc: 96.742 Test acc: 97.020 \n",
      "step: 4894 | Train: G_Loss: 1.147, D_unsup_loss_real: 0.518, D_unsup_loss_fake: 0.599, D_sup_loss: 0.134, D_sup_acc: 97.06 Train acc: 96.835 Test acc: 97.100 \n",
      "step: 4895 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.639, D_unsup_loss_fake: 0.557, D_sup_loss: 0.129, D_sup_acc: 97.14 Train acc: 96.847 Test acc: 97.170 \n",
      "step: 4896 | Train: G_Loss: 1.160, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.610, D_sup_loss: 0.127, D_sup_acc: 97.21 Train acc: 96.882 Test acc: 97.190 \n",
      "step: 4897 | Train: G_Loss: 1.186, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.606, D_sup_loss: 0.127, D_sup_acc: 97.23 Train acc: 96.840 Test acc: 97.100 \n",
      "step: 4898 | Train: G_Loss: 1.096, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.579, D_sup_loss: 0.127, D_sup_acc: 97.14 Train acc: 96.773 Test acc: 97.110 \n",
      "step: 4899 | Train: G_Loss: 1.111, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.571, D_sup_loss: 0.134, D_sup_acc: 97.15 Train acc: 96.853 Test acc: 97.170 \n",
      "step: 4900 | Train: G_Loss: 1.226, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.574, D_sup_loss: 0.125, D_sup_acc: 97.21 Train acc: 96.728 Test acc: 97.040 \n",
      "Train Classifier Accuracy: 96.728%\n",
      "\n",
      "Test Classifier Accuracy: 97.040%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_4900.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_4900.h5\n",
      "step: 4901 | Train: G_Loss: 1.077, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.530, D_sup_loss: 0.138, D_sup_acc: 97.08 Train acc: 96.765 Test acc: 97.020 \n",
      "step: 4902 | Train: G_Loss: 1.187, D_unsup_loss_real: 0.542, D_unsup_loss_fake: 0.579, D_sup_loss: 0.136, D_sup_acc: 97.06 Train acc: 96.868 Test acc: 97.140 \n",
      "step: 4903 | Train: G_Loss: 1.155, D_unsup_loss_real: 0.631, D_unsup_loss_fake: 0.662, D_sup_loss: 0.124, D_sup_acc: 97.18 Train acc: 96.810 Test acc: 97.070 \n",
      "step: 4904 | Train: G_Loss: 1.123, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.618, D_sup_loss: 0.128, D_sup_acc: 97.11 Train acc: 96.833 Test acc: 97.080 \n",
      "step: 4905 | Train: G_Loss: 1.215, D_unsup_loss_real: 0.497, D_unsup_loss_fake: 0.551, D_sup_loss: 0.126, D_sup_acc: 97.12 Train acc: 96.857 Test acc: 97.170 \n",
      "step: 4906 | Train: G_Loss: 1.169, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.530, D_sup_loss: 0.126, D_sup_acc: 97.21 Train acc: 96.815 Test acc: 97.110 \n",
      "step: 4907 | Train: G_Loss: 1.167, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.656, D_sup_loss: 0.126, D_sup_acc: 97.15 Train acc: 96.765 Test acc: 97.040 \n",
      "step: 4908 | Train: G_Loss: 1.132, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.579, D_sup_loss: 0.130, D_sup_acc: 97.08 Train acc: 96.815 Test acc: 97.140 \n",
      "step: 4909 | Train: G_Loss: 1.156, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.605, D_sup_loss: 0.125, D_sup_acc: 97.18 Train acc: 96.812 Test acc: 97.100 \n",
      "step: 4910 | Train: G_Loss: 1.015, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.609, D_sup_loss: 0.123, D_sup_acc: 97.14 Train acc: 96.697 Test acc: 96.990 \n",
      "step: 4911 | Train: G_Loss: 1.184, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.622, D_sup_loss: 0.134, D_sup_acc: 97.03 Train acc: 96.872 Test acc: 97.130 \n",
      "step: 4912 | Train: G_Loss: 1.183, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.579, D_sup_loss: 0.121, D_sup_acc: 97.17 Train acc: 96.630 Test acc: 96.940 \n",
      "step: 4913 | Train: G_Loss: 1.214, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.605, D_sup_loss: 0.138, D_sup_acc: 96.98 Train acc: 96.785 Test acc: 97.060 \n",
      "step: 4914 | Train: G_Loss: 1.217, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.569, D_sup_loss: 0.126, D_sup_acc: 97.10 Train acc: 96.483 Test acc: 96.750 \n",
      "step: 4915 | Train: G_Loss: 1.116, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.663, D_sup_loss: 0.146, D_sup_acc: 96.79 Train acc: 96.732 Test acc: 97.060 \n",
      "step: 4916 | Train: G_Loss: 1.170, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.595, D_sup_loss: 0.130, D_sup_acc: 97.10 Train acc: 96.697 Test acc: 96.970 \n",
      "step: 4917 | Train: G_Loss: 1.069, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.488, D_sup_loss: 0.136, D_sup_acc: 97.01 Train acc: 96.773 Test acc: 97.000 \n",
      "step: 4918 | Train: G_Loss: 1.190, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.573, D_sup_loss: 0.125, D_sup_acc: 97.04 Train acc: 96.665 Test acc: 96.940 \n",
      "step: 4919 | Train: G_Loss: 1.080, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.558, D_sup_loss: 0.140, D_sup_acc: 96.98 Train acc: 96.692 Test acc: 96.990 \n",
      "step: 4920 | Train: G_Loss: 1.120, D_unsup_loss_real: 0.534, D_unsup_loss_fake: 0.714, D_sup_loss: 0.132, D_sup_acc: 97.03 Train acc: 96.533 Test acc: 96.920 \n",
      "step: 4921 | Train: G_Loss: 1.157, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.572, D_sup_loss: 0.141, D_sup_acc: 96.96 Train acc: 96.690 Test acc: 96.990 \n",
      "step: 4922 | Train: G_Loss: 1.084, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.598, D_sup_loss: 0.131, D_sup_acc: 97.03 Train acc: 96.757 Test acc: 97.020 \n",
      "step: 4923 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.494, D_unsup_loss_fake: 0.629, D_sup_loss: 0.128, D_sup_acc: 97.06 Train acc: 96.715 Test acc: 96.990 \n",
      "step: 4924 | Train: G_Loss: 1.089, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.641, D_sup_loss: 0.132, D_sup_acc: 97.03 Train acc: 96.573 Test acc: 96.850 \n",
      "step: 4925 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.698, D_unsup_loss_fake: 0.587, D_sup_loss: 0.142, D_sup_acc: 96.89 Train acc: 96.572 Test acc: 96.910 \n",
      "step: 4926 | Train: G_Loss: 1.108, D_unsup_loss_real: 0.657, D_unsup_loss_fake: 0.636, D_sup_loss: 0.144, D_sup_acc: 96.95 Train acc: 96.728 Test acc: 97.050 \n",
      "step: 4927 | Train: G_Loss: 1.034, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.556, D_sup_loss: 0.131, D_sup_acc: 97.09 Train acc: 96.798 Test acc: 97.130 \n",
      "step: 4928 | Train: G_Loss: 1.114, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.609, D_sup_loss: 0.125, D_sup_acc: 97.17 Train acc: 96.777 Test acc: 97.130 \n",
      "step: 4929 | Train: G_Loss: 1.056, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.541, D_sup_loss: 0.126, D_sup_acc: 97.17 Train acc: 96.748 Test acc: 97.160 \n",
      "step: 4930 | Train: G_Loss: 1.128, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.687, D_sup_loss: 0.130, D_sup_acc: 97.20 Train acc: 96.733 Test acc: 96.950 \n",
      "step: 4931 | Train: G_Loss: 1.087, D_unsup_loss_real: 0.644, D_unsup_loss_fake: 0.568, D_sup_loss: 0.128, D_sup_acc: 96.99 Train acc: 96.588 Test acc: 96.900 \n",
      "step: 4932 | Train: G_Loss: 1.061, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.569, D_sup_loss: 0.146, D_sup_acc: 96.94 Train acc: 96.698 Test acc: 97.000 \n",
      "step: 4933 | Train: G_Loss: 0.995, D_unsup_loss_real: 0.656, D_unsup_loss_fake: 0.615, D_sup_loss: 0.138, D_sup_acc: 97.04 Train acc: 96.745 Test acc: 97.030 \n",
      "step: 4934 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.582, D_sup_loss: 0.132, D_sup_acc: 97.07 Train acc: 96.712 Test acc: 96.970 \n",
      "step: 4935 | Train: G_Loss: 1.171, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.618, D_sup_loss: 0.135, D_sup_acc: 97.01 Train acc: 96.817 Test acc: 97.020 \n",
      "step: 4936 | Train: G_Loss: 1.101, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.584, D_sup_loss: 0.130, D_sup_acc: 97.06 Train acc: 96.837 Test acc: 97.070 \n",
      "step: 4937 | Train: G_Loss: 1.080, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.586, D_sup_loss: 0.125, D_sup_acc: 97.11 Train acc: 96.705 Test acc: 97.060 \n",
      "step: 4938 | Train: G_Loss: 1.050, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.566, D_sup_loss: 0.134, D_sup_acc: 97.10 Train acc: 96.708 Test acc: 97.090 \n",
      "step: 4939 | Train: G_Loss: 1.227, D_unsup_loss_real: 0.635, D_unsup_loss_fake: 0.594, D_sup_loss: 0.133, D_sup_acc: 97.13 Train acc: 96.772 Test acc: 97.120 \n",
      "step: 4940 | Train: G_Loss: 1.153, D_unsup_loss_real: 0.510, D_unsup_loss_fake: 0.638, D_sup_loss: 0.130, D_sup_acc: 97.16 Train acc: 96.772 Test acc: 97.110 \n",
      "step: 4941 | Train: G_Loss: 1.133, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.664, D_sup_loss: 0.127, D_sup_acc: 97.15 Train acc: 96.675 Test acc: 97.040 \n",
      "step: 4942 | Train: G_Loss: 1.135, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.609, D_sup_loss: 0.136, D_sup_acc: 97.08 Train acc: 96.795 Test acc: 97.130 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4943 | Train: G_Loss: 1.141, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.580, D_sup_loss: 0.127, D_sup_acc: 97.17 Train acc: 96.840 Test acc: 97.140 \n",
      "step: 4944 | Train: G_Loss: 1.152, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.592, D_sup_loss: 0.124, D_sup_acc: 97.18 Train acc: 96.828 Test acc: 97.140 \n",
      "step: 4945 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.549, D_sup_loss: 0.125, D_sup_acc: 97.18 Train acc: 96.768 Test acc: 97.140 \n",
      "step: 4946 | Train: G_Loss: 1.173, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.557, D_sup_loss: 0.126, D_sup_acc: 97.18 Train acc: 96.763 Test acc: 96.970 \n",
      "step: 4947 | Train: G_Loss: 1.092, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.639, D_sup_loss: 0.131, D_sup_acc: 97.01 Train acc: 96.707 Test acc: 96.960 \n",
      "step: 4948 | Train: G_Loss: 1.186, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.549, D_sup_loss: 0.133, D_sup_acc: 97.00 Train acc: 96.775 Test acc: 97.150 \n",
      "step: 4949 | Train: G_Loss: 1.097, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.656, D_sup_loss: 0.126, D_sup_acc: 97.19 Train acc: 96.698 Test acc: 97.080 \n",
      "step: 4950 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.548, D_sup_loss: 0.131, D_sup_acc: 97.12 Train acc: 96.740 Test acc: 97.080 \n",
      "step: 4951 | Train: G_Loss: 1.087, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.630, D_sup_loss: 0.130, D_sup_acc: 97.12 Train acc: 96.765 Test acc: 97.100 \n",
      "step: 4952 | Train: G_Loss: 1.177, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.557, D_sup_loss: 0.136, D_sup_acc: 97.14 Train acc: 96.760 Test acc: 97.140 \n",
      "step: 4953 | Train: G_Loss: 1.197, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.643, D_sup_loss: 0.130, D_sup_acc: 97.18 Train acc: 96.737 Test acc: 97.090 \n",
      "step: 4954 | Train: G_Loss: 1.070, D_unsup_loss_real: 0.639, D_unsup_loss_fake: 0.624, D_sup_loss: 0.135, D_sup_acc: 97.13 Train acc: 96.768 Test acc: 97.060 \n",
      "step: 4955 | Train: G_Loss: 1.156, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.617, D_sup_loss: 0.133, D_sup_acc: 97.10 Train acc: 96.785 Test acc: 97.050 \n",
      "step: 4956 | Train: G_Loss: 1.186, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.563, D_sup_loss: 0.132, D_sup_acc: 97.09 Train acc: 96.778 Test acc: 97.130 \n",
      "step: 4957 | Train: G_Loss: 1.162, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.566, D_sup_loss: 0.133, D_sup_acc: 97.17 Train acc: 96.765 Test acc: 97.150 \n",
      "step: 4958 | Train: G_Loss: 1.064, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.585, D_sup_loss: 0.132, D_sup_acc: 97.19 Train acc: 96.730 Test acc: 97.180 \n",
      "step: 4959 | Train: G_Loss: 1.161, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.640, D_sup_loss: 0.134, D_sup_acc: 97.22 Train acc: 96.787 Test acc: 97.180 \n",
      "step: 4960 | Train: G_Loss: 1.190, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.657, D_sup_loss: 0.131, D_sup_acc: 97.22 Train acc: 96.768 Test acc: 97.140 \n",
      "step: 4961 | Train: G_Loss: 1.149, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.576, D_sup_loss: 0.131, D_sup_acc: 97.18 Train acc: 96.862 Test acc: 97.140 \n",
      "step: 4962 | Train: G_Loss: 1.230, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.588, D_sup_loss: 0.128, D_sup_acc: 97.18 Train acc: 96.778 Test acc: 97.170 \n",
      "step: 4963 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.606, D_sup_loss: 0.135, D_sup_acc: 97.21 Train acc: 96.770 Test acc: 97.130 \n",
      "step: 4964 | Train: G_Loss: 1.137, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.542, D_sup_loss: 0.133, D_sup_acc: 97.17 Train acc: 96.627 Test acc: 96.980 \n",
      "step: 4965 | Train: G_Loss: 1.088, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.613, D_sup_loss: 0.141, D_sup_acc: 97.02 Train acc: 96.908 Test acc: 97.180 \n",
      "step: 4966 | Train: G_Loss: 1.128, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.588, D_sup_loss: 0.128, D_sup_acc: 97.22 Train acc: 96.847 Test acc: 97.260 \n",
      "step: 4967 | Train: G_Loss: 1.235, D_unsup_loss_real: 0.541, D_unsup_loss_fake: 0.594, D_sup_loss: 0.129, D_sup_acc: 97.29 Train acc: 96.868 Test acc: 97.130 \n",
      "step: 4968 | Train: G_Loss: 1.082, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.581, D_sup_loss: 0.130, D_sup_acc: 97.17 Train acc: 96.757 Test acc: 97.050 \n",
      "step: 4969 | Train: G_Loss: 1.130, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.575, D_sup_loss: 0.134, D_sup_acc: 97.09 Train acc: 96.832 Test acc: 97.110 \n",
      "step: 4970 | Train: G_Loss: 1.078, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.602, D_sup_loss: 0.128, D_sup_acc: 97.15 Train acc: 96.488 Test acc: 96.760 \n",
      "step: 4971 | Train: G_Loss: 1.038, D_unsup_loss_real: 0.528, D_unsup_loss_fake: 0.604, D_sup_loss: 0.148, D_sup_acc: 96.80 Train acc: 96.597 Test acc: 96.950 \n",
      "step: 4972 | Train: G_Loss: 1.166, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.621, D_sup_loss: 0.145, D_sup_acc: 96.99 Train acc: 96.837 Test acc: 97.160 \n",
      "step: 4973 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.598, D_sup_loss: 0.128, D_sup_acc: 97.20 Train acc: 96.733 Test acc: 97.120 \n",
      "step: 4974 | Train: G_Loss: 1.132, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.569, D_sup_loss: 0.133, D_sup_acc: 97.16 Train acc: 96.867 Test acc: 97.160 \n",
      "step: 4975 | Train: G_Loss: 1.175, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.595, D_sup_loss: 0.123, D_sup_acc: 97.20 Train acc: 96.687 Test acc: 97.100 \n",
      "step: 4976 | Train: G_Loss: 1.183, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.572, D_sup_loss: 0.134, D_sup_acc: 97.14 Train acc: 96.658 Test acc: 97.130 \n",
      "step: 4977 | Train: G_Loss: 1.191, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.566, D_sup_loss: 0.138, D_sup_acc: 97.17 Train acc: 96.838 Test acc: 97.190 \n",
      "step: 4978 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.666, D_sup_loss: 0.127, D_sup_acc: 97.23 Train acc: 96.585 Test acc: 97.040 \n",
      "step: 4979 | Train: G_Loss: 1.090, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.615, D_sup_loss: 0.144, D_sup_acc: 97.08 Train acc: 96.808 Test acc: 97.120 \n",
      "step: 4980 | Train: G_Loss: 1.166, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.587, D_sup_loss: 0.129, D_sup_acc: 97.16 Train acc: 96.893 Test acc: 97.160 \n",
      "step: 4981 | Train: G_Loss: 1.265, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.642, D_sup_loss: 0.121, D_sup_acc: 97.20 Train acc: 96.793 Test acc: 97.120 \n",
      "step: 4982 | Train: G_Loss: 1.173, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.561, D_sup_loss: 0.134, D_sup_acc: 97.16 Train acc: 96.713 Test acc: 97.120 \n",
      "step: 4983 | Train: G_Loss: 1.122, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.608, D_sup_loss: 0.136, D_sup_acc: 97.16 Train acc: 96.863 Test acc: 97.160 \n",
      "step: 4984 | Train: G_Loss: 1.259, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.650, D_sup_loss: 0.128, D_sup_acc: 97.20 Train acc: 96.792 Test acc: 97.030 \n",
      "step: 4985 | Train: G_Loss: 1.226, D_unsup_loss_real: 0.669, D_unsup_loss_fake: 0.609, D_sup_loss: 0.126, D_sup_acc: 97.07 Train acc: 96.313 Test acc: 96.700 \n",
      "step: 4986 | Train: G_Loss: 1.206, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.600, D_sup_loss: 0.155, D_sup_acc: 96.74 Train acc: 96.845 Test acc: 97.170 \n",
      "step: 4987 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.605, D_sup_loss: 0.127, D_sup_acc: 97.21 Train acc: 96.807 Test acc: 97.130 \n",
      "step: 4988 | Train: G_Loss: 1.132, D_unsup_loss_real: 0.537, D_unsup_loss_fake: 0.571, D_sup_loss: 0.129, D_sup_acc: 97.17 Train acc: 96.843 Test acc: 97.220 \n",
      "step: 4989 | Train: G_Loss: 1.118, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.615, D_sup_loss: 0.127, D_sup_acc: 97.26 Train acc: 96.883 Test acc: 97.310 \n",
      "step: 4990 | Train: G_Loss: 1.176, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.593, D_sup_loss: 0.124, D_sup_acc: 97.34 Train acc: 96.925 Test acc: 97.240 \n",
      "step: 4991 | Train: G_Loss: 1.040, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.669, D_sup_loss: 0.122, D_sup_acc: 97.27 Train acc: 96.810 Test acc: 97.160 \n",
      "step: 4992 | Train: G_Loss: 1.058, D_unsup_loss_real: 0.649, D_unsup_loss_fake: 0.670, D_sup_loss: 0.129, D_sup_acc: 97.20 Train acc: 96.480 Test acc: 96.800 \n",
      "step: 4993 | Train: G_Loss: 1.120, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.626, D_sup_loss: 0.147, D_sup_acc: 96.84 Train acc: 96.942 Test acc: 97.220 \n",
      "step: 4994 | Train: G_Loss: 1.188, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.616, D_sup_loss: 0.121, D_sup_acc: 97.26 Train acc: 96.933 Test acc: 97.200 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 4995 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.652, D_unsup_loss_fake: 0.588, D_sup_loss: 0.126, D_sup_acc: 97.24 Train acc: 96.932 Test acc: 97.100 \n",
      "step: 4996 | Train: G_Loss: 1.201, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.583, D_sup_loss: 0.124, D_sup_acc: 97.14 Train acc: 96.835 Test acc: 97.120 \n",
      "step: 4997 | Train: G_Loss: 1.162, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.626, D_sup_loss: 0.129, D_sup_acc: 97.16 Train acc: 96.807 Test acc: 97.090 \n",
      "step: 4998 | Train: G_Loss: 1.157, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.596, D_sup_loss: 0.132, D_sup_acc: 97.13 Train acc: 96.663 Test acc: 97.060 \n",
      "step: 4999 | Train: G_Loss: 1.071, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.581, D_sup_loss: 0.138, D_sup_acc: 97.10 Train acc: 96.787 Test acc: 97.060 \n",
      "step: 5000 | Train: G_Loss: 1.218, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.597, D_sup_loss: 0.131, D_sup_acc: 97.10 Train acc: 96.762 Test acc: 97.110 \n",
      "Train Classifier Accuracy: 96.762%\n",
      "\n",
      "Test Classifier Accuracy: 97.110%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_5000.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_5000.h5\n",
      "step: 5001 | Train: G_Loss: 1.067, D_unsup_loss_real: 0.657, D_unsup_loss_fake: 0.600, D_sup_loss: 0.129, D_sup_acc: 97.15 Train acc: 96.703 Test acc: 97.120 \n",
      "step: 5002 | Train: G_Loss: 1.022, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.581, D_sup_loss: 0.138, D_sup_acc: 97.16 Train acc: 96.815 Test acc: 97.150 \n",
      "step: 5003 | Train: G_Loss: 1.157, D_unsup_loss_real: 0.491, D_unsup_loss_fake: 0.585, D_sup_loss: 0.132, D_sup_acc: 97.19 Train acc: 96.732 Test acc: 97.140 \n",
      "step: 5004 | Train: G_Loss: 1.103, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.624, D_sup_loss: 0.136, D_sup_acc: 97.18 Train acc: 96.720 Test acc: 97.070 \n",
      "step: 5005 | Train: G_Loss: 1.140, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.651, D_sup_loss: 0.140, D_sup_acc: 97.11 Train acc: 96.735 Test acc: 97.000 \n",
      "step: 5006 | Train: G_Loss: 1.123, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.566, D_sup_loss: 0.137, D_sup_acc: 97.04 Train acc: 96.833 Test acc: 97.050 \n",
      "step: 5007 | Train: G_Loss: 1.158, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.565, D_sup_loss: 0.127, D_sup_acc: 97.09 Train acc: 96.885 Test acc: 97.200 \n",
      "step: 5008 | Train: G_Loss: 1.136, D_unsup_loss_real: 0.659, D_unsup_loss_fake: 0.647, D_sup_loss: 0.124, D_sup_acc: 97.24 Train acc: 96.775 Test acc: 97.040 \n",
      "step: 5009 | Train: G_Loss: 1.061, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.618, D_sup_loss: 0.136, D_sup_acc: 97.08 Train acc: 96.668 Test acc: 96.990 \n",
      "step: 5010 | Train: G_Loss: 1.109, D_unsup_loss_real: 0.631, D_unsup_loss_fake: 0.594, D_sup_loss: 0.143, D_sup_acc: 97.03 Train acc: 96.868 Test acc: 97.150 \n",
      "step: 5011 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.592, D_sup_loss: 0.129, D_sup_acc: 97.19 Train acc: 96.805 Test acc: 97.030 \n",
      "step: 5012 | Train: G_Loss: 1.151, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.545, D_sup_loss: 0.132, D_sup_acc: 97.07 Train acc: 96.653 Test acc: 96.950 \n",
      "step: 5013 | Train: G_Loss: 1.203, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.668, D_sup_loss: 0.142, D_sup_acc: 96.99 Train acc: 96.885 Test acc: 97.170 \n",
      "step: 5014 | Train: G_Loss: 1.132, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.625, D_sup_loss: 0.124, D_sup_acc: 97.21 Train acc: 96.663 Test acc: 96.870 \n",
      "step: 5015 | Train: G_Loss: 1.200, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.576, D_sup_loss: 0.142, D_sup_acc: 96.91 Train acc: 96.832 Test acc: 97.080 \n",
      "step: 5016 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.676, D_sup_loss: 0.130, D_sup_acc: 97.12 Train acc: 96.358 Test acc: 96.690 \n",
      "step: 5017 | Train: G_Loss: 1.120, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.591, D_sup_loss: 0.157, D_sup_acc: 96.73 Train acc: 96.732 Test acc: 97.030 \n",
      "step: 5018 | Train: G_Loss: 1.187, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.493, D_sup_loss: 0.137, D_sup_acc: 97.07 Train acc: 96.852 Test acc: 97.020 \n",
      "step: 5019 | Train: G_Loss: 1.183, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.591, D_sup_loss: 0.124, D_sup_acc: 97.06 Train acc: 96.868 Test acc: 97.100 \n",
      "step: 5020 | Train: G_Loss: 1.158, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.579, D_sup_loss: 0.126, D_sup_acc: 97.14 Train acc: 96.933 Test acc: 97.190 \n",
      "step: 5021 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.658, D_sup_loss: 0.127, D_sup_acc: 97.23 Train acc: 96.810 Test acc: 97.040 \n",
      "step: 5022 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.600, D_sup_loss: 0.134, D_sup_acc: 97.08 Train acc: 96.802 Test acc: 97.070 \n",
      "step: 5023 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.654, D_unsup_loss_fake: 0.584, D_sup_loss: 0.136, D_sup_acc: 97.11 Train acc: 96.762 Test acc: 97.180 \n",
      "step: 5024 | Train: G_Loss: 1.066, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.600, D_sup_loss: 0.137, D_sup_acc: 97.22 Train acc: 96.872 Test acc: 97.080 \n",
      "step: 5025 | Train: G_Loss: 1.044, D_unsup_loss_real: 0.646, D_unsup_loss_fake: 0.637, D_sup_loss: 0.129, D_sup_acc: 97.12 Train acc: 96.773 Test acc: 97.070 \n",
      "step: 5026 | Train: G_Loss: 1.099, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.589, D_sup_loss: 0.138, D_sup_acc: 97.11 Train acc: 96.797 Test acc: 97.130 \n",
      "step: 5027 | Train: G_Loss: 1.006, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.580, D_sup_loss: 0.128, D_sup_acc: 97.17 Train acc: 96.497 Test acc: 97.020 \n",
      "step: 5028 | Train: G_Loss: 1.111, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.592, D_sup_loss: 0.151, D_sup_acc: 97.06 Train acc: 96.967 Test acc: 97.190 \n",
      "step: 5029 | Train: G_Loss: 1.137, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.692, D_sup_loss: 0.123, D_sup_acc: 97.23 Train acc: 96.917 Test acc: 97.140 \n",
      "step: 5030 | Train: G_Loss: 1.078, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.638, D_sup_loss: 0.127, D_sup_acc: 97.18 Train acc: 96.967 Test acc: 97.170 \n",
      "step: 5031 | Train: G_Loss: 1.075, D_unsup_loss_real: 0.672, D_unsup_loss_fake: 0.603, D_sup_loss: 0.124, D_sup_acc: 97.21 Train acc: 96.947 Test acc: 97.190 \n",
      "step: 5032 | Train: G_Loss: 1.099, D_unsup_loss_real: 0.677, D_unsup_loss_fake: 0.582, D_sup_loss: 0.126, D_sup_acc: 97.23 Train acc: 96.913 Test acc: 97.160 \n",
      "step: 5033 | Train: G_Loss: 1.143, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.673, D_sup_loss: 0.131, D_sup_acc: 97.20 Train acc: 96.898 Test acc: 97.200 \n",
      "step: 5034 | Train: G_Loss: 1.132, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.554, D_sup_loss: 0.130, D_sup_acc: 97.24 Train acc: 96.873 Test acc: 97.140 \n",
      "step: 5035 | Train: G_Loss: 1.075, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.559, D_sup_loss: 0.131, D_sup_acc: 97.18 Train acc: 96.883 Test acc: 97.130 \n",
      "step: 5036 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.628, D_sup_loss: 0.128, D_sup_acc: 97.17 Train acc: 96.708 Test acc: 96.960 \n",
      "step: 5037 | Train: G_Loss: 1.082, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.592, D_sup_loss: 0.139, D_sup_acc: 97.00 Train acc: 96.757 Test acc: 97.000 \n",
      "step: 5038 | Train: G_Loss: 1.146, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.483, D_sup_loss: 0.136, D_sup_acc: 97.04 Train acc: 96.980 Test acc: 97.260 \n",
      "step: 5039 | Train: G_Loss: 1.015, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.579, D_sup_loss: 0.119, D_sup_acc: 97.29 Train acc: 96.637 Test acc: 96.880 \n",
      "step: 5040 | Train: G_Loss: 1.129, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.580, D_sup_loss: 0.143, D_sup_acc: 96.92 Train acc: 96.888 Test acc: 97.040 \n",
      "step: 5041 | Train: G_Loss: 1.165, D_unsup_loss_real: 0.521, D_unsup_loss_fake: 0.589, D_sup_loss: 0.127, D_sup_acc: 97.08 Train acc: 96.997 Test acc: 97.250 \n",
      "step: 5042 | Train: G_Loss: 1.206, D_unsup_loss_real: 0.516, D_unsup_loss_fake: 0.620, D_sup_loss: 0.118, D_sup_acc: 97.28 Train acc: 97.003 Test acc: 97.170 \n",
      "step: 5043 | Train: G_Loss: 1.169, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.638, D_sup_loss: 0.117, D_sup_acc: 97.21 Train acc: 96.903 Test acc: 97.120 \n",
      "step: 5044 | Train: G_Loss: 1.171, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.605, D_sup_loss: 0.123, D_sup_acc: 97.16 Train acc: 96.788 Test acc: 97.110 \n",
      "step: 5045 | Train: G_Loss: 1.318, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.633, D_sup_loss: 0.127, D_sup_acc: 97.15 Train acc: 96.783 Test acc: 97.130 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5046 | Train: G_Loss: 1.133, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.596, D_sup_loss: 0.130, D_sup_acc: 97.17 Train acc: 96.740 Test acc: 97.090 \n",
      "step: 5047 | Train: G_Loss: 1.218, D_unsup_loss_real: 0.678, D_unsup_loss_fake: 0.568, D_sup_loss: 0.132, D_sup_acc: 97.13 Train acc: 96.637 Test acc: 96.960 \n",
      "step: 5048 | Train: G_Loss: 1.135, D_unsup_loss_real: 0.639, D_unsup_loss_fake: 0.540, D_sup_loss: 0.135, D_sup_acc: 97.00 Train acc: 96.660 Test acc: 96.960 \n",
      "step: 5049 | Train: G_Loss: 1.182, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.584, D_sup_loss: 0.134, D_sup_acc: 97.00 Train acc: 96.798 Test acc: 97.090 \n",
      "step: 5050 | Train: G_Loss: 1.089, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.617, D_sup_loss: 0.124, D_sup_acc: 97.13 Train acc: 96.585 Test acc: 97.030 \n",
      "step: 5051 | Train: G_Loss: 1.157, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.631, D_sup_loss: 0.137, D_sup_acc: 97.07 Train acc: 96.710 Test acc: 97.050 \n",
      "step: 5052 | Train: G_Loss: 1.075, D_unsup_loss_real: 0.542, D_unsup_loss_fake: 0.611, D_sup_loss: 0.127, D_sup_acc: 97.09 Train acc: 96.643 Test acc: 96.970 \n",
      "step: 5053 | Train: G_Loss: 1.114, D_unsup_loss_real: 0.646, D_unsup_loss_fake: 0.554, D_sup_loss: 0.132, D_sup_acc: 97.01 Train acc: 96.815 Test acc: 97.020 \n",
      "step: 5054 | Train: G_Loss: 1.190, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.622, D_sup_loss: 0.124, D_sup_acc: 97.06 Train acc: 96.782 Test acc: 97.020 \n",
      "step: 5055 | Train: G_Loss: 1.165, D_unsup_loss_real: 0.532, D_unsup_loss_fake: 0.574, D_sup_loss: 0.126, D_sup_acc: 97.06 Train acc: 96.783 Test acc: 97.000 \n",
      "step: 5056 | Train: G_Loss: 1.072, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.592, D_sup_loss: 0.123, D_sup_acc: 97.04 Train acc: 96.875 Test acc: 97.120 \n",
      "step: 5057 | Train: G_Loss: 1.184, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.606, D_sup_loss: 0.118, D_sup_acc: 97.16 Train acc: 96.843 Test acc: 97.140 \n",
      "step: 5058 | Train: G_Loss: 1.094, D_unsup_loss_real: 0.545, D_unsup_loss_fake: 0.549, D_sup_loss: 0.123, D_sup_acc: 97.18 Train acc: 96.707 Test acc: 96.950 \n",
      "step: 5059 | Train: G_Loss: 1.082, D_unsup_loss_real: 0.527, D_unsup_loss_fake: 0.578, D_sup_loss: 0.131, D_sup_acc: 96.99 Train acc: 96.848 Test acc: 97.150 \n",
      "step: 5060 | Train: G_Loss: 1.211, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.606, D_sup_loss: 0.122, D_sup_acc: 97.19 Train acc: 96.708 Test acc: 97.010 \n",
      "step: 5061 | Train: G_Loss: 1.143, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.592, D_sup_loss: 0.136, D_sup_acc: 97.05 Train acc: 96.658 Test acc: 96.940 \n",
      "step: 5062 | Train: G_Loss: 1.236, D_unsup_loss_real: 0.542, D_unsup_loss_fake: 0.557, D_sup_loss: 0.138, D_sup_acc: 96.98 Train acc: 96.728 Test acc: 96.900 \n",
      "step: 5063 | Train: G_Loss: 1.203, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.519, D_sup_loss: 0.130, D_sup_acc: 96.94 Train acc: 96.797 Test acc: 97.090 \n",
      "step: 5064 | Train: G_Loss: 1.067, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.583, D_sup_loss: 0.123, D_sup_acc: 97.13 Train acc: 96.702 Test acc: 96.960 \n",
      "step: 5065 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.582, D_sup_loss: 0.131, D_sup_acc: 97.00 Train acc: 96.710 Test acc: 97.030 \n",
      "step: 5066 | Train: G_Loss: 1.212, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.578, D_sup_loss: 0.131, D_sup_acc: 97.07 Train acc: 96.702 Test acc: 97.040 \n",
      "step: 5067 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.565, D_sup_loss: 0.127, D_sup_acc: 97.08 Train acc: 96.663 Test acc: 97.130 \n",
      "step: 5068 | Train: G_Loss: 1.050, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.580, D_sup_loss: 0.134, D_sup_acc: 97.17 Train acc: 96.707 Test acc: 97.080 \n",
      "step: 5069 | Train: G_Loss: 1.160, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.647, D_sup_loss: 0.129, D_sup_acc: 97.12 Train acc: 96.800 Test acc: 97.130 \n",
      "step: 5070 | Train: G_Loss: 1.150, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.700, D_sup_loss: 0.125, D_sup_acc: 97.17 Train acc: 96.780 Test acc: 97.000 \n",
      "step: 5071 | Train: G_Loss: 1.152, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.624, D_sup_loss: 0.132, D_sup_acc: 97.04 Train acc: 96.693 Test acc: 96.980 \n",
      "step: 5072 | Train: G_Loss: 1.127, D_unsup_loss_real: 0.537, D_unsup_loss_fake: 0.561, D_sup_loss: 0.134, D_sup_acc: 97.02 Train acc: 96.735 Test acc: 96.920 \n",
      "step: 5073 | Train: G_Loss: 1.149, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.553, D_sup_loss: 0.133, D_sup_acc: 96.96 Train acc: 96.790 Test acc: 96.980 \n",
      "step: 5074 | Train: G_Loss: 1.054, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.648, D_sup_loss: 0.130, D_sup_acc: 97.02 Train acc: 96.715 Test acc: 97.020 \n",
      "step: 5075 | Train: G_Loss: 1.183, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.559, D_sup_loss: 0.131, D_sup_acc: 97.06 Train acc: 96.788 Test acc: 97.100 \n",
      "step: 5076 | Train: G_Loss: 1.198, D_unsup_loss_real: 0.532, D_unsup_loss_fake: 0.655, D_sup_loss: 0.127, D_sup_acc: 97.14 Train acc: 96.802 Test acc: 97.070 \n",
      "step: 5077 | Train: G_Loss: 1.088, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.643, D_sup_loss: 0.132, D_sup_acc: 97.11 Train acc: 96.548 Test acc: 96.980 \n",
      "step: 5078 | Train: G_Loss: 1.208, D_unsup_loss_real: 0.647, D_unsup_loss_fake: 0.612, D_sup_loss: 0.145, D_sup_acc: 97.02 Train acc: 96.827 Test acc: 97.130 \n",
      "step: 5079 | Train: G_Loss: 1.105, D_unsup_loss_real: 0.651, D_unsup_loss_fake: 0.589, D_sup_loss: 0.127, D_sup_acc: 97.17 Train acc: 96.848 Test acc: 97.010 \n",
      "step: 5080 | Train: G_Loss: 1.190, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.645, D_sup_loss: 0.124, D_sup_acc: 97.05 Train acc: 96.827 Test acc: 96.990 \n",
      "step: 5081 | Train: G_Loss: 1.146, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.625, D_sup_loss: 0.129, D_sup_acc: 97.03 Train acc: 96.793 Test acc: 97.060 \n",
      "step: 5082 | Train: G_Loss: 1.172, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.564, D_sup_loss: 0.133, D_sup_acc: 97.10 Train acc: 96.725 Test acc: 97.030 \n",
      "step: 5083 | Train: G_Loss: 1.212, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.609, D_sup_loss: 0.134, D_sup_acc: 97.07 Train acc: 96.695 Test acc: 96.930 \n",
      "step: 5084 | Train: G_Loss: 1.237, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.655, D_sup_loss: 0.127, D_sup_acc: 96.97 Train acc: 96.685 Test acc: 96.920 \n",
      "step: 5085 | Train: G_Loss: 1.109, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.509, D_sup_loss: 0.140, D_sup_acc: 96.96 Train acc: 96.667 Test acc: 96.910 \n",
      "step: 5086 | Train: G_Loss: 1.106, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.637, D_sup_loss: 0.138, D_sup_acc: 96.95 Train acc: 96.763 Test acc: 97.030 \n",
      "step: 5087 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.629, D_sup_loss: 0.134, D_sup_acc: 97.07 Train acc: 96.560 Test acc: 96.870 \n",
      "step: 5088 | Train: G_Loss: 1.103, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.598, D_sup_loss: 0.140, D_sup_acc: 96.91 Train acc: 96.572 Test acc: 96.910 \n",
      "step: 5089 | Train: G_Loss: 1.178, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.599, D_sup_loss: 0.139, D_sup_acc: 96.95 Train acc: 96.700 Test acc: 96.990 \n",
      "step: 5090 | Train: G_Loss: 1.096, D_unsup_loss_real: 0.642, D_unsup_loss_fake: 0.650, D_sup_loss: 0.133, D_sup_acc: 97.03 Train acc: 96.620 Test acc: 97.060 \n",
      "step: 5091 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.591, D_sup_loss: 0.138, D_sup_acc: 97.10 Train acc: 96.502 Test acc: 96.940 \n",
      "step: 5092 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.550, D_sup_loss: 0.146, D_sup_acc: 96.98 Train acc: 96.602 Test acc: 96.990 \n",
      "step: 5093 | Train: G_Loss: 1.270, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.559, D_sup_loss: 0.142, D_sup_acc: 97.03 Train acc: 96.677 Test acc: 97.000 \n",
      "step: 5094 | Train: G_Loss: 1.233, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.636, D_sup_loss: 0.137, D_sup_acc: 97.04 Train acc: 96.778 Test acc: 97.100 \n",
      "step: 5095 | Train: G_Loss: 1.143, D_unsup_loss_real: 0.628, D_unsup_loss_fake: 0.592, D_sup_loss: 0.132, D_sup_acc: 97.14 Train acc: 96.725 Test acc: 97.060 \n",
      "step: 5096 | Train: G_Loss: 1.196, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.546, D_sup_loss: 0.135, D_sup_acc: 97.10 Train acc: 96.712 Test acc: 96.960 \n",
      "step: 5097 | Train: G_Loss: 1.099, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.583, D_sup_loss: 0.135, D_sup_acc: 97.00 Train acc: 96.725 Test acc: 97.010 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5098 | Train: G_Loss: 1.153, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.605, D_sup_loss: 0.132, D_sup_acc: 97.05 Train acc: 96.833 Test acc: 97.210 \n",
      "step: 5099 | Train: G_Loss: 1.070, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.551, D_sup_loss: 0.123, D_sup_acc: 97.25 Train acc: 96.775 Test acc: 97.160 \n",
      "step: 5100 | Train: G_Loss: 1.087, D_unsup_loss_real: 0.520, D_unsup_loss_fake: 0.576, D_sup_loss: 0.130, D_sup_acc: 97.20 Train acc: 96.845 Test acc: 97.240 \n",
      "Train Classifier Accuracy: 96.845%\n",
      "\n",
      "Test Classifier Accuracy: 97.240%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_5100.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_5100.h5\n",
      "step: 5101 | Train: G_Loss: 1.203, D_unsup_loss_real: 0.528, D_unsup_loss_fake: 0.620, D_sup_loss: 0.126, D_sup_acc: 97.27 Train acc: 96.783 Test acc: 97.170 \n",
      "step: 5102 | Train: G_Loss: 1.276, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.655, D_sup_loss: 0.128, D_sup_acc: 97.21 Train acc: 96.653 Test acc: 96.880 \n",
      "step: 5103 | Train: G_Loss: 1.171, D_unsup_loss_real: 0.700, D_unsup_loss_fake: 0.603, D_sup_loss: 0.124, D_sup_acc: 96.92 Train acc: 96.273 Test acc: 96.680 \n",
      "step: 5104 | Train: G_Loss: 1.194, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.568, D_sup_loss: 0.160, D_sup_acc: 96.72 Train acc: 96.972 Test acc: 97.200 \n",
      "step: 5105 | Train: G_Loss: 1.143, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.609, D_sup_loss: 0.116, D_sup_acc: 97.24 Train acc: 96.807 Test acc: 97.150 \n",
      "step: 5106 | Train: G_Loss: 1.201, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.603, D_sup_loss: 0.126, D_sup_acc: 97.19 Train acc: 96.882 Test acc: 97.170 \n",
      "step: 5107 | Train: G_Loss: 1.080, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.559, D_sup_loss: 0.121, D_sup_acc: 97.21 Train acc: 96.847 Test acc: 97.160 \n",
      "step: 5108 | Train: G_Loss: 1.185, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.581, D_sup_loss: 0.119, D_sup_acc: 97.20 Train acc: 96.807 Test acc: 97.180 \n",
      "step: 5109 | Train: G_Loss: 1.115, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.591, D_sup_loss: 0.124, D_sup_acc: 97.22 Train acc: 96.373 Test acc: 96.740 \n",
      "step: 5110 | Train: G_Loss: 1.110, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.568, D_sup_loss: 0.146, D_sup_acc: 96.78 Train acc: 96.588 Test acc: 96.980 \n",
      "step: 5111 | Train: G_Loss: 1.100, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.628, D_sup_loss: 0.134, D_sup_acc: 97.02 Train acc: 96.567 Test acc: 96.970 \n",
      "step: 5112 | Train: G_Loss: 1.216, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.555, D_sup_loss: 0.133, D_sup_acc: 97.01 Train acc: 96.838 Test acc: 97.180 \n",
      "step: 5113 | Train: G_Loss: 1.010, D_unsup_loss_real: 0.644, D_unsup_loss_fake: 0.611, D_sup_loss: 0.121, D_sup_acc: 97.22 Train acc: 96.742 Test acc: 97.170 \n",
      "step: 5114 | Train: G_Loss: 1.049, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.651, D_sup_loss: 0.127, D_sup_acc: 97.21 Train acc: 96.790 Test acc: 97.200 \n",
      "step: 5115 | Train: G_Loss: 1.133, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.636, D_sup_loss: 0.126, D_sup_acc: 97.24 Train acc: 96.832 Test acc: 97.190 \n",
      "step: 5116 | Train: G_Loss: 1.188, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.615, D_sup_loss: 0.122, D_sup_acc: 97.23 Train acc: 96.820 Test acc: 97.140 \n",
      "step: 5117 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.614, D_sup_loss: 0.125, D_sup_acc: 97.18 Train acc: 96.845 Test acc: 97.130 \n",
      "step: 5118 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.636, D_sup_loss: 0.125, D_sup_acc: 97.17 Train acc: 96.723 Test acc: 97.030 \n",
      "step: 5119 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.566, D_sup_loss: 0.136, D_sup_acc: 97.07 Train acc: 96.750 Test acc: 97.070 \n",
      "step: 5120 | Train: G_Loss: 1.241, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.569, D_sup_loss: 0.132, D_sup_acc: 97.11 Train acc: 96.792 Test acc: 97.170 \n",
      "step: 5121 | Train: G_Loss: 1.058, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.558, D_sup_loss: 0.127, D_sup_acc: 97.21 Train acc: 96.702 Test acc: 97.140 \n",
      "step: 5122 | Train: G_Loss: 1.159, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.584, D_sup_loss: 0.133, D_sup_acc: 97.18 Train acc: 96.843 Test acc: 97.230 \n",
      "step: 5123 | Train: G_Loss: 1.090, D_unsup_loss_real: 0.522, D_unsup_loss_fake: 0.641, D_sup_loss: 0.125, D_sup_acc: 97.27 Train acc: 96.907 Test acc: 97.270 \n",
      "step: 5124 | Train: G_Loss: 1.156, D_unsup_loss_real: 0.496, D_unsup_loss_fake: 0.607, D_sup_loss: 0.121, D_sup_acc: 97.30 Train acc: 96.880 Test acc: 97.260 \n",
      "step: 5125 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.599, D_sup_loss: 0.122, D_sup_acc: 97.29 Train acc: 96.668 Test acc: 97.080 \n",
      "step: 5126 | Train: G_Loss: 1.118, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.616, D_sup_loss: 0.136, D_sup_acc: 97.12 Train acc: 96.622 Test acc: 96.960 \n",
      "step: 5127 | Train: G_Loss: 1.092, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.536, D_sup_loss: 0.139, D_sup_acc: 97.00 Train acc: 96.665 Test acc: 97.050 \n",
      "step: 5128 | Train: G_Loss: 1.265, D_unsup_loss_real: 0.537, D_unsup_loss_fake: 0.626, D_sup_loss: 0.133, D_sup_acc: 97.09 Train acc: 96.855 Test acc: 97.300 \n",
      "step: 5129 | Train: G_Loss: 1.213, D_unsup_loss_real: 0.521, D_unsup_loss_fake: 0.566, D_sup_loss: 0.120, D_sup_acc: 97.33 Train acc: 96.913 Test acc: 97.340 \n",
      "step: 5130 | Train: G_Loss: 1.266, D_unsup_loss_real: 0.642, D_unsup_loss_fake: 0.594, D_sup_loss: 0.115, D_sup_acc: 97.37 Train acc: 96.867 Test acc: 97.200 \n",
      "step: 5131 | Train: G_Loss: 1.156, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.583, D_sup_loss: 0.123, D_sup_acc: 97.24 Train acc: 96.888 Test acc: 97.280 \n",
      "step: 5132 | Train: G_Loss: 1.107, D_unsup_loss_real: 0.705, D_unsup_loss_fake: 0.576, D_sup_loss: 0.119, D_sup_acc: 97.31 Train acc: 96.573 Test acc: 97.030 \n",
      "step: 5133 | Train: G_Loss: 1.169, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.600, D_sup_loss: 0.139, D_sup_acc: 97.07 Train acc: 96.565 Test acc: 96.920 \n",
      "step: 5134 | Train: G_Loss: 1.147, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.594, D_sup_loss: 0.138, D_sup_acc: 96.96 Train acc: 96.658 Test acc: 96.980 \n",
      "step: 5135 | Train: G_Loss: 1.168, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.526, D_sup_loss: 0.135, D_sup_acc: 97.02 Train acc: 96.807 Test acc: 97.180 \n",
      "step: 5136 | Train: G_Loss: 1.182, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.620, D_sup_loss: 0.123, D_sup_acc: 97.22 Train acc: 96.587 Test acc: 96.960 \n",
      "step: 5137 | Train: G_Loss: 1.079, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.571, D_sup_loss: 0.136, D_sup_acc: 97.00 Train acc: 96.788 Test acc: 97.100 \n",
      "step: 5138 | Train: G_Loss: 1.084, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.614, D_sup_loss: 0.126, D_sup_acc: 97.14 Train acc: 96.865 Test acc: 97.190 \n",
      "step: 5139 | Train: G_Loss: 1.245, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.659, D_sup_loss: 0.123, D_sup_acc: 97.23 Train acc: 96.807 Test acc: 97.070 \n",
      "step: 5140 | Train: G_Loss: 1.244, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.606, D_sup_loss: 0.130, D_sup_acc: 97.11 Train acc: 96.417 Test acc: 96.840 \n",
      "step: 5141 | Train: G_Loss: 1.253, D_unsup_loss_real: 0.662, D_unsup_loss_fake: 0.571, D_sup_loss: 0.144, D_sup_acc: 96.88 Train acc: 96.508 Test acc: 96.980 \n",
      "step: 5142 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.585, D_sup_loss: 0.149, D_sup_acc: 97.02 Train acc: 96.905 Test acc: 97.250 \n",
      "step: 5143 | Train: G_Loss: 1.087, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.674, D_sup_loss: 0.122, D_sup_acc: 97.28 Train acc: 96.923 Test acc: 97.200 \n",
      "step: 5144 | Train: G_Loss: 1.161, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.583, D_sup_loss: 0.120, D_sup_acc: 97.24 Train acc: 96.953 Test acc: 97.240 \n",
      "step: 5145 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.596, D_sup_loss: 0.118, D_sup_acc: 97.27 Train acc: 96.865 Test acc: 97.200 \n",
      "step: 5146 | Train: G_Loss: 1.141, D_unsup_loss_real: 0.516, D_unsup_loss_fake: 0.538, D_sup_loss: 0.124, D_sup_acc: 97.24 Train acc: 96.957 Test acc: 97.210 \n",
      "step: 5147 | Train: G_Loss: 1.126, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.629, D_sup_loss: 0.116, D_sup_acc: 97.25 Train acc: 96.807 Test acc: 97.160 \n",
      "step: 5148 | Train: G_Loss: 1.070, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.584, D_sup_loss: 0.128, D_sup_acc: 97.20 Train acc: 96.758 Test acc: 97.110 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5149 | Train: G_Loss: 1.139, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.595, D_sup_loss: 0.127, D_sup_acc: 97.15 Train acc: 96.758 Test acc: 97.120 \n",
      "step: 5150 | Train: G_Loss: 1.028, D_unsup_loss_real: 0.655, D_unsup_loss_fake: 0.616, D_sup_loss: 0.128, D_sup_acc: 97.16 Train acc: 96.755 Test acc: 97.160 \n",
      "step: 5151 | Train: G_Loss: 1.159, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.575, D_sup_loss: 0.129, D_sup_acc: 97.20 Train acc: 96.742 Test acc: 97.040 \n",
      "step: 5152 | Train: G_Loss: 1.087, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.625, D_sup_loss: 0.129, D_sup_acc: 97.08 Train acc: 96.950 Test acc: 97.250 \n",
      "step: 5153 | Train: G_Loss: 1.208, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.605, D_sup_loss: 0.120, D_sup_acc: 97.28 Train acc: 96.838 Test acc: 97.150 \n",
      "step: 5154 | Train: G_Loss: 1.082, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.534, D_sup_loss: 0.123, D_sup_acc: 97.19 Train acc: 96.985 Test acc: 97.310 \n",
      "step: 5155 | Train: G_Loss: 1.184, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.577, D_sup_loss: 0.113, D_sup_acc: 97.34 Train acc: 97.003 Test acc: 97.320 \n",
      "step: 5156 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.626, D_sup_loss: 0.113, D_sup_acc: 97.35 Train acc: 96.758 Test acc: 97.080 \n",
      "step: 5157 | Train: G_Loss: 1.155, D_unsup_loss_real: 0.541, D_unsup_loss_fake: 0.662, D_sup_loss: 0.126, D_sup_acc: 97.12 Train acc: 96.750 Test acc: 97.020 \n",
      "step: 5158 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.596, D_sup_loss: 0.129, D_sup_acc: 97.06 Train acc: 96.678 Test acc: 97.080 \n",
      "step: 5159 | Train: G_Loss: 1.155, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.629, D_sup_loss: 0.131, D_sup_acc: 97.12 Train acc: 96.887 Test acc: 97.230 \n",
      "step: 5160 | Train: G_Loss: 1.228, D_unsup_loss_real: 0.522, D_unsup_loss_fake: 0.625, D_sup_loss: 0.120, D_sup_acc: 97.27 Train acc: 96.920 Test acc: 97.280 \n",
      "step: 5161 | Train: G_Loss: 1.208, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.610, D_sup_loss: 0.119, D_sup_acc: 97.31 Train acc: 96.713 Test acc: 97.130 \n",
      "step: 5162 | Train: G_Loss: 1.125, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.582, D_sup_loss: 0.131, D_sup_acc: 97.17 Train acc: 96.798 Test acc: 97.220 \n",
      "step: 5163 | Train: G_Loss: 1.189, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.605, D_sup_loss: 0.123, D_sup_acc: 97.26 Train acc: 96.753 Test acc: 97.140 \n",
      "step: 5164 | Train: G_Loss: 1.285, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.592, D_sup_loss: 0.125, D_sup_acc: 97.18 Train acc: 96.822 Test acc: 97.200 \n",
      "step: 5165 | Train: G_Loss: 1.237, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.616, D_sup_loss: 0.122, D_sup_acc: 97.24 Train acc: 96.673 Test acc: 97.190 \n",
      "step: 5166 | Train: G_Loss: 1.193, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.575, D_sup_loss: 0.129, D_sup_acc: 97.23 Train acc: 96.602 Test acc: 97.090 \n",
      "step: 5167 | Train: G_Loss: 1.235, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.497, D_sup_loss: 0.132, D_sup_acc: 97.13 Train acc: 96.645 Test acc: 97.170 \n",
      "step: 5168 | Train: G_Loss: 1.139, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.625, D_sup_loss: 0.128, D_sup_acc: 97.21 Train acc: 96.717 Test acc: 97.150 \n",
      "step: 5169 | Train: G_Loss: 1.066, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.598, D_sup_loss: 0.128, D_sup_acc: 97.19 Train acc: 96.830 Test acc: 97.260 \n",
      "step: 5170 | Train: G_Loss: 1.013, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.620, D_sup_loss: 0.125, D_sup_acc: 97.29 Train acc: 96.852 Test acc: 97.190 \n",
      "step: 5171 | Train: G_Loss: 1.203, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.556, D_sup_loss: 0.125, D_sup_acc: 97.23 Train acc: 96.858 Test acc: 97.220 \n",
      "step: 5172 | Train: G_Loss: 1.153, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.599, D_sup_loss: 0.125, D_sup_acc: 97.26 Train acc: 96.617 Test acc: 96.930 \n",
      "step: 5173 | Train: G_Loss: 1.198, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.637, D_sup_loss: 0.140, D_sup_acc: 96.97 Train acc: 96.755 Test acc: 97.050 \n",
      "step: 5174 | Train: G_Loss: 1.058, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.563, D_sup_loss: 0.130, D_sup_acc: 97.09 Train acc: 96.798 Test acc: 97.070 \n",
      "step: 5175 | Train: G_Loss: 1.091, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.583, D_sup_loss: 0.132, D_sup_acc: 97.11 Train acc: 96.742 Test acc: 96.990 \n",
      "step: 5176 | Train: G_Loss: 1.108, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.613, D_sup_loss: 0.133, D_sup_acc: 97.03 Train acc: 96.783 Test acc: 97.060 \n",
      "step: 5177 | Train: G_Loss: 1.082, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.589, D_sup_loss: 0.130, D_sup_acc: 97.10 Train acc: 96.753 Test acc: 97.090 \n",
      "step: 5178 | Train: G_Loss: 1.111, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.597, D_sup_loss: 0.130, D_sup_acc: 97.13 Train acc: 96.748 Test acc: 97.090 \n",
      "step: 5179 | Train: G_Loss: 1.187, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.578, D_sup_loss: 0.132, D_sup_acc: 97.13 Train acc: 96.775 Test acc: 97.080 \n",
      "step: 5180 | Train: G_Loss: 1.194, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.634, D_sup_loss: 0.129, D_sup_acc: 97.12 Train acc: 96.725 Test acc: 97.060 \n",
      "step: 5181 | Train: G_Loss: 1.094, D_unsup_loss_real: 0.663, D_unsup_loss_fake: 0.628, D_sup_loss: 0.138, D_sup_acc: 97.10 Train acc: 96.717 Test acc: 97.040 \n",
      "step: 5182 | Train: G_Loss: 1.200, D_unsup_loss_real: 0.489, D_unsup_loss_fake: 0.572, D_sup_loss: 0.134, D_sup_acc: 97.08 Train acc: 96.698 Test acc: 96.950 \n",
      "step: 5183 | Train: G_Loss: 1.238, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.538, D_sup_loss: 0.132, D_sup_acc: 96.99 Train acc: 96.698 Test acc: 97.080 \n",
      "step: 5184 | Train: G_Loss: 1.285, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.639, D_sup_loss: 0.135, D_sup_acc: 97.12 Train acc: 96.753 Test acc: 97.010 \n",
      "step: 5185 | Train: G_Loss: 1.120, D_unsup_loss_real: 0.691, D_unsup_loss_fake: 0.608, D_sup_loss: 0.128, D_sup_acc: 97.05 Train acc: 96.502 Test acc: 96.800 \n",
      "step: 5186 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.485, D_sup_loss: 0.150, D_sup_acc: 96.84 Train acc: 96.765 Test acc: 97.030 \n",
      "step: 5187 | Train: G_Loss: 1.190, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.655, D_sup_loss: 0.131, D_sup_acc: 97.07 Train acc: 96.793 Test acc: 97.100 \n",
      "step: 5188 | Train: G_Loss: 1.083, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.608, D_sup_loss: 0.127, D_sup_acc: 97.14 Train acc: 96.610 Test acc: 96.990 \n",
      "step: 5189 | Train: G_Loss: 1.086, D_unsup_loss_real: 0.517, D_unsup_loss_fake: 0.594, D_sup_loss: 0.141, D_sup_acc: 97.03 Train acc: 96.642 Test acc: 97.010 \n",
      "step: 5190 | Train: G_Loss: 1.150, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.591, D_sup_loss: 0.138, D_sup_acc: 97.05 Train acc: 96.828 Test acc: 97.050 \n",
      "step: 5191 | Train: G_Loss: 1.090, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.665, D_sup_loss: 0.124, D_sup_acc: 97.09 Train acc: 96.837 Test acc: 97.220 \n",
      "step: 5192 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.534, D_unsup_loss_fake: 0.559, D_sup_loss: 0.127, D_sup_acc: 97.26 Train acc: 96.858 Test acc: 97.250 \n",
      "step: 5193 | Train: G_Loss: 1.133, D_unsup_loss_real: 0.536, D_unsup_loss_fake: 0.641, D_sup_loss: 0.123, D_sup_acc: 97.28 Train acc: 96.882 Test acc: 97.280 \n",
      "step: 5194 | Train: G_Loss: 1.196, D_unsup_loss_real: 0.713, D_unsup_loss_fake: 0.570, D_sup_loss: 0.125, D_sup_acc: 97.31 Train acc: 96.962 Test acc: 97.290 \n",
      "step: 5195 | Train: G_Loss: 1.055, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.539, D_sup_loss: 0.117, D_sup_acc: 97.32 Train acc: 96.703 Test acc: 97.030 \n",
      "step: 5196 | Train: G_Loss: 1.240, D_unsup_loss_real: 0.513, D_unsup_loss_fake: 0.617, D_sup_loss: 0.137, D_sup_acc: 97.07 Train acc: 96.887 Test acc: 97.230 \n",
      "step: 5197 | Train: G_Loss: 1.104, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.605, D_sup_loss: 0.124, D_sup_acc: 97.27 Train acc: 96.928 Test acc: 97.210 \n",
      "step: 5198 | Train: G_Loss: 1.197, D_unsup_loss_real: 0.522, D_unsup_loss_fake: 0.622, D_sup_loss: 0.123, D_sup_acc: 97.25 Train acc: 96.908 Test acc: 97.160 \n",
      "step: 5199 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.653, D_unsup_loss_fake: 0.600, D_sup_loss: 0.123, D_sup_acc: 97.20 Train acc: 96.707 Test acc: 97.030 \n",
      "step: 5200 | Train: G_Loss: 1.163, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.552, D_sup_loss: 0.138, D_sup_acc: 97.07 Train acc: 96.843 Test acc: 97.320 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Classifier Accuracy: 96.843%\n",
      "\n",
      "Test Classifier Accuracy: 97.320%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_5200.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_5200.h5\n",
      "step: 5201 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.525, D_sup_loss: 0.128, D_sup_acc: 97.35 Train acc: 96.868 Test acc: 97.240 \n",
      "step: 5202 | Train: G_Loss: 1.115, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.582, D_sup_loss: 0.127, D_sup_acc: 97.27 Train acc: 96.780 Test acc: 97.040 \n",
      "step: 5203 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.507, D_unsup_loss_fake: 0.653, D_sup_loss: 0.132, D_sup_acc: 97.08 Train acc: 96.702 Test acc: 96.940 \n",
      "step: 5204 | Train: G_Loss: 1.245, D_unsup_loss_real: 0.512, D_unsup_loss_fake: 0.620, D_sup_loss: 0.137, D_sup_acc: 96.98 Train acc: 96.673 Test acc: 97.100 \n",
      "step: 5205 | Train: G_Loss: 1.263, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.612, D_sup_loss: 0.127, D_sup_acc: 97.14 Train acc: 96.738 Test acc: 97.160 \n",
      "step: 5206 | Train: G_Loss: 1.171, D_unsup_loss_real: 0.678, D_unsup_loss_fake: 0.553, D_sup_loss: 0.133, D_sup_acc: 97.20 Train acc: 96.770 Test acc: 97.180 \n",
      "step: 5207 | Train: G_Loss: 1.230, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.675, D_sup_loss: 0.131, D_sup_acc: 97.22 Train acc: 96.718 Test acc: 97.130 \n",
      "step: 5208 | Train: G_Loss: 1.282, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.527, D_sup_loss: 0.134, D_sup_acc: 97.17 Train acc: 96.785 Test acc: 97.170 \n",
      "step: 5209 | Train: G_Loss: 1.169, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.598, D_sup_loss: 0.128, D_sup_acc: 97.21 Train acc: 96.848 Test acc: 97.290 \n",
      "step: 5210 | Train: G_Loss: 1.166, D_unsup_loss_real: 0.659, D_unsup_loss_fake: 0.548, D_sup_loss: 0.122, D_sup_acc: 97.32 Train acc: 96.878 Test acc: 97.300 \n",
      "step: 5211 | Train: G_Loss: 1.130, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.618, D_sup_loss: 0.119, D_sup_acc: 97.33 Train acc: 96.898 Test acc: 97.290 \n",
      "step: 5212 | Train: G_Loss: 1.112, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.605, D_sup_loss: 0.120, D_sup_acc: 97.32 Train acc: 96.820 Test acc: 97.120 \n",
      "step: 5213 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.577, D_sup_loss: 0.128, D_sup_acc: 97.16 Train acc: 96.888 Test acc: 97.210 \n",
      "step: 5214 | Train: G_Loss: 1.111, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.582, D_sup_loss: 0.122, D_sup_acc: 97.25 Train acc: 96.805 Test acc: 97.190 \n",
      "step: 5215 | Train: G_Loss: 1.106, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.545, D_sup_loss: 0.130, D_sup_acc: 97.23 Train acc: 96.752 Test acc: 97.120 \n",
      "step: 5216 | Train: G_Loss: 1.136, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.582, D_sup_loss: 0.130, D_sup_acc: 97.16 Train acc: 96.847 Test acc: 97.230 \n",
      "step: 5217 | Train: G_Loss: 1.075, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.585, D_sup_loss: 0.122, D_sup_acc: 97.27 Train acc: 96.798 Test acc: 97.070 \n",
      "step: 5218 | Train: G_Loss: 1.056, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.575, D_sup_loss: 0.120, D_sup_acc: 97.11 Train acc: 96.683 Test acc: 97.060 \n",
      "step: 5219 | Train: G_Loss: 1.127, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.558, D_sup_loss: 0.134, D_sup_acc: 97.10 Train acc: 96.738 Test acc: 97.110 \n",
      "step: 5220 | Train: G_Loss: 1.211, D_unsup_loss_real: 0.536, D_unsup_loss_fake: 0.623, D_sup_loss: 0.129, D_sup_acc: 97.15 Train acc: 96.893 Test acc: 97.290 \n",
      "step: 5221 | Train: G_Loss: 1.107, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.579, D_sup_loss: 0.121, D_sup_acc: 97.32 Train acc: 96.880 Test acc: 97.210 \n",
      "step: 5222 | Train: G_Loss: 1.147, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.637, D_sup_loss: 0.120, D_sup_acc: 97.25 Train acc: 96.852 Test acc: 97.150 \n",
      "step: 5223 | Train: G_Loss: 1.056, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.557, D_sup_loss: 0.123, D_sup_acc: 97.19 Train acc: 96.852 Test acc: 97.220 \n",
      "step: 5224 | Train: G_Loss: 1.056, D_unsup_loss_real: 0.471, D_unsup_loss_fake: 0.577, D_sup_loss: 0.126, D_sup_acc: 97.26 Train acc: 96.875 Test acc: 97.170 \n",
      "step: 5225 | Train: G_Loss: 1.239, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.536, D_sup_loss: 0.126, D_sup_acc: 97.21 Train acc: 96.885 Test acc: 97.240 \n",
      "step: 5226 | Train: G_Loss: 1.143, D_unsup_loss_real: 0.496, D_unsup_loss_fake: 0.612, D_sup_loss: 0.124, D_sup_acc: 97.27 Train acc: 96.952 Test acc: 97.330 \n",
      "step: 5227 | Train: G_Loss: 1.111, D_unsup_loss_real: 0.674, D_unsup_loss_fake: 0.575, D_sup_loss: 0.119, D_sup_acc: 97.36 Train acc: 96.938 Test acc: 97.210 \n",
      "step: 5228 | Train: G_Loss: 1.241, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.556, D_sup_loss: 0.122, D_sup_acc: 97.25 Train acc: 97.007 Test acc: 97.190 \n",
      "step: 5229 | Train: G_Loss: 1.091, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.629, D_sup_loss: 0.118, D_sup_acc: 97.23 Train acc: 96.810 Test acc: 97.010 \n",
      "step: 5230 | Train: G_Loss: 1.264, D_unsup_loss_real: 0.533, D_unsup_loss_fake: 0.569, D_sup_loss: 0.130, D_sup_acc: 97.05 Train acc: 96.943 Test acc: 97.320 \n",
      "step: 5231 | Train: G_Loss: 1.174, D_unsup_loss_real: 0.518, D_unsup_loss_fake: 0.571, D_sup_loss: 0.120, D_sup_acc: 97.35 Train acc: 96.987 Test acc: 97.370 \n",
      "step: 5232 | Train: G_Loss: 1.314, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.496, D_sup_loss: 0.118, D_sup_acc: 97.40 Train acc: 97.045 Test acc: 97.390 \n",
      "step: 5233 | Train: G_Loss: 1.195, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.607, D_sup_loss: 0.112, D_sup_acc: 97.42 Train acc: 96.953 Test acc: 97.370 \n",
      "step: 5234 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.585, D_sup_loss: 0.119, D_sup_acc: 97.40 Train acc: 96.912 Test acc: 97.290 \n",
      "step: 5235 | Train: G_Loss: 1.123, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.546, D_sup_loss: 0.122, D_sup_acc: 97.32 Train acc: 96.920 Test acc: 97.250 \n",
      "step: 5236 | Train: G_Loss: 1.081, D_unsup_loss_real: 0.550, D_unsup_loss_fake: 0.538, D_sup_loss: 0.122, D_sup_acc: 97.28 Train acc: 96.975 Test acc: 97.310 \n",
      "step: 5237 | Train: G_Loss: 1.114, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.562, D_sup_loss: 0.116, D_sup_acc: 97.34 Train acc: 96.908 Test acc: 97.260 \n",
      "step: 5238 | Train: G_Loss: 1.211, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.584, D_sup_loss: 0.122, D_sup_acc: 97.29 Train acc: 96.910 Test acc: 97.080 \n",
      "step: 5239 | Train: G_Loss: 1.255, D_unsup_loss_real: 0.651, D_unsup_loss_fake: 0.653, D_sup_loss: 0.117, D_sup_acc: 97.12 Train acc: 96.785 Test acc: 97.100 \n",
      "step: 5240 | Train: G_Loss: 1.216, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.589, D_sup_loss: 0.132, D_sup_acc: 97.14 Train acc: 96.850 Test acc: 97.230 \n",
      "step: 5241 | Train: G_Loss: 1.073, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.519, D_sup_loss: 0.127, D_sup_acc: 97.27 Train acc: 96.800 Test acc: 97.180 \n",
      "step: 5242 | Train: G_Loss: 1.191, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.562, D_sup_loss: 0.131, D_sup_acc: 97.22 Train acc: 96.862 Test acc: 97.160 \n",
      "step: 5243 | Train: G_Loss: 1.018, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.567, D_sup_loss: 0.122, D_sup_acc: 97.20 Train acc: 96.945 Test acc: 97.200 \n",
      "step: 5244 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.544, D_unsup_loss_fake: 0.654, D_sup_loss: 0.121, D_sup_acc: 97.24 Train acc: 96.763 Test acc: 97.040 \n",
      "step: 5245 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.580, D_sup_loss: 0.134, D_sup_acc: 97.08 Train acc: 96.903 Test acc: 97.120 \n",
      "step: 5246 | Train: G_Loss: 1.176, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.542, D_sup_loss: 0.123, D_sup_acc: 97.16 Train acc: 96.920 Test acc: 97.120 \n",
      "step: 5247 | Train: G_Loss: 1.084, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.614, D_sup_loss: 0.124, D_sup_acc: 97.16 Train acc: 96.892 Test acc: 97.090 \n",
      "step: 5248 | Train: G_Loss: 1.155, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.574, D_sup_loss: 0.124, D_sup_acc: 97.13 Train acc: 96.920 Test acc: 97.320 \n",
      "step: 5249 | Train: G_Loss: 1.162, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.589, D_sup_loss: 0.123, D_sup_acc: 97.35 Train acc: 96.945 Test acc: 97.260 \n",
      "step: 5250 | Train: G_Loss: 1.174, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.579, D_sup_loss: 0.120, D_sup_acc: 97.29 Train acc: 96.940 Test acc: 97.160 \n",
      "step: 5251 | Train: G_Loss: 1.091, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.680, D_sup_loss: 0.122, D_sup_acc: 97.20 Train acc: 96.927 Test acc: 97.190 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5252 | Train: G_Loss: 1.165, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.528, D_sup_loss: 0.127, D_sup_acc: 97.23 Train acc: 96.928 Test acc: 97.230 \n",
      "step: 5253 | Train: G_Loss: 1.229, D_unsup_loss_real: 0.541, D_unsup_loss_fake: 0.586, D_sup_loss: 0.122, D_sup_acc: 97.27 Train acc: 96.902 Test acc: 97.250 \n",
      "step: 5254 | Train: G_Loss: 1.139, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.537, D_sup_loss: 0.121, D_sup_acc: 97.28 Train acc: 96.948 Test acc: 97.270 \n",
      "step: 5255 | Train: G_Loss: 1.165, D_unsup_loss_real: 0.510, D_unsup_loss_fake: 0.633, D_sup_loss: 0.119, D_sup_acc: 97.30 Train acc: 96.918 Test acc: 97.320 \n",
      "step: 5256 | Train: G_Loss: 1.236, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.600, D_sup_loss: 0.125, D_sup_acc: 97.35 Train acc: 96.855 Test acc: 97.160 \n",
      "step: 5257 | Train: G_Loss: 1.233, D_unsup_loss_real: 0.544, D_unsup_loss_fake: 0.575, D_sup_loss: 0.128, D_sup_acc: 97.20 Train acc: 96.803 Test acc: 97.090 \n",
      "step: 5258 | Train: G_Loss: 1.282, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.565, D_sup_loss: 0.122, D_sup_acc: 97.13 Train acc: 96.890 Test acc: 97.240 \n",
      "step: 5259 | Train: G_Loss: 1.132, D_unsup_loss_real: 0.644, D_unsup_loss_fake: 0.563, D_sup_loss: 0.130, D_sup_acc: 97.27 Train acc: 97.007 Test acc: 97.380 \n",
      "step: 5260 | Train: G_Loss: 1.161, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.637, D_sup_loss: 0.116, D_sup_acc: 97.41 Train acc: 96.872 Test acc: 97.150 \n",
      "step: 5261 | Train: G_Loss: 1.112, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.561, D_sup_loss: 0.127, D_sup_acc: 97.19 Train acc: 96.957 Test acc: 97.200 \n",
      "step: 5262 | Train: G_Loss: 1.217, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.561, D_sup_loss: 0.123, D_sup_acc: 97.24 Train acc: 96.893 Test acc: 97.220 \n",
      "step: 5263 | Train: G_Loss: 1.166, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.592, D_sup_loss: 0.125, D_sup_acc: 97.26 Train acc: 96.883 Test acc: 97.180 \n",
      "step: 5264 | Train: G_Loss: 1.235, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.607, D_sup_loss: 0.125, D_sup_acc: 97.22 Train acc: 96.855 Test acc: 97.170 \n",
      "step: 5265 | Train: G_Loss: 1.239, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.622, D_sup_loss: 0.121, D_sup_acc: 97.21 Train acc: 96.713 Test acc: 96.990 \n",
      "step: 5266 | Train: G_Loss: 1.189, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.645, D_sup_loss: 0.139, D_sup_acc: 97.03 Train acc: 96.967 Test acc: 97.150 \n",
      "step: 5267 | Train: G_Loss: 1.193, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.569, D_sup_loss: 0.116, D_sup_acc: 97.19 Train acc: 96.848 Test acc: 97.130 \n",
      "step: 5268 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.577, D_sup_loss: 0.125, D_sup_acc: 97.17 Train acc: 96.682 Test acc: 97.060 \n",
      "step: 5269 | Train: G_Loss: 1.176, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.656, D_sup_loss: 0.135, D_sup_acc: 97.10 Train acc: 96.712 Test acc: 97.040 \n",
      "step: 5270 | Train: G_Loss: 1.149, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.593, D_sup_loss: 0.133, D_sup_acc: 97.08 Train acc: 96.860 Test acc: 97.130 \n",
      "step: 5271 | Train: G_Loss: 1.202, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.597, D_sup_loss: 0.123, D_sup_acc: 97.17 Train acc: 96.863 Test acc: 97.150 \n",
      "step: 5272 | Train: G_Loss: 1.246, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.564, D_sup_loss: 0.122, D_sup_acc: 97.19 Train acc: 96.873 Test acc: 97.160 \n",
      "step: 5273 | Train: G_Loss: 1.210, D_unsup_loss_real: 0.541, D_unsup_loss_fake: 0.659, D_sup_loss: 0.121, D_sup_acc: 97.20 Train acc: 96.918 Test acc: 97.190 \n",
      "step: 5274 | Train: G_Loss: 1.167, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.617, D_sup_loss: 0.120, D_sup_acc: 97.23 Train acc: 96.828 Test acc: 97.090 \n",
      "step: 5275 | Train: G_Loss: 1.094, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.591, D_sup_loss: 0.128, D_sup_acc: 97.13 Train acc: 96.692 Test acc: 97.050 \n",
      "step: 5276 | Train: G_Loss: 1.165, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.589, D_sup_loss: 0.136, D_sup_acc: 97.09 Train acc: 96.828 Test acc: 97.110 \n",
      "step: 5277 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.523, D_unsup_loss_fake: 0.590, D_sup_loss: 0.127, D_sup_acc: 97.15 Train acc: 96.897 Test acc: 97.220 \n",
      "step: 5278 | Train: G_Loss: 1.263, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.561, D_sup_loss: 0.122, D_sup_acc: 97.26 Train acc: 96.867 Test acc: 97.180 \n",
      "step: 5279 | Train: G_Loss: 1.248, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.626, D_sup_loss: 0.120, D_sup_acc: 97.22 Train acc: 96.862 Test acc: 97.200 \n",
      "step: 5280 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.596, D_sup_loss: 0.124, D_sup_acc: 97.24 Train acc: 96.858 Test acc: 97.190 \n",
      "step: 5281 | Train: G_Loss: 1.189, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.534, D_sup_loss: 0.124, D_sup_acc: 97.23 Train acc: 96.903 Test acc: 97.260 \n",
      "step: 5282 | Train: G_Loss: 1.191, D_unsup_loss_real: 0.664, D_unsup_loss_fake: 0.608, D_sup_loss: 0.121, D_sup_acc: 97.29 Train acc: 96.928 Test acc: 97.210 \n",
      "step: 5283 | Train: G_Loss: 1.189, D_unsup_loss_real: 0.651, D_unsup_loss_fake: 0.619, D_sup_loss: 0.118, D_sup_acc: 97.25 Train acc: 96.970 Test acc: 97.240 \n",
      "step: 5284 | Train: G_Loss: 1.140, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.566, D_sup_loss: 0.115, D_sup_acc: 97.27 Train acc: 96.717 Test acc: 97.050 \n",
      "step: 5285 | Train: G_Loss: 1.176, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.581, D_sup_loss: 0.131, D_sup_acc: 97.09 Train acc: 96.785 Test acc: 97.090 \n",
      "step: 5286 | Train: G_Loss: 1.171, D_unsup_loss_real: 0.545, D_unsup_loss_fake: 0.521, D_sup_loss: 0.127, D_sup_acc: 97.13 Train acc: 96.803 Test acc: 97.170 \n",
      "step: 5287 | Train: G_Loss: 1.175, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.609, D_sup_loss: 0.129, D_sup_acc: 97.21 Train acc: 96.853 Test acc: 97.070 \n",
      "step: 5288 | Train: G_Loss: 1.301, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.568, D_sup_loss: 0.124, D_sup_acc: 97.11 Train acc: 96.755 Test acc: 97.010 \n",
      "step: 5289 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.557, D_sup_loss: 0.130, D_sup_acc: 97.05 Train acc: 96.673 Test acc: 97.000 \n",
      "step: 5290 | Train: G_Loss: 1.158, D_unsup_loss_real: 0.682, D_unsup_loss_fake: 0.588, D_sup_loss: 0.134, D_sup_acc: 97.04 Train acc: 96.815 Test acc: 97.080 \n",
      "step: 5291 | Train: G_Loss: 1.067, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.574, D_sup_loss: 0.128, D_sup_acc: 97.12 Train acc: 96.890 Test acc: 97.080 \n",
      "step: 5292 | Train: G_Loss: 1.228, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.560, D_sup_loss: 0.127, D_sup_acc: 97.12 Train acc: 96.887 Test acc: 97.170 \n",
      "step: 5293 | Train: G_Loss: 1.088, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.540, D_sup_loss: 0.122, D_sup_acc: 97.21 Train acc: 96.928 Test acc: 97.310 \n",
      "step: 5294 | Train: G_Loss: 1.309, D_unsup_loss_real: 0.484, D_unsup_loss_fake: 0.575, D_sup_loss: 0.123, D_sup_acc: 97.34 Train acc: 96.988 Test acc: 97.150 \n",
      "step: 5295 | Train: G_Loss: 1.115, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.572, D_sup_loss: 0.114, D_sup_acc: 97.19 Train acc: 96.955 Test acc: 97.240 \n",
      "step: 5296 | Train: G_Loss: 1.191, D_unsup_loss_real: 0.497, D_unsup_loss_fake: 0.665, D_sup_loss: 0.121, D_sup_acc: 97.27 Train acc: 96.888 Test acc: 97.140 \n",
      "step: 5297 | Train: G_Loss: 1.161, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.586, D_sup_loss: 0.124, D_sup_acc: 97.18 Train acc: 96.940 Test acc: 97.240 \n",
      "step: 5298 | Train: G_Loss: 1.167, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.618, D_sup_loss: 0.120, D_sup_acc: 97.27 Train acc: 96.822 Test acc: 97.050 \n",
      "step: 5299 | Train: G_Loss: 1.230, D_unsup_loss_real: 0.657, D_unsup_loss_fake: 0.631, D_sup_loss: 0.131, D_sup_acc: 97.09 Train acc: 96.907 Test acc: 97.120 \n",
      "step: 5300 | Train: G_Loss: 1.143, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.563, D_sup_loss: 0.127, D_sup_acc: 97.16 Train acc: 96.933 Test acc: 97.190 \n",
      "Train Classifier Accuracy: 96.933%\n",
      "\n",
      "Test Classifier Accuracy: 97.190%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_5300.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_5300.h5\n",
      "step: 5301 | Train: G_Loss: 1.150, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.619, D_sup_loss: 0.124, D_sup_acc: 97.23 Train acc: 96.875 Test acc: 97.170 \n",
      "step: 5302 | Train: G_Loss: 1.092, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.533, D_sup_loss: 0.130, D_sup_acc: 97.21 Train acc: 97.047 Test acc: 97.350 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5303 | Train: G_Loss: 1.072, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.589, D_sup_loss: 0.117, D_sup_acc: 97.38 Train acc: 96.960 Test acc: 97.210 \n",
      "step: 5304 | Train: G_Loss: 1.176, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.624, D_sup_loss: 0.126, D_sup_acc: 97.25 Train acc: 96.925 Test acc: 97.110 \n",
      "step: 5305 | Train: G_Loss: 1.120, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.571, D_sup_loss: 0.127, D_sup_acc: 97.15 Train acc: 96.887 Test acc: 97.160 \n",
      "step: 5306 | Train: G_Loss: 1.114, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.611, D_sup_loss: 0.125, D_sup_acc: 97.20 Train acc: 96.727 Test acc: 96.990 \n",
      "step: 5307 | Train: G_Loss: 1.187, D_unsup_loss_real: 0.514, D_unsup_loss_fake: 0.547, D_sup_loss: 0.138, D_sup_acc: 97.03 Train acc: 96.818 Test acc: 97.140 \n",
      "step: 5308 | Train: G_Loss: 1.136, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.581, D_sup_loss: 0.126, D_sup_acc: 97.18 Train acc: 96.937 Test acc: 97.190 \n",
      "step: 5309 | Train: G_Loss: 1.113, D_unsup_loss_real: 0.654, D_unsup_loss_fake: 0.581, D_sup_loss: 0.125, D_sup_acc: 97.23 Train acc: 96.722 Test acc: 96.930 \n",
      "step: 5310 | Train: G_Loss: 1.188, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.632, D_sup_loss: 0.138, D_sup_acc: 96.97 Train acc: 96.885 Test acc: 97.120 \n",
      "step: 5311 | Train: G_Loss: 1.112, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.550, D_sup_loss: 0.123, D_sup_acc: 97.16 Train acc: 96.888 Test acc: 97.120 \n",
      "step: 5312 | Train: G_Loss: 1.120, D_unsup_loss_real: 0.537, D_unsup_loss_fake: 0.616, D_sup_loss: 0.124, D_sup_acc: 97.16 Train acc: 96.823 Test acc: 97.110 \n",
      "step: 5313 | Train: G_Loss: 1.100, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.664, D_sup_loss: 0.133, D_sup_acc: 97.15 Train acc: 96.820 Test acc: 97.040 \n",
      "step: 5314 | Train: G_Loss: 1.141, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.545, D_sup_loss: 0.134, D_sup_acc: 97.08 Train acc: 96.790 Test acc: 97.110 \n",
      "step: 5315 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.595, D_sup_loss: 0.132, D_sup_acc: 97.15 Train acc: 96.750 Test acc: 97.110 \n",
      "step: 5316 | Train: G_Loss: 1.156, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.537, D_sup_loss: 0.132, D_sup_acc: 97.15 Train acc: 96.852 Test acc: 97.140 \n",
      "step: 5317 | Train: G_Loss: 1.191, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.572, D_sup_loss: 0.126, D_sup_acc: 97.18 Train acc: 96.928 Test acc: 97.220 \n",
      "step: 5318 | Train: G_Loss: 1.014, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.554, D_sup_loss: 0.122, D_sup_acc: 97.26 Train acc: 96.815 Test acc: 97.090 \n",
      "step: 5319 | Train: G_Loss: 1.147, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.592, D_sup_loss: 0.129, D_sup_acc: 97.13 Train acc: 96.920 Test acc: 97.210 \n",
      "step: 5320 | Train: G_Loss: 1.090, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.584, D_sup_loss: 0.123, D_sup_acc: 97.25 Train acc: 96.735 Test acc: 97.030 \n",
      "step: 5321 | Train: G_Loss: 1.137, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.593, D_sup_loss: 0.134, D_sup_acc: 97.07 Train acc: 96.925 Test acc: 97.180 \n",
      "step: 5322 | Train: G_Loss: 1.077, D_unsup_loss_real: 0.500, D_unsup_loss_fake: 0.564, D_sup_loss: 0.121, D_sup_acc: 97.22 Train acc: 96.975 Test acc: 97.190 \n",
      "step: 5323 | Train: G_Loss: 1.254, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.611, D_sup_loss: 0.118, D_sup_acc: 97.23 Train acc: 96.915 Test acc: 97.200 \n",
      "step: 5324 | Train: G_Loss: 1.228, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.569, D_sup_loss: 0.125, D_sup_acc: 97.24 Train acc: 96.882 Test acc: 97.200 \n",
      "step: 5325 | Train: G_Loss: 1.270, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.616, D_sup_loss: 0.125, D_sup_acc: 97.24 Train acc: 96.815 Test acc: 97.220 \n",
      "step: 5326 | Train: G_Loss: 1.229, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.595, D_sup_loss: 0.130, D_sup_acc: 97.26 Train acc: 96.787 Test acc: 97.180 \n",
      "step: 5327 | Train: G_Loss: 1.331, D_unsup_loss_real: 0.647, D_unsup_loss_fake: 0.559, D_sup_loss: 0.132, D_sup_acc: 97.22 Train acc: 96.867 Test acc: 97.110 \n",
      "step: 5328 | Train: G_Loss: 1.190, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.585, D_sup_loss: 0.122, D_sup_acc: 97.15 Train acc: 96.152 Test acc: 96.450 \n",
      "step: 5329 | Train: G_Loss: 1.157, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.630, D_sup_loss: 0.167, D_sup_acc: 96.49 Train acc: 96.663 Test acc: 96.980 \n",
      "step: 5330 | Train: G_Loss: 1.170, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.582, D_sup_loss: 0.137, D_sup_acc: 97.02 Train acc: 97.010 Test acc: 97.210 \n",
      "step: 5331 | Train: G_Loss: 1.031, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.560, D_sup_loss: 0.118, D_sup_acc: 97.25 Train acc: 97.107 Test acc: 97.320 \n",
      "step: 5332 | Train: G_Loss: 1.128, D_unsup_loss_real: 0.531, D_unsup_loss_fake: 0.558, D_sup_loss: 0.115, D_sup_acc: 97.35 Train acc: 97.070 Test acc: 97.330 \n",
      "step: 5333 | Train: G_Loss: 1.163, D_unsup_loss_real: 0.524, D_unsup_loss_fake: 0.616, D_sup_loss: 0.116, D_sup_acc: 97.36 Train acc: 96.902 Test acc: 97.110 \n",
      "step: 5334 | Train: G_Loss: 1.194, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.649, D_sup_loss: 0.128, D_sup_acc: 97.15 Train acc: 96.803 Test acc: 97.050 \n",
      "step: 5335 | Train: G_Loss: 1.186, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.646, D_sup_loss: 0.133, D_sup_acc: 97.09 Train acc: 96.897 Test acc: 97.180 \n",
      "step: 5336 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.601, D_sup_loss: 0.124, D_sup_acc: 97.22 Train acc: 96.757 Test acc: 96.970 \n",
      "step: 5337 | Train: G_Loss: 1.176, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.566, D_sup_loss: 0.136, D_sup_acc: 97.01 Train acc: 96.907 Test acc: 97.070 \n",
      "step: 5338 | Train: G_Loss: 1.085, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.547, D_sup_loss: 0.126, D_sup_acc: 97.11 Train acc: 96.783 Test acc: 96.930 \n",
      "step: 5339 | Train: G_Loss: 1.211, D_unsup_loss_real: 0.651, D_unsup_loss_fake: 0.560, D_sup_loss: 0.134, D_sup_acc: 96.97 Train acc: 96.920 Test acc: 97.100 \n",
      "step: 5340 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.599, D_sup_loss: 0.122, D_sup_acc: 97.14 Train acc: 96.888 Test acc: 97.190 \n",
      "step: 5341 | Train: G_Loss: 1.075, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.617, D_sup_loss: 0.129, D_sup_acc: 97.23 Train acc: 96.903 Test acc: 97.240 \n",
      "step: 5342 | Train: G_Loss: 1.072, D_unsup_loss_real: 0.673, D_unsup_loss_fake: 0.673, D_sup_loss: 0.129, D_sup_acc: 97.27 Train acc: 96.902 Test acc: 97.180 \n",
      "step: 5343 | Train: G_Loss: 1.179, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.517, D_sup_loss: 0.127, D_sup_acc: 97.22 Train acc: 96.932 Test acc: 97.240 \n",
      "step: 5344 | Train: G_Loss: 1.155, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.635, D_sup_loss: 0.122, D_sup_acc: 97.27 Train acc: 96.923 Test acc: 97.210 \n",
      "step: 5345 | Train: G_Loss: 1.157, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.563, D_sup_loss: 0.124, D_sup_acc: 97.25 Train acc: 96.882 Test acc: 97.140 \n",
      "step: 5346 | Train: G_Loss: 1.152, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.626, D_sup_loss: 0.128, D_sup_acc: 97.18 Train acc: 96.795 Test acc: 96.970 \n",
      "step: 5347 | Train: G_Loss: 1.096, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.563, D_sup_loss: 0.139, D_sup_acc: 97.01 Train acc: 96.945 Test acc: 97.170 \n",
      "step: 5348 | Train: G_Loss: 1.023, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.644, D_sup_loss: 0.130, D_sup_acc: 97.21 Train acc: 96.835 Test acc: 97.110 \n",
      "step: 5349 | Train: G_Loss: 1.067, D_unsup_loss_real: 0.535, D_unsup_loss_fake: 0.592, D_sup_loss: 0.137, D_sup_acc: 97.15 Train acc: 96.783 Test acc: 97.000 \n",
      "step: 5350 | Train: G_Loss: 1.103, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.582, D_sup_loss: 0.142, D_sup_acc: 97.04 Train acc: 96.942 Test acc: 97.160 \n",
      "step: 5351 | Train: G_Loss: 1.226, D_unsup_loss_real: 0.530, D_unsup_loss_fake: 0.601, D_sup_loss: 0.129, D_sup_acc: 97.20 Train acc: 96.957 Test acc: 97.200 \n",
      "step: 5352 | Train: G_Loss: 1.171, D_unsup_loss_real: 0.681, D_unsup_loss_fake: 0.561, D_sup_loss: 0.125, D_sup_acc: 97.24 Train acc: 96.895 Test acc: 97.110 \n",
      "step: 5353 | Train: G_Loss: 1.143, D_unsup_loss_real: 0.542, D_unsup_loss_fake: 0.612, D_sup_loss: 0.131, D_sup_acc: 97.15 Train acc: 96.822 Test acc: 96.970 \n",
      "step: 5354 | Train: G_Loss: 1.304, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.583, D_sup_loss: 0.138, D_sup_acc: 97.01 Train acc: 96.983 Test acc: 97.160 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5355 | Train: G_Loss: 1.137, D_unsup_loss_real: 0.702, D_unsup_loss_fake: 0.549, D_sup_loss: 0.124, D_sup_acc: 97.20 Train acc: 96.617 Test acc: 96.940 \n",
      "step: 5356 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.598, D_sup_loss: 0.155, D_sup_acc: 96.98 Train acc: 96.982 Test acc: 97.180 \n",
      "step: 5357 | Train: G_Loss: 1.111, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.562, D_sup_loss: 0.127, D_sup_acc: 97.22 Train acc: 96.945 Test acc: 97.110 \n",
      "step: 5358 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.499, D_unsup_loss_fake: 0.514, D_sup_loss: 0.125, D_sup_acc: 97.15 Train acc: 97.025 Test acc: 97.210 \n",
      "step: 5359 | Train: G_Loss: 1.205, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.680, D_sup_loss: 0.120, D_sup_acc: 97.25 Train acc: 96.968 Test acc: 97.160 \n",
      "step: 5360 | Train: G_Loss: 1.116, D_unsup_loss_real: 0.527, D_unsup_loss_fake: 0.630, D_sup_loss: 0.124, D_sup_acc: 97.20 Train acc: 96.845 Test acc: 97.100 \n",
      "step: 5361 | Train: G_Loss: 1.137, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.706, D_sup_loss: 0.133, D_sup_acc: 97.14 Train acc: 96.850 Test acc: 97.170 \n",
      "step: 5362 | Train: G_Loss: 1.143, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.547, D_sup_loss: 0.135, D_sup_acc: 97.21 Train acc: 96.792 Test acc: 97.140 \n",
      "step: 5363 | Train: G_Loss: 1.084, D_unsup_loss_real: 0.705, D_unsup_loss_fake: 0.611, D_sup_loss: 0.136, D_sup_acc: 97.18 Train acc: 96.882 Test acc: 97.130 \n",
      "step: 5364 | Train: G_Loss: 1.200, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.631, D_sup_loss: 0.132, D_sup_acc: 97.17 Train acc: 96.773 Test acc: 96.980 \n",
      "step: 5365 | Train: G_Loss: 1.049, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.613, D_sup_loss: 0.139, D_sup_acc: 97.02 Train acc: 96.803 Test acc: 97.090 \n",
      "step: 5366 | Train: G_Loss: 1.123, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.515, D_sup_loss: 0.137, D_sup_acc: 97.13 Train acc: 96.875 Test acc: 97.170 \n",
      "step: 5367 | Train: G_Loss: 1.136, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.583, D_sup_loss: 0.133, D_sup_acc: 97.21 Train acc: 96.822 Test acc: 97.010 \n",
      "step: 5368 | Train: G_Loss: 1.255, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.586, D_sup_loss: 0.136, D_sup_acc: 97.05 Train acc: 96.812 Test acc: 97.050 \n",
      "step: 5369 | Train: G_Loss: 1.127, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.588, D_sup_loss: 0.136, D_sup_acc: 97.09 Train acc: 96.635 Test acc: 96.890 \n",
      "step: 5370 | Train: G_Loss: 1.230, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.533, D_sup_loss: 0.147, D_sup_acc: 96.93 Train acc: 96.870 Test acc: 97.110 \n",
      "step: 5371 | Train: G_Loss: 1.222, D_unsup_loss_real: 0.676, D_unsup_loss_fake: 0.509, D_sup_loss: 0.132, D_sup_acc: 97.15 Train acc: 96.867 Test acc: 96.980 \n",
      "step: 5372 | Train: G_Loss: 1.090, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.525, D_sup_loss: 0.133, D_sup_acc: 97.02 Train acc: 96.790 Test acc: 96.980 \n",
      "step: 5373 | Train: G_Loss: 1.146, D_unsup_loss_real: 0.441, D_unsup_loss_fake: 0.639, D_sup_loss: 0.135, D_sup_acc: 97.02 Train acc: 96.948 Test acc: 97.190 \n",
      "step: 5374 | Train: G_Loss: 1.195, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.549, D_sup_loss: 0.126, D_sup_acc: 97.23 Train acc: 96.982 Test acc: 97.300 \n",
      "step: 5375 | Train: G_Loss: 1.161, D_unsup_loss_real: 0.520, D_unsup_loss_fake: 0.527, D_sup_loss: 0.123, D_sup_acc: 97.33 Train acc: 97.048 Test acc: 97.300 \n",
      "step: 5376 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.569, D_sup_loss: 0.114, D_sup_acc: 97.33 Train acc: 97.008 Test acc: 97.320 \n",
      "step: 5377 | Train: G_Loss: 1.225, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.588, D_sup_loss: 0.120, D_sup_acc: 97.35 Train acc: 96.953 Test acc: 97.190 \n",
      "step: 5378 | Train: G_Loss: 1.061, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.618, D_sup_loss: 0.127, D_sup_acc: 97.23 Train acc: 96.887 Test acc: 97.060 \n",
      "step: 5379 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.642, D_sup_loss: 0.132, D_sup_acc: 97.10 Train acc: 97.060 Test acc: 97.340 \n",
      "step: 5380 | Train: G_Loss: 1.003, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.694, D_sup_loss: 0.122, D_sup_acc: 97.37 Train acc: 96.877 Test acc: 97.090 \n",
      "step: 5381 | Train: G_Loss: 1.190, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.554, D_sup_loss: 0.133, D_sup_acc: 97.13 Train acc: 96.722 Test acc: 97.020 \n",
      "step: 5382 | Train: G_Loss: 1.216, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.578, D_sup_loss: 0.138, D_sup_acc: 97.06 Train acc: 97.062 Test acc: 97.240 \n",
      "step: 5383 | Train: G_Loss: 1.117, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.593, D_sup_loss: 0.123, D_sup_acc: 97.27 Train acc: 96.875 Test acc: 97.090 \n",
      "step: 5384 | Train: G_Loss: 1.372, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.557, D_sup_loss: 0.135, D_sup_acc: 97.13 Train acc: 97.002 Test acc: 97.220 \n",
      "step: 5385 | Train: G_Loss: 1.126, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.560, D_sup_loss: 0.125, D_sup_acc: 97.26 Train acc: 96.845 Test acc: 97.100 \n",
      "step: 5386 | Train: G_Loss: 1.044, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.561, D_sup_loss: 0.136, D_sup_acc: 97.14 Train acc: 96.847 Test acc: 97.120 \n",
      "step: 5387 | Train: G_Loss: 1.240, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.597, D_sup_loss: 0.134, D_sup_acc: 97.16 Train acc: 96.798 Test acc: 97.060 \n",
      "step: 5388 | Train: G_Loss: 1.101, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.587, D_sup_loss: 0.123, D_sup_acc: 97.10 Train acc: 96.853 Test acc: 97.170 \n",
      "step: 5389 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.626, D_sup_loss: 0.136, D_sup_acc: 97.21 Train acc: 97.013 Test acc: 97.220 \n",
      "step: 5390 | Train: G_Loss: 1.143, D_unsup_loss_real: 0.523, D_unsup_loss_fake: 0.579, D_sup_loss: 0.121, D_sup_acc: 97.26 Train acc: 97.022 Test acc: 97.160 \n",
      "step: 5391 | Train: G_Loss: 1.207, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.525, D_sup_loss: 0.122, D_sup_acc: 97.20 Train acc: 96.978 Test acc: 97.160 \n",
      "step: 5392 | Train: G_Loss: 1.109, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.592, D_sup_loss: 0.121, D_sup_acc: 97.20 Train acc: 96.772 Test acc: 96.930 \n",
      "step: 5393 | Train: G_Loss: 1.094, D_unsup_loss_real: 0.527, D_unsup_loss_fake: 0.615, D_sup_loss: 0.135, D_sup_acc: 96.97 Train acc: 96.657 Test acc: 96.800 \n",
      "step: 5394 | Train: G_Loss: 1.080, D_unsup_loss_real: 0.523, D_unsup_loss_fake: 0.546, D_sup_loss: 0.141, D_sup_acc: 96.84 Train acc: 96.792 Test acc: 96.870 \n",
      "step: 5395 | Train: G_Loss: 1.115, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.540, D_sup_loss: 0.134, D_sup_acc: 96.91 Train acc: 97.118 Test acc: 97.210 \n",
      "step: 5396 | Train: G_Loss: 1.172, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.633, D_sup_loss: 0.120, D_sup_acc: 97.25 Train acc: 97.038 Test acc: 97.230 \n",
      "step: 5397 | Train: G_Loss: 1.163, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.557, D_sup_loss: 0.122, D_sup_acc: 97.27 Train acc: 97.027 Test acc: 97.230 \n",
      "step: 5398 | Train: G_Loss: 1.216, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.628, D_sup_loss: 0.122, D_sup_acc: 97.27 Train acc: 96.960 Test acc: 97.000 \n",
      "step: 5399 | Train: G_Loss: 1.184, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.635, D_sup_loss: 0.119, D_sup_acc: 97.04 Train acc: 96.958 Test acc: 97.050 \n",
      "step: 5400 | Train: G_Loss: 1.113, D_unsup_loss_real: 0.663, D_unsup_loss_fake: 0.645, D_sup_loss: 0.129, D_sup_acc: 97.09 Train acc: 96.955 Test acc: 97.150 \n",
      "Train Classifier Accuracy: 96.955%\n",
      "\n",
      "Test Classifier Accuracy: 97.150%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_5400.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_5400.h5\n",
      "step: 5401 | Train: G_Loss: 1.166, D_unsup_loss_real: 0.546, D_unsup_loss_fake: 0.592, D_sup_loss: 0.130, D_sup_acc: 97.19 Train acc: 97.022 Test acc: 97.190 \n",
      "step: 5402 | Train: G_Loss: 1.265, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.614, D_sup_loss: 0.121, D_sup_acc: 97.23 Train acc: 97.017 Test acc: 97.160 \n",
      "step: 5403 | Train: G_Loss: 1.169, D_unsup_loss_real: 0.733, D_unsup_loss_fake: 0.601, D_sup_loss: 0.119, D_sup_acc: 97.20 Train acc: 97.027 Test acc: 97.220 \n",
      "step: 5404 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.494, D_unsup_loss_fake: 0.561, D_sup_loss: 0.120, D_sup_acc: 97.26 Train acc: 97.053 Test acc: 97.270 \n",
      "step: 5405 | Train: G_Loss: 1.027, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.569, D_sup_loss: 0.116, D_sup_acc: 97.30 Train acc: 96.932 Test acc: 97.130 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5406 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.665, D_sup_loss: 0.119, D_sup_acc: 97.17 Train acc: 96.955 Test acc: 97.160 \n",
      "step: 5407 | Train: G_Loss: 1.185, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.537, D_sup_loss: 0.125, D_sup_acc: 97.20 Train acc: 96.952 Test acc: 97.270 \n",
      "step: 5408 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.588, D_sup_loss: 0.121, D_sup_acc: 97.30 Train acc: 96.745 Test acc: 96.880 \n",
      "step: 5409 | Train: G_Loss: 1.135, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.636, D_sup_loss: 0.137, D_sup_acc: 96.92 Train acc: 96.937 Test acc: 97.130 \n",
      "step: 5410 | Train: G_Loss: 1.167, D_unsup_loss_real: 0.527, D_unsup_loss_fake: 0.586, D_sup_loss: 0.125, D_sup_acc: 97.17 Train acc: 96.935 Test acc: 97.200 \n",
      "step: 5411 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.620, D_sup_loss: 0.122, D_sup_acc: 97.24 Train acc: 96.832 Test acc: 97.090 \n",
      "step: 5412 | Train: G_Loss: 1.147, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.520, D_sup_loss: 0.129, D_sup_acc: 97.13 Train acc: 96.837 Test acc: 97.010 \n",
      "step: 5413 | Train: G_Loss: 1.182, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.518, D_sup_loss: 0.131, D_sup_acc: 97.05 Train acc: 96.968 Test acc: 97.230 \n",
      "step: 5414 | Train: G_Loss: 1.028, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.540, D_sup_loss: 0.117, D_sup_acc: 97.27 Train acc: 97.002 Test acc: 97.150 \n",
      "step: 5415 | Train: G_Loss: 1.184, D_unsup_loss_real: 0.546, D_unsup_loss_fake: 0.619, D_sup_loss: 0.122, D_sup_acc: 97.19 Train acc: 96.953 Test acc: 97.110 \n",
      "step: 5416 | Train: G_Loss: 1.074, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.603, D_sup_loss: 0.124, D_sup_acc: 97.15 Train acc: 96.987 Test acc: 97.180 \n",
      "step: 5417 | Train: G_Loss: 1.088, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.694, D_sup_loss: 0.120, D_sup_acc: 97.22 Train acc: 96.912 Test acc: 97.040 \n",
      "step: 5418 | Train: G_Loss: 1.163, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.578, D_sup_loss: 0.128, D_sup_acc: 97.08 Train acc: 96.907 Test acc: 97.170 \n",
      "step: 5419 | Train: G_Loss: 1.177, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.634, D_sup_loss: 0.127, D_sup_acc: 97.21 Train acc: 96.807 Test acc: 97.140 \n",
      "step: 5420 | Train: G_Loss: 1.076, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.641, D_sup_loss: 0.132, D_sup_acc: 97.18 Train acc: 96.868 Test acc: 97.100 \n",
      "step: 5421 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.522, D_sup_loss: 0.131, D_sup_acc: 97.14 Train acc: 96.883 Test acc: 97.210 \n",
      "step: 5422 | Train: G_Loss: 1.070, D_unsup_loss_real: 0.628, D_unsup_loss_fake: 0.596, D_sup_loss: 0.128, D_sup_acc: 97.25 Train acc: 96.632 Test acc: 96.830 \n",
      "step: 5423 | Train: G_Loss: 1.049, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.523, D_sup_loss: 0.143, D_sup_acc: 96.87 Train acc: 96.877 Test acc: 97.090 \n",
      "step: 5424 | Train: G_Loss: 1.080, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.612, D_sup_loss: 0.123, D_sup_acc: 97.13 Train acc: 96.868 Test acc: 97.050 \n",
      "step: 5425 | Train: G_Loss: 1.223, D_unsup_loss_real: 0.535, D_unsup_loss_fake: 0.669, D_sup_loss: 0.125, D_sup_acc: 97.09 Train acc: 96.882 Test acc: 97.250 \n",
      "step: 5426 | Train: G_Loss: 1.176, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.587, D_sup_loss: 0.124, D_sup_acc: 97.28 Train acc: 96.933 Test acc: 97.160 \n",
      "step: 5427 | Train: G_Loss: 1.032, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.670, D_sup_loss: 0.121, D_sup_acc: 97.20 Train acc: 96.812 Test acc: 97.070 \n",
      "step: 5428 | Train: G_Loss: 1.096, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.595, D_sup_loss: 0.133, D_sup_acc: 97.11 Train acc: 96.877 Test acc: 97.130 \n",
      "step: 5429 | Train: G_Loss: 1.201, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.566, D_sup_loss: 0.128, D_sup_acc: 97.17 Train acc: 96.945 Test acc: 97.130 \n",
      "step: 5430 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.643, D_sup_loss: 0.122, D_sup_acc: 97.17 Train acc: 96.697 Test acc: 97.020 \n",
      "step: 5431 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.599, D_sup_loss: 0.144, D_sup_acc: 97.06 Train acc: 96.820 Test acc: 96.910 \n",
      "step: 5432 | Train: G_Loss: 1.218, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.574, D_sup_loss: 0.130, D_sup_acc: 96.95 Train acc: 96.862 Test acc: 97.030 \n",
      "step: 5433 | Train: G_Loss: 1.085, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.606, D_sup_loss: 0.126, D_sup_acc: 97.07 Train acc: 96.892 Test acc: 97.130 \n",
      "step: 5434 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.544, D_unsup_loss_fake: 0.603, D_sup_loss: 0.132, D_sup_acc: 97.17 Train acc: 96.887 Test acc: 97.150 \n",
      "step: 5435 | Train: G_Loss: 1.232, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.618, D_sup_loss: 0.129, D_sup_acc: 97.19 Train acc: 96.622 Test acc: 96.800 \n",
      "step: 5436 | Train: G_Loss: 1.183, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.726, D_sup_loss: 0.145, D_sup_acc: 96.84 Train acc: 96.843 Test acc: 97.050 \n",
      "step: 5437 | Train: G_Loss: 1.137, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.622, D_sup_loss: 0.137, D_sup_acc: 97.09 Train acc: 96.985 Test acc: 97.190 \n",
      "step: 5438 | Train: G_Loss: 1.108, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.609, D_sup_loss: 0.127, D_sup_acc: 97.23 Train acc: 97.005 Test acc: 97.250 \n",
      "step: 5439 | Train: G_Loss: 1.261, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.621, D_sup_loss: 0.124, D_sup_acc: 97.28 Train acc: 96.982 Test acc: 97.160 \n",
      "step: 5440 | Train: G_Loss: 1.143, D_unsup_loss_real: 0.682, D_unsup_loss_fake: 0.619, D_sup_loss: 0.119, D_sup_acc: 97.20 Train acc: 96.783 Test acc: 97.120 \n",
      "step: 5441 | Train: G_Loss: 1.207, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.622, D_sup_loss: 0.141, D_sup_acc: 97.16 Train acc: 97.008 Test acc: 97.240 \n",
      "step: 5442 | Train: G_Loss: 1.201, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.569, D_sup_loss: 0.126, D_sup_acc: 97.27 Train acc: 96.950 Test acc: 97.210 \n",
      "step: 5443 | Train: G_Loss: 1.216, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.635, D_sup_loss: 0.126, D_sup_acc: 97.25 Train acc: 96.970 Test acc: 97.340 \n",
      "step: 5444 | Train: G_Loss: 1.109, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.582, D_sup_loss: 0.123, D_sup_acc: 97.37 Train acc: 96.897 Test acc: 97.240 \n",
      "step: 5445 | Train: G_Loss: 1.221, D_unsup_loss_real: 0.492, D_unsup_loss_fake: 0.523, D_sup_loss: 0.130, D_sup_acc: 97.27 Train acc: 97.027 Test acc: 97.310 \n",
      "step: 5446 | Train: G_Loss: 1.173, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.561, D_sup_loss: 0.122, D_sup_acc: 97.34 Train acc: 96.888 Test acc: 97.220 \n",
      "step: 5447 | Train: G_Loss: 1.180, D_unsup_loss_real: 0.526, D_unsup_loss_fake: 0.671, D_sup_loss: 0.131, D_sup_acc: 97.26 Train acc: 97.087 Test acc: 97.420 \n",
      "step: 5448 | Train: G_Loss: 1.089, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.603, D_sup_loss: 0.119, D_sup_acc: 97.45 Train acc: 96.917 Test acc: 97.230 \n",
      "step: 5449 | Train: G_Loss: 1.199, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.566, D_sup_loss: 0.131, D_sup_acc: 97.27 Train acc: 96.982 Test acc: 97.250 \n",
      "step: 5450 | Train: G_Loss: 1.061, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.583, D_sup_loss: 0.121, D_sup_acc: 97.28 Train acc: 96.965 Test acc: 97.290 \n",
      "step: 5451 | Train: G_Loss: 1.099, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.605, D_sup_loss: 0.125, D_sup_acc: 97.32 Train acc: 96.958 Test acc: 97.180 \n",
      "step: 5452 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.518, D_unsup_loss_fake: 0.663, D_sup_loss: 0.125, D_sup_acc: 97.22 Train acc: 96.927 Test acc: 97.080 \n",
      "step: 5453 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.542, D_sup_loss: 0.131, D_sup_acc: 97.12 Train acc: 96.872 Test acc: 97.090 \n",
      "step: 5454 | Train: G_Loss: 1.128, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.569, D_sup_loss: 0.137, D_sup_acc: 97.13 Train acc: 96.955 Test acc: 97.000 \n",
      "step: 5455 | Train: G_Loss: 1.164, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.635, D_sup_loss: 0.129, D_sup_acc: 97.04 Train acc: 96.907 Test acc: 97.060 \n",
      "step: 5456 | Train: G_Loss: 1.041, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.566, D_sup_loss: 0.133, D_sup_acc: 97.10 Train acc: 96.995 Test acc: 97.130 \n",
      "step: 5457 | Train: G_Loss: 1.083, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.611, D_sup_loss: 0.128, D_sup_acc: 97.17 Train acc: 96.918 Test acc: 97.250 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5458 | Train: G_Loss: 1.118, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.640, D_sup_loss: 0.129, D_sup_acc: 97.28 Train acc: 96.963 Test acc: 97.280 \n",
      "step: 5459 | Train: G_Loss: 1.174, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.611, D_sup_loss: 0.126, D_sup_acc: 97.31 Train acc: 96.835 Test acc: 97.100 \n",
      "step: 5460 | Train: G_Loss: 1.136, D_unsup_loss_real: 0.514, D_unsup_loss_fake: 0.688, D_sup_loss: 0.137, D_sup_acc: 97.14 Train acc: 96.948 Test acc: 97.160 \n",
      "step: 5461 | Train: G_Loss: 1.116, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.551, D_sup_loss: 0.132, D_sup_acc: 97.20 Train acc: 97.067 Test acc: 97.200 \n",
      "step: 5462 | Train: G_Loss: 1.127, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.599, D_sup_loss: 0.123, D_sup_acc: 97.24 Train acc: 96.957 Test acc: 97.140 \n",
      "step: 5463 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.550, D_unsup_loss_fake: 0.634, D_sup_loss: 0.133, D_sup_acc: 97.18 Train acc: 97.017 Test acc: 97.190 \n",
      "step: 5464 | Train: G_Loss: 1.176, D_unsup_loss_real: 0.647, D_unsup_loss_fake: 0.643, D_sup_loss: 0.126, D_sup_acc: 97.23 Train acc: 96.968 Test acc: 97.190 \n",
      "step: 5465 | Train: G_Loss: 1.125, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.637, D_sup_loss: 0.126, D_sup_acc: 97.23 Train acc: 96.947 Test acc: 97.190 \n",
      "step: 5466 | Train: G_Loss: 1.189, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.598, D_sup_loss: 0.130, D_sup_acc: 97.23 Train acc: 97.012 Test acc: 97.190 \n",
      "step: 5467 | Train: G_Loss: 1.058, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.612, D_sup_loss: 0.127, D_sup_acc: 97.23 Train acc: 96.747 Test acc: 97.080 \n",
      "step: 5468 | Train: G_Loss: 1.132, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.617, D_sup_loss: 0.141, D_sup_acc: 97.12 Train acc: 96.840 Test acc: 97.150 \n",
      "step: 5469 | Train: G_Loss: 1.157, D_unsup_loss_real: 0.667, D_unsup_loss_fake: 0.643, D_sup_loss: 0.136, D_sup_acc: 97.19 Train acc: 96.903 Test acc: 97.100 \n",
      "step: 5470 | Train: G_Loss: 1.190, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.587, D_sup_loss: 0.130, D_sup_acc: 97.14 Train acc: 96.973 Test acc: 97.200 \n",
      "step: 5471 | Train: G_Loss: 1.139, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.608, D_sup_loss: 0.124, D_sup_acc: 97.24 Train acc: 96.878 Test acc: 97.140 \n",
      "step: 5472 | Train: G_Loss: 1.279, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.563, D_sup_loss: 0.129, D_sup_acc: 97.18 Train acc: 96.743 Test acc: 97.070 \n",
      "step: 5473 | Train: G_Loss: 1.111, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.435, D_sup_loss: 0.133, D_sup_acc: 97.11 Train acc: 96.918 Test acc: 97.230 \n",
      "step: 5474 | Train: G_Loss: 1.314, D_unsup_loss_real: 0.542, D_unsup_loss_fake: 0.576, D_sup_loss: 0.126, D_sup_acc: 97.27 Train acc: 96.977 Test acc: 97.230 \n",
      "step: 5475 | Train: G_Loss: 1.078, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.530, D_sup_loss: 0.120, D_sup_acc: 97.27 Train acc: 96.897 Test acc: 97.310 \n",
      "step: 5476 | Train: G_Loss: 1.198, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.578, D_sup_loss: 0.125, D_sup_acc: 97.34 Train acc: 97.087 Test acc: 97.210 \n",
      "step: 5477 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.668, D_sup_loss: 0.114, D_sup_acc: 97.25 Train acc: 97.080 Test acc: 97.230 \n",
      "step: 5478 | Train: G_Loss: 1.139, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.575, D_sup_loss: 0.118, D_sup_acc: 97.27 Train acc: 97.023 Test acc: 97.280 \n",
      "step: 5479 | Train: G_Loss: 1.062, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.638, D_sup_loss: 0.123, D_sup_acc: 97.31 Train acc: 96.902 Test acc: 97.190 \n",
      "step: 5480 | Train: G_Loss: 1.081, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.578, D_sup_loss: 0.132, D_sup_acc: 97.23 Train acc: 97.007 Test acc: 97.310 \n",
      "step: 5481 | Train: G_Loss: 1.130, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.584, D_sup_loss: 0.124, D_sup_acc: 97.34 Train acc: 96.892 Test acc: 97.110 \n",
      "step: 5482 | Train: G_Loss: 1.153, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.581, D_sup_loss: 0.131, D_sup_acc: 97.15 Train acc: 96.685 Test acc: 96.970 \n",
      "step: 5483 | Train: G_Loss: 1.241, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.523, D_sup_loss: 0.139, D_sup_acc: 97.01 Train acc: 96.892 Test acc: 97.280 \n",
      "step: 5484 | Train: G_Loss: 1.224, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.681, D_sup_loss: 0.125, D_sup_acc: 97.31 Train acc: 96.963 Test acc: 97.270 \n",
      "step: 5485 | Train: G_Loss: 1.162, D_unsup_loss_real: 0.654, D_unsup_loss_fake: 0.541, D_sup_loss: 0.121, D_sup_acc: 97.30 Train acc: 96.950 Test acc: 97.190 \n",
      "step: 5486 | Train: G_Loss: 1.162, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.672, D_sup_loss: 0.124, D_sup_acc: 97.23 Train acc: 96.785 Test acc: 97.040 \n",
      "step: 5487 | Train: G_Loss: 1.231, D_unsup_loss_real: 0.531, D_unsup_loss_fake: 0.625, D_sup_loss: 0.134, D_sup_acc: 97.08 Train acc: 96.913 Test acc: 97.100 \n",
      "step: 5488 | Train: G_Loss: 1.155, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.539, D_sup_loss: 0.124, D_sup_acc: 97.14 Train acc: 96.938 Test acc: 97.290 \n",
      "step: 5489 | Train: G_Loss: 1.199, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.587, D_sup_loss: 0.127, D_sup_acc: 97.32 Train acc: 96.995 Test acc: 97.180 \n",
      "step: 5490 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.552, D_sup_loss: 0.123, D_sup_acc: 97.22 Train acc: 96.870 Test acc: 97.110 \n",
      "step: 5491 | Train: G_Loss: 1.222, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.596, D_sup_loss: 0.131, D_sup_acc: 97.15 Train acc: 97.028 Test acc: 97.220 \n",
      "step: 5492 | Train: G_Loss: 1.220, D_unsup_loss_real: 0.550, D_unsup_loss_fake: 0.654, D_sup_loss: 0.115, D_sup_acc: 97.26 Train acc: 96.975 Test acc: 97.200 \n",
      "step: 5493 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.589, D_sup_loss: 0.119, D_sup_acc: 97.24 Train acc: 96.745 Test acc: 97.030 \n",
      "step: 5494 | Train: G_Loss: 1.163, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.642, D_sup_loss: 0.131, D_sup_acc: 97.07 Train acc: 96.672 Test acc: 97.040 \n",
      "step: 5495 | Train: G_Loss: 1.090, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.684, D_sup_loss: 0.143, D_sup_acc: 97.08 Train acc: 96.868 Test acc: 97.200 \n",
      "step: 5496 | Train: G_Loss: 1.130, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.607, D_sup_loss: 0.130, D_sup_acc: 97.24 Train acc: 96.972 Test acc: 97.190 \n",
      "step: 5497 | Train: G_Loss: 1.120, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.534, D_sup_loss: 0.127, D_sup_acc: 97.23 Train acc: 96.997 Test acc: 97.200 \n",
      "step: 5498 | Train: G_Loss: 1.204, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.561, D_sup_loss: 0.124, D_sup_acc: 97.24 Train acc: 97.058 Test acc: 97.250 \n",
      "step: 5499 | Train: G_Loss: 1.192, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.633, D_sup_loss: 0.119, D_sup_acc: 97.28 Train acc: 96.987 Test acc: 97.180 \n",
      "step: 5500 | Train: G_Loss: 1.178, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.607, D_sup_loss: 0.125, D_sup_acc: 97.22 Train acc: 96.947 Test acc: 97.150 \n",
      "Train Classifier Accuracy: 96.947%\n",
      "\n",
      "Test Classifier Accuracy: 97.150%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_5500.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_5500.h5\n",
      "step: 5501 | Train: G_Loss: 1.139, D_unsup_loss_real: 0.532, D_unsup_loss_fake: 0.610, D_sup_loss: 0.126, D_sup_acc: 97.19 Train acc: 96.842 Test acc: 97.100 \n",
      "step: 5502 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.630, D_sup_loss: 0.128, D_sup_acc: 97.14 Train acc: 96.810 Test acc: 97.030 \n",
      "step: 5503 | Train: G_Loss: 1.169, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.644, D_sup_loss: 0.133, D_sup_acc: 97.07 Train acc: 97.003 Test acc: 97.240 \n",
      "step: 5504 | Train: G_Loss: 1.156, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.563, D_sup_loss: 0.122, D_sup_acc: 97.27 Train acc: 96.885 Test acc: 97.120 \n",
      "step: 5505 | Train: G_Loss: 1.225, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.538, D_sup_loss: 0.127, D_sup_acc: 97.16 Train acc: 96.882 Test acc: 97.160 \n",
      "step: 5506 | Train: G_Loss: 1.159, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.573, D_sup_loss: 0.124, D_sup_acc: 97.20 Train acc: 96.928 Test acc: 97.150 \n",
      "step: 5507 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.627, D_sup_loss: 0.124, D_sup_acc: 97.19 Train acc: 96.850 Test acc: 97.150 \n",
      "step: 5508 | Train: G_Loss: 1.104, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.571, D_sup_loss: 0.129, D_sup_acc: 97.19 Train acc: 96.858 Test acc: 97.140 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5509 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.637, D_sup_loss: 0.127, D_sup_acc: 97.18 Train acc: 96.713 Test acc: 96.940 \n",
      "step: 5510 | Train: G_Loss: 1.151, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.582, D_sup_loss: 0.139, D_sup_acc: 96.98 Train acc: 96.852 Test acc: 97.130 \n",
      "step: 5511 | Train: G_Loss: 1.084, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.609, D_sup_loss: 0.132, D_sup_acc: 97.17 Train acc: 96.765 Test acc: 96.970 \n",
      "step: 5512 | Train: G_Loss: 1.136, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.605, D_sup_loss: 0.140, D_sup_acc: 97.01 Train acc: 96.847 Test acc: 97.040 \n",
      "step: 5513 | Train: G_Loss: 1.147, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.533, D_sup_loss: 0.129, D_sup_acc: 97.08 Train acc: 96.910 Test acc: 97.160 \n",
      "step: 5514 | Train: G_Loss: 1.237, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.631, D_sup_loss: 0.129, D_sup_acc: 97.20 Train acc: 96.775 Test acc: 96.970 \n",
      "step: 5515 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.612, D_sup_loss: 0.140, D_sup_acc: 97.01 Train acc: 96.918 Test acc: 97.160 \n",
      "step: 5516 | Train: G_Loss: 1.165, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.587, D_sup_loss: 0.124, D_sup_acc: 97.20 Train acc: 96.748 Test acc: 96.990 \n",
      "step: 5517 | Train: G_Loss: 1.065, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.588, D_sup_loss: 0.137, D_sup_acc: 97.03 Train acc: 96.910 Test acc: 97.060 \n",
      "step: 5518 | Train: G_Loss: 1.296, D_unsup_loss_real: 0.531, D_unsup_loss_fake: 0.588, D_sup_loss: 0.128, D_sup_acc: 97.10 Train acc: 96.925 Test acc: 97.190 \n",
      "step: 5519 | Train: G_Loss: 1.217, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.669, D_sup_loss: 0.124, D_sup_acc: 97.23 Train acc: 96.875 Test acc: 97.200 \n",
      "step: 5520 | Train: G_Loss: 1.107, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.553, D_sup_loss: 0.129, D_sup_acc: 97.24 Train acc: 96.818 Test acc: 97.090 \n",
      "step: 5521 | Train: G_Loss: 1.101, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.579, D_sup_loss: 0.133, D_sup_acc: 97.13 Train acc: 96.568 Test acc: 96.730 \n",
      "step: 5522 | Train: G_Loss: 1.020, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.683, D_sup_loss: 0.145, D_sup_acc: 96.77 Train acc: 96.618 Test acc: 96.810 \n",
      "step: 5523 | Train: G_Loss: 1.118, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.627, D_sup_loss: 0.145, D_sup_acc: 96.85 Train acc: 96.855 Test acc: 97.100 \n",
      "step: 5524 | Train: G_Loss: 1.180, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.553, D_sup_loss: 0.134, D_sup_acc: 97.14 Train acc: 96.792 Test acc: 97.020 \n",
      "step: 5525 | Train: G_Loss: 1.182, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.632, D_sup_loss: 0.135, D_sup_acc: 97.06 Train acc: 96.862 Test acc: 97.200 \n",
      "step: 5526 | Train: G_Loss: 1.105, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.606, D_sup_loss: 0.128, D_sup_acc: 97.24 Train acc: 96.757 Test acc: 97.080 \n",
      "step: 5527 | Train: G_Loss: 1.096, D_unsup_loss_real: 0.649, D_unsup_loss_fake: 0.582, D_sup_loss: 0.139, D_sup_acc: 97.12 Train acc: 96.747 Test acc: 97.100 \n",
      "step: 5528 | Train: G_Loss: 1.212, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.583, D_sup_loss: 0.140, D_sup_acc: 97.14 Train acc: 96.917 Test acc: 97.170 \n",
      "step: 5529 | Train: G_Loss: 1.128, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.550, D_sup_loss: 0.129, D_sup_acc: 97.21 Train acc: 96.978 Test acc: 97.240 \n",
      "step: 5530 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.606, D_sup_loss: 0.123, D_sup_acc: 97.27 Train acc: 96.898 Test acc: 97.230 \n",
      "step: 5531 | Train: G_Loss: 1.075, D_unsup_loss_real: 0.515, D_unsup_loss_fake: 0.533, D_sup_loss: 0.124, D_sup_acc: 97.27 Train acc: 96.920 Test acc: 97.280 \n",
      "step: 5532 | Train: G_Loss: 1.152, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.543, D_sup_loss: 0.122, D_sup_acc: 97.31 Train acc: 96.915 Test acc: 97.310 \n",
      "step: 5533 | Train: G_Loss: 1.166, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.555, D_sup_loss: 0.122, D_sup_acc: 97.34 Train acc: 96.865 Test acc: 97.110 \n",
      "step: 5534 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.623, D_sup_loss: 0.127, D_sup_acc: 97.15 Train acc: 96.765 Test acc: 96.870 \n",
      "step: 5535 | Train: G_Loss: 1.235, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.633, D_sup_loss: 0.137, D_sup_acc: 96.91 Train acc: 96.928 Test acc: 97.080 \n",
      "step: 5536 | Train: G_Loss: 1.157, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.598, D_sup_loss: 0.125, D_sup_acc: 97.12 Train acc: 96.913 Test acc: 97.120 \n",
      "step: 5537 | Train: G_Loss: 1.244, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.541, D_sup_loss: 0.128, D_sup_acc: 97.16 Train acc: 97.010 Test acc: 97.160 \n",
      "step: 5538 | Train: G_Loss: 1.220, D_unsup_loss_real: 0.664, D_unsup_loss_fake: 0.627, D_sup_loss: 0.120, D_sup_acc: 97.20 Train acc: 96.777 Test acc: 96.930 \n",
      "step: 5539 | Train: G_Loss: 1.165, D_unsup_loss_real: 0.655, D_unsup_loss_fake: 0.593, D_sup_loss: 0.137, D_sup_acc: 96.97 Train acc: 96.788 Test acc: 96.910 \n",
      "step: 5540 | Train: G_Loss: 1.204, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.579, D_sup_loss: 0.128, D_sup_acc: 96.95 Train acc: 96.937 Test acc: 97.000 \n",
      "step: 5541 | Train: G_Loss: 1.109, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.600, D_sup_loss: 0.123, D_sup_acc: 97.04 Train acc: 96.888 Test acc: 97.080 \n",
      "step: 5542 | Train: G_Loss: 1.136, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.533, D_sup_loss: 0.129, D_sup_acc: 97.12 Train acc: 96.845 Test acc: 96.960 \n",
      "step: 5543 | Train: G_Loss: 1.058, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.625, D_sup_loss: 0.131, D_sup_acc: 97.00 Train acc: 96.800 Test acc: 97.000 \n",
      "step: 5544 | Train: G_Loss: 1.132, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.600, D_sup_loss: 0.133, D_sup_acc: 97.04 Train acc: 96.972 Test acc: 97.170 \n",
      "step: 5545 | Train: G_Loss: 1.257, D_unsup_loss_real: 0.509, D_unsup_loss_fake: 0.672, D_sup_loss: 0.125, D_sup_acc: 97.21 Train acc: 96.957 Test acc: 97.160 \n",
      "step: 5546 | Train: G_Loss: 1.109, D_unsup_loss_real: 0.661, D_unsup_loss_fake: 0.647, D_sup_loss: 0.125, D_sup_acc: 97.20 Train acc: 96.958 Test acc: 97.180 \n",
      "step: 5547 | Train: G_Loss: 1.064, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.581, D_sup_loss: 0.128, D_sup_acc: 97.22 Train acc: 96.928 Test acc: 97.180 \n",
      "step: 5548 | Train: G_Loss: 1.126, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.569, D_sup_loss: 0.129, D_sup_acc: 97.22 Train acc: 96.882 Test acc: 97.010 \n",
      "step: 5549 | Train: G_Loss: 1.070, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.566, D_sup_loss: 0.131, D_sup_acc: 97.05 Train acc: 96.942 Test acc: 97.140 \n",
      "step: 5550 | Train: G_Loss: 1.219, D_unsup_loss_real: 0.520, D_unsup_loss_fake: 0.657, D_sup_loss: 0.130, D_sup_acc: 97.18 Train acc: 96.932 Test acc: 97.110 \n",
      "step: 5551 | Train: G_Loss: 1.167, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.683, D_sup_loss: 0.123, D_sup_acc: 97.15 Train acc: 96.712 Test acc: 96.990 \n",
      "step: 5552 | Train: G_Loss: 1.214, D_unsup_loss_real: 0.515, D_unsup_loss_fake: 0.505, D_sup_loss: 0.138, D_sup_acc: 97.03 Train acc: 96.760 Test acc: 96.910 \n",
      "step: 5553 | Train: G_Loss: 1.162, D_unsup_loss_real: 0.536, D_unsup_loss_fake: 0.516, D_sup_loss: 0.132, D_sup_acc: 96.95 Train acc: 96.903 Test acc: 97.060 \n",
      "step: 5554 | Train: G_Loss: 1.178, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.652, D_sup_loss: 0.126, D_sup_acc: 97.10 Train acc: 96.843 Test acc: 97.020 \n",
      "step: 5555 | Train: G_Loss: 1.077, D_unsup_loss_real: 0.546, D_unsup_loss_fake: 0.516, D_sup_loss: 0.130, D_sup_acc: 97.06 Train acc: 96.927 Test acc: 97.190 \n",
      "step: 5556 | Train: G_Loss: 1.233, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.551, D_sup_loss: 0.124, D_sup_acc: 97.23 Train acc: 96.983 Test acc: 97.220 \n",
      "step: 5557 | Train: G_Loss: 1.187, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.548, D_sup_loss: 0.119, D_sup_acc: 97.26 Train acc: 97.053 Test acc: 97.200 \n",
      "step: 5558 | Train: G_Loss: 1.073, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.583, D_sup_loss: 0.119, D_sup_acc: 97.24 Train acc: 97.085 Test acc: 97.130 \n",
      "step: 5559 | Train: G_Loss: 1.119, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.608, D_sup_loss: 0.121, D_sup_acc: 97.17 Train acc: 97.045 Test acc: 97.170 \n",
      "step: 5560 | Train: G_Loss: 1.072, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.638, D_sup_loss: 0.117, D_sup_acc: 97.21 Train acc: 97.043 Test acc: 97.190 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5561 | Train: G_Loss: 1.201, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.599, D_sup_loss: 0.123, D_sup_acc: 97.23 Train acc: 97.035 Test acc: 97.210 \n",
      "step: 5562 | Train: G_Loss: 1.117, D_unsup_loss_real: 0.527, D_unsup_loss_fake: 0.577, D_sup_loss: 0.121, D_sup_acc: 97.25 Train acc: 97.068 Test acc: 97.150 \n",
      "step: 5563 | Train: G_Loss: 1.216, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.524, D_sup_loss: 0.126, D_sup_acc: 97.19 Train acc: 97.008 Test acc: 97.210 \n",
      "step: 5564 | Train: G_Loss: 1.143, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.556, D_sup_loss: 0.119, D_sup_acc: 97.25 Train acc: 96.947 Test acc: 97.120 \n",
      "step: 5565 | Train: G_Loss: 1.186, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.562, D_sup_loss: 0.129, D_sup_acc: 97.16 Train acc: 96.915 Test acc: 97.090 \n",
      "step: 5566 | Train: G_Loss: 1.065, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.634, D_sup_loss: 0.129, D_sup_acc: 97.13 Train acc: 96.500 Test acc: 96.880 \n",
      "step: 5567 | Train: G_Loss: 1.176, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.548, D_sup_loss: 0.155, D_sup_acc: 96.92 Train acc: 96.805 Test acc: 97.010 \n",
      "step: 5568 | Train: G_Loss: 1.214, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.566, D_sup_loss: 0.136, D_sup_acc: 97.05 Train acc: 97.010 Test acc: 97.230 \n",
      "step: 5569 | Train: G_Loss: 1.159, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.587, D_sup_loss: 0.121, D_sup_acc: 97.27 Train acc: 97.020 Test acc: 97.210 \n",
      "step: 5570 | Train: G_Loss: 1.234, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.604, D_sup_loss: 0.123, D_sup_acc: 97.25 Train acc: 96.953 Test acc: 97.220 \n",
      "step: 5571 | Train: G_Loss: 1.141, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.601, D_sup_loss: 0.121, D_sup_acc: 97.26 Train acc: 96.800 Test acc: 97.020 \n",
      "step: 5572 | Train: G_Loss: 1.192, D_unsup_loss_real: 0.647, D_unsup_loss_fake: 0.640, D_sup_loss: 0.136, D_sup_acc: 97.06 Train acc: 96.853 Test acc: 97.060 \n",
      "step: 5573 | Train: G_Loss: 1.180, D_unsup_loss_real: 0.513, D_unsup_loss_fake: 0.596, D_sup_loss: 0.134, D_sup_acc: 97.10 Train acc: 97.020 Test acc: 97.180 \n",
      "step: 5574 | Train: G_Loss: 1.204, D_unsup_loss_real: 0.514, D_unsup_loss_fake: 0.566, D_sup_loss: 0.120, D_sup_acc: 97.22 Train acc: 97.003 Test acc: 97.200 \n",
      "step: 5575 | Train: G_Loss: 1.199, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.516, D_sup_loss: 0.120, D_sup_acc: 97.24 Train acc: 97.032 Test acc: 97.240 \n",
      "step: 5576 | Train: G_Loss: 1.135, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.545, D_sup_loss: 0.119, D_sup_acc: 97.27 Train acc: 96.890 Test acc: 97.100 \n",
      "step: 5577 | Train: G_Loss: 1.120, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.499, D_sup_loss: 0.128, D_sup_acc: 97.14 Train acc: 96.953 Test acc: 97.180 \n",
      "step: 5578 | Train: G_Loss: 1.106, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.650, D_sup_loss: 0.119, D_sup_acc: 97.22 Train acc: 96.983 Test acc: 97.170 \n",
      "step: 5579 | Train: G_Loss: 1.151, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.550, D_sup_loss: 0.122, D_sup_acc: 97.21 Train acc: 96.992 Test acc: 97.170 \n",
      "step: 5580 | Train: G_Loss: 1.096, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.631, D_sup_loss: 0.121, D_sup_acc: 97.21 Train acc: 96.988 Test acc: 97.180 \n",
      "step: 5581 | Train: G_Loss: 1.147, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.561, D_sup_loss: 0.122, D_sup_acc: 97.22 Train acc: 96.995 Test acc: 97.160 \n",
      "step: 5582 | Train: G_Loss: 1.114, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.576, D_sup_loss: 0.123, D_sup_acc: 97.20 Train acc: 96.983 Test acc: 97.060 \n",
      "step: 5583 | Train: G_Loss: 1.172, D_unsup_loss_real: 0.538, D_unsup_loss_fake: 0.614, D_sup_loss: 0.123, D_sup_acc: 97.10 Train acc: 97.018 Test acc: 97.160 \n",
      "step: 5584 | Train: G_Loss: 1.195, D_unsup_loss_real: 0.545, D_unsup_loss_fake: 0.578, D_sup_loss: 0.121, D_sup_acc: 97.20 Train acc: 97.000 Test acc: 97.110 \n",
      "step: 5585 | Train: G_Loss: 1.130, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.604, D_sup_loss: 0.122, D_sup_acc: 97.15 Train acc: 97.080 Test acc: 97.250 \n",
      "step: 5586 | Train: G_Loss: 1.086, D_unsup_loss_real: 0.503, D_unsup_loss_fake: 0.577, D_sup_loss: 0.118, D_sup_acc: 97.28 Train acc: 97.017 Test acc: 97.160 \n",
      "step: 5587 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.585, D_sup_loss: 0.123, D_sup_acc: 97.20 Train acc: 96.565 Test acc: 96.780 \n",
      "step: 5588 | Train: G_Loss: 1.122, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.618, D_sup_loss: 0.144, D_sup_acc: 96.82 Train acc: 96.482 Test acc: 96.680 \n",
      "step: 5589 | Train: G_Loss: 1.123, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.601, D_sup_loss: 0.151, D_sup_acc: 96.72 Train acc: 96.950 Test acc: 97.120 \n",
      "step: 5590 | Train: G_Loss: 1.317, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.604, D_sup_loss: 0.123, D_sup_acc: 97.16 Train acc: 96.995 Test acc: 97.210 \n",
      "step: 5591 | Train: G_Loss: 1.187, D_unsup_loss_real: 0.523, D_unsup_loss_fake: 0.606, D_sup_loss: 0.122, D_sup_acc: 97.25 Train acc: 97.053 Test acc: 97.130 \n",
      "step: 5592 | Train: G_Loss: 1.237, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.568, D_sup_loss: 0.118, D_sup_acc: 97.17 Train acc: 96.938 Test acc: 97.100 \n",
      "step: 5593 | Train: G_Loss: 1.158, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.533, D_sup_loss: 0.128, D_sup_acc: 97.14 Train acc: 96.953 Test acc: 97.180 \n",
      "step: 5594 | Train: G_Loss: 1.114, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.625, D_sup_loss: 0.126, D_sup_acc: 97.22 Train acc: 96.812 Test acc: 97.130 \n",
      "step: 5595 | Train: G_Loss: 0.991, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.600, D_sup_loss: 0.134, D_sup_acc: 97.17 Train acc: 96.842 Test acc: 97.090 \n",
      "step: 5596 | Train: G_Loss: 1.188, D_unsup_loss_real: 0.506, D_unsup_loss_fake: 0.638, D_sup_loss: 0.135, D_sup_acc: 97.13 Train acc: 97.075 Test acc: 97.320 \n",
      "step: 5597 | Train: G_Loss: 1.156, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.568, D_sup_loss: 0.119, D_sup_acc: 97.35 Train acc: 97.087 Test acc: 97.270 \n",
      "step: 5598 | Train: G_Loss: 1.217, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.614, D_sup_loss: 0.121, D_sup_acc: 97.30 Train acc: 97.008 Test acc: 97.180 \n",
      "step: 5599 | Train: G_Loss: 1.172, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.579, D_sup_loss: 0.124, D_sup_acc: 97.22 Train acc: 96.857 Test acc: 97.030 \n",
      "step: 5600 | Train: G_Loss: 1.167, D_unsup_loss_real: 0.653, D_unsup_loss_fake: 0.549, D_sup_loss: 0.130, D_sup_acc: 97.07 Train acc: 97.087 Test acc: 97.230 \n",
      "Train Classifier Accuracy: 97.087%\n",
      "\n",
      "Test Classifier Accuracy: 97.230%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_5600.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_5600.h5\n",
      "step: 5601 | Train: G_Loss: 1.272, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.552, D_sup_loss: 0.120, D_sup_acc: 97.27 Train acc: 97.130 Test acc: 97.220 \n",
      "step: 5602 | Train: G_Loss: 1.328, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.595, D_sup_loss: 0.118, D_sup_acc: 97.26 Train acc: 97.117 Test acc: 97.220 \n",
      "step: 5603 | Train: G_Loss: 1.152, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.591, D_sup_loss: 0.118, D_sup_acc: 97.26 Train acc: 96.975 Test acc: 97.060 \n",
      "step: 5604 | Train: G_Loss: 1.171, D_unsup_loss_real: 0.533, D_unsup_loss_fake: 0.581, D_sup_loss: 0.127, D_sup_acc: 97.10 Train acc: 97.002 Test acc: 97.070 \n",
      "step: 5605 | Train: G_Loss: 1.298, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.636, D_sup_loss: 0.121, D_sup_acc: 97.11 Train acc: 96.922 Test acc: 97.110 \n",
      "step: 5606 | Train: G_Loss: 1.259, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.569, D_sup_loss: 0.124, D_sup_acc: 97.15 Train acc: 97.008 Test acc: 97.170 \n",
      "step: 5607 | Train: G_Loss: 1.174, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.606, D_sup_loss: 0.119, D_sup_acc: 97.21 Train acc: 96.870 Test acc: 97.190 \n",
      "step: 5608 | Train: G_Loss: 1.181, D_unsup_loss_real: 0.538, D_unsup_loss_fake: 0.545, D_sup_loss: 0.130, D_sup_acc: 97.23 Train acc: 96.988 Test acc: 97.300 \n",
      "step: 5609 | Train: G_Loss: 1.032, D_unsup_loss_real: 0.657, D_unsup_loss_fake: 0.565, D_sup_loss: 0.119, D_sup_acc: 97.33 Train acc: 96.883 Test acc: 97.170 \n",
      "step: 5610 | Train: G_Loss: 1.120, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.567, D_sup_loss: 0.133, D_sup_acc: 97.21 Train acc: 96.982 Test acc: 97.250 \n",
      "step: 5611 | Train: G_Loss: 1.154, D_unsup_loss_real: 0.510, D_unsup_loss_fake: 0.529, D_sup_loss: 0.122, D_sup_acc: 97.28 Train acc: 97.068 Test acc: 97.180 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5612 | Train: G_Loss: 1.173, D_unsup_loss_real: 0.502, D_unsup_loss_fake: 0.593, D_sup_loss: 0.114, D_sup_acc: 97.22 Train acc: 97.023 Test acc: 97.210 \n",
      "step: 5613 | Train: G_Loss: 1.136, D_unsup_loss_real: 0.528, D_unsup_loss_fake: 0.578, D_sup_loss: 0.119, D_sup_acc: 97.25 Train acc: 96.883 Test acc: 97.130 \n",
      "step: 5614 | Train: G_Loss: 1.252, D_unsup_loss_real: 0.535, D_unsup_loss_fake: 0.536, D_sup_loss: 0.133, D_sup_acc: 97.17 Train acc: 97.035 Test acc: 97.180 \n",
      "step: 5615 | Train: G_Loss: 1.212, D_unsup_loss_real: 0.510, D_unsup_loss_fake: 0.543, D_sup_loss: 0.117, D_sup_acc: 97.22 Train acc: 97.052 Test acc: 97.110 \n",
      "step: 5616 | Train: G_Loss: 1.130, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.577, D_sup_loss: 0.116, D_sup_acc: 97.15 Train acc: 96.960 Test acc: 97.060 \n",
      "step: 5617 | Train: G_Loss: 1.205, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.570, D_sup_loss: 0.127, D_sup_acc: 97.10 Train acc: 96.938 Test acc: 97.020 \n",
      "step: 5618 | Train: G_Loss: 1.253, D_unsup_loss_real: 0.530, D_unsup_loss_fake: 0.566, D_sup_loss: 0.129, D_sup_acc: 97.06 Train acc: 96.982 Test acc: 97.090 \n",
      "step: 5619 | Train: G_Loss: 1.202, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.527, D_sup_loss: 0.121, D_sup_acc: 97.13 Train acc: 96.915 Test acc: 97.080 \n",
      "step: 5620 | Train: G_Loss: 1.175, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.605, D_sup_loss: 0.126, D_sup_acc: 97.12 Train acc: 96.917 Test acc: 97.160 \n",
      "step: 5621 | Train: G_Loss: 1.168, D_unsup_loss_real: 0.550, D_unsup_loss_fake: 0.621, D_sup_loss: 0.122, D_sup_acc: 97.20 Train acc: 96.847 Test acc: 97.010 \n",
      "step: 5622 | Train: G_Loss: 1.281, D_unsup_loss_real: 0.690, D_unsup_loss_fake: 0.555, D_sup_loss: 0.129, D_sup_acc: 97.05 Train acc: 95.348 Test acc: 95.510 \n",
      "step: 5623 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.778, D_unsup_loss_fake: 0.600, D_sup_loss: 0.221, D_sup_acc: 95.57 Train acc: 97.075 Test acc: 97.220 \n",
      "step: 5624 | Train: G_Loss: 1.129, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.637, D_sup_loss: 0.114, D_sup_acc: 97.26 Train acc: 96.930 Test acc: 97.140 \n",
      "step: 5625 | Train: G_Loss: 1.196, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.603, D_sup_loss: 0.125, D_sup_acc: 97.18 Train acc: 96.967 Test acc: 97.090 \n",
      "step: 5626 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.607, D_sup_loss: 0.124, D_sup_acc: 97.13 Train acc: 97.000 Test acc: 97.180 \n",
      "step: 5627 | Train: G_Loss: 1.141, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.601, D_sup_loss: 0.119, D_sup_acc: 97.22 Train acc: 96.900 Test acc: 97.120 \n",
      "step: 5628 | Train: G_Loss: 1.104, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.533, D_sup_loss: 0.126, D_sup_acc: 97.16 Train acc: 96.990 Test acc: 97.150 \n",
      "step: 5629 | Train: G_Loss: 1.164, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.572, D_sup_loss: 0.121, D_sup_acc: 97.19 Train acc: 96.865 Test acc: 97.060 \n",
      "step: 5630 | Train: G_Loss: 1.136, D_unsup_loss_real: 0.538, D_unsup_loss_fake: 0.601, D_sup_loss: 0.119, D_sup_acc: 97.10 Train acc: 96.853 Test acc: 97.030 \n",
      "step: 5631 | Train: G_Loss: 1.061, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.601, D_sup_loss: 0.127, D_sup_acc: 97.07 Train acc: 96.768 Test acc: 96.950 \n",
      "step: 5632 | Train: G_Loss: 1.188, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.568, D_sup_loss: 0.132, D_sup_acc: 96.99 Train acc: 96.860 Test acc: 97.140 \n",
      "step: 5633 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.591, D_sup_loss: 0.126, D_sup_acc: 97.18 Train acc: 96.763 Test acc: 97.010 \n",
      "step: 5634 | Train: G_Loss: 1.174, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.632, D_sup_loss: 0.132, D_sup_acc: 97.05 Train acc: 96.722 Test acc: 96.930 \n",
      "step: 5635 | Train: G_Loss: 1.200, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.610, D_sup_loss: 0.135, D_sup_acc: 96.97 Train acc: 96.608 Test acc: 96.830 \n",
      "step: 5636 | Train: G_Loss: 1.107, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.577, D_sup_loss: 0.142, D_sup_acc: 96.87 Train acc: 96.702 Test acc: 96.980 \n",
      "step: 5637 | Train: G_Loss: 1.183, D_unsup_loss_real: 0.521, D_unsup_loss_fake: 0.532, D_sup_loss: 0.133, D_sup_acc: 97.02 Train acc: 96.800 Test acc: 97.060 \n",
      "step: 5638 | Train: G_Loss: 1.157, D_unsup_loss_real: 0.657, D_unsup_loss_fake: 0.594, D_sup_loss: 0.127, D_sup_acc: 97.10 Train acc: 96.857 Test acc: 97.060 \n",
      "step: 5639 | Train: G_Loss: 1.159, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.620, D_sup_loss: 0.128, D_sup_acc: 97.10 Train acc: 96.750 Test acc: 96.970 \n",
      "step: 5640 | Train: G_Loss: 1.194, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.551, D_sup_loss: 0.130, D_sup_acc: 97.01 Train acc: 96.870 Test acc: 97.050 \n",
      "step: 5641 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.557, D_sup_loss: 0.125, D_sup_acc: 97.09 Train acc: 96.830 Test acc: 96.980 \n",
      "step: 5642 | Train: G_Loss: 1.110, D_unsup_loss_real: 0.518, D_unsup_loss_fake: 0.603, D_sup_loss: 0.129, D_sup_acc: 97.02 Train acc: 96.778 Test acc: 96.980 \n",
      "step: 5643 | Train: G_Loss: 1.176, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.632, D_sup_loss: 0.131, D_sup_acc: 97.02 Train acc: 96.695 Test acc: 96.870 \n",
      "step: 5644 | Train: G_Loss: 1.162, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.592, D_sup_loss: 0.139, D_sup_acc: 96.91 Train acc: 96.903 Test acc: 97.090 \n",
      "step: 5645 | Train: G_Loss: 1.225, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.560, D_sup_loss: 0.123, D_sup_acc: 97.13 Train acc: 96.930 Test acc: 97.010 \n",
      "step: 5646 | Train: G_Loss: 1.149, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.618, D_sup_loss: 0.125, D_sup_acc: 97.05 Train acc: 96.758 Test acc: 96.910 \n",
      "step: 5647 | Train: G_Loss: 1.128, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.577, D_sup_loss: 0.133, D_sup_acc: 96.95 Train acc: 96.808 Test acc: 97.010 \n",
      "step: 5648 | Train: G_Loss: 1.147, D_unsup_loss_real: 0.542, D_unsup_loss_fake: 0.686, D_sup_loss: 0.131, D_sup_acc: 97.05 Train acc: 96.880 Test acc: 96.960 \n",
      "step: 5649 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.589, D_sup_loss: 0.130, D_sup_acc: 97.00 Train acc: 97.033 Test acc: 97.090 \n",
      "step: 5650 | Train: G_Loss: 1.104, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.603, D_sup_loss: 0.121, D_sup_acc: 97.13 Train acc: 97.003 Test acc: 97.030 \n",
      "step: 5651 | Train: G_Loss: 1.163, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.598, D_sup_loss: 0.126, D_sup_acc: 97.07 Train acc: 97.028 Test acc: 97.070 \n",
      "step: 5652 | Train: G_Loss: 1.182, D_unsup_loss_real: 0.544, D_unsup_loss_fake: 0.611, D_sup_loss: 0.124, D_sup_acc: 97.11 Train acc: 97.013 Test acc: 97.060 \n",
      "step: 5653 | Train: G_Loss: 1.050, D_unsup_loss_real: 0.650, D_unsup_loss_fake: 0.585, D_sup_loss: 0.124, D_sup_acc: 97.10 Train acc: 96.982 Test acc: 97.040 \n",
      "step: 5654 | Train: G_Loss: 1.264, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.562, D_sup_loss: 0.127, D_sup_acc: 97.08 Train acc: 97.020 Test acc: 97.160 \n",
      "step: 5655 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.519, D_unsup_loss_fake: 0.607, D_sup_loss: 0.122, D_sup_acc: 97.20 Train acc: 97.030 Test acc: 97.170 \n",
      "step: 5656 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.515, D_unsup_loss_fake: 0.579, D_sup_loss: 0.121, D_sup_acc: 97.21 Train acc: 97.083 Test acc: 97.260 \n",
      "step: 5657 | Train: G_Loss: 1.178, D_unsup_loss_real: 0.681, D_unsup_loss_fake: 0.656, D_sup_loss: 0.115, D_sup_acc: 97.29 Train acc: 96.845 Test acc: 96.990 \n",
      "step: 5658 | Train: G_Loss: 1.180, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.656, D_sup_loss: 0.136, D_sup_acc: 97.03 Train acc: 96.673 Test acc: 96.800 \n",
      "step: 5659 | Train: G_Loss: 1.258, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.523, D_sup_loss: 0.141, D_sup_acc: 96.84 Train acc: 96.962 Test acc: 97.210 \n",
      "step: 5660 | Train: G_Loss: 1.289, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.603, D_sup_loss: 0.121, D_sup_acc: 97.25 Train acc: 96.953 Test acc: 97.080 \n",
      "step: 5661 | Train: G_Loss: 1.154, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.561, D_sup_loss: 0.120, D_sup_acc: 97.12 Train acc: 96.778 Test acc: 96.900 \n",
      "step: 5662 | Train: G_Loss: 1.171, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.617, D_sup_loss: 0.136, D_sup_acc: 96.94 Train acc: 96.882 Test acc: 97.000 \n",
      "step: 5663 | Train: G_Loss: 1.184, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.669, D_sup_loss: 0.126, D_sup_acc: 97.04 Train acc: 96.690 Test acc: 96.880 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5664 | Train: G_Loss: 1.200, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.547, D_sup_loss: 0.140, D_sup_acc: 96.92 Train acc: 96.797 Test acc: 96.950 \n",
      "step: 5665 | Train: G_Loss: 1.133, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.602, D_sup_loss: 0.131, D_sup_acc: 96.99 Train acc: 96.845 Test acc: 96.980 \n",
      "step: 5666 | Train: G_Loss: 1.289, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.595, D_sup_loss: 0.134, D_sup_acc: 97.02 Train acc: 96.998 Test acc: 97.110 \n",
      "step: 5667 | Train: G_Loss: 1.178, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.513, D_sup_loss: 0.124, D_sup_acc: 97.15 Train acc: 97.045 Test acc: 97.190 \n",
      "step: 5668 | Train: G_Loss: 1.063, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.636, D_sup_loss: 0.122, D_sup_acc: 97.23 Train acc: 96.772 Test acc: 96.820 \n",
      "step: 5669 | Train: G_Loss: 1.193, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.567, D_sup_loss: 0.138, D_sup_acc: 96.86 Train acc: 96.897 Test acc: 97.080 \n",
      "step: 5670 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.655, D_sup_loss: 0.127, D_sup_acc: 97.12 Train acc: 96.818 Test acc: 96.930 \n",
      "step: 5671 | Train: G_Loss: 1.225, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.629, D_sup_loss: 0.136, D_sup_acc: 96.97 Train acc: 96.968 Test acc: 97.140 \n",
      "step: 5672 | Train: G_Loss: 1.115, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.524, D_sup_loss: 0.124, D_sup_acc: 97.18 Train acc: 96.893 Test acc: 97.020 \n",
      "step: 5673 | Train: G_Loss: 1.159, D_unsup_loss_real: 0.545, D_unsup_loss_fake: 0.642, D_sup_loss: 0.128, D_sup_acc: 97.06 Train acc: 96.835 Test acc: 96.910 \n",
      "step: 5674 | Train: G_Loss: 1.156, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.577, D_sup_loss: 0.130, D_sup_acc: 96.95 Train acc: 96.867 Test acc: 97.060 \n",
      "step: 5675 | Train: G_Loss: 1.151, D_unsup_loss_real: 0.535, D_unsup_loss_fake: 0.615, D_sup_loss: 0.128, D_sup_acc: 97.10 Train acc: 96.842 Test acc: 97.040 \n",
      "step: 5676 | Train: G_Loss: 1.193, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.536, D_sup_loss: 0.128, D_sup_acc: 97.08 Train acc: 96.667 Test acc: 96.830 \n",
      "step: 5677 | Train: G_Loss: 1.220, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.588, D_sup_loss: 0.138, D_sup_acc: 96.87 Train acc: 96.835 Test acc: 96.970 \n",
      "step: 5678 | Train: G_Loss: 1.070, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.668, D_sup_loss: 0.129, D_sup_acc: 97.01 Train acc: 96.615 Test acc: 96.820 \n",
      "step: 5679 | Train: G_Loss: 1.143, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.559, D_sup_loss: 0.148, D_sup_acc: 96.86 Train acc: 96.888 Test acc: 97.100 \n",
      "step: 5680 | Train: G_Loss: 1.157, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.641, D_sup_loss: 0.125, D_sup_acc: 97.14 Train acc: 96.732 Test acc: 96.980 \n",
      "step: 5681 | Train: G_Loss: 1.240, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.576, D_sup_loss: 0.137, D_sup_acc: 97.02 Train acc: 96.850 Test acc: 96.970 \n",
      "step: 5682 | Train: G_Loss: 1.194, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.551, D_sup_loss: 0.129, D_sup_acc: 97.01 Train acc: 96.778 Test acc: 96.960 \n",
      "step: 5683 | Train: G_Loss: 1.168, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.560, D_sup_loss: 0.131, D_sup_acc: 97.00 Train acc: 96.700 Test acc: 96.730 \n",
      "step: 5684 | Train: G_Loss: 1.140, D_unsup_loss_real: 0.529, D_unsup_loss_fake: 0.630, D_sup_loss: 0.142, D_sup_acc: 96.77 Train acc: 96.802 Test acc: 96.960 \n",
      "step: 5685 | Train: G_Loss: 1.158, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.559, D_sup_loss: 0.131, D_sup_acc: 97.00 Train acc: 96.743 Test acc: 96.890 \n",
      "step: 5686 | Train: G_Loss: 1.105, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.598, D_sup_loss: 0.132, D_sup_acc: 96.93 Train acc: 96.895 Test acc: 97.200 \n",
      "step: 5687 | Train: G_Loss: 1.156, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.569, D_sup_loss: 0.122, D_sup_acc: 97.24 Train acc: 96.915 Test acc: 97.260 \n",
      "step: 5688 | Train: G_Loss: 1.147, D_unsup_loss_real: 0.491, D_unsup_loss_fake: 0.571, D_sup_loss: 0.123, D_sup_acc: 97.29 Train acc: 96.810 Test acc: 96.970 \n",
      "step: 5689 | Train: G_Loss: 1.114, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.625, D_sup_loss: 0.131, D_sup_acc: 97.01 Train acc: 96.922 Test acc: 97.160 \n",
      "step: 5690 | Train: G_Loss: 1.274, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.542, D_sup_loss: 0.126, D_sup_acc: 97.20 Train acc: 96.845 Test acc: 97.080 \n",
      "step: 5691 | Train: G_Loss: 1.229, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.660, D_sup_loss: 0.133, D_sup_acc: 97.12 Train acc: 96.885 Test acc: 97.170 \n",
      "step: 5692 | Train: G_Loss: 1.213, D_unsup_loss_real: 0.550, D_unsup_loss_fake: 0.574, D_sup_loss: 0.130, D_sup_acc: 97.21 Train acc: 96.923 Test acc: 97.130 \n",
      "step: 5693 | Train: G_Loss: 1.170, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.537, D_sup_loss: 0.125, D_sup_acc: 97.17 Train acc: 96.920 Test acc: 97.160 \n",
      "step: 5694 | Train: G_Loss: 1.110, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.538, D_sup_loss: 0.127, D_sup_acc: 97.20 Train acc: 96.920 Test acc: 97.140 \n",
      "step: 5695 | Train: G_Loss: 1.200, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.618, D_sup_loss: 0.128, D_sup_acc: 97.18 Train acc: 96.907 Test acc: 97.120 \n",
      "step: 5696 | Train: G_Loss: 1.238, D_unsup_loss_real: 0.471, D_unsup_loss_fake: 0.529, D_sup_loss: 0.123, D_sup_acc: 97.16 Train acc: 96.978 Test acc: 97.260 \n",
      "step: 5697 | Train: G_Loss: 1.183, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.600, D_sup_loss: 0.119, D_sup_acc: 97.29 Train acc: 96.853 Test acc: 97.140 \n",
      "step: 5698 | Train: G_Loss: 1.195, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.529, D_sup_loss: 0.126, D_sup_acc: 97.18 Train acc: 96.987 Test acc: 97.240 \n",
      "step: 5699 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.635, D_sup_loss: 0.119, D_sup_acc: 97.27 Train acc: 96.815 Test acc: 96.990 \n",
      "step: 5700 | Train: G_Loss: 1.109, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.645, D_sup_loss: 0.133, D_sup_acc: 97.03 Train acc: 96.987 Test acc: 97.140 \n",
      "Train Classifier Accuracy: 96.987%\n",
      "\n",
      "Test Classifier Accuracy: 97.140%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_5700.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_5700.h5\n",
      "step: 5701 | Train: G_Loss: 1.253, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.603, D_sup_loss: 0.122, D_sup_acc: 97.18 Train acc: 96.637 Test acc: 96.860 \n",
      "step: 5702 | Train: G_Loss: 1.094, D_unsup_loss_real: 0.671, D_unsup_loss_fake: 0.656, D_sup_loss: 0.139, D_sup_acc: 96.90 Train acc: 96.565 Test acc: 96.780 \n",
      "step: 5703 | Train: G_Loss: 1.097, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.577, D_sup_loss: 0.145, D_sup_acc: 96.82 Train acc: 96.800 Test acc: 97.080 \n",
      "step: 5704 | Train: G_Loss: 1.082, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.583, D_sup_loss: 0.129, D_sup_acc: 97.12 Train acc: 96.803 Test acc: 97.190 \n",
      "step: 5705 | Train: G_Loss: 1.177, D_unsup_loss_real: 0.520, D_unsup_loss_fake: 0.639, D_sup_loss: 0.129, D_sup_acc: 97.23 Train acc: 96.893 Test acc: 97.180 \n",
      "step: 5706 | Train: G_Loss: 1.160, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.602, D_sup_loss: 0.125, D_sup_acc: 97.22 Train acc: 96.805 Test acc: 97.090 \n",
      "step: 5707 | Train: G_Loss: 1.156, D_unsup_loss_real: 0.509, D_unsup_loss_fake: 0.594, D_sup_loss: 0.130, D_sup_acc: 97.13 Train acc: 96.817 Test acc: 97.070 \n",
      "step: 5708 | Train: G_Loss: 1.127, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.590, D_sup_loss: 0.131, D_sup_acc: 97.11 Train acc: 96.862 Test acc: 97.150 \n",
      "step: 5709 | Train: G_Loss: 1.211, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.575, D_sup_loss: 0.127, D_sup_acc: 97.19 Train acc: 96.972 Test acc: 97.180 \n",
      "step: 5710 | Train: G_Loss: 1.082, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.570, D_sup_loss: 0.124, D_sup_acc: 97.22 Train acc: 96.953 Test acc: 97.240 \n",
      "step: 5711 | Train: G_Loss: 1.155, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.545, D_sup_loss: 0.123, D_sup_acc: 97.27 Train acc: 96.870 Test acc: 97.120 \n",
      "step: 5712 | Train: G_Loss: 1.113, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.645, D_sup_loss: 0.128, D_sup_acc: 97.16 Train acc: 96.778 Test acc: 97.000 \n",
      "step: 5713 | Train: G_Loss: 1.202, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.551, D_sup_loss: 0.137, D_sup_acc: 97.04 Train acc: 96.915 Test acc: 97.090 \n",
      "step: 5714 | Train: G_Loss: 1.187, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.638, D_sup_loss: 0.126, D_sup_acc: 97.13 Train acc: 96.992 Test acc: 97.210 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5715 | Train: G_Loss: 1.155, D_unsup_loss_real: 0.757, D_unsup_loss_fake: 0.620, D_sup_loss: 0.122, D_sup_acc: 97.25 Train acc: 96.978 Test acc: 97.250 \n",
      "step: 5716 | Train: G_Loss: 1.184, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.510, D_sup_loss: 0.126, D_sup_acc: 97.28 Train acc: 97.132 Test acc: 97.320 \n",
      "step: 5717 | Train: G_Loss: 1.271, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.590, D_sup_loss: 0.114, D_sup_acc: 97.35 Train acc: 97.057 Test acc: 97.200 \n",
      "step: 5718 | Train: G_Loss: 1.206, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.581, D_sup_loss: 0.120, D_sup_acc: 97.24 Train acc: 96.945 Test acc: 97.190 \n",
      "step: 5719 | Train: G_Loss: 1.217, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.561, D_sup_loss: 0.126, D_sup_acc: 97.23 Train acc: 96.977 Test acc: 97.200 \n",
      "step: 5720 | Train: G_Loss: 1.184, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.633, D_sup_loss: 0.122, D_sup_acc: 97.24 Train acc: 97.042 Test acc: 97.300 \n",
      "step: 5721 | Train: G_Loss: 1.141, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.582, D_sup_loss: 0.118, D_sup_acc: 97.33 Train acc: 97.015 Test acc: 97.380 \n",
      "step: 5722 | Train: G_Loss: 1.182, D_unsup_loss_real: 0.535, D_unsup_loss_fake: 0.652, D_sup_loss: 0.120, D_sup_acc: 97.41 Train acc: 97.018 Test acc: 97.220 \n",
      "step: 5723 | Train: G_Loss: 1.227, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.597, D_sup_loss: 0.121, D_sup_acc: 97.26 Train acc: 96.718 Test acc: 96.950 \n",
      "step: 5724 | Train: G_Loss: 1.193, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.636, D_sup_loss: 0.137, D_sup_acc: 96.99 Train acc: 96.803 Test acc: 96.980 \n",
      "step: 5725 | Train: G_Loss: 1.218, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.628, D_sup_loss: 0.130, D_sup_acc: 97.02 Train acc: 96.797 Test acc: 97.200 \n",
      "step: 5726 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.596, D_sup_loss: 0.128, D_sup_acc: 97.24 Train acc: 96.817 Test acc: 97.200 \n",
      "step: 5727 | Train: G_Loss: 1.155, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.567, D_sup_loss: 0.130, D_sup_acc: 97.24 Train acc: 96.850 Test acc: 97.200 \n",
      "step: 5728 | Train: G_Loss: 1.255, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.527, D_sup_loss: 0.130, D_sup_acc: 97.24 Train acc: 96.960 Test acc: 97.260 \n",
      "step: 5729 | Train: G_Loss: 1.288, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.546, D_sup_loss: 0.122, D_sup_acc: 97.29 Train acc: 96.973 Test acc: 97.250 \n",
      "step: 5730 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.600, D_sup_loss: 0.116, D_sup_acc: 97.28 Train acc: 96.762 Test acc: 97.020 \n",
      "step: 5731 | Train: G_Loss: 1.139, D_unsup_loss_real: 0.642, D_unsup_loss_fake: 0.588, D_sup_loss: 0.137, D_sup_acc: 97.06 Train acc: 96.940 Test acc: 97.150 \n",
      "step: 5732 | Train: G_Loss: 1.165, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.648, D_sup_loss: 0.128, D_sup_acc: 97.19 Train acc: 96.937 Test acc: 97.150 \n",
      "step: 5733 | Train: G_Loss: 1.283, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.602, D_sup_loss: 0.130, D_sup_acc: 97.19 Train acc: 96.982 Test acc: 97.210 \n",
      "step: 5734 | Train: G_Loss: 1.192, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.623, D_sup_loss: 0.122, D_sup_acc: 97.25 Train acc: 96.928 Test acc: 97.160 \n",
      "step: 5735 | Train: G_Loss: 1.161, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.514, D_sup_loss: 0.131, D_sup_acc: 97.20 Train acc: 96.812 Test acc: 96.990 \n",
      "step: 5736 | Train: G_Loss: 1.155, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.537, D_sup_loss: 0.137, D_sup_acc: 97.03 Train acc: 96.967 Test acc: 97.230 \n",
      "step: 5737 | Train: G_Loss: 1.050, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.525, D_sup_loss: 0.123, D_sup_acc: 97.27 Train acc: 96.962 Test acc: 97.250 \n",
      "step: 5738 | Train: G_Loss: 1.171, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.584, D_sup_loss: 0.121, D_sup_acc: 97.28 Train acc: 96.802 Test acc: 97.040 \n",
      "step: 5739 | Train: G_Loss: 1.172, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.618, D_sup_loss: 0.131, D_sup_acc: 97.08 Train acc: 96.995 Test acc: 97.280 \n",
      "step: 5740 | Train: G_Loss: 1.072, D_unsup_loss_real: 0.510, D_unsup_loss_fake: 0.531, D_sup_loss: 0.120, D_sup_acc: 97.31 Train acc: 96.948 Test acc: 97.390 \n",
      "step: 5741 | Train: G_Loss: 1.219, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.612, D_sup_loss: 0.116, D_sup_acc: 97.42 Train acc: 96.843 Test acc: 97.270 \n",
      "step: 5742 | Train: G_Loss: 1.091, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.623, D_sup_loss: 0.127, D_sup_acc: 97.30 Train acc: 96.772 Test acc: 97.090 \n",
      "step: 5743 | Train: G_Loss: 1.129, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.608, D_sup_loss: 0.133, D_sup_acc: 97.13 Train acc: 96.877 Test acc: 97.250 \n",
      "step: 5744 | Train: G_Loss: 1.115, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.535, D_sup_loss: 0.126, D_sup_acc: 97.28 Train acc: 96.925 Test acc: 97.310 \n",
      "step: 5745 | Train: G_Loss: 1.187, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.614, D_sup_loss: 0.120, D_sup_acc: 97.34 Train acc: 96.908 Test acc: 97.200 \n",
      "step: 5746 | Train: G_Loss: 1.094, D_unsup_loss_real: 0.664, D_unsup_loss_fake: 0.613, D_sup_loss: 0.122, D_sup_acc: 97.24 Train acc: 96.452 Test acc: 96.740 \n",
      "step: 5747 | Train: G_Loss: 1.075, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.681, D_sup_loss: 0.153, D_sup_acc: 96.78 Train acc: 96.903 Test acc: 97.140 \n",
      "step: 5748 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.594, D_sup_loss: 0.131, D_sup_acc: 97.18 Train acc: 97.040 Test acc: 97.300 \n",
      "step: 5749 | Train: G_Loss: 1.203, D_unsup_loss_real: 0.528, D_unsup_loss_fake: 0.594, D_sup_loss: 0.123, D_sup_acc: 97.33 Train acc: 96.998 Test acc: 97.340 \n",
      "step: 5750 | Train: G_Loss: 1.232, D_unsup_loss_real: 0.529, D_unsup_loss_fake: 0.605, D_sup_loss: 0.122, D_sup_acc: 97.37 Train acc: 96.967 Test acc: 97.280 \n",
      "step: 5751 | Train: G_Loss: 1.097, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.572, D_sup_loss: 0.123, D_sup_acc: 97.31 Train acc: 97.003 Test acc: 97.250 \n",
      "step: 5752 | Train: G_Loss: 1.237, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.560, D_sup_loss: 0.121, D_sup_acc: 97.28 Train acc: 96.900 Test acc: 97.100 \n",
      "step: 5753 | Train: G_Loss: 1.283, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.643, D_sup_loss: 0.123, D_sup_acc: 97.14 Train acc: 96.902 Test acc: 97.220 \n",
      "step: 5754 | Train: G_Loss: 1.260, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.519, D_sup_loss: 0.127, D_sup_acc: 97.26 Train acc: 96.990 Test acc: 97.260 \n",
      "step: 5755 | Train: G_Loss: 1.211, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.564, D_sup_loss: 0.123, D_sup_acc: 97.29 Train acc: 96.802 Test acc: 97.030 \n",
      "step: 5756 | Train: G_Loss: 1.206, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.590, D_sup_loss: 0.133, D_sup_acc: 97.07 Train acc: 97.052 Test acc: 97.230 \n",
      "step: 5757 | Train: G_Loss: 1.171, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.556, D_sup_loss: 0.121, D_sup_acc: 97.27 Train acc: 96.965 Test acc: 97.210 \n",
      "step: 5758 | Train: G_Loss: 1.223, D_unsup_loss_real: 0.517, D_unsup_loss_fake: 0.548, D_sup_loss: 0.127, D_sup_acc: 97.25 Train acc: 97.032 Test acc: 97.300 \n",
      "step: 5759 | Train: G_Loss: 1.033, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.663, D_sup_loss: 0.121, D_sup_acc: 97.33 Train acc: 96.997 Test acc: 97.320 \n",
      "step: 5760 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.522, D_sup_loss: 0.121, D_sup_acc: 97.35 Train acc: 96.945 Test acc: 97.150 \n",
      "step: 5761 | Train: G_Loss: 1.053, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.572, D_sup_loss: 0.127, D_sup_acc: 97.19 Train acc: 96.947 Test acc: 97.260 \n",
      "step: 5762 | Train: G_Loss: 1.119, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.595, D_sup_loss: 0.127, D_sup_acc: 97.29 Train acc: 96.938 Test acc: 97.260 \n",
      "step: 5763 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.631, D_sup_loss: 0.129, D_sup_acc: 97.29 Train acc: 97.085 Test acc: 97.300 \n",
      "step: 5764 | Train: G_Loss: 1.199, D_unsup_loss_real: 0.660, D_unsup_loss_fake: 0.577, D_sup_loss: 0.119, D_sup_acc: 97.33 Train acc: 96.972 Test acc: 97.190 \n",
      "step: 5765 | Train: G_Loss: 1.086, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.554, D_sup_loss: 0.129, D_sup_acc: 97.23 Train acc: 97.087 Test acc: 97.230 \n",
      "step: 5766 | Train: G_Loss: 1.205, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.629, D_sup_loss: 0.117, D_sup_acc: 97.27 Train acc: 96.847 Test acc: 97.190 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5767 | Train: G_Loss: 1.201, D_unsup_loss_real: 0.533, D_unsup_loss_fake: 0.573, D_sup_loss: 0.137, D_sup_acc: 97.23 Train acc: 97.043 Test acc: 97.230 \n",
      "step: 5768 | Train: G_Loss: 1.213, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.574, D_sup_loss: 0.120, D_sup_acc: 97.27 Train acc: 97.000 Test acc: 97.220 \n",
      "step: 5769 | Train: G_Loss: 1.104, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.602, D_sup_loss: 0.121, D_sup_acc: 97.26 Train acc: 97.002 Test acc: 97.240 \n",
      "step: 5770 | Train: G_Loss: 1.224, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.546, D_sup_loss: 0.119, D_sup_acc: 97.27 Train acc: 96.992 Test acc: 97.170 \n",
      "step: 5771 | Train: G_Loss: 1.206, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.610, D_sup_loss: 0.120, D_sup_acc: 97.21 Train acc: 97.063 Test acc: 97.190 \n",
      "step: 5772 | Train: G_Loss: 1.115, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.646, D_sup_loss: 0.120, D_sup_acc: 97.23 Train acc: 96.972 Test acc: 97.210 \n",
      "step: 5773 | Train: G_Loss: 1.156, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.588, D_sup_loss: 0.125, D_sup_acc: 97.25 Train acc: 97.107 Test acc: 97.230 \n",
      "step: 5774 | Train: G_Loss: 1.147, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.524, D_sup_loss: 0.116, D_sup_acc: 97.27 Train acc: 97.130 Test acc: 97.240 \n",
      "step: 5775 | Train: G_Loss: 1.100, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.614, D_sup_loss: 0.114, D_sup_acc: 97.27 Train acc: 97.032 Test acc: 97.220 \n",
      "step: 5776 | Train: G_Loss: 1.196, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.609, D_sup_loss: 0.123, D_sup_acc: 97.26 Train acc: 97.025 Test acc: 97.190 \n",
      "step: 5777 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.583, D_sup_loss: 0.123, D_sup_acc: 97.23 Train acc: 97.075 Test acc: 97.210 \n",
      "step: 5778 | Train: G_Loss: 1.300, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.602, D_sup_loss: 0.118, D_sup_acc: 97.25 Train acc: 97.078 Test acc: 97.210 \n",
      "step: 5779 | Train: G_Loss: 1.151, D_unsup_loss_real: 0.544, D_unsup_loss_fake: 0.648, D_sup_loss: 0.116, D_sup_acc: 97.25 Train acc: 97.103 Test acc: 97.200 \n",
      "step: 5780 | Train: G_Loss: 1.188, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.593, D_sup_loss: 0.117, D_sup_acc: 97.24 Train acc: 97.110 Test acc: 97.210 \n",
      "step: 5781 | Train: G_Loss: 1.220, D_unsup_loss_real: 0.545, D_unsup_loss_fake: 0.632, D_sup_loss: 0.115, D_sup_acc: 97.25 Train acc: 97.073 Test acc: 97.170 \n",
      "step: 5782 | Train: G_Loss: 1.197, D_unsup_loss_real: 0.657, D_unsup_loss_fake: 0.565, D_sup_loss: 0.118, D_sup_acc: 97.21 Train acc: 96.865 Test acc: 96.950 \n",
      "step: 5783 | Train: G_Loss: 1.207, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.625, D_sup_loss: 0.135, D_sup_acc: 96.99 Train acc: 96.947 Test acc: 97.040 \n",
      "step: 5784 | Train: G_Loss: 1.146, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.565, D_sup_loss: 0.130, D_sup_acc: 97.08 Train acc: 96.908 Test acc: 96.990 \n",
      "step: 5785 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.519, D_unsup_loss_fake: 0.543, D_sup_loss: 0.131, D_sup_acc: 97.03 Train acc: 96.987 Test acc: 97.020 \n",
      "step: 5786 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.592, D_sup_loss: 0.120, D_sup_acc: 97.06 Train acc: 96.945 Test acc: 97.100 \n",
      "step: 5787 | Train: G_Loss: 1.249, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.600, D_sup_loss: 0.126, D_sup_acc: 97.14 Train acc: 96.927 Test acc: 97.050 \n",
      "step: 5788 | Train: G_Loss: 1.114, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.610, D_sup_loss: 0.124, D_sup_acc: 97.09 Train acc: 96.735 Test acc: 96.980 \n",
      "step: 5789 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.603, D_sup_loss: 0.135, D_sup_acc: 97.02 Train acc: 96.878 Test acc: 97.130 \n",
      "step: 5790 | Train: G_Loss: 1.266, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.568, D_sup_loss: 0.125, D_sup_acc: 97.17 Train acc: 96.823 Test acc: 97.120 \n",
      "step: 5791 | Train: G_Loss: 1.117, D_unsup_loss_real: 0.516, D_unsup_loss_fake: 0.603, D_sup_loss: 0.124, D_sup_acc: 97.16 Train acc: 96.860 Test acc: 97.220 \n",
      "step: 5792 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.535, D_sup_loss: 0.125, D_sup_acc: 97.26 Train acc: 96.757 Test acc: 97.150 \n",
      "step: 5793 | Train: G_Loss: 1.451, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.641, D_sup_loss: 0.129, D_sup_acc: 97.19 Train acc: 97.018 Test acc: 97.180 \n",
      "step: 5794 | Train: G_Loss: 1.183, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.552, D_sup_loss: 0.117, D_sup_acc: 97.22 Train acc: 96.962 Test acc: 97.190 \n",
      "step: 5795 | Train: G_Loss: 1.196, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.455, D_sup_loss: 0.121, D_sup_acc: 97.23 Train acc: 97.158 Test acc: 97.150 \n",
      "step: 5796 | Train: G_Loss: 1.222, D_unsup_loss_real: 0.530, D_unsup_loss_fake: 0.683, D_sup_loss: 0.112, D_sup_acc: 97.19 Train acc: 96.955 Test acc: 97.140 \n",
      "step: 5797 | Train: G_Loss: 1.226, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.571, D_sup_loss: 0.120, D_sup_acc: 97.18 Train acc: 96.930 Test acc: 97.200 \n",
      "step: 5798 | Train: G_Loss: 1.253, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.576, D_sup_loss: 0.120, D_sup_acc: 97.24 Train acc: 96.820 Test acc: 97.030 \n",
      "step: 5799 | Train: G_Loss: 1.120, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.616, D_sup_loss: 0.130, D_sup_acc: 97.07 Train acc: 96.805 Test acc: 97.110 \n",
      "step: 5800 | Train: G_Loss: 1.123, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.536, D_sup_loss: 0.126, D_sup_acc: 97.15 Train acc: 96.880 Test acc: 97.120 \n",
      "Train Classifier Accuracy: 96.880%\n",
      "\n",
      "Test Classifier Accuracy: 97.120%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_5800.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_5800.h5\n",
      "step: 5801 | Train: G_Loss: 1.071, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.601, D_sup_loss: 0.124, D_sup_acc: 97.16 Train acc: 97.017 Test acc: 97.280 \n",
      "step: 5802 | Train: G_Loss: 1.077, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.587, D_sup_loss: 0.120, D_sup_acc: 97.31 Train acc: 97.015 Test acc: 97.250 \n",
      "step: 5803 | Train: G_Loss: 1.186, D_unsup_loss_real: 0.517, D_unsup_loss_fake: 0.554, D_sup_loss: 0.120, D_sup_acc: 97.28 Train acc: 97.037 Test acc: 97.150 \n",
      "step: 5804 | Train: G_Loss: 1.084, D_unsup_loss_real: 0.648, D_unsup_loss_fake: 0.598, D_sup_loss: 0.111, D_sup_acc: 97.19 Train acc: 96.690 Test acc: 96.940 \n",
      "step: 5805 | Train: G_Loss: 1.193, D_unsup_loss_real: 0.647, D_unsup_loss_fake: 0.569, D_sup_loss: 0.145, D_sup_acc: 96.98 Train acc: 97.000 Test acc: 97.170 \n",
      "step: 5806 | Train: G_Loss: 1.099, D_unsup_loss_real: 0.530, D_unsup_loss_fake: 0.523, D_sup_loss: 0.117, D_sup_acc: 97.21 Train acc: 96.857 Test acc: 97.030 \n",
      "step: 5807 | Train: G_Loss: 1.091, D_unsup_loss_real: 0.525, D_unsup_loss_fake: 0.576, D_sup_loss: 0.125, D_sup_acc: 97.07 Train acc: 97.050 Test acc: 97.210 \n",
      "step: 5808 | Train: G_Loss: 1.252, D_unsup_loss_real: 0.537, D_unsup_loss_fake: 0.612, D_sup_loss: 0.116, D_sup_acc: 97.25 Train acc: 97.100 Test acc: 97.210 \n",
      "step: 5809 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.647, D_sup_loss: 0.111, D_sup_acc: 97.25 Train acc: 96.870 Test acc: 97.150 \n",
      "step: 5810 | Train: G_Loss: 1.257, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.640, D_sup_loss: 0.131, D_sup_acc: 97.19 Train acc: 96.963 Test acc: 97.100 \n",
      "step: 5811 | Train: G_Loss: 1.103, D_unsup_loss_real: 0.532, D_unsup_loss_fake: 0.589, D_sup_loss: 0.123, D_sup_acc: 97.14 Train acc: 96.990 Test acc: 97.070 \n",
      "step: 5812 | Train: G_Loss: 1.130, D_unsup_loss_real: 0.522, D_unsup_loss_fake: 0.577, D_sup_loss: 0.122, D_sup_acc: 97.11 Train acc: 96.870 Test acc: 96.990 \n",
      "step: 5813 | Train: G_Loss: 1.173, D_unsup_loss_real: 0.515, D_unsup_loss_fake: 0.617, D_sup_loss: 0.130, D_sup_acc: 97.03 Train acc: 96.877 Test acc: 97.010 \n",
      "step: 5814 | Train: G_Loss: 1.141, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.568, D_sup_loss: 0.131, D_sup_acc: 97.05 Train acc: 96.998 Test acc: 97.160 \n",
      "step: 5815 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.589, D_sup_loss: 0.122, D_sup_acc: 97.20 Train acc: 96.910 Test acc: 97.140 \n",
      "step: 5816 | Train: G_Loss: 1.172, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.579, D_sup_loss: 0.120, D_sup_acc: 97.18 Train acc: 96.943 Test acc: 97.140 \n",
      "step: 5817 | Train: G_Loss: 1.154, D_unsup_loss_real: 0.544, D_unsup_loss_fake: 0.565, D_sup_loss: 0.122, D_sup_acc: 97.18 Train acc: 96.697 Test acc: 96.820 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5818 | Train: G_Loss: 1.189, D_unsup_loss_real: 0.512, D_unsup_loss_fake: 0.608, D_sup_loss: 0.134, D_sup_acc: 96.86 Train acc: 96.942 Test acc: 97.120 \n",
      "step: 5819 | Train: G_Loss: 1.182, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.578, D_sup_loss: 0.119, D_sup_acc: 97.16 Train acc: 96.842 Test acc: 97.060 \n",
      "step: 5820 | Train: G_Loss: 1.191, D_unsup_loss_real: 0.646, D_unsup_loss_fake: 0.561, D_sup_loss: 0.124, D_sup_acc: 97.10 Train acc: 96.603 Test acc: 96.890 \n",
      "step: 5821 | Train: G_Loss: 1.264, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.578, D_sup_loss: 0.137, D_sup_acc: 96.93 Train acc: 96.987 Test acc: 97.200 \n",
      "step: 5822 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.583, D_sup_loss: 0.116, D_sup_acc: 97.24 Train acc: 96.717 Test acc: 97.010 \n",
      "step: 5823 | Train: G_Loss: 1.243, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.554, D_sup_loss: 0.133, D_sup_acc: 97.05 Train acc: 96.812 Test acc: 97.170 \n",
      "step: 5824 | Train: G_Loss: 1.213, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.549, D_sup_loss: 0.126, D_sup_acc: 97.21 Train acc: 96.727 Test acc: 96.880 \n",
      "step: 5825 | Train: G_Loss: 1.154, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.634, D_sup_loss: 0.133, D_sup_acc: 96.92 Train acc: 96.860 Test acc: 97.140 \n",
      "step: 5826 | Train: G_Loss: 1.139, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.597, D_sup_loss: 0.127, D_sup_acc: 97.18 Train acc: 97.013 Test acc: 97.070 \n",
      "step: 5827 | Train: G_Loss: 1.006, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.635, D_sup_loss: 0.118, D_sup_acc: 97.11 Train acc: 96.975 Test acc: 97.160 \n",
      "step: 5828 | Train: G_Loss: 1.116, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.630, D_sup_loss: 0.123, D_sup_acc: 97.20 Train acc: 96.953 Test acc: 97.140 \n",
      "step: 5829 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.591, D_sup_loss: 0.125, D_sup_acc: 97.18 Train acc: 96.825 Test acc: 97.110 \n",
      "step: 5830 | Train: G_Loss: 1.151, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.632, D_sup_loss: 0.132, D_sup_acc: 97.15 Train acc: 96.997 Test acc: 97.150 \n",
      "step: 5831 | Train: G_Loss: 1.228, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.584, D_sup_loss: 0.119, D_sup_acc: 97.19 Train acc: 97.015 Test acc: 97.220 \n",
      "step: 5832 | Train: G_Loss: 1.115, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.654, D_sup_loss: 0.119, D_sup_acc: 97.26 Train acc: 96.760 Test acc: 97.100 \n",
      "step: 5833 | Train: G_Loss: 1.222, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.550, D_sup_loss: 0.138, D_sup_acc: 97.14 Train acc: 97.008 Test acc: 97.290 \n",
      "step: 5834 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.538, D_unsup_loss_fake: 0.586, D_sup_loss: 0.118, D_sup_acc: 97.32 Train acc: 96.838 Test acc: 97.130 \n",
      "step: 5835 | Train: G_Loss: 1.177, D_unsup_loss_real: 0.639, D_unsup_loss_fake: 0.560, D_sup_loss: 0.128, D_sup_acc: 97.17 Train acc: 96.657 Test acc: 96.860 \n",
      "step: 5836 | Train: G_Loss: 1.107, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.586, D_sup_loss: 0.136, D_sup_acc: 96.90 Train acc: 96.868 Test acc: 97.160 \n",
      "step: 5837 | Train: G_Loss: 1.239, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.543, D_sup_loss: 0.123, D_sup_acc: 97.20 Train acc: 96.900 Test acc: 97.200 \n",
      "step: 5838 | Train: G_Loss: 1.156, D_unsup_loss_real: 0.639, D_unsup_loss_fake: 0.583, D_sup_loss: 0.121, D_sup_acc: 97.24 Train acc: 96.952 Test acc: 97.320 \n",
      "step: 5839 | Train: G_Loss: 1.177, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.586, D_sup_loss: 0.117, D_sup_acc: 97.35 Train acc: 96.902 Test acc: 97.170 \n",
      "step: 5840 | Train: G_Loss: 1.103, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.651, D_sup_loss: 0.123, D_sup_acc: 97.21 Train acc: 96.815 Test acc: 97.130 \n",
      "step: 5841 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.502, D_unsup_loss_fake: 0.647, D_sup_loss: 0.128, D_sup_acc: 97.17 Train acc: 96.807 Test acc: 97.060 \n",
      "step: 5842 | Train: G_Loss: 1.153, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.531, D_sup_loss: 0.130, D_sup_acc: 97.10 Train acc: 96.970 Test acc: 97.260 \n",
      "step: 5843 | Train: G_Loss: 1.155, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.594, D_sup_loss: 0.118, D_sup_acc: 97.29 Train acc: 96.837 Test acc: 97.140 \n",
      "step: 5844 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.608, D_sup_loss: 0.134, D_sup_acc: 97.18 Train acc: 96.743 Test acc: 97.100 \n",
      "step: 5845 | Train: G_Loss: 1.201, D_unsup_loss_real: 0.530, D_unsup_loss_fake: 0.533, D_sup_loss: 0.133, D_sup_acc: 97.14 Train acc: 97.022 Test acc: 97.250 \n",
      "step: 5846 | Train: G_Loss: 1.191, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.588, D_sup_loss: 0.116, D_sup_acc: 97.28 Train acc: 96.935 Test acc: 97.220 \n",
      "step: 5847 | Train: G_Loss: 1.187, D_unsup_loss_real: 0.520, D_unsup_loss_fake: 0.561, D_sup_loss: 0.122, D_sup_acc: 97.26 Train acc: 96.513 Test acc: 96.760 \n",
      "step: 5848 | Train: G_Loss: 1.128, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.603, D_sup_loss: 0.144, D_sup_acc: 96.80 Train acc: 96.795 Test acc: 96.980 \n",
      "step: 5849 | Train: G_Loss: 1.218, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.483, D_sup_loss: 0.133, D_sup_acc: 97.02 Train acc: 96.970 Test acc: 97.130 \n",
      "step: 5850 | Train: G_Loss: 1.268, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.596, D_sup_loss: 0.122, D_sup_acc: 97.17 Train acc: 96.923 Test acc: 97.120 \n",
      "step: 5851 | Train: G_Loss: 1.232, D_unsup_loss_real: 0.536, D_unsup_loss_fake: 0.608, D_sup_loss: 0.121, D_sup_acc: 97.16 Train acc: 97.013 Test acc: 97.170 \n",
      "step: 5852 | Train: G_Loss: 1.187, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.595, D_sup_loss: 0.119, D_sup_acc: 97.21 Train acc: 97.062 Test acc: 97.240 \n",
      "step: 5853 | Train: G_Loss: 1.241, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.602, D_sup_loss: 0.120, D_sup_acc: 97.27 Train acc: 96.992 Test acc: 97.130 \n",
      "step: 5854 | Train: G_Loss: 1.129, D_unsup_loss_real: 0.525, D_unsup_loss_fake: 0.568, D_sup_loss: 0.125, D_sup_acc: 97.17 Train acc: 96.955 Test acc: 97.100 \n",
      "step: 5855 | Train: G_Loss: 1.163, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.569, D_sup_loss: 0.121, D_sup_acc: 97.14 Train acc: 97.082 Test acc: 97.200 \n",
      "step: 5856 | Train: G_Loss: 1.177, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.574, D_sup_loss: 0.114, D_sup_acc: 97.24 Train acc: 96.987 Test acc: 97.190 \n",
      "step: 5857 | Train: G_Loss: 1.084, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.589, D_sup_loss: 0.119, D_sup_acc: 97.23 Train acc: 97.013 Test acc: 97.230 \n",
      "step: 5858 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.578, D_sup_loss: 0.119, D_sup_acc: 97.27 Train acc: 96.958 Test acc: 97.190 \n",
      "step: 5859 | Train: G_Loss: 1.227, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.605, D_sup_loss: 0.127, D_sup_acc: 97.23 Train acc: 96.988 Test acc: 97.270 \n",
      "step: 5860 | Train: G_Loss: 1.188, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.553, D_sup_loss: 0.121, D_sup_acc: 97.30 Train acc: 96.978 Test acc: 97.140 \n",
      "step: 5861 | Train: G_Loss: 1.202, D_unsup_loss_real: 0.526, D_unsup_loss_fake: 0.617, D_sup_loss: 0.122, D_sup_acc: 97.18 Train acc: 96.875 Test acc: 97.040 \n",
      "step: 5862 | Train: G_Loss: 1.181, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.577, D_sup_loss: 0.128, D_sup_acc: 97.08 Train acc: 97.042 Test acc: 97.250 \n",
      "step: 5863 | Train: G_Loss: 1.257, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.577, D_sup_loss: 0.118, D_sup_acc: 97.28 Train acc: 96.828 Test acc: 97.050 \n",
      "step: 5864 | Train: G_Loss: 1.154, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.587, D_sup_loss: 0.129, D_sup_acc: 97.09 Train acc: 96.830 Test acc: 97.030 \n",
      "step: 5865 | Train: G_Loss: 1.108, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.567, D_sup_loss: 0.129, D_sup_acc: 97.07 Train acc: 96.850 Test acc: 97.050 \n",
      "step: 5866 | Train: G_Loss: 1.202, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.629, D_sup_loss: 0.126, D_sup_acc: 97.09 Train acc: 96.785 Test acc: 96.990 \n",
      "step: 5867 | Train: G_Loss: 1.240, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.537, D_sup_loss: 0.130, D_sup_acc: 97.03 Train acc: 96.873 Test acc: 97.170 \n",
      "step: 5868 | Train: G_Loss: 1.143, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.564, D_sup_loss: 0.123, D_sup_acc: 97.21 Train acc: 96.865 Test acc: 97.050 \n",
      "step: 5869 | Train: G_Loss: 1.207, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.589, D_sup_loss: 0.125, D_sup_acc: 97.09 Train acc: 96.477 Test acc: 96.720 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5870 | Train: G_Loss: 1.227, D_unsup_loss_real: 0.657, D_unsup_loss_fake: 0.578, D_sup_loss: 0.127, D_sup_acc: 96.76 Train acc: 96.433 Test acc: 96.660 \n",
      "step: 5871 | Train: G_Loss: 1.158, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.556, D_sup_loss: 0.160, D_sup_acc: 96.70 Train acc: 97.082 Test acc: 97.290 \n",
      "step: 5872 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.620, D_sup_loss: 0.113, D_sup_acc: 97.32 Train acc: 97.152 Test acc: 97.330 \n",
      "step: 5873 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.495, D_unsup_loss_fake: 0.630, D_sup_loss: 0.111, D_sup_acc: 97.36 Train acc: 97.088 Test acc: 97.330 \n",
      "step: 5874 | Train: G_Loss: 1.168, D_unsup_loss_real: 0.644, D_unsup_loss_fake: 0.571, D_sup_loss: 0.112, D_sup_acc: 97.36 Train acc: 96.978 Test acc: 97.290 \n",
      "step: 5875 | Train: G_Loss: 1.182, D_unsup_loss_real: 0.534, D_unsup_loss_fake: 0.611, D_sup_loss: 0.119, D_sup_acc: 97.32 Train acc: 97.073 Test acc: 97.330 \n",
      "step: 5876 | Train: G_Loss: 1.066, D_unsup_loss_real: 0.508, D_unsup_loss_fake: 0.557, D_sup_loss: 0.113, D_sup_acc: 97.36 Train acc: 96.910 Test acc: 97.270 \n",
      "step: 5877 | Train: G_Loss: 1.257, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.563, D_sup_loss: 0.124, D_sup_acc: 97.30 Train acc: 96.980 Test acc: 97.320 \n",
      "step: 5878 | Train: G_Loss: 1.222, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.570, D_sup_loss: 0.121, D_sup_acc: 97.35 Train acc: 97.048 Test acc: 97.320 \n",
      "step: 5879 | Train: G_Loss: 1.217, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.614, D_sup_loss: 0.118, D_sup_acc: 97.35 Train acc: 97.093 Test acc: 97.320 \n",
      "step: 5880 | Train: G_Loss: 1.177, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.511, D_sup_loss: 0.117, D_sup_acc: 97.35 Train acc: 96.872 Test acc: 97.120 \n",
      "step: 5881 | Train: G_Loss: 1.236, D_unsup_loss_real: 0.521, D_unsup_loss_fake: 0.598, D_sup_loss: 0.125, D_sup_acc: 97.16 Train acc: 96.975 Test acc: 97.220 \n",
      "step: 5882 | Train: G_Loss: 1.309, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.565, D_sup_loss: 0.119, D_sup_acc: 97.26 Train acc: 97.050 Test acc: 97.240 \n",
      "step: 5883 | Train: G_Loss: 1.333, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.630, D_sup_loss: 0.114, D_sup_acc: 97.27 Train acc: 96.852 Test acc: 97.100 \n",
      "step: 5884 | Train: G_Loss: 1.181, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.544, D_sup_loss: 0.129, D_sup_acc: 97.14 Train acc: 96.842 Test acc: 97.110 \n",
      "step: 5885 | Train: G_Loss: 1.312, D_unsup_loss_real: 0.653, D_unsup_loss_fake: 0.538, D_sup_loss: 0.133, D_sup_acc: 97.15 Train acc: 97.122 Test acc: 97.290 \n",
      "step: 5886 | Train: G_Loss: 1.212, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.627, D_sup_loss: 0.112, D_sup_acc: 97.32 Train acc: 97.090 Test acc: 97.190 \n",
      "step: 5887 | Train: G_Loss: 1.201, D_unsup_loss_real: 0.521, D_unsup_loss_fake: 0.509, D_sup_loss: 0.116, D_sup_acc: 97.23 Train acc: 97.060 Test acc: 97.160 \n",
      "step: 5888 | Train: G_Loss: 1.161, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.669, D_sup_loss: 0.116, D_sup_acc: 97.20 Train acc: 96.865 Test acc: 97.070 \n",
      "step: 5889 | Train: G_Loss: 1.200, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.587, D_sup_loss: 0.130, D_sup_acc: 97.11 Train acc: 97.100 Test acc: 97.240 \n",
      "step: 5890 | Train: G_Loss: 1.164, D_unsup_loss_real: 0.498, D_unsup_loss_fake: 0.561, D_sup_loss: 0.113, D_sup_acc: 97.27 Train acc: 97.073 Test acc: 97.250 \n",
      "step: 5891 | Train: G_Loss: 1.219, D_unsup_loss_real: 0.533, D_unsup_loss_fake: 0.551, D_sup_loss: 0.116, D_sup_acc: 97.28 Train acc: 97.092 Test acc: 97.280 \n",
      "step: 5892 | Train: G_Loss: 1.222, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.590, D_sup_loss: 0.115, D_sup_acc: 97.31 Train acc: 97.077 Test acc: 97.350 \n",
      "step: 5893 | Train: G_Loss: 1.165, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.576, D_sup_loss: 0.115, D_sup_acc: 97.38 Train acc: 97.113 Test acc: 97.270 \n",
      "step: 5894 | Train: G_Loss: 1.211, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.539, D_sup_loss: 0.116, D_sup_acc: 97.30 Train acc: 96.995 Test acc: 97.130 \n",
      "step: 5895 | Train: G_Loss: 1.155, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.591, D_sup_loss: 0.122, D_sup_acc: 97.17 Train acc: 96.993 Test acc: 97.140 \n",
      "step: 5896 | Train: G_Loss: 1.222, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.652, D_sup_loss: 0.119, D_sup_acc: 97.18 Train acc: 96.748 Test acc: 96.920 \n",
      "step: 5897 | Train: G_Loss: 1.157, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.589, D_sup_loss: 0.134, D_sup_acc: 96.96 Train acc: 96.992 Test acc: 97.220 \n",
      "step: 5898 | Train: G_Loss: 1.235, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.549, D_sup_loss: 0.116, D_sup_acc: 97.26 Train acc: 96.965 Test acc: 97.160 \n",
      "step: 5899 | Train: G_Loss: 1.149, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.719, D_sup_loss: 0.119, D_sup_acc: 97.20 Train acc: 96.698 Test acc: 96.840 \n",
      "step: 5900 | Train: G_Loss: 1.191, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.571, D_sup_loss: 0.137, D_sup_acc: 96.88 Train acc: 96.832 Test acc: 96.980 \n",
      "Train Classifier Accuracy: 96.832%\n",
      "\n",
      "Test Classifier Accuracy: 96.980%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_5900.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_5900.h5\n",
      "step: 5901 | Train: G_Loss: 1.244, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.668, D_sup_loss: 0.132, D_sup_acc: 97.02 Train acc: 96.853 Test acc: 97.000 \n",
      "step: 5902 | Train: G_Loss: 1.274, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.544, D_sup_loss: 0.128, D_sup_acc: 97.04 Train acc: 96.710 Test acc: 96.770 \n",
      "step: 5903 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.674, D_unsup_loss_fake: 0.564, D_sup_loss: 0.134, D_sup_acc: 96.81 Train acc: 96.905 Test acc: 97.010 \n",
      "step: 5904 | Train: G_Loss: 1.110, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.565, D_sup_loss: 0.132, D_sup_acc: 97.05 Train acc: 97.085 Test acc: 97.140 \n",
      "step: 5905 | Train: G_Loss: 1.194, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.567, D_sup_loss: 0.115, D_sup_acc: 97.18 Train acc: 97.037 Test acc: 97.160 \n",
      "step: 5906 | Train: G_Loss: 1.050, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.560, D_sup_loss: 0.121, D_sup_acc: 97.20 Train acc: 96.797 Test acc: 96.880 \n",
      "step: 5907 | Train: G_Loss: 1.162, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.549, D_sup_loss: 0.131, D_sup_acc: 96.92 Train acc: 96.882 Test acc: 96.910 \n",
      "step: 5908 | Train: G_Loss: 1.240, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.609, D_sup_loss: 0.129, D_sup_acc: 96.95 Train acc: 96.863 Test acc: 96.880 \n",
      "step: 5909 | Train: G_Loss: 1.330, D_unsup_loss_real: 0.512, D_unsup_loss_fake: 0.541, D_sup_loss: 0.131, D_sup_acc: 96.92 Train acc: 96.967 Test acc: 97.040 \n",
      "step: 5910 | Train: G_Loss: 1.135, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.575, D_sup_loss: 0.124, D_sup_acc: 97.08 Train acc: 97.012 Test acc: 97.040 \n",
      "step: 5911 | Train: G_Loss: 1.125, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.607, D_sup_loss: 0.119, D_sup_acc: 97.08 Train acc: 96.768 Test acc: 96.850 \n",
      "step: 5912 | Train: G_Loss: 1.162, D_unsup_loss_real: 0.537, D_unsup_loss_fake: 0.574, D_sup_loss: 0.139, D_sup_acc: 96.89 Train acc: 97.087 Test acc: 97.180 \n",
      "step: 5913 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.544, D_unsup_loss_fake: 0.545, D_sup_loss: 0.118, D_sup_acc: 97.22 Train acc: 96.935 Test acc: 97.020 \n",
      "step: 5914 | Train: G_Loss: 1.247, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.504, D_sup_loss: 0.126, D_sup_acc: 97.06 Train acc: 96.853 Test acc: 96.970 \n",
      "step: 5915 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.535, D_sup_loss: 0.128, D_sup_acc: 97.01 Train acc: 96.648 Test acc: 96.800 \n",
      "step: 5916 | Train: G_Loss: 1.220, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.608, D_sup_loss: 0.141, D_sup_acc: 96.84 Train acc: 96.862 Test acc: 96.950 \n",
      "step: 5917 | Train: G_Loss: 1.189, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.587, D_sup_loss: 0.127, D_sup_acc: 96.99 Train acc: 97.015 Test acc: 97.090 \n",
      "step: 5918 | Train: G_Loss: 1.243, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.549, D_sup_loss: 0.116, D_sup_acc: 97.13 Train acc: 96.918 Test acc: 97.150 \n",
      "step: 5919 | Train: G_Loss: 1.068, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.609, D_sup_loss: 0.120, D_sup_acc: 97.19 Train acc: 97.038 Test acc: 97.120 \n",
      "step: 5920 | Train: G_Loss: 1.236, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.609, D_sup_loss: 0.115, D_sup_acc: 97.16 Train acc: 97.037 Test acc: 97.110 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5921 | Train: G_Loss: 1.156, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.628, D_sup_loss: 0.118, D_sup_acc: 97.15 Train acc: 96.935 Test acc: 97.090 \n",
      "step: 5922 | Train: G_Loss: 1.212, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.524, D_sup_loss: 0.123, D_sup_acc: 97.13 Train acc: 96.982 Test acc: 97.180 \n",
      "step: 5923 | Train: G_Loss: 1.228, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.692, D_sup_loss: 0.121, D_sup_acc: 97.22 Train acc: 96.865 Test acc: 97.160 \n",
      "step: 5924 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.660, D_unsup_loss_fake: 0.637, D_sup_loss: 0.122, D_sup_acc: 97.20 Train acc: 96.798 Test acc: 97.090 \n",
      "step: 5925 | Train: G_Loss: 1.126, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.549, D_sup_loss: 0.128, D_sup_acc: 97.13 Train acc: 96.958 Test acc: 97.190 \n",
      "step: 5926 | Train: G_Loss: 1.082, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.677, D_sup_loss: 0.121, D_sup_acc: 97.23 Train acc: 96.882 Test acc: 97.120 \n",
      "step: 5927 | Train: G_Loss: 1.162, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.537, D_sup_loss: 0.132, D_sup_acc: 97.16 Train acc: 96.835 Test acc: 97.060 \n",
      "step: 5928 | Train: G_Loss: 1.096, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.641, D_sup_loss: 0.133, D_sup_acc: 97.10 Train acc: 96.933 Test acc: 97.120 \n",
      "step: 5929 | Train: G_Loss: 1.207, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.579, D_sup_loss: 0.126, D_sup_acc: 97.16 Train acc: 96.987 Test acc: 97.150 \n",
      "step: 5930 | Train: G_Loss: 1.150, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.637, D_sup_loss: 0.129, D_sup_acc: 97.19 Train acc: 97.038 Test acc: 97.140 \n",
      "step: 5931 | Train: G_Loss: 1.143, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.614, D_sup_loss: 0.122, D_sup_acc: 97.18 Train acc: 97.083 Test acc: 97.290 \n",
      "step: 5932 | Train: G_Loss: 1.237, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.597, D_sup_loss: 0.120, D_sup_acc: 97.32 Train acc: 97.047 Test acc: 97.180 \n",
      "step: 5933 | Train: G_Loss: 1.249, D_unsup_loss_real: 0.521, D_unsup_loss_fake: 0.618, D_sup_loss: 0.119, D_sup_acc: 97.22 Train acc: 96.638 Test acc: 96.930 \n",
      "step: 5934 | Train: G_Loss: 1.243, D_unsup_loss_real: 0.655, D_unsup_loss_fake: 0.587, D_sup_loss: 0.149, D_sup_acc: 96.97 Train acc: 96.980 Test acc: 97.180 \n",
      "step: 5935 | Train: G_Loss: 1.257, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.580, D_sup_loss: 0.125, D_sup_acc: 97.22 Train acc: 96.947 Test acc: 97.200 \n",
      "step: 5936 | Train: G_Loss: 1.049, D_unsup_loss_real: 0.631, D_unsup_loss_fake: 0.540, D_sup_loss: 0.125, D_sup_acc: 97.24 Train acc: 96.997 Test acc: 97.160 \n",
      "step: 5937 | Train: G_Loss: 1.187, D_unsup_loss_real: 0.652, D_unsup_loss_fake: 0.633, D_sup_loss: 0.120, D_sup_acc: 97.20 Train acc: 96.990 Test acc: 97.150 \n",
      "step: 5938 | Train: G_Loss: 1.238, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.572, D_sup_loss: 0.129, D_sup_acc: 97.19 Train acc: 97.107 Test acc: 97.260 \n",
      "step: 5939 | Train: G_Loss: 1.018, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.639, D_sup_loss: 0.116, D_sup_acc: 97.29 Train acc: 96.935 Test acc: 97.120 \n",
      "step: 5940 | Train: G_Loss: 1.137, D_unsup_loss_real: 0.530, D_unsup_loss_fake: 0.542, D_sup_loss: 0.129, D_sup_acc: 97.16 Train acc: 97.062 Test acc: 97.160 \n",
      "step: 5941 | Train: G_Loss: 1.127, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.629, D_sup_loss: 0.120, D_sup_acc: 97.20 Train acc: 96.930 Test acc: 97.190 \n",
      "step: 5942 | Train: G_Loss: 1.164, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.592, D_sup_loss: 0.122, D_sup_acc: 97.23 Train acc: 97.022 Test acc: 97.250 \n",
      "step: 5943 | Train: G_Loss: 1.193, D_unsup_loss_real: 0.544, D_unsup_loss_fake: 0.565, D_sup_loss: 0.122, D_sup_acc: 97.28 Train acc: 97.028 Test acc: 97.270 \n",
      "step: 5944 | Train: G_Loss: 1.241, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.578, D_sup_loss: 0.121, D_sup_acc: 97.30 Train acc: 96.862 Test acc: 97.150 \n",
      "step: 5945 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.592, D_sup_loss: 0.129, D_sup_acc: 97.19 Train acc: 96.990 Test acc: 97.310 \n",
      "step: 5946 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.649, D_sup_loss: 0.124, D_sup_acc: 97.34 Train acc: 96.927 Test acc: 97.110 \n",
      "step: 5947 | Train: G_Loss: 1.163, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.654, D_sup_loss: 0.124, D_sup_acc: 97.15 Train acc: 97.073 Test acc: 97.240 \n",
      "step: 5948 | Train: G_Loss: 1.173, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.516, D_sup_loss: 0.117, D_sup_acc: 97.27 Train acc: 97.097 Test acc: 97.240 \n",
      "step: 5949 | Train: G_Loss: 1.164, D_unsup_loss_real: 0.531, D_unsup_loss_fake: 0.613, D_sup_loss: 0.119, D_sup_acc: 97.27 Train acc: 97.045 Test acc: 97.240 \n",
      "step: 5950 | Train: G_Loss: 1.265, D_unsup_loss_real: 0.546, D_unsup_loss_fake: 0.556, D_sup_loss: 0.125, D_sup_acc: 97.27 Train acc: 97.003 Test acc: 97.140 \n",
      "step: 5951 | Train: G_Loss: 1.231, D_unsup_loss_real: 0.535, D_unsup_loss_fake: 0.605, D_sup_loss: 0.123, D_sup_acc: 97.18 Train acc: 96.957 Test acc: 97.110 \n",
      "step: 5952 | Train: G_Loss: 1.215, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.551, D_sup_loss: 0.129, D_sup_acc: 97.15 Train acc: 97.095 Test acc: 97.290 \n",
      "step: 5953 | Train: G_Loss: 1.166, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.549, D_sup_loss: 0.116, D_sup_acc: 97.32 Train acc: 97.078 Test acc: 97.300 \n",
      "step: 5954 | Train: G_Loss: 1.202, D_unsup_loss_real: 0.538, D_unsup_loss_fake: 0.574, D_sup_loss: 0.118, D_sup_acc: 97.33 Train acc: 97.068 Test acc: 97.210 \n",
      "step: 5955 | Train: G_Loss: 1.186, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.635, D_sup_loss: 0.122, D_sup_acc: 97.25 Train acc: 97.062 Test acc: 97.280 \n",
      "step: 5956 | Train: G_Loss: 1.183, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.567, D_sup_loss: 0.122, D_sup_acc: 97.31 Train acc: 97.053 Test acc: 97.230 \n",
      "step: 5957 | Train: G_Loss: 1.181, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.548, D_sup_loss: 0.121, D_sup_acc: 97.27 Train acc: 96.920 Test acc: 97.120 \n",
      "step: 5958 | Train: G_Loss: 1.094, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.569, D_sup_loss: 0.129, D_sup_acc: 97.16 Train acc: 97.005 Test acc: 97.130 \n",
      "step: 5959 | Train: G_Loss: 1.161, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.625, D_sup_loss: 0.130, D_sup_acc: 97.17 Train acc: 96.975 Test acc: 97.130 \n",
      "step: 5960 | Train: G_Loss: 1.078, D_unsup_loss_real: 0.653, D_unsup_loss_fake: 0.612, D_sup_loss: 0.128, D_sup_acc: 97.17 Train acc: 96.957 Test acc: 97.130 \n",
      "step: 5961 | Train: G_Loss: 1.292, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.503, D_sup_loss: 0.130, D_sup_acc: 97.17 Train acc: 97.010 Test acc: 97.210 \n",
      "step: 5962 | Train: G_Loss: 1.380, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.590, D_sup_loss: 0.123, D_sup_acc: 97.25 Train acc: 97.072 Test acc: 97.360 \n",
      "step: 5963 | Train: G_Loss: 1.260, D_unsup_loss_real: 0.529, D_unsup_loss_fake: 0.568, D_sup_loss: 0.116, D_sup_acc: 97.39 Train acc: 97.050 Test acc: 97.270 \n",
      "step: 5964 | Train: G_Loss: 1.181, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.560, D_sup_loss: 0.112, D_sup_acc: 97.30 Train acc: 96.970 Test acc: 97.210 \n",
      "step: 5965 | Train: G_Loss: 1.274, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.603, D_sup_loss: 0.119, D_sup_acc: 97.25 Train acc: 97.042 Test acc: 97.290 \n",
      "step: 5966 | Train: G_Loss: 1.140, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.550, D_sup_loss: 0.113, D_sup_acc: 97.32 Train acc: 97.033 Test acc: 97.270 \n",
      "step: 5967 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.494, D_unsup_loss_fake: 0.558, D_sup_loss: 0.122, D_sup_acc: 97.30 Train acc: 97.045 Test acc: 97.290 \n",
      "step: 5968 | Train: G_Loss: 1.211, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.712, D_sup_loss: 0.118, D_sup_acc: 97.32 Train acc: 96.943 Test acc: 97.170 \n",
      "step: 5969 | Train: G_Loss: 1.211, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.618, D_sup_loss: 0.121, D_sup_acc: 97.21 Train acc: 97.043 Test acc: 97.300 \n",
      "step: 5970 | Train: G_Loss: 1.374, D_unsup_loss_real: 0.517, D_unsup_loss_fake: 0.623, D_sup_loss: 0.116, D_sup_acc: 97.33 Train acc: 97.020 Test acc: 97.310 \n",
      "step: 5971 | Train: G_Loss: 1.318, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.534, D_sup_loss: 0.117, D_sup_acc: 97.34 Train acc: 96.943 Test acc: 97.170 \n",
      "step: 5972 | Train: G_Loss: 1.273, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.626, D_sup_loss: 0.123, D_sup_acc: 97.21 Train acc: 96.952 Test acc: 97.240 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 5973 | Train: G_Loss: 1.247, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.567, D_sup_loss: 0.123, D_sup_acc: 97.27 Train acc: 97.065 Test acc: 97.250 \n",
      "step: 5974 | Train: G_Loss: 1.169, D_unsup_loss_real: 0.719, D_unsup_loss_fake: 0.623, D_sup_loss: 0.113, D_sup_acc: 97.28 Train acc: 96.775 Test acc: 97.020 \n",
      "step: 5975 | Train: G_Loss: 1.216, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.536, D_sup_loss: 0.138, D_sup_acc: 97.06 Train acc: 97.075 Test acc: 97.400 \n",
      "step: 5976 | Train: G_Loss: 1.180, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.576, D_sup_loss: 0.114, D_sup_acc: 97.43 Train acc: 97.060 Test acc: 97.410 \n",
      "step: 5977 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.533, D_unsup_loss_fake: 0.538, D_sup_loss: 0.117, D_sup_acc: 97.44 Train acc: 97.130 Test acc: 97.350 \n",
      "step: 5978 | Train: G_Loss: 1.258, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.599, D_sup_loss: 0.112, D_sup_acc: 97.38 Train acc: 97.135 Test acc: 97.430 \n",
      "step: 5979 | Train: G_Loss: 1.255, D_unsup_loss_real: 0.517, D_unsup_loss_fake: 0.590, D_sup_loss: 0.112, D_sup_acc: 97.46 Train acc: 96.988 Test acc: 97.280 \n",
      "step: 5980 | Train: G_Loss: 1.206, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.638, D_sup_loss: 0.118, D_sup_acc: 97.31 Train acc: 97.058 Test acc: 97.290 \n",
      "step: 5981 | Train: G_Loss: 1.125, D_unsup_loss_real: 0.516, D_unsup_loss_fake: 0.562, D_sup_loss: 0.113, D_sup_acc: 97.32 Train acc: 97.075 Test acc: 97.310 \n",
      "step: 5982 | Train: G_Loss: 1.258, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.620, D_sup_loss: 0.113, D_sup_acc: 97.34 Train acc: 96.963 Test acc: 97.210 \n",
      "step: 5983 | Train: G_Loss: 1.315, D_unsup_loss_real: 0.512, D_unsup_loss_fake: 0.571, D_sup_loss: 0.122, D_sup_acc: 97.25 Train acc: 96.943 Test acc: 97.240 \n",
      "step: 5984 | Train: G_Loss: 1.233, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.548, D_sup_loss: 0.124, D_sup_acc: 97.27 Train acc: 96.940 Test acc: 97.220 \n",
      "step: 5985 | Train: G_Loss: 1.161, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.640, D_sup_loss: 0.128, D_sup_acc: 97.26 Train acc: 97.038 Test acc: 97.170 \n",
      "step: 5986 | Train: G_Loss: 1.126, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.571, D_sup_loss: 0.117, D_sup_acc: 97.21 Train acc: 96.960 Test acc: 97.210 \n",
      "step: 5987 | Train: G_Loss: 1.224, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.527, D_sup_loss: 0.126, D_sup_acc: 97.25 Train acc: 97.077 Test acc: 97.210 \n",
      "step: 5988 | Train: G_Loss: 1.269, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.579, D_sup_loss: 0.116, D_sup_acc: 97.25 Train acc: 97.000 Test acc: 97.220 \n",
      "step: 5989 | Train: G_Loss: 1.207, D_unsup_loss_real: 0.648, D_unsup_loss_fake: 0.556, D_sup_loss: 0.118, D_sup_acc: 97.26 Train acc: 96.887 Test acc: 97.100 \n",
      "step: 5990 | Train: G_Loss: 1.267, D_unsup_loss_real: 0.497, D_unsup_loss_fake: 0.611, D_sup_loss: 0.124, D_sup_acc: 97.14 Train acc: 97.030 Test acc: 97.180 \n",
      "step: 5991 | Train: G_Loss: 1.146, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.537, D_sup_loss: 0.118, D_sup_acc: 97.22 Train acc: 96.935 Test acc: 97.060 \n",
      "step: 5992 | Train: G_Loss: 1.169, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.587, D_sup_loss: 0.126, D_sup_acc: 97.10 Train acc: 96.958 Test acc: 97.200 \n",
      "step: 5993 | Train: G_Loss: 1.210, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.650, D_sup_loss: 0.125, D_sup_acc: 97.24 Train acc: 97.002 Test acc: 97.250 \n",
      "step: 5994 | Train: G_Loss: 1.202, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.599, D_sup_loss: 0.121, D_sup_acc: 97.28 Train acc: 96.798 Test acc: 96.970 \n",
      "step: 5995 | Train: G_Loss: 1.243, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.519, D_sup_loss: 0.133, D_sup_acc: 97.01 Train acc: 97.053 Test acc: 97.120 \n",
      "step: 5996 | Train: G_Loss: 1.236, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.619, D_sup_loss: 0.114, D_sup_acc: 97.16 Train acc: 96.990 Test acc: 97.210 \n",
      "step: 5997 | Train: G_Loss: 1.393, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.697, D_sup_loss: 0.118, D_sup_acc: 97.25 Train acc: 96.755 Test acc: 97.050 \n",
      "step: 5998 | Train: G_Loss: 1.317, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.592, D_sup_loss: 0.133, D_sup_acc: 97.09 Train acc: 97.015 Test acc: 97.130 \n",
      "step: 5999 | Train: G_Loss: 1.171, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.518, D_sup_loss: 0.119, D_sup_acc: 97.17 Train acc: 96.868 Test acc: 97.100 \n",
      "step: 6000 | Train: G_Loss: 1.127, D_unsup_loss_real: 0.684, D_unsup_loss_fake: 0.601, D_sup_loss: 0.125, D_sup_acc: 97.14 Train acc: 96.882 Test acc: 97.100 \n",
      "Train Classifier Accuracy: 96.882%\n",
      "\n",
      "Test Classifier Accuracy: 97.100%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_6000.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_6000.h5\n",
      "step: 6001 | Train: G_Loss: 1.282, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.546, D_sup_loss: 0.126, D_sup_acc: 97.14 Train acc: 97.097 Test acc: 97.240 \n",
      "step: 6002 | Train: G_Loss: 1.120, D_unsup_loss_real: 0.489, D_unsup_loss_fake: 0.557, D_sup_loss: 0.110, D_sup_acc: 97.27 Train acc: 96.715 Test acc: 96.970 \n",
      "step: 6003 | Train: G_Loss: 1.192, D_unsup_loss_real: 0.550, D_unsup_loss_fake: 0.547, D_sup_loss: 0.136, D_sup_acc: 97.01 Train acc: 96.965 Test acc: 97.160 \n",
      "step: 6004 | Train: G_Loss: 1.191, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.676, D_sup_loss: 0.121, D_sup_acc: 97.20 Train acc: 97.132 Test acc: 97.320 \n",
      "step: 6005 | Train: G_Loss: 1.176, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.594, D_sup_loss: 0.116, D_sup_acc: 97.35 Train acc: 97.165 Test acc: 97.290 \n",
      "step: 6006 | Train: G_Loss: 1.156, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.565, D_sup_loss: 0.109, D_sup_acc: 97.32 Train acc: 96.760 Test acc: 97.000 \n",
      "step: 6007 | Train: G_Loss: 1.207, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.614, D_sup_loss: 0.136, D_sup_acc: 97.04 Train acc: 96.898 Test acc: 97.110 \n",
      "step: 6008 | Train: G_Loss: 1.289, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.634, D_sup_loss: 0.131, D_sup_acc: 97.15 Train acc: 97.097 Test acc: 97.150 \n",
      "step: 6009 | Train: G_Loss: 1.146, D_unsup_loss_real: 0.678, D_unsup_loss_fake: 0.580, D_sup_loss: 0.116, D_sup_acc: 97.19 Train acc: 96.977 Test acc: 97.080 \n",
      "step: 6010 | Train: G_Loss: 1.150, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.607, D_sup_loss: 0.123, D_sup_acc: 97.12 Train acc: 96.872 Test acc: 97.050 \n",
      "step: 6011 | Train: G_Loss: 1.184, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.552, D_sup_loss: 0.128, D_sup_acc: 97.09 Train acc: 96.705 Test acc: 97.020 \n",
      "step: 6012 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.605, D_sup_loss: 0.132, D_sup_acc: 97.06 Train acc: 97.043 Test acc: 97.140 \n",
      "step: 6013 | Train: G_Loss: 1.254, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.608, D_sup_loss: 0.114, D_sup_acc: 97.18 Train acc: 96.922 Test acc: 97.180 \n",
      "step: 6014 | Train: G_Loss: 1.046, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.549, D_sup_loss: 0.125, D_sup_acc: 97.22 Train acc: 96.852 Test acc: 97.260 \n",
      "step: 6015 | Train: G_Loss: 1.574, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.573, D_sup_loss: 0.124, D_sup_acc: 97.29 Train acc: 96.677 Test acc: 96.940 \n",
      "step: 6016 | Train: G_Loss: 1.718, D_unsup_loss_real: 0.748, D_unsup_loss_fake: 0.433, D_sup_loss: 0.120, D_sup_acc: 96.98 Train acc: 95.185 Test acc: 95.450 \n",
      "step: 6017 | Train: G_Loss: 1.166, D_unsup_loss_real: 1.042, D_unsup_loss_fake: 0.625, D_sup_loss: 0.227, D_sup_acc: 95.51 Train acc: 96.993 Test acc: 97.210 \n",
      "step: 6018 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.468, D_unsup_loss_fake: 0.657, D_sup_loss: 0.115, D_sup_acc: 97.25 Train acc: 96.988 Test acc: 97.200 \n",
      "step: 6019 | Train: G_Loss: 0.997, D_unsup_loss_real: 0.510, D_unsup_loss_fake: 0.628, D_sup_loss: 0.114, D_sup_acc: 97.24 Train acc: 96.943 Test acc: 97.220 \n",
      "step: 6020 | Train: G_Loss: 1.006, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.613, D_sup_loss: 0.116, D_sup_acc: 97.26 Train acc: 97.058 Test acc: 97.310 \n",
      "step: 6021 | Train: G_Loss: 0.976, D_unsup_loss_real: 0.522, D_unsup_loss_fake: 0.605, D_sup_loss: 0.111, D_sup_acc: 97.34 Train acc: 96.998 Test acc: 97.350 \n",
      "step: 6022 | Train: G_Loss: 1.083, D_unsup_loss_real: 0.518, D_unsup_loss_fake: 0.655, D_sup_loss: 0.111, D_sup_acc: 97.38 Train acc: 96.957 Test acc: 97.220 \n",
      "step: 6023 | Train: G_Loss: 1.115, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.663, D_sup_loss: 0.117, D_sup_acc: 97.26 Train acc: 96.837 Test acc: 97.060 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6024 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.447, D_unsup_loss_fake: 0.646, D_sup_loss: 0.123, D_sup_acc: 97.10 Train acc: 96.767 Test acc: 97.010 \n",
      "step: 6025 | Train: G_Loss: 1.155, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.586, D_sup_loss: 0.129, D_sup_acc: 97.05 Train acc: 96.885 Test acc: 97.170 \n",
      "step: 6026 | Train: G_Loss: 1.239, D_unsup_loss_real: 0.512, D_unsup_loss_fake: 0.605, D_sup_loss: 0.122, D_sup_acc: 97.21 Train acc: 96.890 Test acc: 97.110 \n",
      "step: 6027 | Train: G_Loss: 1.158, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.589, D_sup_loss: 0.125, D_sup_acc: 97.15 Train acc: 96.900 Test acc: 97.110 \n",
      "step: 6028 | Train: G_Loss: 1.183, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.553, D_sup_loss: 0.122, D_sup_acc: 97.15 Train acc: 96.717 Test acc: 96.930 \n",
      "step: 6029 | Train: G_Loss: 1.123, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.615, D_sup_loss: 0.131, D_sup_acc: 96.97 Train acc: 96.683 Test acc: 96.880 \n",
      "step: 6030 | Train: G_Loss: 1.164, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.498, D_sup_loss: 0.134, D_sup_acc: 96.92 Train acc: 96.923 Test acc: 97.180 \n",
      "step: 6031 | Train: G_Loss: 1.252, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.582, D_sup_loss: 0.120, D_sup_acc: 97.22 Train acc: 96.967 Test acc: 97.160 \n",
      "step: 6032 | Train: G_Loss: 1.192, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.636, D_sup_loss: 0.121, D_sup_acc: 97.20 Train acc: 96.963 Test acc: 97.160 \n",
      "step: 6033 | Train: G_Loss: 1.152, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.575, D_sup_loss: 0.124, D_sup_acc: 97.20 Train acc: 96.698 Test acc: 96.970 \n",
      "step: 6034 | Train: G_Loss: 1.219, D_unsup_loss_real: 0.535, D_unsup_loss_fake: 0.603, D_sup_loss: 0.135, D_sup_acc: 97.01 Train acc: 96.948 Test acc: 97.180 \n",
      "step: 6035 | Train: G_Loss: 1.083, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.524, D_sup_loss: 0.120, D_sup_acc: 97.22 Train acc: 96.995 Test acc: 97.230 \n",
      "step: 6036 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.502, D_unsup_loss_fake: 0.572, D_sup_loss: 0.116, D_sup_acc: 97.27 Train acc: 96.848 Test acc: 97.050 \n",
      "step: 6037 | Train: G_Loss: 1.217, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.573, D_sup_loss: 0.124, D_sup_acc: 97.09 Train acc: 97.005 Test acc: 97.200 \n",
      "step: 6038 | Train: G_Loss: 1.018, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.632, D_sup_loss: 0.116, D_sup_acc: 97.24 Train acc: 96.905 Test acc: 97.220 \n",
      "step: 6039 | Train: G_Loss: 1.024, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.652, D_sup_loss: 0.126, D_sup_acc: 97.26 Train acc: 96.952 Test acc: 97.160 \n",
      "step: 6040 | Train: G_Loss: 1.182, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.648, D_sup_loss: 0.122, D_sup_acc: 97.20 Train acc: 96.982 Test acc: 97.070 \n",
      "step: 6041 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.628, D_unsup_loss_fake: 0.603, D_sup_loss: 0.120, D_sup_acc: 97.11 Train acc: 96.967 Test acc: 97.130 \n",
      "step: 6042 | Train: G_Loss: 1.253, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.604, D_sup_loss: 0.124, D_sup_acc: 97.17 Train acc: 96.980 Test acc: 97.160 \n",
      "step: 6043 | Train: G_Loss: 1.361, D_unsup_loss_real: 0.538, D_unsup_loss_fake: 0.537, D_sup_loss: 0.120, D_sup_acc: 97.20 Train acc: 97.037 Test acc: 97.260 \n",
      "step: 6044 | Train: G_Loss: 1.214, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.551, D_sup_loss: 0.116, D_sup_acc: 97.29 Train acc: 97.093 Test acc: 97.290 \n",
      "step: 6045 | Train: G_Loss: 1.200, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.638, D_sup_loss: 0.114, D_sup_acc: 97.32 Train acc: 96.982 Test acc: 97.120 \n",
      "step: 6046 | Train: G_Loss: 1.159, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.564, D_sup_loss: 0.123, D_sup_acc: 97.16 Train acc: 97.070 Test acc: 97.290 \n",
      "step: 6047 | Train: G_Loss: 1.202, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.582, D_sup_loss: 0.114, D_sup_acc: 97.32 Train acc: 97.078 Test acc: 97.370 \n",
      "step: 6048 | Train: G_Loss: 1.151, D_unsup_loss_real: 0.642, D_unsup_loss_fake: 0.652, D_sup_loss: 0.115, D_sup_acc: 97.40 Train acc: 96.883 Test acc: 97.040 \n",
      "step: 6049 | Train: G_Loss: 1.167, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.582, D_sup_loss: 0.131, D_sup_acc: 97.08 Train acc: 96.903 Test acc: 97.100 \n",
      "step: 6050 | Train: G_Loss: 1.171, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.572, D_sup_loss: 0.125, D_sup_acc: 97.14 Train acc: 96.908 Test acc: 97.060 \n",
      "step: 6051 | Train: G_Loss: 1.133, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.555, D_sup_loss: 0.126, D_sup_acc: 97.10 Train acc: 96.980 Test acc: 97.270 \n",
      "step: 6052 | Train: G_Loss: 1.194, D_unsup_loss_real: 0.674, D_unsup_loss_fake: 0.615, D_sup_loss: 0.120, D_sup_acc: 97.30 Train acc: 96.823 Test acc: 97.110 \n",
      "step: 6053 | Train: G_Loss: 1.081, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.588, D_sup_loss: 0.130, D_sup_acc: 97.15 Train acc: 96.872 Test acc: 97.030 \n",
      "step: 6054 | Train: G_Loss: 1.217, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.597, D_sup_loss: 0.129, D_sup_acc: 97.07 Train acc: 96.817 Test acc: 97.050 \n",
      "step: 6055 | Train: G_Loss: 1.188, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.561, D_sup_loss: 0.132, D_sup_acc: 97.09 Train acc: 96.903 Test acc: 97.090 \n",
      "step: 6056 | Train: G_Loss: 1.295, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.562, D_sup_loss: 0.126, D_sup_acc: 97.13 Train acc: 96.948 Test acc: 97.140 \n",
      "step: 6057 | Train: G_Loss: 1.170, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.605, D_sup_loss: 0.123, D_sup_acc: 97.18 Train acc: 97.022 Test acc: 97.250 \n",
      "step: 6058 | Train: G_Loss: 1.112, D_unsup_loss_real: 0.651, D_unsup_loss_fake: 0.574, D_sup_loss: 0.120, D_sup_acc: 97.28 Train acc: 97.042 Test acc: 97.210 \n",
      "step: 6059 | Train: G_Loss: 1.160, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.537, D_sup_loss: 0.123, D_sup_acc: 97.25 Train acc: 97.113 Test acc: 97.370 \n",
      "step: 6060 | Train: G_Loss: 1.187, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.618, D_sup_loss: 0.119, D_sup_acc: 97.40 Train acc: 97.050 Test acc: 97.270 \n",
      "step: 6061 | Train: G_Loss: 1.160, D_unsup_loss_real: 0.529, D_unsup_loss_fake: 0.570, D_sup_loss: 0.123, D_sup_acc: 97.30 Train acc: 97.175 Test acc: 97.350 \n",
      "step: 6062 | Train: G_Loss: 1.143, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.595, D_sup_loss: 0.112, D_sup_acc: 97.38 Train acc: 96.932 Test acc: 97.140 \n",
      "step: 6063 | Train: G_Loss: 1.162, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.571, D_sup_loss: 0.130, D_sup_acc: 97.18 Train acc: 96.997 Test acc: 97.270 \n",
      "step: 6064 | Train: G_Loss: 1.250, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.620, D_sup_loss: 0.124, D_sup_acc: 97.30 Train acc: 97.093 Test acc: 97.360 \n",
      "step: 6065 | Train: G_Loss: 1.212, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.565, D_sup_loss: 0.117, D_sup_acc: 97.39 Train acc: 96.970 Test acc: 97.220 \n",
      "step: 6066 | Train: G_Loss: 1.185, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.570, D_sup_loss: 0.125, D_sup_acc: 97.26 Train acc: 97.060 Test acc: 97.300 \n",
      "step: 6067 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.528, D_sup_loss: 0.120, D_sup_acc: 97.33 Train acc: 96.943 Test acc: 97.270 \n",
      "step: 6068 | Train: G_Loss: 1.179, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.582, D_sup_loss: 0.123, D_sup_acc: 97.30 Train acc: 97.067 Test acc: 97.310 \n",
      "step: 6069 | Train: G_Loss: 1.249, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.588, D_sup_loss: 0.118, D_sup_acc: 97.34 Train acc: 96.977 Test acc: 97.280 \n",
      "step: 6070 | Train: G_Loss: 1.159, D_unsup_loss_real: 0.534, D_unsup_loss_fake: 0.568, D_sup_loss: 0.122, D_sup_acc: 97.31 Train acc: 97.002 Test acc: 97.230 \n",
      "step: 6071 | Train: G_Loss: 1.164, D_unsup_loss_real: 0.635, D_unsup_loss_fake: 0.638, D_sup_loss: 0.120, D_sup_acc: 97.27 Train acc: 96.923 Test acc: 97.100 \n",
      "step: 6072 | Train: G_Loss: 1.184, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.643, D_sup_loss: 0.127, D_sup_acc: 97.14 Train acc: 97.047 Test acc: 97.220 \n",
      "step: 6073 | Train: G_Loss: 1.053, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.679, D_sup_loss: 0.119, D_sup_acc: 97.26 Train acc: 97.027 Test acc: 97.220 \n",
      "step: 6074 | Train: G_Loss: 1.234, D_unsup_loss_real: 0.531, D_unsup_loss_fake: 0.582, D_sup_loss: 0.122, D_sup_acc: 97.26 Train acc: 97.085 Test acc: 97.260 \n",
      "step: 6075 | Train: G_Loss: 1.299, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.650, D_sup_loss: 0.118, D_sup_acc: 97.29 Train acc: 97.008 Test acc: 97.260 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6076 | Train: G_Loss: 1.178, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.503, D_sup_loss: 0.122, D_sup_acc: 97.29 Train acc: 96.647 Test acc: 96.840 \n",
      "step: 6077 | Train: G_Loss: 1.252, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.559, D_sup_loss: 0.142, D_sup_acc: 96.88 Train acc: 97.047 Test acc: 97.190 \n",
      "step: 6078 | Train: G_Loss: 1.308, D_unsup_loss_real: 0.642, D_unsup_loss_fake: 0.559, D_sup_loss: 0.121, D_sup_acc: 97.23 Train acc: 97.138 Test acc: 97.220 \n",
      "step: 6079 | Train: G_Loss: 1.224, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.523, D_sup_loss: 0.115, D_sup_acc: 97.26 Train acc: 97.147 Test acc: 97.270 \n",
      "step: 6080 | Train: G_Loss: 1.299, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.628, D_sup_loss: 0.116, D_sup_acc: 97.30 Train acc: 97.057 Test acc: 97.260 \n",
      "step: 6081 | Train: G_Loss: 1.230, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.584, D_sup_loss: 0.122, D_sup_acc: 97.29 Train acc: 97.032 Test acc: 97.200 \n",
      "step: 6082 | Train: G_Loss: 1.057, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.607, D_sup_loss: 0.124, D_sup_acc: 97.24 Train acc: 96.967 Test acc: 97.240 \n",
      "step: 6083 | Train: G_Loss: 1.176, D_unsup_loss_real: 0.635, D_unsup_loss_fake: 0.632, D_sup_loss: 0.128, D_sup_acc: 97.27 Train acc: 97.073 Test acc: 97.320 \n",
      "step: 6084 | Train: G_Loss: 1.194, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.586, D_sup_loss: 0.124, D_sup_acc: 97.35 Train acc: 96.882 Test acc: 97.170 \n",
      "step: 6085 | Train: G_Loss: 1.212, D_unsup_loss_real: 0.520, D_unsup_loss_fake: 0.559, D_sup_loss: 0.131, D_sup_acc: 97.21 Train acc: 97.075 Test acc: 97.280 \n",
      "step: 6086 | Train: G_Loss: 1.092, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.617, D_sup_loss: 0.118, D_sup_acc: 97.31 Train acc: 96.952 Test acc: 97.300 \n",
      "step: 6087 | Train: G_Loss: 1.101, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.619, D_sup_loss: 0.123, D_sup_acc: 97.33 Train acc: 96.988 Test acc: 97.270 \n",
      "step: 6088 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.628, D_sup_loss: 0.121, D_sup_acc: 97.30 Train acc: 96.748 Test acc: 97.070 \n",
      "step: 6089 | Train: G_Loss: 1.061, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.578, D_sup_loss: 0.135, D_sup_acc: 97.11 Train acc: 96.863 Test acc: 97.180 \n",
      "step: 6090 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.502, D_unsup_loss_fake: 0.640, D_sup_loss: 0.126, D_sup_acc: 97.22 Train acc: 97.008 Test acc: 97.220 \n",
      "step: 6091 | Train: G_Loss: 1.203, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.522, D_sup_loss: 0.116, D_sup_acc: 97.26 Train acc: 96.972 Test acc: 97.300 \n",
      "step: 6092 | Train: G_Loss: 1.126, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.533, D_sup_loss: 0.120, D_sup_acc: 97.33 Train acc: 97.068 Test acc: 97.320 \n",
      "step: 6093 | Train: G_Loss: 1.117, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.565, D_sup_loss: 0.115, D_sup_acc: 97.35 Train acc: 96.892 Test acc: 97.160 \n",
      "step: 6094 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.639, D_sup_loss: 0.123, D_sup_acc: 97.20 Train acc: 96.947 Test acc: 97.320 \n",
      "step: 6095 | Train: G_Loss: 1.113, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.586, D_sup_loss: 0.116, D_sup_acc: 97.35 Train acc: 96.878 Test acc: 97.120 \n",
      "step: 6096 | Train: G_Loss: 1.181, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.642, D_sup_loss: 0.125, D_sup_acc: 97.16 Train acc: 97.037 Test acc: 97.270 \n",
      "step: 6097 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.527, D_unsup_loss_fake: 0.593, D_sup_loss: 0.118, D_sup_acc: 97.30 Train acc: 97.017 Test acc: 97.160 \n",
      "step: 6098 | Train: G_Loss: 1.222, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.529, D_sup_loss: 0.118, D_sup_acc: 97.20 Train acc: 97.025 Test acc: 97.250 \n",
      "step: 6099 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.536, D_sup_loss: 0.115, D_sup_acc: 97.28 Train acc: 96.868 Test acc: 97.100 \n",
      "step: 6100 | Train: G_Loss: 1.228, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.502, D_sup_loss: 0.125, D_sup_acc: 97.14 Train acc: 97.048 Test acc: 97.240 \n",
      "Train Classifier Accuracy: 97.048%\n",
      "\n",
      "Test Classifier Accuracy: 97.240%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_6100.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_6100.h5\n",
      "step: 6101 | Train: G_Loss: 1.209, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.627, D_sup_loss: 0.117, D_sup_acc: 97.27 Train acc: 96.998 Test acc: 97.230 \n",
      "step: 6102 | Train: G_Loss: 1.161, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.681, D_sup_loss: 0.117, D_sup_acc: 97.27 Train acc: 96.907 Test acc: 97.150 \n",
      "step: 6103 | Train: G_Loss: 1.181, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.644, D_sup_loss: 0.121, D_sup_acc: 97.19 Train acc: 97.015 Test acc: 97.290 \n",
      "step: 6104 | Train: G_Loss: 1.197, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.592, D_sup_loss: 0.115, D_sup_acc: 97.32 Train acc: 97.060 Test acc: 97.260 \n",
      "step: 6105 | Train: G_Loss: 1.193, D_unsup_loss_real: 0.519, D_unsup_loss_fake: 0.582, D_sup_loss: 0.111, D_sup_acc: 97.29 Train acc: 97.000 Test acc: 97.230 \n",
      "step: 6106 | Train: G_Loss: 1.152, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.582, D_sup_loss: 0.117, D_sup_acc: 97.27 Train acc: 97.122 Test acc: 97.320 \n",
      "step: 6107 | Train: G_Loss: 1.118, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.613, D_sup_loss: 0.111, D_sup_acc: 97.35 Train acc: 96.850 Test acc: 97.080 \n",
      "step: 6108 | Train: G_Loss: 1.164, D_unsup_loss_real: 0.662, D_unsup_loss_fake: 0.618, D_sup_loss: 0.129, D_sup_acc: 97.12 Train acc: 96.943 Test acc: 97.140 \n",
      "step: 6109 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.579, D_sup_loss: 0.124, D_sup_acc: 97.18 Train acc: 96.917 Test acc: 97.080 \n",
      "step: 6110 | Train: G_Loss: 1.200, D_unsup_loss_real: 0.652, D_unsup_loss_fake: 0.561, D_sup_loss: 0.126, D_sup_acc: 97.12 Train acc: 97.025 Test acc: 97.220 \n",
      "step: 6111 | Train: G_Loss: 1.137, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.596, D_sup_loss: 0.123, D_sup_acc: 97.26 Train acc: 97.015 Test acc: 97.180 \n",
      "step: 6112 | Train: G_Loss: 1.073, D_unsup_loss_real: 0.520, D_unsup_loss_fake: 0.652, D_sup_loss: 0.122, D_sup_acc: 97.22 Train acc: 96.945 Test acc: 97.130 \n",
      "step: 6113 | Train: G_Loss: 1.115, D_unsup_loss_real: 0.537, D_unsup_loss_fake: 0.657, D_sup_loss: 0.126, D_sup_acc: 97.17 Train acc: 97.023 Test acc: 97.140 \n",
      "step: 6114 | Train: G_Loss: 1.175, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.562, D_sup_loss: 0.122, D_sup_acc: 97.18 Train acc: 97.028 Test acc: 97.170 \n",
      "step: 6115 | Train: G_Loss: 1.291, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.555, D_sup_loss: 0.122, D_sup_acc: 97.21 Train acc: 96.962 Test acc: 97.100 \n",
      "step: 6116 | Train: G_Loss: 1.196, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.691, D_sup_loss: 0.124, D_sup_acc: 97.14 Train acc: 96.922 Test acc: 97.150 \n",
      "step: 6117 | Train: G_Loss: 1.189, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.579, D_sup_loss: 0.128, D_sup_acc: 97.19 Train acc: 96.938 Test acc: 97.240 \n",
      "step: 6118 | Train: G_Loss: 1.100, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.574, D_sup_loss: 0.128, D_sup_acc: 97.27 Train acc: 96.808 Test acc: 97.060 \n",
      "step: 6119 | Train: G_Loss: 1.245, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.604, D_sup_loss: 0.135, D_sup_acc: 97.10 Train acc: 97.028 Test acc: 97.150 \n",
      "step: 6120 | Train: G_Loss: 1.273, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.603, D_sup_loss: 0.117, D_sup_acc: 97.19 Train acc: 96.940 Test acc: 97.260 \n",
      "step: 6121 | Train: G_Loss: 1.186, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.547, D_sup_loss: 0.126, D_sup_acc: 97.29 Train acc: 96.648 Test acc: 96.880 \n",
      "step: 6122 | Train: G_Loss: 1.221, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.594, D_sup_loss: 0.142, D_sup_acc: 96.92 Train acc: 97.058 Test acc: 97.240 \n",
      "step: 6123 | Train: G_Loss: 1.154, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.597, D_sup_loss: 0.116, D_sup_acc: 97.27 Train acc: 97.040 Test acc: 97.160 \n",
      "step: 6124 | Train: G_Loss: 1.170, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.561, D_sup_loss: 0.115, D_sup_acc: 97.20 Train acc: 96.557 Test acc: 96.830 \n",
      "step: 6125 | Train: G_Loss: 1.070, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.583, D_sup_loss: 0.154, D_sup_acc: 96.87 Train acc: 96.937 Test acc: 97.150 \n",
      "step: 6126 | Train: G_Loss: 1.156, D_unsup_loss_real: 0.529, D_unsup_loss_fake: 0.537, D_sup_loss: 0.128, D_sup_acc: 97.19 Train acc: 97.037 Test acc: 97.270 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6127 | Train: G_Loss: 1.213, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.676, D_sup_loss: 0.118, D_sup_acc: 97.30 Train acc: 96.917 Test acc: 97.070 \n",
      "step: 6128 | Train: G_Loss: 1.137, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.560, D_sup_loss: 0.126, D_sup_acc: 97.11 Train acc: 96.778 Test acc: 96.890 \n",
      "step: 6129 | Train: G_Loss: 1.291, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.599, D_sup_loss: 0.131, D_sup_acc: 96.93 Train acc: 97.072 Test acc: 97.270 \n",
      "step: 6130 | Train: G_Loss: 1.066, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.575, D_sup_loss: 0.114, D_sup_acc: 97.30 Train acc: 97.053 Test acc: 97.200 \n",
      "step: 6131 | Train: G_Loss: 1.104, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.577, D_sup_loss: 0.115, D_sup_acc: 97.24 Train acc: 96.843 Test acc: 97.090 \n",
      "step: 6132 | Train: G_Loss: 1.141, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.504, D_sup_loss: 0.130, D_sup_acc: 97.13 Train acc: 97.068 Test acc: 97.250 \n",
      "step: 6133 | Train: G_Loss: 1.076, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.610, D_sup_loss: 0.117, D_sup_acc: 97.28 Train acc: 96.825 Test acc: 97.100 \n",
      "step: 6134 | Train: G_Loss: 1.152, D_unsup_loss_real: 0.538, D_unsup_loss_fake: 0.577, D_sup_loss: 0.131, D_sup_acc: 97.14 Train acc: 97.010 Test acc: 97.190 \n",
      "step: 6135 | Train: G_Loss: 1.249, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.588, D_sup_loss: 0.119, D_sup_acc: 97.23 Train acc: 96.993 Test acc: 97.240 \n",
      "step: 6136 | Train: G_Loss: 1.165, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.624, D_sup_loss: 0.119, D_sup_acc: 97.27 Train acc: 96.915 Test acc: 97.080 \n",
      "step: 6137 | Train: G_Loss: 1.050, D_unsup_loss_real: 0.670, D_unsup_loss_fake: 0.594, D_sup_loss: 0.121, D_sup_acc: 97.12 Train acc: 96.897 Test acc: 97.120 \n",
      "step: 6138 | Train: G_Loss: 1.096, D_unsup_loss_real: 0.544, D_unsup_loss_fake: 0.679, D_sup_loss: 0.126, D_sup_acc: 97.16 Train acc: 96.908 Test acc: 97.060 \n",
      "step: 6139 | Train: G_Loss: 1.191, D_unsup_loss_real: 0.528, D_unsup_loss_fake: 0.529, D_sup_loss: 0.128, D_sup_acc: 97.10 Train acc: 96.888 Test acc: 97.130 \n",
      "step: 6140 | Train: G_Loss: 1.228, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.555, D_sup_loss: 0.128, D_sup_acc: 97.17 Train acc: 97.020 Test acc: 97.150 \n",
      "step: 6141 | Train: G_Loss: 1.229, D_unsup_loss_real: 0.513, D_unsup_loss_fake: 0.565, D_sup_loss: 0.123, D_sup_acc: 97.19 Train acc: 96.993 Test acc: 97.230 \n",
      "step: 6142 | Train: G_Loss: 1.165, D_unsup_loss_real: 0.533, D_unsup_loss_fake: 0.608, D_sup_loss: 0.121, D_sup_acc: 97.27 Train acc: 97.062 Test acc: 97.100 \n",
      "step: 6143 | Train: G_Loss: 1.185, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.567, D_sup_loss: 0.115, D_sup_acc: 97.14 Train acc: 96.915 Test acc: 97.120 \n",
      "step: 6144 | Train: G_Loss: 1.216, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.611, D_sup_loss: 0.132, D_sup_acc: 97.16 Train acc: 97.058 Test acc: 97.170 \n",
      "step: 6145 | Train: G_Loss: 1.254, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.560, D_sup_loss: 0.120, D_sup_acc: 97.21 Train acc: 97.063 Test acc: 97.200 \n",
      "step: 6146 | Train: G_Loss: 1.231, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.576, D_sup_loss: 0.118, D_sup_acc: 97.24 Train acc: 96.935 Test acc: 97.110 \n",
      "step: 6147 | Train: G_Loss: 1.189, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.602, D_sup_loss: 0.130, D_sup_acc: 97.15 Train acc: 97.087 Test acc: 97.190 \n",
      "step: 6148 | Train: G_Loss: 1.217, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.640, D_sup_loss: 0.116, D_sup_acc: 97.23 Train acc: 96.925 Test acc: 97.030 \n",
      "step: 6149 | Train: G_Loss: 1.154, D_unsup_loss_real: 0.541, D_unsup_loss_fake: 0.564, D_sup_loss: 0.127, D_sup_acc: 97.07 Train acc: 96.943 Test acc: 97.080 \n",
      "step: 6150 | Train: G_Loss: 1.185, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.549, D_sup_loss: 0.128, D_sup_acc: 97.12 Train acc: 97.008 Test acc: 97.190 \n",
      "step: 6151 | Train: G_Loss: 1.164, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.600, D_sup_loss: 0.116, D_sup_acc: 97.23 Train acc: 96.922 Test acc: 97.140 \n",
      "step: 6152 | Train: G_Loss: 1.172, D_unsup_loss_real: 0.526, D_unsup_loss_fake: 0.631, D_sup_loss: 0.125, D_sup_acc: 97.18 Train acc: 96.992 Test acc: 97.000 \n",
      "step: 6153 | Train: G_Loss: 1.228, D_unsup_loss_real: 0.545, D_unsup_loss_fake: 0.572, D_sup_loss: 0.118, D_sup_acc: 97.04 Train acc: 97.065 Test acc: 97.100 \n",
      "step: 6154 | Train: G_Loss: 1.088, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.625, D_sup_loss: 0.117, D_sup_acc: 97.14 Train acc: 96.635 Test acc: 96.900 \n",
      "step: 6155 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.590, D_sup_loss: 0.148, D_sup_acc: 96.94 Train acc: 96.912 Test acc: 97.100 \n",
      "step: 6156 | Train: G_Loss: 1.153, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.611, D_sup_loss: 0.130, D_sup_acc: 97.14 Train acc: 96.875 Test acc: 97.110 \n",
      "step: 6157 | Train: G_Loss: 1.182, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.540, D_sup_loss: 0.130, D_sup_acc: 97.15 Train acc: 96.982 Test acc: 97.220 \n",
      "step: 6158 | Train: G_Loss: 1.115, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.625, D_sup_loss: 0.122, D_sup_acc: 97.26 Train acc: 96.803 Test acc: 97.010 \n",
      "step: 6159 | Train: G_Loss: 1.286, D_unsup_loss_real: 0.665, D_unsup_loss_fake: 0.573, D_sup_loss: 0.136, D_sup_acc: 97.05 Train acc: 97.012 Test acc: 97.190 \n",
      "step: 6160 | Train: G_Loss: 1.159, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.593, D_sup_loss: 0.120, D_sup_acc: 97.23 Train acc: 96.997 Test acc: 97.140 \n",
      "step: 6161 | Train: G_Loss: 1.229, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.556, D_sup_loss: 0.124, D_sup_acc: 97.18 Train acc: 96.987 Test acc: 97.150 \n",
      "step: 6162 | Train: G_Loss: 1.136, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.549, D_sup_loss: 0.122, D_sup_acc: 97.19 Train acc: 97.028 Test acc: 97.100 \n",
      "step: 6163 | Train: G_Loss: 1.187, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.600, D_sup_loss: 0.118, D_sup_acc: 97.14 Train acc: 97.020 Test acc: 97.090 \n",
      "step: 6164 | Train: G_Loss: 1.050, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.584, D_sup_loss: 0.120, D_sup_acc: 97.13 Train acc: 96.928 Test acc: 97.090 \n",
      "step: 6165 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.647, D_unsup_loss_fake: 0.546, D_sup_loss: 0.123, D_sup_acc: 97.13 Train acc: 96.927 Test acc: 97.070 \n",
      "step: 6166 | Train: G_Loss: 1.185, D_unsup_loss_real: 0.532, D_unsup_loss_fake: 0.660, D_sup_loss: 0.125, D_sup_acc: 97.11 Train acc: 96.703 Test acc: 96.880 \n",
      "step: 6167 | Train: G_Loss: 1.213, D_unsup_loss_real: 0.504, D_unsup_loss_fake: 0.668, D_sup_loss: 0.135, D_sup_acc: 96.92 Train acc: 96.712 Test acc: 96.880 \n",
      "step: 6168 | Train: G_Loss: 1.082, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.578, D_sup_loss: 0.133, D_sup_acc: 96.92 Train acc: 97.105 Test acc: 97.250 \n",
      "step: 6169 | Train: G_Loss: 1.123, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.590, D_sup_loss: 0.117, D_sup_acc: 97.28 Train acc: 96.987 Test acc: 97.120 \n",
      "step: 6170 | Train: G_Loss: 1.058, D_unsup_loss_real: 0.657, D_unsup_loss_fake: 0.533, D_sup_loss: 0.124, D_sup_acc: 97.16 Train acc: 97.008 Test acc: 97.160 \n",
      "step: 6171 | Train: G_Loss: 1.133, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.609, D_sup_loss: 0.122, D_sup_acc: 97.20 Train acc: 97.002 Test acc: 97.160 \n",
      "step: 6172 | Train: G_Loss: 1.157, D_unsup_loss_real: 0.529, D_unsup_loss_fake: 0.588, D_sup_loss: 0.125, D_sup_acc: 97.20 Train acc: 96.833 Test acc: 97.010 \n",
      "step: 6173 | Train: G_Loss: 1.083, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.688, D_sup_loss: 0.133, D_sup_acc: 97.05 Train acc: 96.940 Test acc: 97.120 \n",
      "step: 6174 | Train: G_Loss: 1.160, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.544, D_sup_loss: 0.129, D_sup_acc: 97.16 Train acc: 97.098 Test acc: 97.260 \n",
      "step: 6175 | Train: G_Loss: 1.101, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.598, D_sup_loss: 0.120, D_sup_acc: 97.29 Train acc: 96.972 Test acc: 97.130 \n",
      "step: 6176 | Train: G_Loss: 1.117, D_unsup_loss_real: 0.703, D_unsup_loss_fake: 0.578, D_sup_loss: 0.123, D_sup_acc: 97.17 Train acc: 96.997 Test acc: 97.150 \n",
      "step: 6177 | Train: G_Loss: 1.178, D_unsup_loss_real: 0.537, D_unsup_loss_fake: 0.588, D_sup_loss: 0.126, D_sup_acc: 97.19 Train acc: 97.013 Test acc: 97.230 \n",
      "step: 6178 | Train: G_Loss: 1.210, D_unsup_loss_real: 0.494, D_unsup_loss_fake: 0.604, D_sup_loss: 0.121, D_sup_acc: 97.27 Train acc: 97.097 Test acc: 97.190 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6179 | Train: G_Loss: 1.180, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.579, D_sup_loss: 0.113, D_sup_acc: 97.23 Train acc: 96.952 Test acc: 97.150 \n",
      "step: 6180 | Train: G_Loss: 1.189, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.584, D_sup_loss: 0.128, D_sup_acc: 97.19 Train acc: 97.022 Test acc: 97.080 \n",
      "step: 6181 | Train: G_Loss: 1.083, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.649, D_sup_loss: 0.119, D_sup_acc: 97.12 Train acc: 97.007 Test acc: 97.220 \n",
      "step: 6182 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.574, D_sup_loss: 0.127, D_sup_acc: 97.26 Train acc: 97.123 Test acc: 97.240 \n",
      "step: 6183 | Train: G_Loss: 1.136, D_unsup_loss_real: 0.545, D_unsup_loss_fake: 0.594, D_sup_loss: 0.120, D_sup_acc: 97.27 Train acc: 96.907 Test acc: 97.270 \n",
      "step: 6184 | Train: G_Loss: 1.175, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.622, D_sup_loss: 0.132, D_sup_acc: 97.30 Train acc: 96.793 Test acc: 97.110 \n",
      "step: 6185 | Train: G_Loss: 1.150, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.568, D_sup_loss: 0.138, D_sup_acc: 97.15 Train acc: 97.070 Test acc: 97.190 \n",
      "step: 6186 | Train: G_Loss: 1.267, D_unsup_loss_real: 0.526, D_unsup_loss_fake: 0.552, D_sup_loss: 0.123, D_sup_acc: 97.23 Train acc: 97.027 Test acc: 97.230 \n",
      "step: 6187 | Train: G_Loss: 1.243, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.605, D_sup_loss: 0.126, D_sup_acc: 97.27 Train acc: 97.055 Test acc: 97.230 \n",
      "step: 6188 | Train: G_Loss: 1.162, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.580, D_sup_loss: 0.122, D_sup_acc: 97.27 Train acc: 96.987 Test acc: 97.220 \n",
      "step: 6189 | Train: G_Loss: 1.296, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.578, D_sup_loss: 0.127, D_sup_acc: 97.26 Train acc: 97.122 Test acc: 97.370 \n",
      "step: 6190 | Train: G_Loss: 1.143, D_unsup_loss_real: 0.532, D_unsup_loss_fake: 0.601, D_sup_loss: 0.119, D_sup_acc: 97.40 Train acc: 96.943 Test acc: 97.180 \n",
      "step: 6191 | Train: G_Loss: 1.203, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.510, D_sup_loss: 0.128, D_sup_acc: 97.22 Train acc: 96.973 Test acc: 97.220 \n",
      "step: 6192 | Train: G_Loss: 1.179, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.527, D_sup_loss: 0.125, D_sup_acc: 97.26 Train acc: 96.853 Test acc: 97.100 \n",
      "step: 6193 | Train: G_Loss: 1.191, D_unsup_loss_real: 0.527, D_unsup_loss_fake: 0.594, D_sup_loss: 0.134, D_sup_acc: 97.14 Train acc: 96.915 Test acc: 97.110 \n",
      "step: 6194 | Train: G_Loss: 1.160, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.554, D_sup_loss: 0.128, D_sup_acc: 97.15 Train acc: 96.980 Test acc: 97.180 \n",
      "step: 6195 | Train: G_Loss: 1.151, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.659, D_sup_loss: 0.121, D_sup_acc: 97.22 Train acc: 96.938 Test acc: 97.160 \n",
      "step: 6196 | Train: G_Loss: 1.137, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.620, D_sup_loss: 0.127, D_sup_acc: 97.20 Train acc: 96.972 Test acc: 97.180 \n",
      "step: 6197 | Train: G_Loss: 1.365, D_unsup_loss_real: 0.550, D_unsup_loss_fake: 0.561, D_sup_loss: 0.126, D_sup_acc: 97.22 Train acc: 96.978 Test acc: 97.190 \n",
      "step: 6198 | Train: G_Loss: 1.266, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.482, D_sup_loss: 0.124, D_sup_acc: 97.23 Train acc: 96.947 Test acc: 97.100 \n",
      "step: 6199 | Train: G_Loss: 1.073, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.580, D_sup_loss: 0.123, D_sup_acc: 97.14 Train acc: 96.993 Test acc: 97.150 \n",
      "step: 6200 | Train: G_Loss: 1.109, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.573, D_sup_loss: 0.123, D_sup_acc: 97.19 Train acc: 96.955 Test acc: 97.210 \n",
      "Train Classifier Accuracy: 96.955%\n",
      "\n",
      "Test Classifier Accuracy: 97.210%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_6200.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_6200.h5\n",
      "step: 6201 | Train: G_Loss: 1.147, D_unsup_loss_real: 0.510, D_unsup_loss_fake: 0.557, D_sup_loss: 0.124, D_sup_acc: 97.25 Train acc: 97.112 Test acc: 97.230 \n",
      "step: 6202 | Train: G_Loss: 1.180, D_unsup_loss_real: 0.542, D_unsup_loss_fake: 0.596, D_sup_loss: 0.115, D_sup_acc: 97.27 Train acc: 96.817 Test acc: 97.080 \n",
      "step: 6203 | Train: G_Loss: 1.150, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.564, D_sup_loss: 0.132, D_sup_acc: 97.12 Train acc: 96.977 Test acc: 97.220 \n",
      "step: 6204 | Train: G_Loss: 1.168, D_unsup_loss_real: 0.541, D_unsup_loss_fake: 0.532, D_sup_loss: 0.123, D_sup_acc: 97.26 Train acc: 96.882 Test acc: 97.100 \n",
      "step: 6205 | Train: G_Loss: 1.162, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.517, D_sup_loss: 0.125, D_sup_acc: 97.14 Train acc: 96.767 Test acc: 96.960 \n",
      "step: 6206 | Train: G_Loss: 1.125, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.621, D_sup_loss: 0.130, D_sup_acc: 97.00 Train acc: 96.423 Test acc: 96.670 \n",
      "step: 6207 | Train: G_Loss: 1.105, D_unsup_loss_real: 0.537, D_unsup_loss_fake: 0.587, D_sup_loss: 0.146, D_sup_acc: 96.71 Train acc: 96.773 Test acc: 97.010 \n",
      "step: 6208 | Train: G_Loss: 1.171, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.615, D_sup_loss: 0.132, D_sup_acc: 97.05 Train acc: 96.757 Test acc: 96.950 \n",
      "step: 6209 | Train: G_Loss: 1.172, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.537, D_sup_loss: 0.133, D_sup_acc: 96.99 Train acc: 96.880 Test acc: 97.030 \n",
      "step: 6210 | Train: G_Loss: 1.149, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.565, D_sup_loss: 0.126, D_sup_acc: 97.07 Train acc: 96.957 Test acc: 97.090 \n",
      "step: 6211 | Train: G_Loss: 1.188, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.615, D_sup_loss: 0.124, D_sup_acc: 97.13 Train acc: 97.022 Test acc: 97.230 \n",
      "step: 6212 | Train: G_Loss: 1.358, D_unsup_loss_real: 0.529, D_unsup_loss_fake: 0.563, D_sup_loss: 0.119, D_sup_acc: 97.27 Train acc: 97.085 Test acc: 97.260 \n",
      "step: 6213 | Train: G_Loss: 1.198, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.588, D_sup_loss: 0.116, D_sup_acc: 97.29 Train acc: 96.973 Test acc: 97.210 \n",
      "step: 6214 | Train: G_Loss: 1.141, D_unsup_loss_real: 0.536, D_unsup_loss_fake: 0.636, D_sup_loss: 0.124, D_sup_acc: 97.25 Train acc: 96.838 Test acc: 97.050 \n",
      "step: 6215 | Train: G_Loss: 1.031, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.538, D_sup_loss: 0.131, D_sup_acc: 97.09 Train acc: 96.795 Test acc: 97.020 \n",
      "step: 6216 | Train: G_Loss: 1.254, D_unsup_loss_real: 0.520, D_unsup_loss_fake: 0.605, D_sup_loss: 0.132, D_sup_acc: 97.06 Train acc: 96.728 Test acc: 96.920 \n",
      "step: 6217 | Train: G_Loss: 1.165, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.565, D_sup_loss: 0.133, D_sup_acc: 96.96 Train acc: 96.810 Test acc: 97.060 \n",
      "step: 6218 | Train: G_Loss: 1.232, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.565, D_sup_loss: 0.131, D_sup_acc: 97.10 Train acc: 96.782 Test acc: 97.060 \n",
      "step: 6219 | Train: G_Loss: 1.150, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.578, D_sup_loss: 0.131, D_sup_acc: 97.10 Train acc: 96.708 Test acc: 96.960 \n",
      "step: 6220 | Train: G_Loss: 1.192, D_unsup_loss_real: 0.538, D_unsup_loss_fake: 0.553, D_sup_loss: 0.135, D_sup_acc: 97.00 Train acc: 96.900 Test acc: 97.140 \n",
      "step: 6221 | Train: G_Loss: 1.256, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.563, D_sup_loss: 0.123, D_sup_acc: 97.18 Train acc: 96.717 Test acc: 96.870 \n",
      "step: 6222 | Train: G_Loss: 1.260, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.578, D_sup_loss: 0.138, D_sup_acc: 96.91 Train acc: 97.075 Test acc: 97.200 \n",
      "step: 6223 | Train: G_Loss: 1.216, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.636, D_sup_loss: 0.116, D_sup_acc: 97.24 Train acc: 97.052 Test acc: 97.330 \n",
      "step: 6224 | Train: G_Loss: 1.129, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.573, D_sup_loss: 0.115, D_sup_acc: 97.36 Train acc: 96.905 Test acc: 97.210 \n",
      "step: 6225 | Train: G_Loss: 1.105, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.529, D_sup_loss: 0.128, D_sup_acc: 97.25 Train acc: 96.785 Test acc: 97.150 \n",
      "step: 6226 | Train: G_Loss: 1.184, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.533, D_sup_loss: 0.134, D_sup_acc: 97.19 Train acc: 97.013 Test acc: 97.260 \n",
      "step: 6227 | Train: G_Loss: 1.109, D_unsup_loss_real: 0.536, D_unsup_loss_fake: 0.571, D_sup_loss: 0.116, D_sup_acc: 97.29 Train acc: 96.862 Test acc: 97.060 \n",
      "step: 6228 | Train: G_Loss: 1.165, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.574, D_sup_loss: 0.133, D_sup_acc: 97.10 Train acc: 97.017 Test acc: 97.170 \n",
      "step: 6229 | Train: G_Loss: 1.157, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.571, D_sup_loss: 0.120, D_sup_acc: 97.21 Train acc: 96.795 Test acc: 97.050 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6230 | Train: G_Loss: 1.293, D_unsup_loss_real: 0.531, D_unsup_loss_fake: 0.545, D_sup_loss: 0.132, D_sup_acc: 97.09 Train acc: 96.850 Test acc: 97.110 \n",
      "step: 6231 | Train: G_Loss: 1.100, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.619, D_sup_loss: 0.126, D_sup_acc: 97.15 Train acc: 96.773 Test acc: 97.020 \n",
      "step: 6232 | Train: G_Loss: 1.130, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.737, D_sup_loss: 0.136, D_sup_acc: 97.06 Train acc: 96.938 Test acc: 97.210 \n",
      "step: 6233 | Train: G_Loss: 1.209, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.538, D_sup_loss: 0.120, D_sup_acc: 97.25 Train acc: 96.912 Test acc: 97.140 \n",
      "step: 6234 | Train: G_Loss: 1.224, D_unsup_loss_real: 0.529, D_unsup_loss_fake: 0.514, D_sup_loss: 0.122, D_sup_acc: 97.18 Train acc: 97.007 Test acc: 97.310 \n",
      "step: 6235 | Train: G_Loss: 1.072, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.628, D_sup_loss: 0.116, D_sup_acc: 97.34 Train acc: 96.597 Test acc: 96.760 \n",
      "step: 6236 | Train: G_Loss: 1.187, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.608, D_sup_loss: 0.141, D_sup_acc: 96.80 Train acc: 96.718 Test acc: 96.950 \n",
      "step: 6237 | Train: G_Loss: 1.172, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.577, D_sup_loss: 0.134, D_sup_acc: 96.99 Train acc: 96.738 Test acc: 96.980 \n",
      "step: 6238 | Train: G_Loss: 1.203, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.600, D_sup_loss: 0.132, D_sup_acc: 97.02 Train acc: 96.760 Test acc: 97.070 \n",
      "step: 6239 | Train: G_Loss: 1.401, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.579, D_sup_loss: 0.130, D_sup_acc: 97.11 Train acc: 96.878 Test acc: 97.060 \n",
      "step: 6240 | Train: G_Loss: 1.272, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.596, D_sup_loss: 0.120, D_sup_acc: 97.10 Train acc: 96.343 Test acc: 96.480 \n",
      "step: 6241 | Train: G_Loss: 1.127, D_unsup_loss_real: 0.704, D_unsup_loss_fake: 0.675, D_sup_loss: 0.167, D_sup_acc: 96.52 Train acc: 97.038 Test acc: 97.240 \n",
      "step: 6242 | Train: G_Loss: 1.210, D_unsup_loss_real: 0.639, D_unsup_loss_fake: 0.570, D_sup_loss: 0.115, D_sup_acc: 97.27 Train acc: 96.943 Test acc: 97.160 \n",
      "step: 6243 | Train: G_Loss: 1.256, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.534, D_sup_loss: 0.121, D_sup_acc: 97.20 Train acc: 96.953 Test acc: 97.160 \n",
      "step: 6244 | Train: G_Loss: 1.280, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.616, D_sup_loss: 0.120, D_sup_acc: 97.20 Train acc: 96.842 Test acc: 97.040 \n",
      "step: 6245 | Train: G_Loss: 1.120, D_unsup_loss_real: 0.520, D_unsup_loss_fake: 0.541, D_sup_loss: 0.131, D_sup_acc: 97.08 Train acc: 96.977 Test acc: 97.170 \n",
      "step: 6246 | Train: G_Loss: 1.250, D_unsup_loss_real: 0.529, D_unsup_loss_fake: 0.554, D_sup_loss: 0.122, D_sup_acc: 97.21 Train acc: 97.062 Test acc: 97.170 \n",
      "step: 6247 | Train: G_Loss: 1.211, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.579, D_sup_loss: 0.114, D_sup_acc: 97.21 Train acc: 96.852 Test acc: 97.050 \n",
      "step: 6248 | Train: G_Loss: 1.165, D_unsup_loss_real: 0.536, D_unsup_loss_fake: 0.574, D_sup_loss: 0.131, D_sup_acc: 97.09 Train acc: 96.893 Test acc: 97.190 \n",
      "step: 6249 | Train: G_Loss: 1.071, D_unsup_loss_real: 0.658, D_unsup_loss_fake: 0.602, D_sup_loss: 0.124, D_sup_acc: 97.23 Train acc: 96.900 Test acc: 97.250 \n",
      "step: 6250 | Train: G_Loss: 1.228, D_unsup_loss_real: 0.537, D_unsup_loss_fake: 0.641, D_sup_loss: 0.123, D_sup_acc: 97.28 Train acc: 96.837 Test acc: 97.090 \n",
      "step: 6251 | Train: G_Loss: 1.154, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.626, D_sup_loss: 0.126, D_sup_acc: 97.13 Train acc: 96.753 Test acc: 96.980 \n",
      "step: 6252 | Train: G_Loss: 1.263, D_unsup_loss_real: 0.523, D_unsup_loss_fake: 0.500, D_sup_loss: 0.132, D_sup_acc: 97.02 Train acc: 97.063 Test acc: 97.180 \n",
      "step: 6253 | Train: G_Loss: 1.112, D_unsup_loss_real: 0.686, D_unsup_loss_fake: 0.630, D_sup_loss: 0.110, D_sup_acc: 97.22 Train acc: 96.518 Test acc: 96.840 \n",
      "step: 6254 | Train: G_Loss: 1.163, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.560, D_sup_loss: 0.147, D_sup_acc: 96.88 Train acc: 96.802 Test acc: 97.090 \n",
      "step: 6255 | Train: G_Loss: 1.258, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.699, D_sup_loss: 0.126, D_sup_acc: 97.13 Train acc: 96.705 Test acc: 97.000 \n",
      "step: 6256 | Train: G_Loss: 1.182, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.541, D_sup_loss: 0.130, D_sup_acc: 97.04 Train acc: 96.843 Test acc: 97.030 \n",
      "step: 6257 | Train: G_Loss: 1.119, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.605, D_sup_loss: 0.121, D_sup_acc: 97.07 Train acc: 96.743 Test acc: 97.000 \n",
      "step: 6258 | Train: G_Loss: 1.191, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.615, D_sup_loss: 0.127, D_sup_acc: 97.04 Train acc: 96.737 Test acc: 97.050 \n",
      "step: 6259 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.553, D_sup_loss: 0.126, D_sup_acc: 97.09 Train acc: 96.745 Test acc: 97.100 \n",
      "step: 6260 | Train: G_Loss: 1.149, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.563, D_sup_loss: 0.124, D_sup_acc: 97.14 Train acc: 96.625 Test acc: 97.020 \n",
      "step: 6261 | Train: G_Loss: 1.235, D_unsup_loss_real: 0.517, D_unsup_loss_fake: 0.604, D_sup_loss: 0.131, D_sup_acc: 97.06 Train acc: 96.537 Test acc: 96.830 \n",
      "step: 6262 | Train: G_Loss: 1.176, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.529, D_sup_loss: 0.135, D_sup_acc: 96.87 Train acc: 96.715 Test acc: 96.970 \n",
      "step: 6263 | Train: G_Loss: 1.180, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.538, D_sup_loss: 0.125, D_sup_acc: 97.01 Train acc: 96.705 Test acc: 96.980 \n",
      "step: 6264 | Train: G_Loss: 1.287, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.598, D_sup_loss: 0.127, D_sup_acc: 97.02 Train acc: 96.852 Test acc: 97.120 \n",
      "step: 6265 | Train: G_Loss: 1.103, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.605, D_sup_loss: 0.118, D_sup_acc: 97.16 Train acc: 96.868 Test acc: 97.200 \n",
      "step: 6266 | Train: G_Loss: 1.168, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.576, D_sup_loss: 0.123, D_sup_acc: 97.24 Train acc: 96.773 Test acc: 96.990 \n",
      "step: 6267 | Train: G_Loss: 1.226, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.619, D_sup_loss: 0.128, D_sup_acc: 97.03 Train acc: 96.923 Test acc: 97.230 \n",
      "step: 6268 | Train: G_Loss: 1.157, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.613, D_sup_loss: 0.121, D_sup_acc: 97.27 Train acc: 96.853 Test acc: 97.120 \n",
      "step: 6269 | Train: G_Loss: 1.321, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.572, D_sup_loss: 0.126, D_sup_acc: 97.16 Train acc: 96.772 Test acc: 96.960 \n",
      "step: 6270 | Train: G_Loss: 1.427, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.553, D_sup_loss: 0.130, D_sup_acc: 97.00 Train acc: 96.852 Test acc: 97.050 \n",
      "step: 6271 | Train: G_Loss: 1.282, D_unsup_loss_real: 0.729, D_unsup_loss_fake: 0.592, D_sup_loss: 0.120, D_sup_acc: 97.09 Train acc: 95.850 Test acc: 96.110 \n",
      "step: 6272 | Train: G_Loss: 1.099, D_unsup_loss_real: 0.780, D_unsup_loss_fake: 0.619, D_sup_loss: 0.199, D_sup_acc: 96.16 Train acc: 96.900 Test acc: 97.070 \n",
      "step: 6273 | Train: G_Loss: 1.322, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.590, D_sup_loss: 0.130, D_sup_acc: 97.11 Train acc: 97.113 Test acc: 97.360 \n",
      "step: 6274 | Train: G_Loss: 1.191, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.614, D_sup_loss: 0.114, D_sup_acc: 97.39 Train acc: 97.058 Test acc: 97.310 \n",
      "step: 6275 | Train: G_Loss: 1.221, D_unsup_loss_real: 0.518, D_unsup_loss_fake: 0.645, D_sup_loss: 0.117, D_sup_acc: 97.34 Train acc: 96.940 Test acc: 97.260 \n",
      "step: 6276 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.573, D_sup_loss: 0.123, D_sup_acc: 97.29 Train acc: 97.055 Test acc: 97.350 \n",
      "step: 6277 | Train: G_Loss: 1.220, D_unsup_loss_real: 0.521, D_unsup_loss_fake: 0.580, D_sup_loss: 0.115, D_sup_acc: 97.38 Train acc: 96.943 Test acc: 97.260 \n",
      "step: 6278 | Train: G_Loss: 1.235, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.622, D_sup_loss: 0.123, D_sup_acc: 97.29 Train acc: 97.023 Test acc: 97.310 \n",
      "step: 6279 | Train: G_Loss: 1.202, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.590, D_sup_loss: 0.120, D_sup_acc: 97.34 Train acc: 96.907 Test acc: 97.100 \n",
      "step: 6280 | Train: G_Loss: 1.214, D_unsup_loss_real: 0.516, D_unsup_loss_fake: 0.530, D_sup_loss: 0.126, D_sup_acc: 97.14 Train acc: 96.938 Test acc: 97.140 \n",
      "step: 6281 | Train: G_Loss: 1.164, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.555, D_sup_loss: 0.120, D_sup_acc: 97.18 Train acc: 96.897 Test acc: 97.170 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6282 | Train: G_Loss: 1.208, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.574, D_sup_loss: 0.123, D_sup_acc: 97.21 Train acc: 96.917 Test acc: 97.160 \n",
      "step: 6283 | Train: G_Loss: 1.141, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.577, D_sup_loss: 0.123, D_sup_acc: 97.20 Train acc: 96.793 Test acc: 97.090 \n",
      "step: 6284 | Train: G_Loss: 1.210, D_unsup_loss_real: 0.521, D_unsup_loss_fake: 0.542, D_sup_loss: 0.129, D_sup_acc: 97.13 Train acc: 96.905 Test acc: 97.140 \n",
      "step: 6285 | Train: G_Loss: 1.265, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.559, D_sup_loss: 0.121, D_sup_acc: 97.18 Train acc: 96.975 Test acc: 97.260 \n",
      "step: 6286 | Train: G_Loss: 1.147, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.565, D_sup_loss: 0.119, D_sup_acc: 97.29 Train acc: 96.862 Test acc: 97.010 \n",
      "step: 6287 | Train: G_Loss: 1.198, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.542, D_sup_loss: 0.127, D_sup_acc: 97.05 Train acc: 96.678 Test acc: 96.760 \n",
      "step: 6288 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.510, D_unsup_loss_fake: 0.635, D_sup_loss: 0.136, D_sup_acc: 96.80 Train acc: 96.903 Test acc: 97.140 \n",
      "step: 6289 | Train: G_Loss: 1.227, D_unsup_loss_real: 0.631, D_unsup_loss_fake: 0.564, D_sup_loss: 0.125, D_sup_acc: 97.18 Train acc: 96.942 Test acc: 97.220 \n",
      "step: 6290 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.534, D_sup_loss: 0.123, D_sup_acc: 97.26 Train acc: 96.832 Test acc: 97.030 \n",
      "step: 6291 | Train: G_Loss: 1.153, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.577, D_sup_loss: 0.132, D_sup_acc: 97.07 Train acc: 96.745 Test acc: 96.890 \n",
      "step: 6292 | Train: G_Loss: 1.160, D_unsup_loss_real: 0.653, D_unsup_loss_fake: 0.627, D_sup_loss: 0.135, D_sup_acc: 96.93 Train acc: 96.830 Test acc: 97.100 \n",
      "step: 6293 | Train: G_Loss: 1.137, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.554, D_sup_loss: 0.132, D_sup_acc: 97.14 Train acc: 96.907 Test acc: 97.150 \n",
      "step: 6294 | Train: G_Loss: 1.132, D_unsup_loss_real: 0.527, D_unsup_loss_fake: 0.563, D_sup_loss: 0.124, D_sup_acc: 97.19 Train acc: 96.893 Test acc: 97.100 \n",
      "step: 6295 | Train: G_Loss: 1.234, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.566, D_sup_loss: 0.127, D_sup_acc: 97.14 Train acc: 96.990 Test acc: 97.280 \n",
      "step: 6296 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.675, D_unsup_loss_fake: 0.579, D_sup_loss: 0.117, D_sup_acc: 97.31 Train acc: 96.608 Test acc: 96.760 \n",
      "step: 6297 | Train: G_Loss: 1.171, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.565, D_sup_loss: 0.142, D_sup_acc: 96.80 Train acc: 96.907 Test acc: 97.230 \n",
      "step: 6298 | Train: G_Loss: 1.157, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.571, D_sup_loss: 0.123, D_sup_acc: 97.27 Train acc: 96.988 Test acc: 97.380 \n",
      "step: 6299 | Train: G_Loss: 1.149, D_unsup_loss_real: 0.512, D_unsup_loss_fake: 0.574, D_sup_loss: 0.119, D_sup_acc: 97.41 Train acc: 96.945 Test acc: 97.310 \n",
      "step: 6300 | Train: G_Loss: 1.173, D_unsup_loss_real: 0.537, D_unsup_loss_fake: 0.664, D_sup_loss: 0.121, D_sup_acc: 97.34 Train acc: 96.737 Test acc: 97.010 \n",
      "Train Classifier Accuracy: 96.737%\n",
      "\n",
      "Test Classifier Accuracy: 97.010%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_6300.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_6300.h5\n",
      "step: 6301 | Train: G_Loss: 1.191, D_unsup_loss_real: 0.649, D_unsup_loss_fake: 0.641, D_sup_loss: 0.132, D_sup_acc: 97.05 Train acc: 96.480 Test acc: 96.570 \n",
      "step: 6302 | Train: G_Loss: 1.157, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.580, D_sup_loss: 0.149, D_sup_acc: 96.61 Train acc: 96.947 Test acc: 97.150 \n",
      "step: 6303 | Train: G_Loss: 1.192, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.664, D_sup_loss: 0.126, D_sup_acc: 97.19 Train acc: 96.725 Test acc: 96.980 \n",
      "step: 6304 | Train: G_Loss: 1.223, D_unsup_loss_real: 0.514, D_unsup_loss_fake: 0.569, D_sup_loss: 0.136, D_sup_acc: 97.02 Train acc: 97.022 Test acc: 97.310 \n",
      "step: 6305 | Train: G_Loss: 1.279, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.625, D_sup_loss: 0.115, D_sup_acc: 97.34 Train acc: 97.055 Test acc: 97.320 \n",
      "step: 6306 | Train: G_Loss: 1.297, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.568, D_sup_loss: 0.120, D_sup_acc: 97.35 Train acc: 96.987 Test acc: 97.200 \n",
      "step: 6307 | Train: G_Loss: 1.254, D_unsup_loss_real: 0.631, D_unsup_loss_fake: 0.571, D_sup_loss: 0.126, D_sup_acc: 97.24 Train acc: 97.187 Test acc: 97.450 \n",
      "step: 6308 | Train: G_Loss: 1.244, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.623, D_sup_loss: 0.111, D_sup_acc: 97.48 Train acc: 97.068 Test acc: 97.410 \n",
      "step: 6309 | Train: G_Loss: 1.273, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.547, D_sup_loss: 0.117, D_sup_acc: 97.44 Train acc: 97.048 Test acc: 97.280 \n",
      "step: 6310 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.612, D_sup_loss: 0.119, D_sup_acc: 97.31 Train acc: 97.020 Test acc: 97.250 \n",
      "step: 6311 | Train: G_Loss: 1.122, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.600, D_sup_loss: 0.120, D_sup_acc: 97.28 Train acc: 96.923 Test acc: 97.200 \n",
      "step: 6312 | Train: G_Loss: 1.221, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.578, D_sup_loss: 0.127, D_sup_acc: 97.24 Train acc: 96.798 Test acc: 96.970 \n",
      "step: 6313 | Train: G_Loss: 1.167, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.590, D_sup_loss: 0.135, D_sup_acc: 97.01 Train acc: 96.853 Test acc: 97.110 \n",
      "step: 6314 | Train: G_Loss: 1.183, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.531, D_sup_loss: 0.132, D_sup_acc: 97.15 Train acc: 97.042 Test acc: 97.320 \n",
      "step: 6315 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.590, D_sup_loss: 0.118, D_sup_acc: 97.35 Train acc: 96.943 Test acc: 97.170 \n",
      "step: 6316 | Train: G_Loss: 1.088, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.574, D_sup_loss: 0.128, D_sup_acc: 97.21 Train acc: 96.805 Test acc: 97.050 \n",
      "step: 6317 | Train: G_Loss: 1.104, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.598, D_sup_loss: 0.138, D_sup_acc: 97.09 Train acc: 96.905 Test acc: 97.190 \n",
      "step: 6318 | Train: G_Loss: 1.189, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.548, D_sup_loss: 0.130, D_sup_acc: 97.23 Train acc: 96.963 Test acc: 97.200 \n",
      "step: 6319 | Train: G_Loss: 1.197, D_unsup_loss_real: 0.544, D_unsup_loss_fake: 0.586, D_sup_loss: 0.121, D_sup_acc: 97.24 Train acc: 96.750 Test acc: 96.900 \n",
      "step: 6320 | Train: G_Loss: 1.207, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.578, D_sup_loss: 0.134, D_sup_acc: 96.94 Train acc: 97.023 Test acc: 97.260 \n",
      "step: 6321 | Train: G_Loss: 1.089, D_unsup_loss_real: 0.541, D_unsup_loss_fake: 0.588, D_sup_loss: 0.122, D_sup_acc: 97.29 Train acc: 97.007 Test acc: 97.230 \n",
      "step: 6322 | Train: G_Loss: 1.246, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.539, D_sup_loss: 0.121, D_sup_acc: 97.27 Train acc: 96.958 Test acc: 97.190 \n",
      "step: 6323 | Train: G_Loss: 1.306, D_unsup_loss_real: 0.511, D_unsup_loss_fake: 0.554, D_sup_loss: 0.118, D_sup_acc: 97.23 Train acc: 96.805 Test acc: 97.190 \n",
      "step: 6324 | Train: G_Loss: 1.277, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.586, D_sup_loss: 0.128, D_sup_acc: 97.23 Train acc: 97.028 Test acc: 97.310 \n",
      "step: 6325 | Train: G_Loss: 1.237, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.613, D_sup_loss: 0.122, D_sup_acc: 97.34 Train acc: 96.955 Test acc: 97.220 \n",
      "step: 6326 | Train: G_Loss: 1.234, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.571, D_sup_loss: 0.127, D_sup_acc: 97.26 Train acc: 97.050 Test acc: 97.290 \n",
      "step: 6327 | Train: G_Loss: 1.284, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.631, D_sup_loss: 0.116, D_sup_acc: 97.32 Train acc: 97.063 Test acc: 97.290 \n",
      "step: 6328 | Train: G_Loss: 1.128, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.555, D_sup_loss: 0.120, D_sup_acc: 97.32 Train acc: 96.775 Test acc: 96.950 \n",
      "step: 6329 | Train: G_Loss: 1.177, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.576, D_sup_loss: 0.140, D_sup_acc: 96.99 Train acc: 96.962 Test acc: 97.190 \n",
      "step: 6330 | Train: G_Loss: 1.175, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.570, D_sup_loss: 0.129, D_sup_acc: 97.23 Train acc: 96.868 Test acc: 97.010 \n",
      "step: 6331 | Train: G_Loss: 1.227, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.565, D_sup_loss: 0.134, D_sup_acc: 97.05 Train acc: 96.990 Test acc: 97.110 \n",
      "step: 6332 | Train: G_Loss: 1.173, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.580, D_sup_loss: 0.122, D_sup_acc: 97.15 Train acc: 96.968 Test acc: 97.000 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6333 | Train: G_Loss: 1.183, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.563, D_sup_loss: 0.125, D_sup_acc: 97.04 Train acc: 96.495 Test acc: 96.740 \n",
      "step: 6334 | Train: G_Loss: 1.136, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.579, D_sup_loss: 0.150, D_sup_acc: 96.78 Train acc: 96.787 Test acc: 96.980 \n",
      "step: 6335 | Train: G_Loss: 1.205, D_unsup_loss_real: 0.517, D_unsup_loss_fake: 0.562, D_sup_loss: 0.133, D_sup_acc: 97.02 Train acc: 97.025 Test acc: 97.240 \n",
      "step: 6336 | Train: G_Loss: 1.132, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.558, D_sup_loss: 0.116, D_sup_acc: 97.27 Train acc: 96.917 Test acc: 97.150 \n",
      "step: 6337 | Train: G_Loss: 1.135, D_unsup_loss_real: 0.480, D_unsup_loss_fake: 0.618, D_sup_loss: 0.127, D_sup_acc: 97.19 Train acc: 96.863 Test acc: 97.140 \n",
      "step: 6338 | Train: G_Loss: 1.202, D_unsup_loss_real: 0.545, D_unsup_loss_fake: 0.541, D_sup_loss: 0.131, D_sup_acc: 97.18 Train acc: 96.988 Test acc: 97.250 \n",
      "step: 6339 | Train: G_Loss: 1.256, D_unsup_loss_real: 0.544, D_unsup_loss_fake: 0.593, D_sup_loss: 0.118, D_sup_acc: 97.28 Train acc: 96.853 Test acc: 97.000 \n",
      "step: 6340 | Train: G_Loss: 1.233, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.594, D_sup_loss: 0.126, D_sup_acc: 97.04 Train acc: 97.067 Test acc: 97.360 \n",
      "step: 6341 | Train: G_Loss: 1.223, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.548, D_sup_loss: 0.113, D_sup_acc: 97.39 Train acc: 96.920 Test acc: 97.140 \n",
      "step: 6342 | Train: G_Loss: 1.096, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.567, D_sup_loss: 0.125, D_sup_acc: 97.18 Train acc: 96.932 Test acc: 97.170 \n",
      "step: 6343 | Train: G_Loss: 1.273, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.589, D_sup_loss: 0.124, D_sup_acc: 97.21 Train acc: 97.060 Test acc: 97.250 \n",
      "step: 6344 | Train: G_Loss: 1.193, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.549, D_sup_loss: 0.115, D_sup_acc: 97.28 Train acc: 97.045 Test acc: 97.280 \n",
      "step: 6345 | Train: G_Loss: 1.221, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.596, D_sup_loss: 0.118, D_sup_acc: 97.31 Train acc: 97.093 Test acc: 97.320 \n",
      "step: 6346 | Train: G_Loss: 1.187, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.602, D_sup_loss: 0.116, D_sup_acc: 97.35 Train acc: 96.982 Test acc: 97.280 \n",
      "step: 6347 | Train: G_Loss: 1.100, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.591, D_sup_loss: 0.124, D_sup_acc: 97.31 Train acc: 96.868 Test acc: 97.090 \n",
      "step: 6348 | Train: G_Loss: 1.246, D_unsup_loss_real: 0.504, D_unsup_loss_fake: 0.564, D_sup_loss: 0.134, D_sup_acc: 97.13 Train acc: 96.857 Test acc: 97.080 \n",
      "step: 6349 | Train: G_Loss: 1.296, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.562, D_sup_loss: 0.130, D_sup_acc: 97.12 Train acc: 96.908 Test acc: 97.080 \n",
      "step: 6350 | Train: G_Loss: 1.125, D_unsup_loss_real: 0.700, D_unsup_loss_fake: 0.582, D_sup_loss: 0.130, D_sup_acc: 97.12 Train acc: 96.895 Test acc: 97.090 \n",
      "step: 6351 | Train: G_Loss: 1.233, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.535, D_sup_loss: 0.133, D_sup_acc: 97.13 Train acc: 97.033 Test acc: 97.150 \n",
      "step: 6352 | Train: G_Loss: 1.221, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.552, D_sup_loss: 0.118, D_sup_acc: 97.19 Train acc: 97.002 Test acc: 97.180 \n",
      "step: 6353 | Train: G_Loss: 1.218, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.614, D_sup_loss: 0.121, D_sup_acc: 97.22 Train acc: 96.773 Test acc: 96.950 \n",
      "step: 6354 | Train: G_Loss: 1.234, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.601, D_sup_loss: 0.137, D_sup_acc: 96.99 Train acc: 96.935 Test acc: 97.190 \n",
      "step: 6355 | Train: G_Loss: 1.163, D_unsup_loss_real: 0.528, D_unsup_loss_fake: 0.579, D_sup_loss: 0.124, D_sup_acc: 97.23 Train acc: 96.820 Test acc: 97.030 \n",
      "step: 6356 | Train: G_Loss: 1.192, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.540, D_sup_loss: 0.128, D_sup_acc: 97.07 Train acc: 96.762 Test acc: 97.060 \n",
      "step: 6357 | Train: G_Loss: 1.273, D_unsup_loss_real: 0.502, D_unsup_loss_fake: 0.580, D_sup_loss: 0.126, D_sup_acc: 97.10 Train acc: 96.937 Test acc: 97.150 \n",
      "step: 6358 | Train: G_Loss: 1.170, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.598, D_sup_loss: 0.124, D_sup_acc: 97.19 Train acc: 96.817 Test acc: 97.080 \n",
      "step: 6359 | Train: G_Loss: 1.172, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.561, D_sup_loss: 0.128, D_sup_acc: 97.12 Train acc: 96.818 Test acc: 97.080 \n",
      "step: 6360 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.570, D_sup_loss: 0.130, D_sup_acc: 97.12 Train acc: 96.945 Test acc: 97.080 \n",
      "step: 6361 | Train: G_Loss: 1.287, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.612, D_sup_loss: 0.124, D_sup_acc: 97.12 Train acc: 96.988 Test acc: 97.140 \n",
      "step: 6362 | Train: G_Loss: 1.170, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.600, D_sup_loss: 0.119, D_sup_acc: 97.18 Train acc: 96.922 Test acc: 96.960 \n",
      "step: 6363 | Train: G_Loss: 1.287, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.574, D_sup_loss: 0.128, D_sup_acc: 97.00 Train acc: 96.998 Test acc: 97.160 \n",
      "step: 6364 | Train: G_Loss: 1.290, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.554, D_sup_loss: 0.120, D_sup_acc: 97.20 Train acc: 97.085 Test acc: 97.270 \n",
      "step: 6365 | Train: G_Loss: 1.323, D_unsup_loss_real: 0.682, D_unsup_loss_fake: 0.538, D_sup_loss: 0.109, D_sup_acc: 97.30 Train acc: 96.395 Test acc: 96.470 \n",
      "step: 6366 | Train: G_Loss: 1.115, D_unsup_loss_real: 0.783, D_unsup_loss_fake: 0.598, D_sup_loss: 0.172, D_sup_acc: 96.51 Train acc: 97.118 Test acc: 97.370 \n",
      "step: 6367 | Train: G_Loss: 1.116, D_unsup_loss_real: 0.510, D_unsup_loss_fake: 0.624, D_sup_loss: 0.111, D_sup_acc: 97.40 Train acc: 97.017 Test acc: 97.270 \n",
      "step: 6368 | Train: G_Loss: 1.167, D_unsup_loss_real: 0.527, D_unsup_loss_fake: 0.573, D_sup_loss: 0.117, D_sup_acc: 97.30 Train acc: 96.993 Test acc: 97.210 \n",
      "step: 6369 | Train: G_Loss: 1.161, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.536, D_sup_loss: 0.117, D_sup_acc: 97.25 Train acc: 97.003 Test acc: 97.150 \n",
      "step: 6370 | Train: G_Loss: 1.264, D_unsup_loss_real: 0.524, D_unsup_loss_fake: 0.588, D_sup_loss: 0.120, D_sup_acc: 97.19 Train acc: 96.977 Test acc: 97.190 \n",
      "step: 6371 | Train: G_Loss: 1.183, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.539, D_sup_loss: 0.116, D_sup_acc: 97.23 Train acc: 96.915 Test acc: 97.030 \n",
      "step: 6372 | Train: G_Loss: 1.192, D_unsup_loss_real: 0.520, D_unsup_loss_fake: 0.620, D_sup_loss: 0.123, D_sup_acc: 97.07 Train acc: 97.028 Test acc: 97.250 \n",
      "step: 6373 | Train: G_Loss: 1.106, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.536, D_sup_loss: 0.111, D_sup_acc: 97.28 Train acc: 96.712 Test acc: 96.870 \n",
      "step: 6374 | Train: G_Loss: 1.123, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.550, D_sup_loss: 0.135, D_sup_acc: 96.91 Train acc: 96.882 Test acc: 97.030 \n",
      "step: 6375 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.661, D_unsup_loss_fake: 0.616, D_sup_loss: 0.124, D_sup_acc: 97.07 Train acc: 96.963 Test acc: 97.130 \n",
      "step: 6376 | Train: G_Loss: 1.167, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.579, D_sup_loss: 0.119, D_sup_acc: 97.17 Train acc: 96.930 Test acc: 97.190 \n",
      "step: 6377 | Train: G_Loss: 1.090, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.557, D_sup_loss: 0.120, D_sup_acc: 97.23 Train acc: 96.947 Test acc: 97.260 \n",
      "step: 6378 | Train: G_Loss: 1.137, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.591, D_sup_loss: 0.121, D_sup_acc: 97.29 Train acc: 96.890 Test acc: 97.100 \n",
      "step: 6379 | Train: G_Loss: 1.234, D_unsup_loss_real: 0.526, D_unsup_loss_fake: 0.550, D_sup_loss: 0.129, D_sup_acc: 97.14 Train acc: 96.762 Test acc: 96.870 \n",
      "step: 6380 | Train: G_Loss: 1.168, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.646, D_sup_loss: 0.131, D_sup_acc: 96.91 Train acc: 96.858 Test acc: 97.170 \n",
      "step: 6381 | Train: G_Loss: 1.104, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.585, D_sup_loss: 0.127, D_sup_acc: 97.21 Train acc: 96.748 Test acc: 96.910 \n",
      "step: 6382 | Train: G_Loss: 1.248, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.607, D_sup_loss: 0.137, D_sup_acc: 96.95 Train acc: 96.988 Test acc: 97.250 \n",
      "step: 6383 | Train: G_Loss: 1.143, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.640, D_sup_loss: 0.118, D_sup_acc: 97.28 Train acc: 96.837 Test acc: 97.210 \n",
      "step: 6384 | Train: G_Loss: 1.243, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.540, D_sup_loss: 0.129, D_sup_acc: 97.25 Train acc: 96.823 Test acc: 97.170 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6385 | Train: G_Loss: 1.162, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.504, D_sup_loss: 0.129, D_sup_acc: 97.21 Train acc: 96.958 Test acc: 97.280 \n",
      "step: 6386 | Train: G_Loss: 1.187, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.594, D_sup_loss: 0.121, D_sup_acc: 97.31 Train acc: 96.827 Test acc: 97.250 \n",
      "step: 6387 | Train: G_Loss: 1.350, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.584, D_sup_loss: 0.127, D_sup_acc: 97.28 Train acc: 96.812 Test acc: 97.190 \n",
      "step: 6388 | Train: G_Loss: 1.225, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.589, D_sup_loss: 0.128, D_sup_acc: 97.23 Train acc: 96.738 Test acc: 96.970 \n",
      "step: 6389 | Train: G_Loss: 1.163, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.618, D_sup_loss: 0.133, D_sup_acc: 97.01 Train acc: 97.055 Test acc: 97.380 \n",
      "step: 6390 | Train: G_Loss: 1.150, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.618, D_sup_loss: 0.115, D_sup_acc: 97.41 Train acc: 97.007 Test acc: 97.400 \n",
      "step: 6391 | Train: G_Loss: 1.122, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.544, D_sup_loss: 0.115, D_sup_acc: 97.43 Train acc: 96.387 Test acc: 96.610 \n",
      "step: 6392 | Train: G_Loss: 1.284, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.551, D_sup_loss: 0.155, D_sup_acc: 96.65 Train acc: 96.837 Test acc: 97.090 \n",
      "step: 6393 | Train: G_Loss: 1.235, D_unsup_loss_real: 0.534, D_unsup_loss_fake: 0.613, D_sup_loss: 0.127, D_sup_acc: 97.13 Train acc: 96.897 Test acc: 97.120 \n",
      "step: 6394 | Train: G_Loss: 1.045, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.621, D_sup_loss: 0.124, D_sup_acc: 97.16 Train acc: 96.958 Test acc: 97.290 \n",
      "step: 6395 | Train: G_Loss: 1.192, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.557, D_sup_loss: 0.118, D_sup_acc: 97.32 Train acc: 96.888 Test acc: 97.260 \n",
      "step: 6396 | Train: G_Loss: 1.280, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.558, D_sup_loss: 0.121, D_sup_acc: 97.29 Train acc: 96.960 Test acc: 97.280 \n",
      "step: 6397 | Train: G_Loss: 1.069, D_unsup_loss_real: 0.527, D_unsup_loss_fake: 0.539, D_sup_loss: 0.115, D_sup_acc: 97.31 Train acc: 96.855 Test acc: 97.090 \n",
      "step: 6398 | Train: G_Loss: 1.192, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.542, D_sup_loss: 0.127, D_sup_acc: 97.13 Train acc: 97.027 Test acc: 97.270 \n",
      "step: 6399 | Train: G_Loss: 1.167, D_unsup_loss_real: 0.527, D_unsup_loss_fake: 0.629, D_sup_loss: 0.115, D_sup_acc: 97.30 Train acc: 97.045 Test acc: 97.230 \n",
      "step: 6400 | Train: G_Loss: 1.200, D_unsup_loss_real: 0.523, D_unsup_loss_fake: 0.550, D_sup_loss: 0.113, D_sup_acc: 97.27 Train acc: 96.985 Test acc: 97.260 \n",
      "Train Classifier Accuracy: 96.985%\n",
      "\n",
      "Test Classifier Accuracy: 97.260%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_6400.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_6400.h5\n",
      "step: 6401 | Train: G_Loss: 1.241, D_unsup_loss_real: 0.500, D_unsup_loss_fake: 0.536, D_sup_loss: 0.121, D_sup_acc: 97.29 Train acc: 96.973 Test acc: 97.300 \n",
      "step: 6402 | Train: G_Loss: 1.161, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.573, D_sup_loss: 0.119, D_sup_acc: 97.33 Train acc: 96.940 Test acc: 97.260 \n",
      "step: 6403 | Train: G_Loss: 1.168, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.566, D_sup_loss: 0.120, D_sup_acc: 97.29 Train acc: 96.768 Test acc: 97.080 \n",
      "step: 6404 | Train: G_Loss: 1.100, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.575, D_sup_loss: 0.135, D_sup_acc: 97.12 Train acc: 96.705 Test acc: 97.060 \n",
      "step: 6405 | Train: G_Loss: 1.294, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.636, D_sup_loss: 0.131, D_sup_acc: 97.10 Train acc: 96.858 Test acc: 97.220 \n",
      "step: 6406 | Train: G_Loss: 1.249, D_unsup_loss_real: 0.541, D_unsup_loss_fake: 0.540, D_sup_loss: 0.124, D_sup_acc: 97.26 Train acc: 96.873 Test acc: 97.130 \n",
      "step: 6407 | Train: G_Loss: 1.173, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.560, D_sup_loss: 0.123, D_sup_acc: 97.17 Train acc: 96.848 Test acc: 97.050 \n",
      "step: 6408 | Train: G_Loss: 1.181, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.581, D_sup_loss: 0.124, D_sup_acc: 97.09 Train acc: 96.867 Test acc: 97.110 \n",
      "step: 6409 | Train: G_Loss: 1.205, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.633, D_sup_loss: 0.121, D_sup_acc: 97.15 Train acc: 96.733 Test acc: 97.060 \n",
      "step: 6410 | Train: G_Loss: 1.219, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.569, D_sup_loss: 0.132, D_sup_acc: 97.10 Train acc: 96.830 Test acc: 97.090 \n",
      "step: 6411 | Train: G_Loss: 1.209, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.585, D_sup_loss: 0.124, D_sup_acc: 97.13 Train acc: 96.997 Test acc: 97.260 \n",
      "step: 6412 | Train: G_Loss: 1.150, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.610, D_sup_loss: 0.115, D_sup_acc: 97.29 Train acc: 96.848 Test acc: 97.030 \n",
      "step: 6413 | Train: G_Loss: 1.185, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.552, D_sup_loss: 0.128, D_sup_acc: 97.07 Train acc: 97.013 Test acc: 97.200 \n",
      "step: 6414 | Train: G_Loss: 1.322, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.577, D_sup_loss: 0.115, D_sup_acc: 97.24 Train acc: 96.957 Test acc: 97.170 \n",
      "step: 6415 | Train: G_Loss: 1.232, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.626, D_sup_loss: 0.125, D_sup_acc: 97.21 Train acc: 96.940 Test acc: 97.130 \n",
      "step: 6416 | Train: G_Loss: 1.156, D_unsup_loss_real: 0.497, D_unsup_loss_fake: 0.489, D_sup_loss: 0.126, D_sup_acc: 97.17 Train acc: 97.055 Test acc: 97.180 \n",
      "step: 6417 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.599, D_sup_loss: 0.115, D_sup_acc: 97.22 Train acc: 97.047 Test acc: 97.170 \n",
      "step: 6418 | Train: G_Loss: 1.273, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.596, D_sup_loss: 0.119, D_sup_acc: 97.21 Train acc: 97.003 Test acc: 97.130 \n",
      "step: 6419 | Train: G_Loss: 1.233, D_unsup_loss_real: 0.524, D_unsup_loss_fake: 0.585, D_sup_loss: 0.125, D_sup_acc: 97.17 Train acc: 96.965 Test acc: 97.110 \n",
      "step: 6420 | Train: G_Loss: 1.251, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.598, D_sup_loss: 0.127, D_sup_acc: 97.15 Train acc: 96.990 Test acc: 97.190 \n",
      "step: 6421 | Train: G_Loss: 1.176, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.605, D_sup_loss: 0.120, D_sup_acc: 97.23 Train acc: 96.737 Test acc: 96.970 \n",
      "step: 6422 | Train: G_Loss: 1.220, D_unsup_loss_real: 0.647, D_unsup_loss_fake: 0.577, D_sup_loss: 0.134, D_sup_acc: 97.01 Train acc: 96.903 Test acc: 97.190 \n",
      "step: 6423 | Train: G_Loss: 1.123, D_unsup_loss_real: 0.687, D_unsup_loss_fake: 0.595, D_sup_loss: 0.119, D_sup_acc: 97.23 Train acc: 96.700 Test acc: 97.000 \n",
      "step: 6424 | Train: G_Loss: 1.202, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.596, D_sup_loss: 0.141, D_sup_acc: 97.04 Train acc: 96.865 Test acc: 97.170 \n",
      "step: 6425 | Train: G_Loss: 1.188, D_unsup_loss_real: 0.529, D_unsup_loss_fake: 0.587, D_sup_loss: 0.130, D_sup_acc: 97.21 Train acc: 97.032 Test acc: 97.270 \n",
      "step: 6426 | Train: G_Loss: 1.187, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.590, D_sup_loss: 0.117, D_sup_acc: 97.30 Train acc: 96.753 Test acc: 97.030 \n",
      "step: 6427 | Train: G_Loss: 1.201, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.556, D_sup_loss: 0.131, D_sup_acc: 97.07 Train acc: 96.727 Test acc: 97.010 \n",
      "step: 6428 | Train: G_Loss: 1.178, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.581, D_sup_loss: 0.130, D_sup_acc: 97.05 Train acc: 96.967 Test acc: 97.190 \n",
      "step: 6429 | Train: G_Loss: 1.210, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.594, D_sup_loss: 0.120, D_sup_acc: 97.23 Train acc: 96.958 Test acc: 97.230 \n",
      "step: 6430 | Train: G_Loss: 1.188, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.564, D_sup_loss: 0.122, D_sup_acc: 97.27 Train acc: 96.925 Test acc: 97.180 \n",
      "step: 6431 | Train: G_Loss: 1.288, D_unsup_loss_real: 0.458, D_unsup_loss_fake: 0.599, D_sup_loss: 0.126, D_sup_acc: 97.22 Train acc: 97.055 Test acc: 97.230 \n",
      "step: 6432 | Train: G_Loss: 1.073, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.615, D_sup_loss: 0.113, D_sup_acc: 97.27 Train acc: 96.965 Test acc: 97.130 \n",
      "step: 6433 | Train: G_Loss: 1.253, D_unsup_loss_real: 0.529, D_unsup_loss_fake: 0.548, D_sup_loss: 0.120, D_sup_acc: 97.17 Train acc: 96.997 Test acc: 97.240 \n",
      "step: 6434 | Train: G_Loss: 1.118, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.585, D_sup_loss: 0.117, D_sup_acc: 97.27 Train acc: 96.873 Test acc: 97.060 \n",
      "step: 6435 | Train: G_Loss: 1.123, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.581, D_sup_loss: 0.125, D_sup_acc: 97.10 Train acc: 96.893 Test acc: 97.110 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6436 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.677, D_unsup_loss_fake: 0.531, D_sup_loss: 0.122, D_sup_acc: 97.15 Train acc: 96.850 Test acc: 97.060 \n",
      "step: 6437 | Train: G_Loss: 1.227, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.586, D_sup_loss: 0.129, D_sup_acc: 97.10 Train acc: 96.960 Test acc: 97.250 \n",
      "step: 6438 | Train: G_Loss: 1.203, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.665, D_sup_loss: 0.121, D_sup_acc: 97.28 Train acc: 96.868 Test acc: 97.140 \n",
      "step: 6439 | Train: G_Loss: 1.107, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.591, D_sup_loss: 0.127, D_sup_acc: 97.18 Train acc: 96.758 Test acc: 97.070 \n",
      "step: 6440 | Train: G_Loss: 1.170, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.557, D_sup_loss: 0.131, D_sup_acc: 97.11 Train acc: 96.822 Test acc: 97.100 \n",
      "step: 6441 | Train: G_Loss: 1.194, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.613, D_sup_loss: 0.128, D_sup_acc: 97.14 Train acc: 96.888 Test acc: 97.180 \n",
      "step: 6442 | Train: G_Loss: 1.210, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.635, D_sup_loss: 0.123, D_sup_acc: 97.22 Train acc: 96.818 Test acc: 97.180 \n",
      "step: 6443 | Train: G_Loss: 1.260, D_unsup_loss_real: 0.529, D_unsup_loss_fake: 0.528, D_sup_loss: 0.129, D_sup_acc: 97.22 Train acc: 96.988 Test acc: 97.310 \n",
      "step: 6444 | Train: G_Loss: 1.278, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.586, D_sup_loss: 0.119, D_sup_acc: 97.34 Train acc: 96.978 Test acc: 97.310 \n",
      "step: 6445 | Train: G_Loss: 1.180, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.526, D_sup_loss: 0.125, D_sup_acc: 97.34 Train acc: 97.002 Test acc: 97.380 \n",
      "step: 6446 | Train: G_Loss: 1.058, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.602, D_sup_loss: 0.123, D_sup_acc: 97.41 Train acc: 97.047 Test acc: 97.310 \n",
      "step: 6447 | Train: G_Loss: 1.225, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.596, D_sup_loss: 0.120, D_sup_acc: 97.34 Train acc: 97.033 Test acc: 97.300 \n",
      "step: 6448 | Train: G_Loss: 1.202, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.580, D_sup_loss: 0.118, D_sup_acc: 97.33 Train acc: 96.983 Test acc: 97.340 \n",
      "step: 6449 | Train: G_Loss: 1.193, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.574, D_sup_loss: 0.128, D_sup_acc: 97.37 Train acc: 97.070 Test acc: 97.430 \n",
      "step: 6450 | Train: G_Loss: 1.204, D_unsup_loss_real: 0.533, D_unsup_loss_fake: 0.611, D_sup_loss: 0.119, D_sup_acc: 97.46 Train acc: 97.032 Test acc: 97.330 \n",
      "step: 6451 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.524, D_unsup_loss_fake: 0.506, D_sup_loss: 0.124, D_sup_acc: 97.36 Train acc: 97.112 Test acc: 97.310 \n",
      "step: 6452 | Train: G_Loss: 1.151, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.553, D_sup_loss: 0.115, D_sup_acc: 97.34 Train acc: 97.007 Test acc: 97.300 \n",
      "step: 6453 | Train: G_Loss: 1.180, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.623, D_sup_loss: 0.120, D_sup_acc: 97.33 Train acc: 97.138 Test acc: 97.380 \n",
      "step: 6454 | Train: G_Loss: 1.170, D_unsup_loss_real: 0.664, D_unsup_loss_fake: 0.617, D_sup_loss: 0.110, D_sup_acc: 97.41 Train acc: 96.893 Test acc: 97.260 \n",
      "step: 6455 | Train: G_Loss: 1.073, D_unsup_loss_real: 0.526, D_unsup_loss_fake: 0.561, D_sup_loss: 0.128, D_sup_acc: 97.29 Train acc: 96.875 Test acc: 97.130 \n",
      "step: 6456 | Train: G_Loss: 1.207, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.579, D_sup_loss: 0.126, D_sup_acc: 97.17 Train acc: 96.905 Test acc: 97.120 \n",
      "step: 6457 | Train: G_Loss: 1.130, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.625, D_sup_loss: 0.127, D_sup_acc: 97.16 Train acc: 97.013 Test acc: 97.130 \n",
      "step: 6458 | Train: G_Loss: 1.206, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.636, D_sup_loss: 0.116, D_sup_acc: 97.17 Train acc: 96.840 Test acc: 97.120 \n",
      "step: 6459 | Train: G_Loss: 1.341, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.499, D_sup_loss: 0.129, D_sup_acc: 97.16 Train acc: 96.940 Test acc: 97.220 \n",
      "step: 6460 | Train: G_Loss: 1.252, D_unsup_loss_real: 0.658, D_unsup_loss_fake: 0.584, D_sup_loss: 0.118, D_sup_acc: 97.26 Train acc: 96.730 Test acc: 97.070 \n",
      "step: 6461 | Train: G_Loss: 1.195, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.585, D_sup_loss: 0.142, D_sup_acc: 97.11 Train acc: 97.003 Test acc: 97.100 \n",
      "step: 6462 | Train: G_Loss: 1.152, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.581, D_sup_loss: 0.121, D_sup_acc: 97.14 Train acc: 96.955 Test acc: 97.090 \n",
      "step: 6463 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.618, D_sup_loss: 0.124, D_sup_acc: 97.13 Train acc: 97.005 Test acc: 97.210 \n",
      "step: 6464 | Train: G_Loss: 1.151, D_unsup_loss_real: 0.541, D_unsup_loss_fake: 0.520, D_sup_loss: 0.120, D_sup_acc: 97.25 Train acc: 96.827 Test acc: 96.990 \n",
      "step: 6465 | Train: G_Loss: 1.191, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.521, D_sup_loss: 0.130, D_sup_acc: 97.03 Train acc: 97.028 Test acc: 97.150 \n",
      "step: 6466 | Train: G_Loss: 1.099, D_unsup_loss_real: 0.550, D_unsup_loss_fake: 0.615, D_sup_loss: 0.116, D_sup_acc: 97.19 Train acc: 96.832 Test acc: 97.130 \n",
      "step: 6467 | Train: G_Loss: 1.209, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.577, D_sup_loss: 0.129, D_sup_acc: 97.17 Train acc: 96.962 Test acc: 97.170 \n",
      "step: 6468 | Train: G_Loss: 1.187, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.544, D_sup_loss: 0.122, D_sup_acc: 97.21 Train acc: 97.043 Test acc: 97.280 \n",
      "step: 6469 | Train: G_Loss: 1.153, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.585, D_sup_loss: 0.116, D_sup_acc: 97.31 Train acc: 96.702 Test acc: 96.960 \n",
      "step: 6470 | Train: G_Loss: 1.172, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.589, D_sup_loss: 0.134, D_sup_acc: 97.00 Train acc: 96.612 Test acc: 96.870 \n",
      "step: 6471 | Train: G_Loss: 1.338, D_unsup_loss_real: 0.653, D_unsup_loss_fake: 0.574, D_sup_loss: 0.134, D_sup_acc: 96.91 Train acc: 96.892 Test acc: 97.220 \n",
      "step: 6472 | Train: G_Loss: 1.184, D_unsup_loss_real: 0.546, D_unsup_loss_fake: 0.521, D_sup_loss: 0.119, D_sup_acc: 97.26 Train acc: 96.792 Test acc: 97.130 \n",
      "step: 6473 | Train: G_Loss: 1.184, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.482, D_sup_loss: 0.127, D_sup_acc: 97.17 Train acc: 96.672 Test acc: 97.040 \n",
      "step: 6474 | Train: G_Loss: 1.197, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.596, D_sup_loss: 0.128, D_sup_acc: 97.08 Train acc: 96.810 Test acc: 97.130 \n",
      "step: 6475 | Train: G_Loss: 1.259, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.578, D_sup_loss: 0.122, D_sup_acc: 97.17 Train acc: 96.868 Test acc: 97.160 \n",
      "step: 6476 | Train: G_Loss: 1.184, D_unsup_loss_real: 0.514, D_unsup_loss_fake: 0.567, D_sup_loss: 0.123, D_sup_acc: 97.20 Train acc: 96.920 Test acc: 97.150 \n",
      "step: 6477 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.561, D_sup_loss: 0.114, D_sup_acc: 97.19 Train acc: 96.822 Test acc: 97.080 \n",
      "step: 6478 | Train: G_Loss: 1.198, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.568, D_sup_loss: 0.121, D_sup_acc: 97.12 Train acc: 96.843 Test acc: 97.190 \n",
      "step: 6479 | Train: G_Loss: 1.046, D_unsup_loss_real: 0.514, D_unsup_loss_fake: 0.599, D_sup_loss: 0.118, D_sup_acc: 97.23 Train acc: 96.942 Test acc: 97.220 \n",
      "step: 6480 | Train: G_Loss: 1.153, D_unsup_loss_real: 0.526, D_unsup_loss_fake: 0.538, D_sup_loss: 0.116, D_sup_acc: 97.26 Train acc: 96.940 Test acc: 97.300 \n",
      "step: 6481 | Train: G_Loss: 1.200, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.621, D_sup_loss: 0.116, D_sup_acc: 97.33 Train acc: 97.043 Test acc: 97.280 \n",
      "step: 6482 | Train: G_Loss: 1.279, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.547, D_sup_loss: 0.115, D_sup_acc: 97.31 Train acc: 96.983 Test acc: 97.200 \n",
      "step: 6483 | Train: G_Loss: 1.275, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.565, D_sup_loss: 0.119, D_sup_acc: 97.24 Train acc: 96.885 Test acc: 97.240 \n",
      "step: 6484 | Train: G_Loss: 1.139, D_unsup_loss_real: 0.631, D_unsup_loss_fake: 0.539, D_sup_loss: 0.121, D_sup_acc: 97.27 Train acc: 96.890 Test acc: 97.180 \n",
      "step: 6485 | Train: G_Loss: 1.177, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.584, D_sup_loss: 0.126, D_sup_acc: 97.22 Train acc: 96.970 Test acc: 97.290 \n",
      "step: 6486 | Train: G_Loss: 1.176, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.543, D_sup_loss: 0.116, D_sup_acc: 97.32 Train acc: 96.928 Test acc: 97.320 \n",
      "step: 6487 | Train: G_Loss: 1.221, D_unsup_loss_real: 0.533, D_unsup_loss_fake: 0.625, D_sup_loss: 0.118, D_sup_acc: 97.35 Train acc: 96.763 Test acc: 97.030 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6488 | Train: G_Loss: 1.216, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.530, D_sup_loss: 0.127, D_sup_acc: 97.07 Train acc: 97.007 Test acc: 97.320 \n",
      "step: 6489 | Train: G_Loss: 1.184, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.598, D_sup_loss: 0.116, D_sup_acc: 97.35 Train acc: 96.953 Test acc: 97.240 \n",
      "step: 6490 | Train: G_Loss: 1.204, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.567, D_sup_loss: 0.120, D_sup_acc: 97.27 Train acc: 96.858 Test acc: 97.220 \n",
      "step: 6491 | Train: G_Loss: 1.307, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.553, D_sup_loss: 0.129, D_sup_acc: 97.26 Train acc: 97.045 Test acc: 97.310 \n",
      "step: 6492 | Train: G_Loss: 1.201, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.643, D_sup_loss: 0.112, D_sup_acc: 97.34 Train acc: 97.065 Test acc: 97.350 \n",
      "step: 6493 | Train: G_Loss: 1.169, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.619, D_sup_loss: 0.112, D_sup_acc: 97.38 Train acc: 96.930 Test acc: 97.270 \n",
      "step: 6494 | Train: G_Loss: 1.280, D_unsup_loss_real: 0.664, D_unsup_loss_fake: 0.536, D_sup_loss: 0.125, D_sup_acc: 97.30 Train acc: 96.925 Test acc: 97.280 \n",
      "step: 6495 | Train: G_Loss: 1.201, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.596, D_sup_loss: 0.122, D_sup_acc: 97.31 Train acc: 96.935 Test acc: 97.220 \n",
      "step: 6496 | Train: G_Loss: 1.195, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.581, D_sup_loss: 0.119, D_sup_acc: 97.26 Train acc: 96.877 Test acc: 97.270 \n",
      "step: 6497 | Train: G_Loss: 1.227, D_unsup_loss_real: 0.665, D_unsup_loss_fake: 0.589, D_sup_loss: 0.123, D_sup_acc: 97.30 Train acc: 96.760 Test acc: 97.110 \n",
      "step: 6498 | Train: G_Loss: 1.176, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.589, D_sup_loss: 0.131, D_sup_acc: 97.15 Train acc: 96.348 Test acc: 96.680 \n",
      "step: 6499 | Train: G_Loss: 1.261, D_unsup_loss_real: 0.544, D_unsup_loss_fake: 0.534, D_sup_loss: 0.147, D_sup_acc: 96.72 Train acc: 96.785 Test acc: 97.130 \n",
      "step: 6500 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.596, D_sup_loss: 0.125, D_sup_acc: 97.17 Train acc: 96.905 Test acc: 97.270 \n",
      "Train Classifier Accuracy: 96.905%\n",
      "\n",
      "Test Classifier Accuracy: 97.270%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_6500.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_6500.h5\n",
      "step: 6501 | Train: G_Loss: 1.210, D_unsup_loss_real: 0.538, D_unsup_loss_fake: 0.564, D_sup_loss: 0.120, D_sup_acc: 97.30 Train acc: 96.908 Test acc: 97.240 \n",
      "step: 6502 | Train: G_Loss: 1.197, D_unsup_loss_real: 0.510, D_unsup_loss_fake: 0.574, D_sup_loss: 0.120, D_sup_acc: 97.27 Train acc: 96.830 Test acc: 97.100 \n",
      "step: 6503 | Train: G_Loss: 1.250, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.495, D_sup_loss: 0.123, D_sup_acc: 97.14 Train acc: 96.905 Test acc: 97.180 \n",
      "step: 6504 | Train: G_Loss: 1.219, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.584, D_sup_loss: 0.119, D_sup_acc: 97.22 Train acc: 97.007 Test acc: 97.310 \n",
      "step: 6505 | Train: G_Loss: 1.180, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.599, D_sup_loss: 0.115, D_sup_acc: 97.34 Train acc: 96.892 Test acc: 97.280 \n",
      "step: 6506 | Train: G_Loss: 1.168, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.611, D_sup_loss: 0.125, D_sup_acc: 97.31 Train acc: 96.937 Test acc: 97.260 \n",
      "step: 6507 | Train: G_Loss: 1.331, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.528, D_sup_loss: 0.121, D_sup_acc: 97.29 Train acc: 97.113 Test acc: 97.290 \n",
      "step: 6508 | Train: G_Loss: 1.191, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.485, D_sup_loss: 0.111, D_sup_acc: 97.32 Train acc: 97.037 Test acc: 97.300 \n",
      "step: 6509 | Train: G_Loss: 1.154, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.530, D_sup_loss: 0.122, D_sup_acc: 97.33 Train acc: 97.047 Test acc: 97.260 \n",
      "step: 6510 | Train: G_Loss: 1.067, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.600, D_sup_loss: 0.117, D_sup_acc: 97.29 Train acc: 96.772 Test acc: 96.900 \n",
      "step: 6511 | Train: G_Loss: 1.197, D_unsup_loss_real: 0.530, D_unsup_loss_fake: 0.550, D_sup_loss: 0.132, D_sup_acc: 96.94 Train acc: 96.932 Test acc: 97.120 \n",
      "step: 6512 | Train: G_Loss: 1.065, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.618, D_sup_loss: 0.119, D_sup_acc: 97.16 Train acc: 96.593 Test acc: 96.810 \n",
      "step: 6513 | Train: G_Loss: 1.159, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.523, D_sup_loss: 0.138, D_sup_acc: 96.85 Train acc: 97.088 Test acc: 97.300 \n",
      "step: 6514 | Train: G_Loss: 1.055, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.596, D_sup_loss: 0.112, D_sup_acc: 97.33 Train acc: 96.967 Test acc: 97.250 \n",
      "step: 6515 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.504, D_unsup_loss_fake: 0.569, D_sup_loss: 0.119, D_sup_acc: 97.28 Train acc: 97.137 Test acc: 97.230 \n",
      "step: 6516 | Train: G_Loss: 1.209, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.571, D_sup_loss: 0.112, D_sup_acc: 97.27 Train acc: 96.757 Test acc: 97.000 \n",
      "step: 6517 | Train: G_Loss: 1.149, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.596, D_sup_loss: 0.131, D_sup_acc: 97.04 Train acc: 97.050 Test acc: 97.200 \n",
      "step: 6518 | Train: G_Loss: 1.072, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.548, D_sup_loss: 0.114, D_sup_acc: 97.24 Train acc: 97.020 Test acc: 97.220 \n",
      "step: 6519 | Train: G_Loss: 1.231, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.623, D_sup_loss: 0.114, D_sup_acc: 97.26 Train acc: 97.018 Test acc: 97.270 \n",
      "step: 6520 | Train: G_Loss: 1.179, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.602, D_sup_loss: 0.115, D_sup_acc: 97.30 Train acc: 96.885 Test acc: 97.100 \n",
      "step: 6521 | Train: G_Loss: 1.207, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.565, D_sup_loss: 0.126, D_sup_acc: 97.14 Train acc: 96.975 Test acc: 97.190 \n",
      "step: 6522 | Train: G_Loss: 1.130, D_unsup_loss_real: 0.505, D_unsup_loss_fake: 0.598, D_sup_loss: 0.120, D_sup_acc: 97.23 Train acc: 96.957 Test acc: 97.280 \n",
      "step: 6523 | Train: G_Loss: 1.151, D_unsup_loss_real: 0.524, D_unsup_loss_fake: 0.510, D_sup_loss: 0.124, D_sup_acc: 97.31 Train acc: 97.107 Test acc: 97.340 \n",
      "step: 6524 | Train: G_Loss: 1.161, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.593, D_sup_loss: 0.110, D_sup_acc: 97.37 Train acc: 96.900 Test acc: 97.160 \n",
      "step: 6525 | Train: G_Loss: 1.361, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.538, D_sup_loss: 0.123, D_sup_acc: 97.20 Train acc: 96.825 Test acc: 96.920 \n",
      "step: 6526 | Train: G_Loss: 1.276, D_unsup_loss_real: 0.659, D_unsup_loss_fake: 0.666, D_sup_loss: 0.124, D_sup_acc: 96.96 Train acc: 96.545 Test acc: 96.780 \n",
      "step: 6527 | Train: G_Loss: 1.216, D_unsup_loss_real: 0.713, D_unsup_loss_fake: 0.617, D_sup_loss: 0.158, D_sup_acc: 96.82 Train acc: 96.998 Test acc: 97.240 \n",
      "step: 6528 | Train: G_Loss: 1.267, D_unsup_loss_real: 0.536, D_unsup_loss_fake: 0.621, D_sup_loss: 0.112, D_sup_acc: 97.27 Train acc: 96.928 Test acc: 97.300 \n",
      "step: 6529 | Train: G_Loss: 1.127, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.591, D_sup_loss: 0.117, D_sup_acc: 97.33 Train acc: 96.790 Test acc: 97.040 \n",
      "step: 6530 | Train: G_Loss: 1.187, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.560, D_sup_loss: 0.126, D_sup_acc: 97.08 Train acc: 96.925 Test acc: 97.250 \n",
      "step: 6531 | Train: G_Loss: 1.218, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.573, D_sup_loss: 0.117, D_sup_acc: 97.28 Train acc: 96.935 Test acc: 97.280 \n",
      "step: 6532 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.551, D_sup_loss: 0.117, D_sup_acc: 97.31 Train acc: 96.897 Test acc: 97.220 \n",
      "step: 6533 | Train: G_Loss: 1.206, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.574, D_sup_loss: 0.120, D_sup_acc: 97.26 Train acc: 96.942 Test acc: 97.360 \n",
      "step: 6534 | Train: G_Loss: 1.122, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.598, D_sup_loss: 0.116, D_sup_acc: 97.39 Train acc: 96.783 Test acc: 97.210 \n",
      "step: 6535 | Train: G_Loss: 1.158, D_unsup_loss_real: 0.528, D_unsup_loss_fake: 0.562, D_sup_loss: 0.125, D_sup_acc: 97.25 Train acc: 96.930 Test acc: 97.300 \n",
      "step: 6536 | Train: G_Loss: 1.069, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.637, D_sup_loss: 0.115, D_sup_acc: 97.33 Train acc: 96.787 Test acc: 97.090 \n",
      "step: 6537 | Train: G_Loss: 1.157, D_unsup_loss_real: 0.520, D_unsup_loss_fake: 0.548, D_sup_loss: 0.122, D_sup_acc: 97.13 Train acc: 96.698 Test acc: 96.950 \n",
      "step: 6538 | Train: G_Loss: 1.201, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.559, D_sup_loss: 0.129, D_sup_acc: 96.99 Train acc: 96.857 Test acc: 97.110 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6539 | Train: G_Loss: 1.227, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.626, D_sup_loss: 0.122, D_sup_acc: 97.15 Train acc: 96.878 Test acc: 97.080 \n",
      "step: 6540 | Train: G_Loss: 1.236, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.551, D_sup_loss: 0.125, D_sup_acc: 97.12 Train acc: 97.017 Test acc: 97.320 \n",
      "step: 6541 | Train: G_Loss: 1.156, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.563, D_sup_loss: 0.116, D_sup_acc: 97.35 Train acc: 96.772 Test acc: 96.960 \n",
      "step: 6542 | Train: G_Loss: 1.276, D_unsup_loss_real: 0.517, D_unsup_loss_fake: 0.553, D_sup_loss: 0.130, D_sup_acc: 97.00 Train acc: 96.932 Test acc: 97.130 \n",
      "step: 6543 | Train: G_Loss: 1.211, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.657, D_sup_loss: 0.119, D_sup_acc: 97.17 Train acc: 96.498 Test acc: 96.740 \n",
      "step: 6544 | Train: G_Loss: 1.161, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.570, D_sup_loss: 0.140, D_sup_acc: 96.78 Train acc: 96.933 Test acc: 97.120 \n",
      "step: 6545 | Train: G_Loss: 1.160, D_unsup_loss_real: 0.657, D_unsup_loss_fake: 0.520, D_sup_loss: 0.120, D_sup_acc: 97.16 Train acc: 96.907 Test acc: 97.230 \n",
      "step: 6546 | Train: G_Loss: 1.298, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.603, D_sup_loss: 0.122, D_sup_acc: 97.27 Train acc: 97.052 Test acc: 97.210 \n",
      "step: 6547 | Train: G_Loss: 1.202, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.594, D_sup_loss: 0.111, D_sup_acc: 97.25 Train acc: 96.770 Test acc: 97.200 \n",
      "step: 6548 | Train: G_Loss: 1.172, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.586, D_sup_loss: 0.131, D_sup_acc: 97.24 Train acc: 96.882 Test acc: 97.270 \n",
      "step: 6549 | Train: G_Loss: 1.259, D_unsup_loss_real: 0.518, D_unsup_loss_fake: 0.541, D_sup_loss: 0.126, D_sup_acc: 97.30 Train acc: 97.035 Test acc: 97.290 \n",
      "step: 6550 | Train: G_Loss: 1.215, D_unsup_loss_real: 0.657, D_unsup_loss_fake: 0.561, D_sup_loss: 0.114, D_sup_acc: 97.32 Train acc: 96.832 Test acc: 97.060 \n",
      "step: 6551 | Train: G_Loss: 1.226, D_unsup_loss_real: 0.658, D_unsup_loss_fake: 0.587, D_sup_loss: 0.123, D_sup_acc: 97.10 Train acc: 96.955 Test acc: 97.230 \n",
      "step: 6552 | Train: G_Loss: 1.181, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.642, D_sup_loss: 0.123, D_sup_acc: 97.27 Train acc: 96.933 Test acc: 97.100 \n",
      "step: 6553 | Train: G_Loss: 1.192, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.513, D_sup_loss: 0.123, D_sup_acc: 97.14 Train acc: 96.978 Test acc: 97.160 \n",
      "step: 6554 | Train: G_Loss: 1.210, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.565, D_sup_loss: 0.114, D_sup_acc: 97.20 Train acc: 96.753 Test acc: 96.890 \n",
      "step: 6555 | Train: G_Loss: 1.090, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.535, D_sup_loss: 0.130, D_sup_acc: 96.93 Train acc: 96.965 Test acc: 97.200 \n",
      "step: 6556 | Train: G_Loss: 1.281, D_unsup_loss_real: 0.524, D_unsup_loss_fake: 0.546, D_sup_loss: 0.121, D_sup_acc: 97.24 Train acc: 97.042 Test acc: 97.290 \n",
      "step: 6557 | Train: G_Loss: 1.178, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.506, D_sup_loss: 0.114, D_sup_acc: 97.32 Train acc: 97.022 Test acc: 97.270 \n",
      "step: 6558 | Train: G_Loss: 1.160, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.600, D_sup_loss: 0.112, D_sup_acc: 97.30 Train acc: 96.742 Test acc: 96.970 \n",
      "step: 6559 | Train: G_Loss: 1.172, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.578, D_sup_loss: 0.132, D_sup_acc: 97.01 Train acc: 96.882 Test acc: 97.110 \n",
      "step: 6560 | Train: G_Loss: 1.273, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.556, D_sup_loss: 0.122, D_sup_acc: 97.15 Train acc: 96.915 Test acc: 97.190 \n",
      "step: 6561 | Train: G_Loss: 1.151, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.553, D_sup_loss: 0.116, D_sup_acc: 97.23 Train acc: 96.850 Test acc: 97.050 \n",
      "step: 6562 | Train: G_Loss: 1.249, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.587, D_sup_loss: 0.122, D_sup_acc: 97.09 Train acc: 96.917 Test acc: 97.180 \n",
      "step: 6563 | Train: G_Loss: 1.217, D_unsup_loss_real: 0.502, D_unsup_loss_fake: 0.677, D_sup_loss: 0.118, D_sup_acc: 97.22 Train acc: 96.878 Test acc: 97.210 \n",
      "step: 6564 | Train: G_Loss: 1.189, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.629, D_sup_loss: 0.123, D_sup_acc: 97.25 Train acc: 96.647 Test acc: 96.850 \n",
      "step: 6565 | Train: G_Loss: 1.185, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.554, D_sup_loss: 0.138, D_sup_acc: 96.89 Train acc: 96.883 Test acc: 97.060 \n",
      "step: 6566 | Train: G_Loss: 1.250, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.526, D_sup_loss: 0.125, D_sup_acc: 97.10 Train acc: 96.992 Test acc: 97.220 \n",
      "step: 6567 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.592, D_sup_loss: 0.118, D_sup_acc: 97.26 Train acc: 96.940 Test acc: 97.140 \n",
      "step: 6568 | Train: G_Loss: 1.342, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.521, D_sup_loss: 0.122, D_sup_acc: 97.18 Train acc: 96.898 Test acc: 97.180 \n",
      "step: 6569 | Train: G_Loss: 1.157, D_unsup_loss_real: 0.475, D_unsup_loss_fake: 0.601, D_sup_loss: 0.120, D_sup_acc: 97.22 Train acc: 96.803 Test acc: 97.010 \n",
      "step: 6570 | Train: G_Loss: 1.178, D_unsup_loss_real: 0.536, D_unsup_loss_fake: 0.503, D_sup_loss: 0.130, D_sup_acc: 97.05 Train acc: 96.905 Test acc: 97.210 \n",
      "step: 6571 | Train: G_Loss: 1.253, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.638, D_sup_loss: 0.120, D_sup_acc: 97.25 Train acc: 96.893 Test acc: 97.070 \n",
      "step: 6572 | Train: G_Loss: 1.338, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.527, D_sup_loss: 0.123, D_sup_acc: 97.11 Train acc: 97.115 Test acc: 97.360 \n",
      "step: 6573 | Train: G_Loss: 1.231, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.664, D_sup_loss: 0.110, D_sup_acc: 97.39 Train acc: 96.917 Test acc: 97.070 \n",
      "step: 6574 | Train: G_Loss: 1.262, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.575, D_sup_loss: 0.126, D_sup_acc: 97.11 Train acc: 97.063 Test acc: 97.190 \n",
      "step: 6575 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.593, D_sup_loss: 0.111, D_sup_acc: 97.23 Train acc: 96.887 Test acc: 97.010 \n",
      "step: 6576 | Train: G_Loss: 1.260, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.555, D_sup_loss: 0.125, D_sup_acc: 97.05 Train acc: 97.042 Test acc: 97.210 \n",
      "step: 6577 | Train: G_Loss: 1.276, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.585, D_sup_loss: 0.116, D_sup_acc: 97.25 Train acc: 96.932 Test acc: 97.150 \n",
      "step: 6578 | Train: G_Loss: 1.063, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.526, D_sup_loss: 0.125, D_sup_acc: 97.19 Train acc: 96.938 Test acc: 97.120 \n",
      "step: 6579 | Train: G_Loss: 1.203, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.650, D_sup_loss: 0.121, D_sup_acc: 97.16 Train acc: 97.030 Test acc: 97.300 \n",
      "step: 6580 | Train: G_Loss: 1.247, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.584, D_sup_loss: 0.120, D_sup_acc: 97.33 Train acc: 97.012 Test acc: 97.280 \n",
      "step: 6581 | Train: G_Loss: 1.225, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.549, D_sup_loss: 0.121, D_sup_acc: 97.31 Train acc: 96.942 Test acc: 97.190 \n",
      "step: 6582 | Train: G_Loss: 1.224, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.561, D_sup_loss: 0.124, D_sup_acc: 97.23 Train acc: 97.022 Test acc: 97.240 \n",
      "step: 6583 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.541, D_unsup_loss_fake: 0.639, D_sup_loss: 0.116, D_sup_acc: 97.27 Train acc: 96.825 Test acc: 97.050 \n",
      "step: 6584 | Train: G_Loss: 1.260, D_unsup_loss_real: 0.500, D_unsup_loss_fake: 0.559, D_sup_loss: 0.133, D_sup_acc: 97.09 Train acc: 96.900 Test acc: 97.120 \n",
      "step: 6585 | Train: G_Loss: 1.243, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.561, D_sup_loss: 0.127, D_sup_acc: 97.16 Train acc: 96.638 Test acc: 96.840 \n",
      "step: 6586 | Train: G_Loss: 1.318, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.586, D_sup_loss: 0.143, D_sup_acc: 96.88 Train acc: 96.888 Test acc: 97.160 \n",
      "step: 6587 | Train: G_Loss: 1.259, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.579, D_sup_loss: 0.127, D_sup_acc: 97.20 Train acc: 96.933 Test acc: 97.200 \n",
      "step: 6588 | Train: G_Loss: 1.241, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.566, D_sup_loss: 0.126, D_sup_acc: 97.24 Train acc: 96.930 Test acc: 97.250 \n",
      "step: 6589 | Train: G_Loss: 1.240, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.565, D_sup_loss: 0.123, D_sup_acc: 97.28 Train acc: 97.025 Test acc: 97.340 \n",
      "step: 6590 | Train: G_Loss: 1.179, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.569, D_sup_loss: 0.117, D_sup_acc: 97.37 Train acc: 96.730 Test acc: 97.110 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6591 | Train: G_Loss: 1.236, D_unsup_loss_real: 0.649, D_unsup_loss_fake: 0.561, D_sup_loss: 0.134, D_sup_acc: 97.15 Train acc: 96.942 Test acc: 97.160 \n",
      "step: 6592 | Train: G_Loss: 1.243, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.579, D_sup_loss: 0.118, D_sup_acc: 97.20 Train acc: 96.945 Test acc: 97.160 \n",
      "step: 6593 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.536, D_sup_loss: 0.120, D_sup_acc: 97.20 Train acc: 96.807 Test acc: 97.010 \n",
      "step: 6594 | Train: G_Loss: 1.178, D_unsup_loss_real: 0.544, D_unsup_loss_fake: 0.590, D_sup_loss: 0.124, D_sup_acc: 97.05 Train acc: 96.992 Test acc: 97.240 \n",
      "step: 6595 | Train: G_Loss: 1.171, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.537, D_sup_loss: 0.113, D_sup_acc: 97.27 Train acc: 96.773 Test acc: 96.970 \n",
      "step: 6596 | Train: G_Loss: 1.209, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.558, D_sup_loss: 0.129, D_sup_acc: 97.01 Train acc: 97.032 Test acc: 97.250 \n",
      "step: 6597 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.628, D_sup_loss: 0.112, D_sup_acc: 97.28 Train acc: 96.687 Test acc: 96.950 \n",
      "step: 6598 | Train: G_Loss: 1.165, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.556, D_sup_loss: 0.132, D_sup_acc: 96.99 Train acc: 96.767 Test acc: 96.890 \n",
      "step: 6599 | Train: G_Loss: 1.226, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.656, D_sup_loss: 0.127, D_sup_acc: 96.93 Train acc: 96.967 Test acc: 97.190 \n",
      "step: 6600 | Train: G_Loss: 1.273, D_unsup_loss_real: 0.541, D_unsup_loss_fake: 0.632, D_sup_loss: 0.117, D_sup_acc: 97.23 Train acc: 96.705 Test acc: 96.970 \n",
      "Train Classifier Accuracy: 96.705%\n",
      "\n",
      "Test Classifier Accuracy: 96.970%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_6600.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_6600.h5\n",
      "step: 6601 | Train: G_Loss: 1.215, D_unsup_loss_real: 0.628, D_unsup_loss_fake: 0.580, D_sup_loss: 0.131, D_sup_acc: 97.01 Train acc: 96.813 Test acc: 97.040 \n",
      "step: 6602 | Train: G_Loss: 1.163, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.569, D_sup_loss: 0.125, D_sup_acc: 97.08 Train acc: 96.810 Test acc: 96.990 \n",
      "step: 6603 | Train: G_Loss: 1.267, D_unsup_loss_real: 0.535, D_unsup_loss_fake: 0.591, D_sup_loss: 0.122, D_sup_acc: 97.03 Train acc: 96.458 Test acc: 96.620 \n",
      "step: 6604 | Train: G_Loss: 1.280, D_unsup_loss_real: 0.537, D_unsup_loss_fake: 0.521, D_sup_loss: 0.139, D_sup_acc: 96.66 Train acc: 96.727 Test acc: 96.850 \n",
      "step: 6605 | Train: G_Loss: 1.181, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.529, D_sup_loss: 0.127, D_sup_acc: 96.89 Train acc: 96.932 Test acc: 97.130 \n",
      "step: 6606 | Train: G_Loss: 1.135, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.570, D_sup_loss: 0.120, D_sup_acc: 97.17 Train acc: 96.958 Test acc: 97.120 \n",
      "step: 6607 | Train: G_Loss: 1.264, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.561, D_sup_loss: 0.118, D_sup_acc: 97.16 Train acc: 96.920 Test acc: 97.050 \n",
      "step: 6608 | Train: G_Loss: 1.226, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.522, D_sup_loss: 0.118, D_sup_acc: 97.09 Train acc: 96.910 Test acc: 97.110 \n",
      "step: 6609 | Train: G_Loss: 1.182, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.618, D_sup_loss: 0.119, D_sup_acc: 97.15 Train acc: 96.903 Test acc: 97.040 \n",
      "step: 6610 | Train: G_Loss: 1.159, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.603, D_sup_loss: 0.123, D_sup_acc: 97.08 Train acc: 96.923 Test acc: 97.120 \n",
      "step: 6611 | Train: G_Loss: 1.208, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.619, D_sup_loss: 0.120, D_sup_acc: 97.16 Train acc: 96.822 Test acc: 96.980 \n",
      "step: 6612 | Train: G_Loss: 1.281, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.642, D_sup_loss: 0.128, D_sup_acc: 97.02 Train acc: 96.755 Test acc: 97.010 \n",
      "step: 6613 | Train: G_Loss: 1.239, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.560, D_sup_loss: 0.132, D_sup_acc: 97.05 Train acc: 96.963 Test acc: 97.070 \n",
      "step: 6614 | Train: G_Loss: 1.227, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.581, D_sup_loss: 0.121, D_sup_acc: 97.11 Train acc: 96.978 Test acc: 97.190 \n",
      "step: 6615 | Train: G_Loss: 1.203, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.546, D_sup_loss: 0.118, D_sup_acc: 97.23 Train acc: 96.967 Test acc: 97.140 \n",
      "step: 6616 | Train: G_Loss: 1.208, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.529, D_sup_loss: 0.119, D_sup_acc: 97.18 Train acc: 96.917 Test acc: 97.120 \n",
      "step: 6617 | Train: G_Loss: 1.077, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.563, D_sup_loss: 0.123, D_sup_acc: 97.16 Train acc: 96.852 Test acc: 97.070 \n",
      "step: 6618 | Train: G_Loss: 1.208, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.544, D_sup_loss: 0.126, D_sup_acc: 97.11 Train acc: 96.902 Test acc: 97.070 \n",
      "step: 6619 | Train: G_Loss: 1.238, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.555, D_sup_loss: 0.124, D_sup_acc: 97.11 Train acc: 96.997 Test acc: 97.260 \n",
      "step: 6620 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.607, D_sup_loss: 0.116, D_sup_acc: 97.29 Train acc: 96.635 Test acc: 96.870 \n",
      "step: 6621 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.550, D_unsup_loss_fake: 0.589, D_sup_loss: 0.142, D_sup_acc: 96.91 Train acc: 96.835 Test acc: 97.140 \n",
      "step: 6622 | Train: G_Loss: 1.213, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.560, D_sup_loss: 0.128, D_sup_acc: 97.18 Train acc: 96.798 Test acc: 97.080 \n",
      "step: 6623 | Train: G_Loss: 1.115, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.584, D_sup_loss: 0.128, D_sup_acc: 97.12 Train acc: 96.853 Test acc: 97.060 \n",
      "step: 6624 | Train: G_Loss: 1.247, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.625, D_sup_loss: 0.132, D_sup_acc: 97.10 Train acc: 96.945 Test acc: 97.140 \n",
      "step: 6625 | Train: G_Loss: 1.272, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.625, D_sup_loss: 0.122, D_sup_acc: 97.18 Train acc: 97.003 Test acc: 97.180 \n",
      "step: 6626 | Train: G_Loss: 1.227, D_unsup_loss_real: 0.500, D_unsup_loss_fake: 0.587, D_sup_loss: 0.119, D_sup_acc: 97.22 Train acc: 96.930 Test acc: 97.080 \n",
      "step: 6627 | Train: G_Loss: 1.192, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.533, D_sup_loss: 0.124, D_sup_acc: 97.12 Train acc: 96.888 Test acc: 97.050 \n",
      "step: 6628 | Train: G_Loss: 1.349, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.551, D_sup_loss: 0.127, D_sup_acc: 97.09 Train acc: 96.973 Test acc: 97.230 \n",
      "step: 6629 | Train: G_Loss: 1.237, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.640, D_sup_loss: 0.119, D_sup_acc: 97.27 Train acc: 96.820 Test acc: 96.990 \n",
      "step: 6630 | Train: G_Loss: 1.224, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.533, D_sup_loss: 0.131, D_sup_acc: 97.03 Train acc: 97.032 Test acc: 97.230 \n",
      "step: 6631 | Train: G_Loss: 1.332, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.566, D_sup_loss: 0.114, D_sup_acc: 97.27 Train acc: 97.052 Test acc: 97.270 \n",
      "step: 6632 | Train: G_Loss: 1.187, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.620, D_sup_loss: 0.114, D_sup_acc: 97.30 Train acc: 97.090 Test acc: 97.260 \n",
      "step: 6633 | Train: G_Loss: 1.211, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.537, D_sup_loss: 0.118, D_sup_acc: 97.29 Train acc: 97.040 Test acc: 97.280 \n",
      "step: 6634 | Train: G_Loss: 1.299, D_unsup_loss_real: 0.524, D_unsup_loss_fake: 0.627, D_sup_loss: 0.119, D_sup_acc: 97.31 Train acc: 96.817 Test acc: 96.970 \n",
      "step: 6635 | Train: G_Loss: 1.256, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.544, D_sup_loss: 0.131, D_sup_acc: 97.01 Train acc: 96.915 Test acc: 97.080 \n",
      "step: 6636 | Train: G_Loss: 1.332, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.566, D_sup_loss: 0.123, D_sup_acc: 97.12 Train acc: 96.888 Test acc: 97.100 \n",
      "step: 6637 | Train: G_Loss: 1.120, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.525, D_sup_loss: 0.124, D_sup_acc: 97.14 Train acc: 96.930 Test acc: 97.100 \n",
      "step: 6638 | Train: G_Loss: 1.342, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.543, D_sup_loss: 0.121, D_sup_acc: 97.14 Train acc: 97.025 Test acc: 97.200 \n",
      "step: 6639 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.689, D_unsup_loss_fake: 0.558, D_sup_loss: 0.109, D_sup_acc: 97.24 Train acc: 96.652 Test acc: 96.850 \n",
      "step: 6640 | Train: G_Loss: 1.251, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.627, D_sup_loss: 0.141, D_sup_acc: 96.89 Train acc: 96.862 Test acc: 97.030 \n",
      "step: 6641 | Train: G_Loss: 1.336, D_unsup_loss_real: 0.532, D_unsup_loss_fake: 0.489, D_sup_loss: 0.125, D_sup_acc: 97.07 Train acc: 97.022 Test acc: 97.170 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6642 | Train: G_Loss: 1.276, D_unsup_loss_real: 0.639, D_unsup_loss_fake: 0.598, D_sup_loss: 0.111, D_sup_acc: 97.21 Train acc: 96.108 Test acc: 96.430 \n",
      "step: 6643 | Train: G_Loss: 1.244, D_unsup_loss_real: 0.644, D_unsup_loss_fake: 0.645, D_sup_loss: 0.169, D_sup_acc: 96.48 Train acc: 96.867 Test acc: 97.070 \n",
      "step: 6644 | Train: G_Loss: 1.190, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.593, D_sup_loss: 0.124, D_sup_acc: 97.11 Train acc: 96.777 Test acc: 96.910 \n",
      "step: 6645 | Train: G_Loss: 1.163, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.556, D_sup_loss: 0.129, D_sup_acc: 96.95 Train acc: 96.697 Test acc: 96.920 \n",
      "step: 6646 | Train: G_Loss: 1.171, D_unsup_loss_real: 0.504, D_unsup_loss_fake: 0.578, D_sup_loss: 0.132, D_sup_acc: 96.96 Train acc: 96.692 Test acc: 96.850 \n",
      "step: 6647 | Train: G_Loss: 1.303, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.603, D_sup_loss: 0.128, D_sup_acc: 96.89 Train acc: 97.028 Test acc: 97.160 \n",
      "step: 6648 | Train: G_Loss: 1.092, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.572, D_sup_loss: 0.113, D_sup_acc: 97.20 Train acc: 96.627 Test acc: 96.760 \n",
      "step: 6649 | Train: G_Loss: 1.153, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.561, D_sup_loss: 0.140, D_sup_acc: 96.80 Train acc: 96.958 Test acc: 97.110 \n",
      "step: 6650 | Train: G_Loss: 1.192, D_unsup_loss_real: 0.520, D_unsup_loss_fake: 0.597, D_sup_loss: 0.122, D_sup_acc: 97.15 Train acc: 96.943 Test acc: 97.120 \n",
      "step: 6651 | Train: G_Loss: 1.167, D_unsup_loss_real: 0.644, D_unsup_loss_fake: 0.580, D_sup_loss: 0.121, D_sup_acc: 97.16 Train acc: 96.908 Test acc: 97.010 \n",
      "step: 6652 | Train: G_Loss: 1.090, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.598, D_sup_loss: 0.121, D_sup_acc: 97.05 Train acc: 96.977 Test acc: 97.070 \n",
      "step: 6653 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.577, D_sup_loss: 0.121, D_sup_acc: 97.11 Train acc: 96.863 Test acc: 96.960 \n",
      "step: 6654 | Train: G_Loss: 1.230, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.521, D_sup_loss: 0.127, D_sup_acc: 97.00 Train acc: 96.745 Test acc: 96.900 \n",
      "step: 6655 | Train: G_Loss: 1.133, D_unsup_loss_real: 0.531, D_unsup_loss_fake: 0.579, D_sup_loss: 0.135, D_sup_acc: 96.94 Train acc: 96.735 Test acc: 96.790 \n",
      "step: 6656 | Train: G_Loss: 1.194, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.604, D_sup_loss: 0.134, D_sup_acc: 96.83 Train acc: 96.985 Test acc: 97.120 \n",
      "step: 6657 | Train: G_Loss: 1.297, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.656, D_sup_loss: 0.121, D_sup_acc: 97.16 Train acc: 96.535 Test acc: 96.530 \n",
      "step: 6658 | Train: G_Loss: 1.194, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.579, D_sup_loss: 0.139, D_sup_acc: 96.57 Train acc: 96.927 Test acc: 97.050 \n",
      "step: 6659 | Train: G_Loss: 1.270, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.518, D_sup_loss: 0.125, D_sup_acc: 97.09 Train acc: 96.968 Test acc: 97.290 \n",
      "step: 6660 | Train: G_Loss: 1.137, D_unsup_loss_real: 0.485, D_unsup_loss_fake: 0.535, D_sup_loss: 0.120, D_sup_acc: 97.32 Train acc: 97.092 Test acc: 97.290 \n",
      "step: 6661 | Train: G_Loss: 1.184, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.557, D_sup_loss: 0.111, D_sup_acc: 97.32 Train acc: 96.813 Test acc: 96.920 \n",
      "step: 6662 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.628, D_sup_loss: 0.133, D_sup_acc: 96.96 Train acc: 96.832 Test acc: 97.000 \n",
      "step: 6663 | Train: G_Loss: 1.214, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.578, D_sup_loss: 0.124, D_sup_acc: 97.04 Train acc: 96.710 Test acc: 96.960 \n",
      "step: 6664 | Train: G_Loss: 1.140, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.625, D_sup_loss: 0.134, D_sup_acc: 97.00 Train acc: 96.858 Test acc: 97.100 \n",
      "step: 6665 | Train: G_Loss: 1.239, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.513, D_sup_loss: 0.126, D_sup_acc: 97.14 Train acc: 96.983 Test acc: 97.200 \n",
      "step: 6666 | Train: G_Loss: 1.202, D_unsup_loss_real: 0.494, D_unsup_loss_fake: 0.581, D_sup_loss: 0.116, D_sup_acc: 97.24 Train acc: 96.808 Test acc: 96.970 \n",
      "step: 6667 | Train: G_Loss: 1.330, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.575, D_sup_loss: 0.127, D_sup_acc: 97.01 Train acc: 96.875 Test acc: 97.010 \n",
      "step: 6668 | Train: G_Loss: 1.208, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.606, D_sup_loss: 0.124, D_sup_acc: 97.05 Train acc: 96.965 Test acc: 97.110 \n",
      "step: 6669 | Train: G_Loss: 1.343, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.567, D_sup_loss: 0.120, D_sup_acc: 97.15 Train acc: 96.830 Test acc: 97.070 \n",
      "step: 6670 | Train: G_Loss: 1.228, D_unsup_loss_real: 0.510, D_unsup_loss_fake: 0.568, D_sup_loss: 0.130, D_sup_acc: 97.11 Train acc: 97.043 Test acc: 97.220 \n",
      "step: 6671 | Train: G_Loss: 1.157, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.602, D_sup_loss: 0.119, D_sup_acc: 97.26 Train acc: 97.057 Test acc: 97.270 \n",
      "step: 6672 | Train: G_Loss: 1.206, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.567, D_sup_loss: 0.116, D_sup_acc: 97.30 Train acc: 96.947 Test acc: 97.140 \n",
      "step: 6673 | Train: G_Loss: 1.211, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.575, D_sup_loss: 0.124, D_sup_acc: 97.18 Train acc: 97.002 Test acc: 97.220 \n",
      "step: 6674 | Train: G_Loss: 1.097, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.627, D_sup_loss: 0.118, D_sup_acc: 97.26 Train acc: 96.817 Test acc: 97.090 \n",
      "step: 6675 | Train: G_Loss: 1.190, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.511, D_sup_loss: 0.134, D_sup_acc: 97.13 Train acc: 97.013 Test acc: 97.220 \n",
      "step: 6676 | Train: G_Loss: 1.242, D_unsup_loss_real: 0.523, D_unsup_loss_fake: 0.595, D_sup_loss: 0.119, D_sup_acc: 97.26 Train acc: 97.042 Test acc: 97.240 \n",
      "step: 6677 | Train: G_Loss: 1.362, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.578, D_sup_loss: 0.117, D_sup_acc: 97.27 Train acc: 97.138 Test acc: 97.170 \n",
      "step: 6678 | Train: G_Loss: 1.182, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.595, D_sup_loss: 0.111, D_sup_acc: 97.21 Train acc: 96.970 Test acc: 96.910 \n",
      "step: 6679 | Train: G_Loss: 1.252, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.560, D_sup_loss: 0.125, D_sup_acc: 96.95 Train acc: 97.010 Test acc: 97.110 \n",
      "step: 6680 | Train: G_Loss: 1.172, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.525, D_sup_loss: 0.119, D_sup_acc: 97.15 Train acc: 97.072 Test acc: 97.180 \n",
      "step: 6681 | Train: G_Loss: 1.287, D_unsup_loss_real: 0.506, D_unsup_loss_fake: 0.553, D_sup_loss: 0.115, D_sup_acc: 97.22 Train acc: 97.115 Test acc: 97.180 \n",
      "step: 6682 | Train: G_Loss: 1.164, D_unsup_loss_real: 0.485, D_unsup_loss_fake: 0.590, D_sup_loss: 0.114, D_sup_acc: 97.22 Train acc: 97.078 Test acc: 97.210 \n",
      "step: 6683 | Train: G_Loss: 1.237, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.583, D_sup_loss: 0.117, D_sup_acc: 97.25 Train acc: 96.955 Test acc: 96.940 \n",
      "step: 6684 | Train: G_Loss: 1.227, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.557, D_sup_loss: 0.126, D_sup_acc: 96.98 Train acc: 97.117 Test acc: 97.140 \n",
      "step: 6685 | Train: G_Loss: 1.173, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.585, D_sup_loss: 0.114, D_sup_acc: 97.18 Train acc: 97.043 Test acc: 97.110 \n",
      "step: 6686 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.585, D_sup_loss: 0.120, D_sup_acc: 97.15 Train acc: 97.158 Test acc: 97.290 \n",
      "step: 6687 | Train: G_Loss: 1.161, D_unsup_loss_real: 0.652, D_unsup_loss_fake: 0.604, D_sup_loss: 0.111, D_sup_acc: 97.32 Train acc: 96.997 Test acc: 97.070 \n",
      "step: 6688 | Train: G_Loss: 1.176, D_unsup_loss_real: 0.518, D_unsup_loss_fake: 0.575, D_sup_loss: 0.125, D_sup_acc: 97.11 Train acc: 96.843 Test acc: 97.000 \n",
      "step: 6689 | Train: G_Loss: 1.158, D_unsup_loss_real: 0.663, D_unsup_loss_fake: 0.528, D_sup_loss: 0.131, D_sup_acc: 97.04 Train acc: 97.008 Test acc: 97.080 \n",
      "step: 6690 | Train: G_Loss: 1.276, D_unsup_loss_real: 0.440, D_unsup_loss_fake: 0.521, D_sup_loss: 0.120, D_sup_acc: 97.12 Train acc: 97.063 Test acc: 97.020 \n",
      "step: 6691 | Train: G_Loss: 1.200, D_unsup_loss_real: 0.670, D_unsup_loss_fake: 0.594, D_sup_loss: 0.113, D_sup_acc: 97.06 Train acc: 97.012 Test acc: 97.020 \n",
      "step: 6692 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.495, D_unsup_loss_fake: 0.557, D_sup_loss: 0.118, D_sup_acc: 97.06 Train acc: 96.883 Test acc: 97.030 \n",
      "step: 6693 | Train: G_Loss: 1.139, D_unsup_loss_real: 0.550, D_unsup_loss_fake: 0.558, D_sup_loss: 0.121, D_sup_acc: 97.07 Train acc: 96.973 Test acc: 97.130 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6694 | Train: G_Loss: 1.109, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.567, D_sup_loss: 0.117, D_sup_acc: 97.17 Train acc: 96.975 Test acc: 97.190 \n",
      "step: 6695 | Train: G_Loss: 1.197, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.618, D_sup_loss: 0.115, D_sup_acc: 97.23 Train acc: 96.912 Test acc: 97.070 \n",
      "step: 6696 | Train: G_Loss: 1.292, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.663, D_sup_loss: 0.124, D_sup_acc: 97.11 Train acc: 96.942 Test acc: 97.030 \n",
      "step: 6697 | Train: G_Loss: 1.133, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.653, D_sup_loss: 0.122, D_sup_acc: 97.07 Train acc: 97.077 Test acc: 97.330 \n",
      "step: 6698 | Train: G_Loss: 1.234, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.618, D_sup_loss: 0.113, D_sup_acc: 97.36 Train acc: 97.007 Test acc: 97.130 \n",
      "step: 6699 | Train: G_Loss: 1.135, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.521, D_sup_loss: 0.121, D_sup_acc: 97.17 Train acc: 97.032 Test acc: 97.240 \n",
      "step: 6700 | Train: G_Loss: 1.183, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.515, D_sup_loss: 0.116, D_sup_acc: 97.27 Train acc: 97.018 Test acc: 97.210 \n",
      "Train Classifier Accuracy: 97.018%\n",
      "\n",
      "Test Classifier Accuracy: 97.210%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_6700.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_6700.h5\n",
      "step: 6701 | Train: G_Loss: 1.100, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.549, D_sup_loss: 0.112, D_sup_acc: 97.25 Train acc: 97.062 Test acc: 97.160 \n",
      "step: 6702 | Train: G_Loss: 1.156, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.515, D_sup_loss: 0.116, D_sup_acc: 97.20 Train acc: 97.067 Test acc: 97.130 \n",
      "step: 6703 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.577, D_sup_loss: 0.115, D_sup_acc: 97.17 Train acc: 96.935 Test acc: 97.030 \n",
      "step: 6704 | Train: G_Loss: 1.155, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.665, D_sup_loss: 0.122, D_sup_acc: 97.07 Train acc: 97.027 Test acc: 97.070 \n",
      "step: 6705 | Train: G_Loss: 1.191, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.591, D_sup_loss: 0.118, D_sup_acc: 97.11 Train acc: 97.022 Test acc: 97.140 \n",
      "step: 6706 | Train: G_Loss: 1.255, D_unsup_loss_real: 0.518, D_unsup_loss_fake: 0.563, D_sup_loss: 0.116, D_sup_acc: 97.18 Train acc: 97.023 Test acc: 97.070 \n",
      "step: 6707 | Train: G_Loss: 1.241, D_unsup_loss_real: 0.510, D_unsup_loss_fake: 0.595, D_sup_loss: 0.120, D_sup_acc: 97.11 Train acc: 96.935 Test acc: 97.090 \n",
      "step: 6708 | Train: G_Loss: 1.128, D_unsup_loss_real: 0.686, D_unsup_loss_fake: 0.585, D_sup_loss: 0.123, D_sup_acc: 97.13 Train acc: 97.137 Test acc: 97.270 \n",
      "step: 6709 | Train: G_Loss: 1.167, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.586, D_sup_loss: 0.113, D_sup_acc: 97.30 Train acc: 97.097 Test acc: 97.160 \n",
      "step: 6710 | Train: G_Loss: 1.200, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.618, D_sup_loss: 0.121, D_sup_acc: 97.20 Train acc: 96.990 Test acc: 97.110 \n",
      "step: 6711 | Train: G_Loss: 1.238, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.579, D_sup_loss: 0.122, D_sup_acc: 97.15 Train acc: 97.130 Test acc: 97.220 \n",
      "step: 6712 | Train: G_Loss: 1.221, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.602, D_sup_loss: 0.112, D_sup_acc: 97.26 Train acc: 96.872 Test acc: 97.050 \n",
      "step: 6713 | Train: G_Loss: 1.181, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.574, D_sup_loss: 0.128, D_sup_acc: 97.09 Train acc: 97.162 Test acc: 97.260 \n",
      "step: 6714 | Train: G_Loss: 1.233, D_unsup_loss_real: 0.495, D_unsup_loss_fake: 0.551, D_sup_loss: 0.110, D_sup_acc: 97.29 Train acc: 97.093 Test acc: 97.160 \n",
      "step: 6715 | Train: G_Loss: 1.259, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.566, D_sup_loss: 0.116, D_sup_acc: 97.20 Train acc: 96.640 Test acc: 96.820 \n",
      "step: 6716 | Train: G_Loss: 1.371, D_unsup_loss_real: 0.526, D_unsup_loss_fake: 0.632, D_sup_loss: 0.139, D_sup_acc: 96.86 Train acc: 97.043 Test acc: 97.190 \n",
      "step: 6717 | Train: G_Loss: 1.293, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.555, D_sup_loss: 0.115, D_sup_acc: 97.23 Train acc: 97.028 Test acc: 97.180 \n",
      "step: 6718 | Train: G_Loss: 1.179, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.570, D_sup_loss: 0.114, D_sup_acc: 97.22 Train acc: 97.065 Test acc: 97.260 \n",
      "step: 6719 | Train: G_Loss: 1.167, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.579, D_sup_loss: 0.115, D_sup_acc: 97.29 Train acc: 97.010 Test acc: 97.250 \n",
      "step: 6720 | Train: G_Loss: 1.246, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.614, D_sup_loss: 0.118, D_sup_acc: 97.28 Train acc: 97.028 Test acc: 97.250 \n",
      "step: 6721 | Train: G_Loss: 1.155, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.539, D_sup_loss: 0.117, D_sup_acc: 97.28 Train acc: 97.097 Test acc: 97.320 \n",
      "step: 6722 | Train: G_Loss: 1.283, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.543, D_sup_loss: 0.111, D_sup_acc: 97.35 Train acc: 96.945 Test acc: 97.180 \n",
      "step: 6723 | Train: G_Loss: 1.314, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.604, D_sup_loss: 0.120, D_sup_acc: 97.22 Train acc: 97.065 Test acc: 97.230 \n",
      "step: 6724 | Train: G_Loss: 1.278, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.625, D_sup_loss: 0.114, D_sup_acc: 97.27 Train acc: 97.008 Test acc: 97.070 \n",
      "step: 6725 | Train: G_Loss: 1.280, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.605, D_sup_loss: 0.120, D_sup_acc: 97.11 Train acc: 97.045 Test acc: 97.160 \n",
      "step: 6726 | Train: G_Loss: 1.303, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.569, D_sup_loss: 0.116, D_sup_acc: 97.20 Train acc: 97.038 Test acc: 97.250 \n",
      "step: 6727 | Train: G_Loss: 1.185, D_unsup_loss_real: 0.695, D_unsup_loss_fake: 0.644, D_sup_loss: 0.110, D_sup_acc: 97.28 Train acc: 97.042 Test acc: 97.280 \n",
      "step: 6728 | Train: G_Loss: 1.267, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.575, D_sup_loss: 0.118, D_sup_acc: 97.31 Train acc: 97.125 Test acc: 97.280 \n",
      "step: 6729 | Train: G_Loss: 1.247, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.612, D_sup_loss: 0.110, D_sup_acc: 97.31 Train acc: 97.050 Test acc: 97.200 \n",
      "step: 6730 | Train: G_Loss: 1.202, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.539, D_sup_loss: 0.114, D_sup_acc: 97.24 Train acc: 96.968 Test acc: 97.180 \n",
      "step: 6731 | Train: G_Loss: 1.241, D_unsup_loss_real: 0.483, D_unsup_loss_fake: 0.500, D_sup_loss: 0.122, D_sup_acc: 97.22 Train acc: 97.023 Test acc: 97.180 \n",
      "step: 6732 | Train: G_Loss: 1.220, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.606, D_sup_loss: 0.116, D_sup_acc: 97.22 Train acc: 96.720 Test acc: 96.920 \n",
      "step: 6733 | Train: G_Loss: 1.171, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.625, D_sup_loss: 0.130, D_sup_acc: 96.96 Train acc: 97.035 Test acc: 97.220 \n",
      "step: 6734 | Train: G_Loss: 1.324, D_unsup_loss_real: 0.550, D_unsup_loss_fake: 0.565, D_sup_loss: 0.120, D_sup_acc: 97.26 Train acc: 97.108 Test acc: 97.170 \n",
      "step: 6735 | Train: G_Loss: 1.182, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.613, D_sup_loss: 0.113, D_sup_acc: 97.21 Train acc: 96.992 Test acc: 97.110 \n",
      "step: 6736 | Train: G_Loss: 1.201, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.563, D_sup_loss: 0.121, D_sup_acc: 97.15 Train acc: 96.953 Test acc: 97.050 \n",
      "step: 6737 | Train: G_Loss: 1.311, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.526, D_sup_loss: 0.122, D_sup_acc: 97.09 Train acc: 96.988 Test acc: 97.060 \n",
      "step: 6738 | Train: G_Loss: 1.206, D_unsup_loss_real: 0.501, D_unsup_loss_fake: 0.545, D_sup_loss: 0.120, D_sup_acc: 97.10 Train acc: 96.932 Test acc: 96.990 \n",
      "step: 6739 | Train: G_Loss: 1.239, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.587, D_sup_loss: 0.120, D_sup_acc: 97.03 Train acc: 96.890 Test acc: 97.080 \n",
      "step: 6740 | Train: G_Loss: 1.170, D_unsup_loss_real: 0.651, D_unsup_loss_fake: 0.579, D_sup_loss: 0.120, D_sup_acc: 97.12 Train acc: 97.037 Test acc: 97.160 \n",
      "step: 6741 | Train: G_Loss: 1.244, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.510, D_sup_loss: 0.119, D_sup_acc: 97.20 Train acc: 97.103 Test acc: 97.180 \n",
      "step: 6742 | Train: G_Loss: 1.184, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.575, D_sup_loss: 0.111, D_sup_acc: 97.22 Train acc: 96.963 Test acc: 97.140 \n",
      "step: 6743 | Train: G_Loss: 1.174, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.636, D_sup_loss: 0.121, D_sup_acc: 97.18 Train acc: 96.993 Test acc: 97.100 \n",
      "step: 6744 | Train: G_Loss: 1.363, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.602, D_sup_loss: 0.118, D_sup_acc: 97.14 Train acc: 96.887 Test acc: 97.020 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6745 | Train: G_Loss: 1.187, D_unsup_loss_real: 0.473, D_unsup_loss_fake: 0.526, D_sup_loss: 0.121, D_sup_acc: 97.06 Train acc: 96.838 Test acc: 97.040 \n",
      "step: 6746 | Train: G_Loss: 1.217, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.599, D_sup_loss: 0.122, D_sup_acc: 97.08 Train acc: 96.592 Test acc: 96.920 \n",
      "step: 6747 | Train: G_Loss: 1.261, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.553, D_sup_loss: 0.139, D_sup_acc: 96.96 Train acc: 96.823 Test acc: 97.000 \n",
      "step: 6748 | Train: G_Loss: 1.242, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.660, D_sup_loss: 0.120, D_sup_acc: 97.04 Train acc: 96.833 Test acc: 96.920 \n",
      "step: 6749 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.515, D_sup_loss: 0.123, D_sup_acc: 96.96 Train acc: 96.593 Test acc: 96.800 \n",
      "step: 6750 | Train: G_Loss: 1.190, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.546, D_sup_loss: 0.133, D_sup_acc: 96.84 Train acc: 96.635 Test acc: 96.850 \n",
      "step: 6751 | Train: G_Loss: 1.158, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.596, D_sup_loss: 0.131, D_sup_acc: 96.89 Train acc: 96.833 Test acc: 96.960 \n",
      "step: 6752 | Train: G_Loss: 1.253, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.554, D_sup_loss: 0.120, D_sup_acc: 97.00 Train acc: 96.712 Test acc: 96.970 \n",
      "step: 6753 | Train: G_Loss: 1.127, D_unsup_loss_real: 0.506, D_unsup_loss_fake: 0.541, D_sup_loss: 0.128, D_sup_acc: 97.01 Train acc: 96.568 Test acc: 96.830 \n",
      "step: 6754 | Train: G_Loss: 1.287, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.563, D_sup_loss: 0.134, D_sup_acc: 96.87 Train acc: 96.795 Test acc: 97.040 \n",
      "step: 6755 | Train: G_Loss: 1.384, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.535, D_sup_loss: 0.122, D_sup_acc: 97.08 Train acc: 96.887 Test acc: 97.120 \n",
      "step: 6756 | Train: G_Loss: 1.370, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.557, D_sup_loss: 0.112, D_sup_acc: 97.16 Train acc: 95.703 Test acc: 95.930 \n",
      "step: 6757 | Train: G_Loss: 1.274, D_unsup_loss_real: 0.799, D_unsup_loss_fake: 0.561, D_sup_loss: 0.193, D_sup_acc: 95.98 Train acc: 97.075 Test acc: 97.110 \n",
      "step: 6758 | Train: G_Loss: 1.151, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.687, D_sup_loss: 0.110, D_sup_acc: 97.15 Train acc: 96.980 Test acc: 97.200 \n",
      "step: 6759 | Train: G_Loss: 1.193, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.618, D_sup_loss: 0.120, D_sup_acc: 97.24 Train acc: 96.880 Test acc: 97.060 \n",
      "step: 6760 | Train: G_Loss: 1.236, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.660, D_sup_loss: 0.124, D_sup_acc: 97.10 Train acc: 96.895 Test acc: 97.110 \n",
      "step: 6761 | Train: G_Loss: 1.208, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.623, D_sup_loss: 0.120, D_sup_acc: 97.15 Train acc: 97.165 Test acc: 97.330 \n",
      "step: 6762 | Train: G_Loss: 1.207, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.685, D_sup_loss: 0.107, D_sup_acc: 97.36 Train acc: 97.080 Test acc: 97.290 \n",
      "step: 6763 | Train: G_Loss: 1.090, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.581, D_sup_loss: 0.112, D_sup_acc: 97.32 Train acc: 97.065 Test acc: 97.240 \n",
      "step: 6764 | Train: G_Loss: 1.182, D_unsup_loss_real: 0.531, D_unsup_loss_fake: 0.483, D_sup_loss: 0.115, D_sup_acc: 97.27 Train acc: 96.947 Test acc: 97.160 \n",
      "step: 6765 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.528, D_unsup_loss_fake: 0.610, D_sup_loss: 0.119, D_sup_acc: 97.20 Train acc: 96.820 Test acc: 97.110 \n",
      "step: 6766 | Train: G_Loss: 1.371, D_unsup_loss_real: 0.490, D_unsup_loss_fake: 0.616, D_sup_loss: 0.127, D_sup_acc: 97.15 Train acc: 96.968 Test acc: 97.180 \n",
      "step: 6767 | Train: G_Loss: 1.118, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.625, D_sup_loss: 0.116, D_sup_acc: 97.22 Train acc: 96.810 Test acc: 97.050 \n",
      "step: 6768 | Train: G_Loss: 1.078, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.550, D_sup_loss: 0.131, D_sup_acc: 97.09 Train acc: 97.008 Test acc: 97.210 \n",
      "step: 6769 | Train: G_Loss: 1.193, D_unsup_loss_real: 0.670, D_unsup_loss_fake: 0.545, D_sup_loss: 0.118, D_sup_acc: 97.25 Train acc: 96.985 Test acc: 97.210 \n",
      "step: 6770 | Train: G_Loss: 1.209, D_unsup_loss_real: 0.517, D_unsup_loss_fake: 0.634, D_sup_loss: 0.122, D_sup_acc: 97.25 Train acc: 96.908 Test acc: 97.150 \n",
      "step: 6771 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.704, D_sup_loss: 0.122, D_sup_acc: 97.19 Train acc: 96.690 Test acc: 97.010 \n",
      "step: 6772 | Train: G_Loss: 1.197, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.507, D_sup_loss: 0.136, D_sup_acc: 97.05 Train acc: 96.820 Test acc: 97.120 \n",
      "step: 6773 | Train: G_Loss: 1.267, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.515, D_sup_loss: 0.126, D_sup_acc: 97.16 Train acc: 96.960 Test acc: 97.150 \n",
      "step: 6774 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.596, D_sup_loss: 0.119, D_sup_acc: 97.19 Train acc: 96.495 Test acc: 96.730 \n",
      "step: 6775 | Train: G_Loss: 1.143, D_unsup_loss_real: 0.523, D_unsup_loss_fake: 0.560, D_sup_loss: 0.147, D_sup_acc: 96.77 Train acc: 96.855 Test acc: 97.050 \n",
      "step: 6776 | Train: G_Loss: 1.184, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.585, D_sup_loss: 0.122, D_sup_acc: 97.09 Train acc: 96.860 Test acc: 97.070 \n",
      "step: 6777 | Train: G_Loss: 1.161, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.566, D_sup_loss: 0.122, D_sup_acc: 97.11 Train acc: 96.787 Test acc: 96.940 \n",
      "step: 6778 | Train: G_Loss: 1.200, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.570, D_sup_loss: 0.130, D_sup_acc: 96.98 Train acc: 96.883 Test acc: 97.020 \n",
      "step: 6779 | Train: G_Loss: 1.180, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.610, D_sup_loss: 0.122, D_sup_acc: 97.06 Train acc: 96.757 Test acc: 96.880 \n",
      "step: 6780 | Train: G_Loss: 1.155, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.614, D_sup_loss: 0.132, D_sup_acc: 96.92 Train acc: 96.695 Test acc: 96.850 \n",
      "step: 6781 | Train: G_Loss: 1.335, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.562, D_sup_loss: 0.131, D_sup_acc: 96.89 Train acc: 96.900 Test acc: 97.110 \n",
      "step: 6782 | Train: G_Loss: 1.154, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.568, D_sup_loss: 0.117, D_sup_acc: 97.15 Train acc: 96.817 Test acc: 97.060 \n",
      "step: 6783 | Train: G_Loss: 1.179, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.554, D_sup_loss: 0.129, D_sup_acc: 97.10 Train acc: 96.847 Test acc: 97.110 \n",
      "step: 6784 | Train: G_Loss: 1.173, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.563, D_sup_loss: 0.127, D_sup_acc: 97.15 Train acc: 96.777 Test acc: 97.000 \n",
      "step: 6785 | Train: G_Loss: 1.135, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.612, D_sup_loss: 0.129, D_sup_acc: 97.04 Train acc: 96.683 Test acc: 97.000 \n",
      "step: 6786 | Train: G_Loss: 1.344, D_unsup_loss_real: 0.492, D_unsup_loss_fake: 0.516, D_sup_loss: 0.133, D_sup_acc: 97.04 Train acc: 96.915 Test acc: 97.150 \n",
      "step: 6787 | Train: G_Loss: 1.230, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.574, D_sup_loss: 0.115, D_sup_acc: 97.19 Train acc: 96.695 Test acc: 96.950 \n",
      "step: 6788 | Train: G_Loss: 1.312, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.656, D_sup_loss: 0.131, D_sup_acc: 96.99 Train acc: 96.882 Test acc: 97.150 \n",
      "step: 6789 | Train: G_Loss: 1.270, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.494, D_sup_loss: 0.123, D_sup_acc: 97.19 Train acc: 96.952 Test acc: 97.160 \n",
      "step: 6790 | Train: G_Loss: 1.250, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.609, D_sup_loss: 0.118, D_sup_acc: 97.20 Train acc: 97.035 Test acc: 97.280 \n",
      "step: 6791 | Train: G_Loss: 1.319, D_unsup_loss_real: 0.526, D_unsup_loss_fake: 0.608, D_sup_loss: 0.111, D_sup_acc: 97.31 Train acc: 96.943 Test acc: 97.180 \n",
      "step: 6792 | Train: G_Loss: 1.147, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.537, D_sup_loss: 0.115, D_sup_acc: 97.22 Train acc: 96.553 Test acc: 96.870 \n",
      "step: 6793 | Train: G_Loss: 1.222, D_unsup_loss_real: 0.525, D_unsup_loss_fake: 0.548, D_sup_loss: 0.139, D_sup_acc: 96.91 Train acc: 96.880 Test acc: 97.130 \n",
      "step: 6794 | Train: G_Loss: 1.228, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.582, D_sup_loss: 0.121, D_sup_acc: 97.17 Train acc: 97.183 Test acc: 97.290 \n",
      "step: 6795 | Train: G_Loss: 1.226, D_unsup_loss_real: 0.541, D_unsup_loss_fake: 0.680, D_sup_loss: 0.110, D_sup_acc: 97.32 Train acc: 97.103 Test acc: 97.320 \n",
      "step: 6796 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.518, D_unsup_loss_fake: 0.503, D_sup_loss: 0.112, D_sup_acc: 97.35 Train acc: 97.112 Test acc: 97.310 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6797 | Train: G_Loss: 1.257, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.606, D_sup_loss: 0.109, D_sup_acc: 97.34 Train acc: 97.035 Test acc: 97.240 \n",
      "step: 6798 | Train: G_Loss: 1.266, D_unsup_loss_real: 0.530, D_unsup_loss_fake: 0.589, D_sup_loss: 0.116, D_sup_acc: 97.27 Train acc: 96.900 Test acc: 97.050 \n",
      "step: 6799 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.522, D_unsup_loss_fake: 0.573, D_sup_loss: 0.127, D_sup_acc: 97.09 Train acc: 96.817 Test acc: 96.960 \n",
      "step: 6800 | Train: G_Loss: 1.125, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.669, D_sup_loss: 0.125, D_sup_acc: 97.00 Train acc: 96.890 Test acc: 97.120 \n",
      "Train Classifier Accuracy: 96.890%\n",
      "\n",
      "Test Classifier Accuracy: 97.120%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_6800.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_6800.h5\n",
      "step: 6801 | Train: G_Loss: 1.230, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.552, D_sup_loss: 0.123, D_sup_acc: 97.16 Train acc: 96.988 Test acc: 97.170 \n",
      "step: 6802 | Train: G_Loss: 1.262, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.597, D_sup_loss: 0.119, D_sup_acc: 97.21 Train acc: 97.075 Test acc: 97.190 \n",
      "step: 6803 | Train: G_Loss: 1.217, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.575, D_sup_loss: 0.114, D_sup_acc: 97.23 Train acc: 97.007 Test acc: 97.190 \n",
      "step: 6804 | Train: G_Loss: 1.269, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.517, D_sup_loss: 0.119, D_sup_acc: 97.23 Train acc: 96.937 Test acc: 97.130 \n",
      "step: 6805 | Train: G_Loss: 1.281, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.599, D_sup_loss: 0.123, D_sup_acc: 97.17 Train acc: 97.048 Test acc: 97.260 \n",
      "step: 6806 | Train: G_Loss: 1.264, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.485, D_sup_loss: 0.113, D_sup_acc: 97.29 Train acc: 96.852 Test acc: 97.030 \n",
      "step: 6807 | Train: G_Loss: 1.228, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.583, D_sup_loss: 0.125, D_sup_acc: 97.07 Train acc: 97.040 Test acc: 97.270 \n",
      "step: 6808 | Train: G_Loss: 1.194, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.604, D_sup_loss: 0.111, D_sup_acc: 97.30 Train acc: 96.985 Test acc: 97.220 \n",
      "step: 6809 | Train: G_Loss: 1.206, D_unsup_loss_real: 0.499, D_unsup_loss_fake: 0.677, D_sup_loss: 0.121, D_sup_acc: 97.26 Train acc: 96.977 Test acc: 97.180 \n",
      "step: 6810 | Train: G_Loss: 1.264, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.575, D_sup_loss: 0.119, D_sup_acc: 97.22 Train acc: 96.843 Test acc: 97.200 \n",
      "step: 6811 | Train: G_Loss: 1.288, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.524, D_sup_loss: 0.128, D_sup_acc: 97.24 Train acc: 97.072 Test acc: 97.260 \n",
      "step: 6812 | Train: G_Loss: 1.276, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.532, D_sup_loss: 0.110, D_sup_acc: 97.29 Train acc: 96.875 Test acc: 97.060 \n",
      "step: 6813 | Train: G_Loss: 1.118, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.609, D_sup_loss: 0.119, D_sup_acc: 97.10 Train acc: 96.868 Test acc: 97.140 \n",
      "step: 6814 | Train: G_Loss: 1.195, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.581, D_sup_loss: 0.127, D_sup_acc: 97.18 Train acc: 96.835 Test acc: 97.150 \n",
      "step: 6815 | Train: G_Loss: 1.171, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.625, D_sup_loss: 0.124, D_sup_acc: 97.19 Train acc: 96.612 Test acc: 96.860 \n",
      "step: 6816 | Train: G_Loss: 1.243, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.592, D_sup_loss: 0.137, D_sup_acc: 96.90 Train acc: 96.943 Test acc: 97.070 \n",
      "step: 6817 | Train: G_Loss: 1.150, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.503, D_sup_loss: 0.120, D_sup_acc: 97.11 Train acc: 96.677 Test acc: 96.880 \n",
      "step: 6818 | Train: G_Loss: 1.241, D_unsup_loss_real: 0.499, D_unsup_loss_fake: 0.563, D_sup_loss: 0.135, D_sup_acc: 96.92 Train acc: 96.973 Test acc: 97.190 \n",
      "step: 6819 | Train: G_Loss: 1.244, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.593, D_sup_loss: 0.117, D_sup_acc: 97.23 Train acc: 96.682 Test acc: 96.880 \n",
      "step: 6820 | Train: G_Loss: 1.215, D_unsup_loss_real: 0.518, D_unsup_loss_fake: 0.597, D_sup_loss: 0.133, D_sup_acc: 96.92 Train acc: 96.767 Test acc: 96.980 \n",
      "step: 6821 | Train: G_Loss: 1.307, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.541, D_sup_loss: 0.131, D_sup_acc: 97.02 Train acc: 96.888 Test acc: 97.060 \n",
      "step: 6822 | Train: G_Loss: 1.165, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.572, D_sup_loss: 0.124, D_sup_acc: 97.10 Train acc: 96.930 Test acc: 97.070 \n",
      "step: 6823 | Train: G_Loss: 1.260, D_unsup_loss_real: 0.536, D_unsup_loss_fake: 0.570, D_sup_loss: 0.126, D_sup_acc: 97.11 Train acc: 97.005 Test acc: 97.180 \n",
      "step: 6824 | Train: G_Loss: 1.177, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.576, D_sup_loss: 0.117, D_sup_acc: 97.22 Train acc: 97.102 Test acc: 97.270 \n",
      "step: 6825 | Train: G_Loss: 1.161, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.585, D_sup_loss: 0.114, D_sup_acc: 97.30 Train acc: 96.783 Test acc: 97.010 \n",
      "step: 6826 | Train: G_Loss: 1.229, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.530, D_sup_loss: 0.129, D_sup_acc: 97.05 Train acc: 96.772 Test acc: 97.030 \n",
      "step: 6827 | Train: G_Loss: 1.209, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.639, D_sup_loss: 0.133, D_sup_acc: 97.07 Train acc: 97.090 Test acc: 97.250 \n",
      "step: 6828 | Train: G_Loss: 1.196, D_unsup_loss_real: 0.545, D_unsup_loss_fake: 0.691, D_sup_loss: 0.114, D_sup_acc: 97.28 Train acc: 97.005 Test acc: 97.130 \n",
      "step: 6829 | Train: G_Loss: 1.160, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.564, D_sup_loss: 0.121, D_sup_acc: 97.17 Train acc: 97.018 Test acc: 97.180 \n",
      "step: 6830 | Train: G_Loss: 1.276, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.610, D_sup_loss: 0.120, D_sup_acc: 97.22 Train acc: 96.665 Test acc: 96.980 \n",
      "step: 6831 | Train: G_Loss: 1.204, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.589, D_sup_loss: 0.134, D_sup_acc: 97.02 Train acc: 96.875 Test acc: 97.150 \n",
      "step: 6832 | Train: G_Loss: 1.198, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.590, D_sup_loss: 0.123, D_sup_acc: 97.19 Train acc: 96.813 Test acc: 97.020 \n",
      "step: 6833 | Train: G_Loss: 1.237, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.534, D_sup_loss: 0.129, D_sup_acc: 97.06 Train acc: 96.868 Test acc: 97.100 \n",
      "step: 6834 | Train: G_Loss: 1.160, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.621, D_sup_loss: 0.123, D_sup_acc: 97.14 Train acc: 96.647 Test acc: 96.870 \n",
      "step: 6835 | Train: G_Loss: 1.228, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.594, D_sup_loss: 0.138, D_sup_acc: 96.91 Train acc: 97.028 Test acc: 97.190 \n",
      "step: 6836 | Train: G_Loss: 1.282, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.545, D_sup_loss: 0.119, D_sup_acc: 97.23 Train acc: 97.042 Test acc: 97.170 \n",
      "step: 6837 | Train: G_Loss: 1.203, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.563, D_sup_loss: 0.119, D_sup_acc: 97.21 Train acc: 96.998 Test acc: 97.140 \n",
      "step: 6838 | Train: G_Loss: 1.213, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.499, D_sup_loss: 0.123, D_sup_acc: 97.18 Train acc: 96.923 Test acc: 97.140 \n",
      "step: 6839 | Train: G_Loss: 1.195, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.602, D_sup_loss: 0.127, D_sup_acc: 97.18 Train acc: 96.847 Test acc: 97.100 \n",
      "step: 6840 | Train: G_Loss: 1.249, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.511, D_sup_loss: 0.132, D_sup_acc: 97.14 Train acc: 96.970 Test acc: 97.110 \n",
      "step: 6841 | Train: G_Loss: 1.162, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.636, D_sup_loss: 0.115, D_sup_acc: 97.15 Train acc: 96.643 Test acc: 96.880 \n",
      "step: 6842 | Train: G_Loss: 1.110, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.585, D_sup_loss: 0.143, D_sup_acc: 96.92 Train acc: 96.927 Test acc: 97.010 \n",
      "step: 6843 | Train: G_Loss: 1.165, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.611, D_sup_loss: 0.127, D_sup_acc: 97.05 Train acc: 97.022 Test acc: 97.140 \n",
      "step: 6844 | Train: G_Loss: 1.136, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.554, D_sup_loss: 0.119, D_sup_acc: 97.18 Train acc: 96.947 Test acc: 97.050 \n",
      "step: 6845 | Train: G_Loss: 1.181, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.610, D_sup_loss: 0.124, D_sup_acc: 97.09 Train acc: 96.933 Test acc: 97.060 \n",
      "step: 6846 | Train: G_Loss: 1.166, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.659, D_sup_loss: 0.126, D_sup_acc: 97.10 Train acc: 96.908 Test acc: 97.030 \n",
      "step: 6847 | Train: G_Loss: 1.321, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.592, D_sup_loss: 0.125, D_sup_acc: 97.07 Train acc: 96.918 Test acc: 97.070 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6848 | Train: G_Loss: 1.090, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.560, D_sup_loss: 0.122, D_sup_acc: 97.11 Train acc: 96.890 Test acc: 97.000 \n",
      "step: 6849 | Train: G_Loss: 1.193, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.643, D_sup_loss: 0.125, D_sup_acc: 97.04 Train acc: 96.922 Test acc: 97.090 \n",
      "step: 6850 | Train: G_Loss: 1.161, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.586, D_sup_loss: 0.123, D_sup_acc: 97.13 Train acc: 96.888 Test acc: 97.030 \n",
      "step: 6851 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.507, D_unsup_loss_fake: 0.636, D_sup_loss: 0.130, D_sup_acc: 97.07 Train acc: 96.925 Test acc: 97.080 \n",
      "step: 6852 | Train: G_Loss: 1.223, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.502, D_sup_loss: 0.125, D_sup_acc: 97.12 Train acc: 96.877 Test acc: 97.080 \n",
      "step: 6853 | Train: G_Loss: 1.141, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.612, D_sup_loss: 0.128, D_sup_acc: 97.12 Train acc: 96.968 Test acc: 97.120 \n",
      "step: 6854 | Train: G_Loss: 1.099, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.613, D_sup_loss: 0.119, D_sup_acc: 97.16 Train acc: 96.942 Test acc: 97.160 \n",
      "step: 6855 | Train: G_Loss: 1.229, D_unsup_loss_real: 0.550, D_unsup_loss_fake: 0.552, D_sup_loss: 0.121, D_sup_acc: 97.20 Train acc: 97.038 Test acc: 97.220 \n",
      "step: 6856 | Train: G_Loss: 1.199, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.618, D_sup_loss: 0.115, D_sup_acc: 97.26 Train acc: 96.995 Test acc: 97.130 \n",
      "step: 6857 | Train: G_Loss: 1.120, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.539, D_sup_loss: 0.120, D_sup_acc: 97.17 Train acc: 96.837 Test acc: 96.960 \n",
      "step: 6858 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.621, D_sup_loss: 0.128, D_sup_acc: 97.00 Train acc: 96.665 Test acc: 96.860 \n",
      "step: 6859 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.611, D_sup_loss: 0.135, D_sup_acc: 96.90 Train acc: 96.953 Test acc: 96.980 \n",
      "step: 6860 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.516, D_unsup_loss_fake: 0.512, D_sup_loss: 0.122, D_sup_acc: 97.02 Train acc: 96.755 Test acc: 96.930 \n",
      "step: 6861 | Train: G_Loss: 1.235, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.575, D_sup_loss: 0.135, D_sup_acc: 96.97 Train acc: 96.998 Test acc: 97.070 \n",
      "step: 6862 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.512, D_unsup_loss_fake: 0.629, D_sup_loss: 0.123, D_sup_acc: 97.11 Train acc: 97.077 Test acc: 97.080 \n",
      "step: 6863 | Train: G_Loss: 1.182, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.608, D_sup_loss: 0.119, D_sup_acc: 97.12 Train acc: 96.952 Test acc: 97.090 \n",
      "step: 6864 | Train: G_Loss: 1.175, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.520, D_sup_loss: 0.129, D_sup_acc: 97.13 Train acc: 97.035 Test acc: 97.100 \n",
      "step: 6865 | Train: G_Loss: 1.195, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.622, D_sup_loss: 0.121, D_sup_acc: 97.14 Train acc: 96.932 Test acc: 97.100 \n",
      "step: 6866 | Train: G_Loss: 1.126, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.610, D_sup_loss: 0.128, D_sup_acc: 97.14 Train acc: 96.907 Test acc: 97.000 \n",
      "step: 6867 | Train: G_Loss: 1.268, D_unsup_loss_real: 0.546, D_unsup_loss_fake: 0.543, D_sup_loss: 0.128, D_sup_acc: 97.04 Train acc: 96.973 Test acc: 97.070 \n",
      "step: 6868 | Train: G_Loss: 1.218, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.585, D_sup_loss: 0.123, D_sup_acc: 97.11 Train acc: 96.793 Test acc: 96.910 \n",
      "step: 6869 | Train: G_Loss: 1.258, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.584, D_sup_loss: 0.137, D_sup_acc: 96.95 Train acc: 96.998 Test acc: 97.130 \n",
      "step: 6870 | Train: G_Loss: 1.245, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.543, D_sup_loss: 0.120, D_sup_acc: 97.17 Train acc: 96.983 Test acc: 97.020 \n",
      "step: 6871 | Train: G_Loss: 1.199, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.512, D_sup_loss: 0.124, D_sup_acc: 97.06 Train acc: 97.005 Test acc: 97.070 \n",
      "step: 6872 | Train: G_Loss: 1.178, D_unsup_loss_real: 0.631, D_unsup_loss_fake: 0.505, D_sup_loss: 0.121, D_sup_acc: 97.11 Train acc: 96.472 Test acc: 96.630 \n",
      "step: 6873 | Train: G_Loss: 1.230, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.695, D_sup_loss: 0.161, D_sup_acc: 96.67 Train acc: 97.038 Test acc: 97.230 \n",
      "step: 6874 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.608, D_sup_loss: 0.120, D_sup_acc: 97.27 Train acc: 97.048 Test acc: 97.240 \n",
      "step: 6875 | Train: G_Loss: 1.172, D_unsup_loss_real: 0.532, D_unsup_loss_fake: 0.549, D_sup_loss: 0.118, D_sup_acc: 97.27 Train acc: 97.035 Test acc: 97.240 \n",
      "step: 6876 | Train: G_Loss: 1.128, D_unsup_loss_real: 0.536, D_unsup_loss_fake: 0.523, D_sup_loss: 0.120, D_sup_acc: 97.27 Train acc: 97.093 Test acc: 97.220 \n",
      "step: 6877 | Train: G_Loss: 1.217, D_unsup_loss_real: 0.468, D_unsup_loss_fake: 0.608, D_sup_loss: 0.112, D_sup_acc: 97.26 Train acc: 97.067 Test acc: 97.290 \n",
      "step: 6878 | Train: G_Loss: 1.184, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.601, D_sup_loss: 0.116, D_sup_acc: 97.32 Train acc: 96.988 Test acc: 97.240 \n",
      "step: 6879 | Train: G_Loss: 1.305, D_unsup_loss_real: 0.544, D_unsup_loss_fake: 0.543, D_sup_loss: 0.124, D_sup_acc: 97.27 Train acc: 97.223 Test acc: 97.350 \n",
      "step: 6880 | Train: G_Loss: 1.189, D_unsup_loss_real: 0.528, D_unsup_loss_fake: 0.557, D_sup_loss: 0.106, D_sup_acc: 97.38 Train acc: 96.988 Test acc: 97.250 \n",
      "step: 6881 | Train: G_Loss: 1.184, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.549, D_sup_loss: 0.122, D_sup_acc: 97.28 Train acc: 97.087 Test acc: 97.330 \n",
      "step: 6882 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.538, D_unsup_loss_fake: 0.638, D_sup_loss: 0.115, D_sup_acc: 97.36 Train acc: 96.977 Test acc: 97.280 \n",
      "step: 6883 | Train: G_Loss: 1.123, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.622, D_sup_loss: 0.124, D_sup_acc: 97.31 Train acc: 97.007 Test acc: 97.160 \n",
      "step: 6884 | Train: G_Loss: 1.101, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.595, D_sup_loss: 0.122, D_sup_acc: 97.20 Train acc: 96.902 Test acc: 97.070 \n",
      "step: 6885 | Train: G_Loss: 1.338, D_unsup_loss_real: 0.542, D_unsup_loss_fake: 0.610, D_sup_loss: 0.128, D_sup_acc: 97.11 Train acc: 97.100 Test acc: 97.310 \n",
      "step: 6886 | Train: G_Loss: 1.232, D_unsup_loss_real: 0.649, D_unsup_loss_fake: 0.562, D_sup_loss: 0.113, D_sup_acc: 97.34 Train acc: 97.048 Test acc: 97.240 \n",
      "step: 6887 | Train: G_Loss: 1.178, D_unsup_loss_real: 0.544, D_unsup_loss_fake: 0.559, D_sup_loss: 0.118, D_sup_acc: 97.27 Train acc: 96.977 Test acc: 97.190 \n",
      "step: 6888 | Train: G_Loss: 1.137, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.587, D_sup_loss: 0.121, D_sup_acc: 97.23 Train acc: 97.060 Test acc: 97.260 \n",
      "step: 6889 | Train: G_Loss: 1.191, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.590, D_sup_loss: 0.113, D_sup_acc: 97.29 Train acc: 96.845 Test acc: 97.100 \n",
      "step: 6890 | Train: G_Loss: 1.259, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.549, D_sup_loss: 0.133, D_sup_acc: 97.14 Train acc: 97.003 Test acc: 97.160 \n",
      "step: 6891 | Train: G_Loss: 1.154, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.549, D_sup_loss: 0.121, D_sup_acc: 97.20 Train acc: 97.067 Test acc: 97.210 \n",
      "step: 6892 | Train: G_Loss: 1.228, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.598, D_sup_loss: 0.122, D_sup_acc: 97.25 Train acc: 97.077 Test acc: 97.220 \n",
      "step: 6893 | Train: G_Loss: 1.202, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.575, D_sup_loss: 0.118, D_sup_acc: 97.26 Train acc: 97.027 Test acc: 97.280 \n",
      "step: 6894 | Train: G_Loss: 1.079, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.540, D_sup_loss: 0.120, D_sup_acc: 97.31 Train acc: 97.113 Test acc: 97.290 \n",
      "step: 6895 | Train: G_Loss: 1.156, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.633, D_sup_loss: 0.117, D_sup_acc: 97.32 Train acc: 97.053 Test acc: 97.220 \n",
      "step: 6896 | Train: G_Loss: 1.191, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.588, D_sup_loss: 0.120, D_sup_acc: 97.26 Train acc: 96.913 Test acc: 97.210 \n",
      "step: 6897 | Train: G_Loss: 1.226, D_unsup_loss_real: 0.498, D_unsup_loss_fake: 0.585, D_sup_loss: 0.124, D_sup_acc: 97.25 Train acc: 96.843 Test acc: 97.130 \n",
      "step: 6898 | Train: G_Loss: 1.267, D_unsup_loss_real: 0.651, D_unsup_loss_fake: 0.577, D_sup_loss: 0.128, D_sup_acc: 97.17 Train acc: 96.913 Test acc: 97.230 \n",
      "step: 6899 | Train: G_Loss: 1.280, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.578, D_sup_loss: 0.123, D_sup_acc: 97.27 Train acc: 96.868 Test acc: 97.120 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6900 | Train: G_Loss: 1.141, D_unsup_loss_real: 0.631, D_unsup_loss_fake: 0.546, D_sup_loss: 0.130, D_sup_acc: 97.16 Train acc: 97.060 Test acc: 97.210 \n",
      "Train Classifier Accuracy: 97.060%\n",
      "\n",
      "Test Classifier Accuracy: 97.210%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_6900.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_6900.h5\n",
      "step: 6901 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.592, D_sup_loss: 0.118, D_sup_acc: 97.25 Train acc: 96.912 Test acc: 97.100 \n",
      "step: 6902 | Train: G_Loss: 1.383, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.558, D_sup_loss: 0.127, D_sup_acc: 97.14 Train acc: 96.943 Test acc: 97.060 \n",
      "step: 6903 | Train: G_Loss: 1.056, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.579, D_sup_loss: 0.114, D_sup_acc: 97.10 Train acc: 96.755 Test acc: 97.090 \n",
      "step: 6904 | Train: G_Loss: 1.252, D_unsup_loss_real: 0.647, D_unsup_loss_fake: 0.568, D_sup_loss: 0.136, D_sup_acc: 97.13 Train acc: 97.148 Test acc: 97.310 \n",
      "step: 6905 | Train: G_Loss: 1.156, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.541, D_sup_loss: 0.111, D_sup_acc: 97.34 Train acc: 97.070 Test acc: 97.380 \n",
      "step: 6906 | Train: G_Loss: 1.101, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.570, D_sup_loss: 0.114, D_sup_acc: 97.41 Train acc: 97.087 Test acc: 97.360 \n",
      "step: 6907 | Train: G_Loss: 1.155, D_unsup_loss_real: 0.497, D_unsup_loss_fake: 0.549, D_sup_loss: 0.112, D_sup_acc: 97.39 Train acc: 97.028 Test acc: 97.200 \n",
      "step: 6908 | Train: G_Loss: 1.212, D_unsup_loss_real: 0.500, D_unsup_loss_fake: 0.601, D_sup_loss: 0.116, D_sup_acc: 97.24 Train acc: 97.015 Test acc: 97.210 \n",
      "step: 6909 | Train: G_Loss: 1.349, D_unsup_loss_real: 0.534, D_unsup_loss_fake: 0.611, D_sup_loss: 0.118, D_sup_acc: 97.25 Train acc: 96.858 Test acc: 97.150 \n",
      "step: 6910 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.631, D_sup_loss: 0.127, D_sup_acc: 97.19 Train acc: 96.782 Test acc: 97.090 \n",
      "step: 6911 | Train: G_Loss: 1.369, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.580, D_sup_loss: 0.130, D_sup_acc: 97.13 Train acc: 96.992 Test acc: 97.190 \n",
      "step: 6912 | Train: G_Loss: 1.193, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.550, D_sup_loss: 0.116, D_sup_acc: 97.23 Train acc: 96.945 Test acc: 97.100 \n",
      "step: 6913 | Train: G_Loss: 1.110, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.583, D_sup_loss: 0.122, D_sup_acc: 97.14 Train acc: 97.073 Test acc: 97.220 \n",
      "step: 6914 | Train: G_Loss: 1.188, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.623, D_sup_loss: 0.117, D_sup_acc: 97.26 Train acc: 97.055 Test acc: 97.210 \n",
      "step: 6915 | Train: G_Loss: 1.322, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.546, D_sup_loss: 0.118, D_sup_acc: 97.25 Train acc: 97.162 Test acc: 97.280 \n",
      "step: 6916 | Train: G_Loss: 1.276, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.623, D_sup_loss: 0.110, D_sup_acc: 97.31 Train acc: 97.047 Test acc: 97.240 \n",
      "step: 6917 | Train: G_Loss: 1.237, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.599, D_sup_loss: 0.118, D_sup_acc: 97.27 Train acc: 96.958 Test acc: 97.120 \n",
      "step: 6918 | Train: G_Loss: 1.247, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.600, D_sup_loss: 0.124, D_sup_acc: 97.16 Train acc: 97.025 Test acc: 97.270 \n",
      "step: 6919 | Train: G_Loss: 1.163, D_unsup_loss_real: 0.513, D_unsup_loss_fake: 0.519, D_sup_loss: 0.116, D_sup_acc: 97.30 Train acc: 97.155 Test acc: 97.280 \n",
      "step: 6920 | Train: G_Loss: 1.214, D_unsup_loss_real: 0.525, D_unsup_loss_fake: 0.581, D_sup_loss: 0.114, D_sup_acc: 97.31 Train acc: 97.078 Test acc: 97.320 \n",
      "step: 6921 | Train: G_Loss: 1.308, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.607, D_sup_loss: 0.119, D_sup_acc: 97.35 Train acc: 96.868 Test acc: 97.120 \n",
      "step: 6922 | Train: G_Loss: 1.364, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.519, D_sup_loss: 0.128, D_sup_acc: 97.16 Train acc: 97.035 Test acc: 97.270 \n",
      "step: 6923 | Train: G_Loss: 1.243, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.540, D_sup_loss: 0.120, D_sup_acc: 97.30 Train acc: 96.898 Test acc: 97.130 \n",
      "step: 6924 | Train: G_Loss: 1.277, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.562, D_sup_loss: 0.128, D_sup_acc: 97.17 Train acc: 97.135 Test acc: 97.270 \n",
      "step: 6925 | Train: G_Loss: 1.240, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.639, D_sup_loss: 0.114, D_sup_acc: 97.30 Train acc: 97.090 Test acc: 97.270 \n",
      "step: 6926 | Train: G_Loss: 1.129, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.614, D_sup_loss: 0.121, D_sup_acc: 97.30 Train acc: 96.972 Test acc: 97.120 \n",
      "step: 6927 | Train: G_Loss: 1.119, D_unsup_loss_real: 0.546, D_unsup_loss_fake: 0.560, D_sup_loss: 0.126, D_sup_acc: 97.16 Train acc: 96.870 Test acc: 97.100 \n",
      "step: 6928 | Train: G_Loss: 1.212, D_unsup_loss_real: 0.642, D_unsup_loss_fake: 0.516, D_sup_loss: 0.128, D_sup_acc: 97.14 Train acc: 96.852 Test acc: 96.980 \n",
      "step: 6929 | Train: G_Loss: 1.226, D_unsup_loss_real: 0.524, D_unsup_loss_fake: 0.602, D_sup_loss: 0.130, D_sup_acc: 97.02 Train acc: 97.003 Test acc: 97.120 \n",
      "step: 6930 | Train: G_Loss: 1.216, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.528, D_sup_loss: 0.122, D_sup_acc: 97.16 Train acc: 97.063 Test acc: 97.170 \n",
      "step: 6931 | Train: G_Loss: 1.316, D_unsup_loss_real: 0.532, D_unsup_loss_fake: 0.624, D_sup_loss: 0.117, D_sup_acc: 97.21 Train acc: 96.987 Test acc: 97.130 \n",
      "step: 6932 | Train: G_Loss: 1.202, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.534, D_sup_loss: 0.123, D_sup_acc: 97.17 Train acc: 97.022 Test acc: 97.160 \n",
      "step: 6933 | Train: G_Loss: 1.045, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.535, D_sup_loss: 0.120, D_sup_acc: 97.20 Train acc: 96.818 Test acc: 97.100 \n",
      "step: 6934 | Train: G_Loss: 1.299, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.555, D_sup_loss: 0.131, D_sup_acc: 97.14 Train acc: 97.097 Test acc: 97.260 \n",
      "step: 6935 | Train: G_Loss: 1.244, D_unsup_loss_real: 0.499, D_unsup_loss_fake: 0.574, D_sup_loss: 0.112, D_sup_acc: 97.29 Train acc: 97.017 Test acc: 97.180 \n",
      "step: 6936 | Train: G_Loss: 1.251, D_unsup_loss_real: 0.538, D_unsup_loss_fake: 0.500, D_sup_loss: 0.112, D_sup_acc: 97.22 Train acc: 96.925 Test acc: 97.290 \n",
      "step: 6937 | Train: G_Loss: 1.210, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.594, D_sup_loss: 0.124, D_sup_acc: 97.32 Train acc: 97.055 Test acc: 97.290 \n",
      "step: 6938 | Train: G_Loss: 1.194, D_unsup_loss_real: 0.488, D_unsup_loss_fake: 0.586, D_sup_loss: 0.115, D_sup_acc: 97.32 Train acc: 96.925 Test acc: 97.180 \n",
      "step: 6939 | Train: G_Loss: 1.205, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.607, D_sup_loss: 0.122, D_sup_acc: 97.22 Train acc: 96.880 Test acc: 97.110 \n",
      "step: 6940 | Train: G_Loss: 1.301, D_unsup_loss_real: 0.545, D_unsup_loss_fake: 0.545, D_sup_loss: 0.126, D_sup_acc: 97.15 Train acc: 96.948 Test acc: 97.180 \n",
      "step: 6941 | Train: G_Loss: 1.287, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.582, D_sup_loss: 0.120, D_sup_acc: 97.22 Train acc: 96.987 Test acc: 97.190 \n",
      "step: 6942 | Train: G_Loss: 1.390, D_unsup_loss_real: 0.518, D_unsup_loss_fake: 0.556, D_sup_loss: 0.119, D_sup_acc: 97.23 Train acc: 97.145 Test acc: 97.350 \n",
      "step: 6943 | Train: G_Loss: 1.258, D_unsup_loss_real: 0.722, D_unsup_loss_fake: 0.568, D_sup_loss: 0.106, D_sup_acc: 97.38 Train acc: 96.423 Test acc: 96.630 \n",
      "step: 6944 | Train: G_Loss: 1.108, D_unsup_loss_real: 0.649, D_unsup_loss_fake: 0.541, D_sup_loss: 0.160, D_sup_acc: 96.67 Train acc: 97.045 Test acc: 97.280 \n",
      "step: 6945 | Train: G_Loss: 1.183, D_unsup_loss_real: 0.504, D_unsup_loss_fake: 0.625, D_sup_loss: 0.114, D_sup_acc: 97.31 Train acc: 96.998 Test acc: 97.190 \n",
      "step: 6946 | Train: G_Loss: 1.367, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.548, D_sup_loss: 0.118, D_sup_acc: 97.23 Train acc: 97.078 Test acc: 97.430 \n",
      "step: 6947 | Train: G_Loss: 1.174, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.644, D_sup_loss: 0.111, D_sup_acc: 97.46 Train acc: 96.702 Test acc: 96.880 \n",
      "step: 6948 | Train: G_Loss: 1.062, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.557, D_sup_loss: 0.139, D_sup_acc: 96.92 Train acc: 97.060 Test acc: 97.320 \n",
      "step: 6949 | Train: G_Loss: 1.266, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.549, D_sup_loss: 0.116, D_sup_acc: 97.35 Train acc: 97.197 Test acc: 97.450 \n",
      "step: 6950 | Train: G_Loss: 1.185, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.608, D_sup_loss: 0.106, D_sup_acc: 97.48 Train acc: 97.035 Test acc: 97.280 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 6951 | Train: G_Loss: 1.112, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.585, D_sup_loss: 0.120, D_sup_acc: 97.31 Train acc: 97.022 Test acc: 97.270 \n",
      "step: 6952 | Train: G_Loss: 1.104, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.530, D_sup_loss: 0.118, D_sup_acc: 97.30 Train acc: 97.028 Test acc: 97.330 \n",
      "step: 6953 | Train: G_Loss: 1.160, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.625, D_sup_loss: 0.117, D_sup_acc: 97.36 Train acc: 97.058 Test acc: 97.280 \n",
      "step: 6954 | Train: G_Loss: 1.155, D_unsup_loss_real: 0.550, D_unsup_loss_fake: 0.685, D_sup_loss: 0.117, D_sup_acc: 97.31 Train acc: 97.042 Test acc: 97.330 \n",
      "step: 6955 | Train: G_Loss: 1.136, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.695, D_sup_loss: 0.120, D_sup_acc: 97.36 Train acc: 96.907 Test acc: 97.170 \n",
      "step: 6956 | Train: G_Loss: 1.168, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.580, D_sup_loss: 0.128, D_sup_acc: 97.21 Train acc: 97.053 Test acc: 97.300 \n",
      "step: 6957 | Train: G_Loss: 1.227, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.602, D_sup_loss: 0.116, D_sup_acc: 97.33 Train acc: 97.003 Test acc: 97.230 \n",
      "step: 6958 | Train: G_Loss: 1.343, D_unsup_loss_real: 0.509, D_unsup_loss_fake: 0.574, D_sup_loss: 0.116, D_sup_acc: 97.27 Train acc: 96.978 Test acc: 97.250 \n",
      "step: 6959 | Train: G_Loss: 1.173, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.533, D_sup_loss: 0.121, D_sup_acc: 97.28 Train acc: 96.623 Test acc: 96.830 \n",
      "step: 6960 | Train: G_Loss: 1.107, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.577, D_sup_loss: 0.141, D_sup_acc: 96.87 Train acc: 97.040 Test acc: 97.210 \n",
      "step: 6961 | Train: G_Loss: 1.218, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.569, D_sup_loss: 0.120, D_sup_acc: 97.25 Train acc: 96.967 Test acc: 97.080 \n",
      "step: 6962 | Train: G_Loss: 1.215, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.593, D_sup_loss: 0.120, D_sup_acc: 97.12 Train acc: 97.075 Test acc: 97.240 \n",
      "step: 6963 | Train: G_Loss: 1.162, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.562, D_sup_loss: 0.114, D_sup_acc: 97.27 Train acc: 96.838 Test acc: 96.950 \n",
      "step: 6964 | Train: G_Loss: 1.241, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.645, D_sup_loss: 0.128, D_sup_acc: 96.99 Train acc: 96.895 Test acc: 97.050 \n",
      "step: 6965 | Train: G_Loss: 1.166, D_unsup_loss_real: 0.546, D_unsup_loss_fake: 0.579, D_sup_loss: 0.123, D_sup_acc: 97.09 Train acc: 97.027 Test acc: 97.170 \n",
      "step: 6966 | Train: G_Loss: 1.161, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.632, D_sup_loss: 0.117, D_sup_acc: 97.21 Train acc: 97.082 Test acc: 97.270 \n",
      "step: 6967 | Train: G_Loss: 1.312, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.616, D_sup_loss: 0.118, D_sup_acc: 97.30 Train acc: 96.960 Test acc: 97.180 \n",
      "step: 6968 | Train: G_Loss: 1.252, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.570, D_sup_loss: 0.128, D_sup_acc: 97.22 Train acc: 97.115 Test acc: 97.420 \n",
      "step: 6969 | Train: G_Loss: 1.309, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.554, D_sup_loss: 0.114, D_sup_acc: 97.45 Train acc: 96.850 Test acc: 96.970 \n",
      "step: 6970 | Train: G_Loss: 1.236, D_unsup_loss_real: 0.514, D_unsup_loss_fake: 0.522, D_sup_loss: 0.129, D_sup_acc: 97.01 Train acc: 97.078 Test acc: 97.350 \n",
      "step: 6971 | Train: G_Loss: 1.169, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.608, D_sup_loss: 0.114, D_sup_acc: 97.38 Train acc: 96.867 Test acc: 97.140 \n",
      "step: 6972 | Train: G_Loss: 1.220, D_unsup_loss_real: 0.642, D_unsup_loss_fake: 0.556, D_sup_loss: 0.130, D_sup_acc: 97.18 Train acc: 97.048 Test acc: 97.210 \n",
      "step: 6973 | Train: G_Loss: 1.201, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.578, D_sup_loss: 0.112, D_sup_acc: 97.25 Train acc: 97.042 Test acc: 97.240 \n",
      "step: 6974 | Train: G_Loss: 1.132, D_unsup_loss_real: 0.518, D_unsup_loss_fake: 0.502, D_sup_loss: 0.120, D_sup_acc: 97.27 Train acc: 97.023 Test acc: 97.170 \n",
      "step: 6975 | Train: G_Loss: 1.176, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.555, D_sup_loss: 0.118, D_sup_acc: 97.21 Train acc: 96.992 Test acc: 97.240 \n",
      "step: 6976 | Train: G_Loss: 1.296, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.547, D_sup_loss: 0.118, D_sup_acc: 97.27 Train acc: 97.050 Test acc: 97.270 \n",
      "step: 6977 | Train: G_Loss: 1.176, D_unsup_loss_real: 0.500, D_unsup_loss_fake: 0.577, D_sup_loss: 0.114, D_sup_acc: 97.30 Train acc: 96.968 Test acc: 97.140 \n",
      "step: 6978 | Train: G_Loss: 1.244, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.572, D_sup_loss: 0.115, D_sup_acc: 97.18 Train acc: 96.863 Test acc: 97.060 \n",
      "step: 6979 | Train: G_Loss: 1.195, D_unsup_loss_real: 0.544, D_unsup_loss_fake: 0.580, D_sup_loss: 0.124, D_sup_acc: 97.10 Train acc: 96.957 Test acc: 97.140 \n",
      "step: 6980 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.620, D_sup_loss: 0.119, D_sup_acc: 97.18 Train acc: 96.968 Test acc: 97.140 \n",
      "step: 6981 | Train: G_Loss: 1.186, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.581, D_sup_loss: 0.120, D_sup_acc: 97.18 Train acc: 96.983 Test acc: 97.170 \n",
      "step: 6982 | Train: G_Loss: 1.152, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.528, D_sup_loss: 0.126, D_sup_acc: 97.21 Train acc: 97.135 Test acc: 97.240 \n",
      "step: 6983 | Train: G_Loss: 1.194, D_unsup_loss_real: 0.521, D_unsup_loss_fake: 0.511, D_sup_loss: 0.111, D_sup_acc: 97.27 Train acc: 97.037 Test acc: 97.200 \n",
      "step: 6984 | Train: G_Loss: 1.290, D_unsup_loss_real: 0.482, D_unsup_loss_fake: 0.597, D_sup_loss: 0.114, D_sup_acc: 97.24 Train acc: 96.847 Test acc: 96.920 \n",
      "step: 6985 | Train: G_Loss: 1.298, D_unsup_loss_real: 0.474, D_unsup_loss_fake: 0.513, D_sup_loss: 0.125, D_sup_acc: 96.96 Train acc: 97.020 Test acc: 97.140 \n",
      "step: 6986 | Train: G_Loss: 1.107, D_unsup_loss_real: 0.715, D_unsup_loss_fake: 0.546, D_sup_loss: 0.114, D_sup_acc: 97.18 Train acc: 96.370 Test acc: 96.590 \n",
      "step: 6987 | Train: G_Loss: 1.162, D_unsup_loss_real: 0.653, D_unsup_loss_fake: 0.667, D_sup_loss: 0.161, D_sup_acc: 96.63 Train acc: 97.053 Test acc: 97.280 \n",
      "step: 6988 | Train: G_Loss: 1.228, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.501, D_sup_loss: 0.116, D_sup_acc: 97.31 Train acc: 97.133 Test acc: 97.350 \n",
      "step: 6989 | Train: G_Loss: 1.182, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.629, D_sup_loss: 0.108, D_sup_acc: 97.38 Train acc: 97.070 Test acc: 97.240 \n",
      "step: 6990 | Train: G_Loss: 1.097, D_unsup_loss_real: 0.502, D_unsup_loss_fake: 0.535, D_sup_loss: 0.118, D_sup_acc: 97.27 Train acc: 97.138 Test acc: 97.370 \n",
      "step: 6991 | Train: G_Loss: 1.156, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.610, D_sup_loss: 0.113, D_sup_acc: 97.40 Train acc: 96.897 Test acc: 97.160 \n",
      "step: 6992 | Train: G_Loss: 1.211, D_unsup_loss_real: 0.542, D_unsup_loss_fake: 0.626, D_sup_loss: 0.128, D_sup_acc: 97.20 Train acc: 97.043 Test acc: 97.310 \n",
      "step: 6993 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.544, D_sup_loss: 0.117, D_sup_acc: 97.34 Train acc: 96.843 Test acc: 97.180 \n",
      "step: 6994 | Train: G_Loss: 1.304, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.526, D_sup_loss: 0.126, D_sup_acc: 97.22 Train acc: 97.073 Test acc: 97.230 \n",
      "step: 6995 | Train: G_Loss: 1.139, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.595, D_sup_loss: 0.110, D_sup_acc: 97.27 Train acc: 96.993 Test acc: 97.200 \n",
      "step: 6996 | Train: G_Loss: 1.207, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.644, D_sup_loss: 0.121, D_sup_acc: 97.24 Train acc: 96.977 Test acc: 97.140 \n",
      "step: 6997 | Train: G_Loss: 1.351, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.584, D_sup_loss: 0.122, D_sup_acc: 97.18 Train acc: 97.155 Test acc: 97.350 \n",
      "step: 6998 | Train: G_Loss: 1.163, D_unsup_loss_real: 0.631, D_unsup_loss_fake: 0.591, D_sup_loss: 0.109, D_sup_acc: 97.38 Train acc: 96.857 Test acc: 97.200 \n",
      "step: 6999 | Train: G_Loss: 1.175, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.557, D_sup_loss: 0.133, D_sup_acc: 97.24 Train acc: 96.922 Test acc: 97.120 \n",
      "step: 7000 | Train: G_Loss: 1.202, D_unsup_loss_real: 0.498, D_unsup_loss_fake: 0.588, D_sup_loss: 0.126, D_sup_acc: 97.16 Train acc: 96.895 Test acc: 97.060 \n",
      "Train Classifier Accuracy: 96.895%\n",
      "\n",
      "Test Classifier Accuracy: 97.060%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_7000.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_7000.h5\n",
      "step: 7001 | Train: G_Loss: 1.168, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.571, D_sup_loss: 0.122, D_sup_acc: 97.10 Train acc: 97.033 Test acc: 97.170 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 7002 | Train: G_Loss: 1.309, D_unsup_loss_real: 0.527, D_unsup_loss_fake: 0.619, D_sup_loss: 0.120, D_sup_acc: 97.21 Train acc: 97.052 Test acc: 97.190 \n",
      "step: 7003 | Train: G_Loss: 1.287, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.510, D_sup_loss: 0.117, D_sup_acc: 97.23 Train acc: 97.103 Test acc: 97.260 \n",
      "step: 7004 | Train: G_Loss: 1.316, D_unsup_loss_real: 0.659, D_unsup_loss_fake: 0.579, D_sup_loss: 0.112, D_sup_acc: 97.29 Train acc: 97.008 Test acc: 97.100 \n",
      "step: 7005 | Train: G_Loss: 1.166, D_unsup_loss_real: 0.503, D_unsup_loss_fake: 0.513, D_sup_loss: 0.120, D_sup_acc: 97.14 Train acc: 97.028 Test acc: 97.180 \n",
      "step: 7006 | Train: G_Loss: 1.127, D_unsup_loss_real: 0.685, D_unsup_loss_fake: 0.560, D_sup_loss: 0.114, D_sup_acc: 97.22 Train acc: 97.087 Test acc: 97.200 \n",
      "step: 7007 | Train: G_Loss: 1.282, D_unsup_loss_real: 0.523, D_unsup_loss_fake: 0.596, D_sup_loss: 0.115, D_sup_acc: 97.24 Train acc: 97.155 Test acc: 97.160 \n",
      "step: 7008 | Train: G_Loss: 1.181, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.585, D_sup_loss: 0.109, D_sup_acc: 97.20 Train acc: 96.848 Test acc: 96.970 \n",
      "step: 7009 | Train: G_Loss: 1.275, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.517, D_sup_loss: 0.128, D_sup_acc: 97.01 Train acc: 97.057 Test acc: 97.210 \n",
      "step: 7010 | Train: G_Loss: 1.256, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.532, D_sup_loss: 0.114, D_sup_acc: 97.25 Train acc: 97.110 Test acc: 97.190 \n",
      "step: 7011 | Train: G_Loss: 1.220, D_unsup_loss_real: 0.526, D_unsup_loss_fake: 0.624, D_sup_loss: 0.112, D_sup_acc: 97.23 Train acc: 96.890 Test acc: 96.940 \n",
      "step: 7012 | Train: G_Loss: 1.252, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.562, D_sup_loss: 0.125, D_sup_acc: 96.98 Train acc: 97.090 Test acc: 97.280 \n",
      "step: 7013 | Train: G_Loss: 1.195, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.524, D_sup_loss: 0.110, D_sup_acc: 97.31 Train acc: 96.977 Test acc: 97.210 \n",
      "step: 7014 | Train: G_Loss: 1.129, D_unsup_loss_real: 0.639, D_unsup_loss_fake: 0.660, D_sup_loss: 0.121, D_sup_acc: 97.25 Train acc: 96.940 Test acc: 97.100 \n",
      "step: 7015 | Train: G_Loss: 1.157, D_unsup_loss_real: 0.526, D_unsup_loss_fake: 0.535, D_sup_loss: 0.121, D_sup_acc: 97.14 Train acc: 97.032 Test acc: 97.150 \n",
      "step: 7016 | Train: G_Loss: 1.282, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.577, D_sup_loss: 0.113, D_sup_acc: 97.19 Train acc: 96.815 Test acc: 96.920 \n",
      "step: 7017 | Train: G_Loss: 1.259, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.545, D_sup_loss: 0.124, D_sup_acc: 96.96 Train acc: 97.070 Test acc: 97.150 \n",
      "step: 7018 | Train: G_Loss: 1.156, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.592, D_sup_loss: 0.109, D_sup_acc: 97.19 Train acc: 96.698 Test acc: 96.770 \n",
      "step: 7019 | Train: G_Loss: 1.130, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.565, D_sup_loss: 0.133, D_sup_acc: 96.81 Train acc: 96.763 Test acc: 96.870 \n",
      "step: 7020 | Train: G_Loss: 1.223, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.583, D_sup_loss: 0.126, D_sup_acc: 96.91 Train acc: 96.782 Test acc: 96.970 \n",
      "step: 7021 | Train: G_Loss: 1.168, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.588, D_sup_loss: 0.125, D_sup_acc: 97.01 Train acc: 96.737 Test acc: 96.830 \n",
      "step: 7022 | Train: G_Loss: 1.196, D_unsup_loss_real: 0.538, D_unsup_loss_fake: 0.573, D_sup_loss: 0.127, D_sup_acc: 96.87 Train acc: 96.847 Test acc: 96.950 \n",
      "step: 7023 | Train: G_Loss: 1.232, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.531, D_sup_loss: 0.121, D_sup_acc: 96.99 Train acc: 96.853 Test acc: 97.070 \n",
      "step: 7024 | Train: G_Loss: 1.247, D_unsup_loss_real: 0.502, D_unsup_loss_fake: 0.594, D_sup_loss: 0.120, D_sup_acc: 97.11 Train acc: 97.043 Test acc: 97.220 \n",
      "step: 7025 | Train: G_Loss: 1.204, D_unsup_loss_real: 0.517, D_unsup_loss_fake: 0.630, D_sup_loss: 0.113, D_sup_acc: 97.26 Train acc: 96.802 Test acc: 96.960 \n",
      "step: 7026 | Train: G_Loss: 1.253, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.531, D_sup_loss: 0.127, D_sup_acc: 97.00 Train acc: 96.972 Test acc: 97.100 \n",
      "step: 7027 | Train: G_Loss: 1.165, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.597, D_sup_loss: 0.118, D_sup_acc: 97.14 Train acc: 96.990 Test acc: 97.190 \n",
      "step: 7028 | Train: G_Loss: 1.328, D_unsup_loss_real: 0.495, D_unsup_loss_fake: 0.584, D_sup_loss: 0.115, D_sup_acc: 97.23 Train acc: 96.983 Test acc: 97.140 \n",
      "step: 7029 | Train: G_Loss: 1.209, D_unsup_loss_real: 0.531, D_unsup_loss_fake: 0.556, D_sup_loss: 0.115, D_sup_acc: 97.18 Train acc: 96.830 Test acc: 96.980 \n",
      "step: 7030 | Train: G_Loss: 1.234, D_unsup_loss_real: 0.528, D_unsup_loss_fake: 0.567, D_sup_loss: 0.124, D_sup_acc: 97.02 Train acc: 97.030 Test acc: 97.170 \n",
      "step: 7031 | Train: G_Loss: 1.209, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.541, D_sup_loss: 0.116, D_sup_acc: 97.21 Train acc: 96.823 Test acc: 97.020 \n",
      "step: 7032 | Train: G_Loss: 1.176, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.685, D_sup_loss: 0.129, D_sup_acc: 97.06 Train acc: 96.822 Test acc: 96.940 \n",
      "step: 7033 | Train: G_Loss: 1.332, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.567, D_sup_loss: 0.136, D_sup_acc: 96.98 Train acc: 96.965 Test acc: 96.980 \n",
      "step: 7034 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.573, D_sup_loss: 0.121, D_sup_acc: 97.02 Train acc: 96.930 Test acc: 97.080 \n",
      "step: 7035 | Train: G_Loss: 1.184, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.581, D_sup_loss: 0.123, D_sup_acc: 97.12 Train acc: 96.737 Test acc: 96.940 \n",
      "step: 7036 | Train: G_Loss: 1.141, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.536, D_sup_loss: 0.131, D_sup_acc: 96.98 Train acc: 96.865 Test acc: 97.030 \n",
      "step: 7037 | Train: G_Loss: 1.239, D_unsup_loss_real: 0.523, D_unsup_loss_fake: 0.586, D_sup_loss: 0.127, D_sup_acc: 97.07 Train acc: 97.097 Test acc: 97.190 \n",
      "step: 7038 | Train: G_Loss: 1.304, D_unsup_loss_real: 0.512, D_unsup_loss_fake: 0.561, D_sup_loss: 0.111, D_sup_acc: 97.23 Train acc: 97.123 Test acc: 97.250 \n",
      "step: 7039 | Train: G_Loss: 1.084, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.605, D_sup_loss: 0.112, D_sup_acc: 97.28 Train acc: 96.873 Test acc: 97.030 \n",
      "step: 7040 | Train: G_Loss: 1.180, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.618, D_sup_loss: 0.122, D_sup_acc: 97.07 Train acc: 96.640 Test acc: 96.860 \n",
      "step: 7041 | Train: G_Loss: 1.210, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.556, D_sup_loss: 0.143, D_sup_acc: 96.90 Train acc: 97.038 Test acc: 97.130 \n",
      "step: 7042 | Train: G_Loss: 1.193, D_unsup_loss_real: 0.537, D_unsup_loss_fake: 0.569, D_sup_loss: 0.114, D_sup_acc: 97.17 Train acc: 97.022 Test acc: 97.110 \n",
      "step: 7043 | Train: G_Loss: 1.251, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.577, D_sup_loss: 0.118, D_sup_acc: 97.15 Train acc: 96.997 Test acc: 97.100 \n",
      "step: 7044 | Train: G_Loss: 1.250, D_unsup_loss_real: 0.533, D_unsup_loss_fake: 0.553, D_sup_loss: 0.118, D_sup_acc: 97.14 Train acc: 96.843 Test acc: 97.010 \n",
      "step: 7045 | Train: G_Loss: 1.218, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.550, D_sup_loss: 0.128, D_sup_acc: 97.05 Train acc: 96.868 Test acc: 97.020 \n",
      "step: 7046 | Train: G_Loss: 1.218, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.511, D_sup_loss: 0.123, D_sup_acc: 97.06 Train acc: 96.890 Test acc: 97.000 \n",
      "step: 7047 | Train: G_Loss: 1.170, D_unsup_loss_real: 0.518, D_unsup_loss_fake: 0.611, D_sup_loss: 0.120, D_sup_acc: 97.04 Train acc: 96.753 Test acc: 96.920 \n",
      "step: 7048 | Train: G_Loss: 1.184, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.646, D_sup_loss: 0.131, D_sup_acc: 96.96 Train acc: 96.953 Test acc: 97.050 \n",
      "step: 7049 | Train: G_Loss: 1.231, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.578, D_sup_loss: 0.121, D_sup_acc: 97.09 Train acc: 96.933 Test acc: 97.100 \n",
      "step: 7050 | Train: G_Loss: 1.156, D_unsup_loss_real: 0.484, D_unsup_loss_fake: 0.535, D_sup_loss: 0.119, D_sup_acc: 97.14 Train acc: 97.058 Test acc: 97.110 \n",
      "step: 7051 | Train: G_Loss: 1.259, D_unsup_loss_real: 0.657, D_unsup_loss_fake: 0.506, D_sup_loss: 0.113, D_sup_acc: 97.15 Train acc: 96.883 Test acc: 97.040 \n",
      "step: 7052 | Train: G_Loss: 1.201, D_unsup_loss_real: 0.507, D_unsup_loss_fake: 0.565, D_sup_loss: 0.126, D_sup_acc: 97.08 Train acc: 96.817 Test acc: 97.050 \n",
      "step: 7053 | Train: G_Loss: 1.187, D_unsup_loss_real: 0.546, D_unsup_loss_fake: 0.594, D_sup_loss: 0.126, D_sup_acc: 97.09 Train acc: 96.913 Test acc: 97.040 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 7054 | Train: G_Loss: 1.153, D_unsup_loss_real: 0.534, D_unsup_loss_fake: 0.540, D_sup_loss: 0.123, D_sup_acc: 97.08 Train acc: 96.937 Test acc: 97.040 \n",
      "step: 7055 | Train: G_Loss: 1.305, D_unsup_loss_real: 0.516, D_unsup_loss_fake: 0.558, D_sup_loss: 0.123, D_sup_acc: 97.08 Train acc: 97.008 Test acc: 97.100 \n",
      "step: 7056 | Train: G_Loss: 1.341, D_unsup_loss_real: 0.521, D_unsup_loss_fake: 0.496, D_sup_loss: 0.116, D_sup_acc: 97.14 Train acc: 96.953 Test acc: 97.120 \n",
      "step: 7057 | Train: G_Loss: 1.279, D_unsup_loss_real: 0.541, D_unsup_loss_fake: 0.553, D_sup_loss: 0.115, D_sup_acc: 97.16 Train acc: 96.910 Test acc: 97.070 \n",
      "step: 7058 | Train: G_Loss: 1.208, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.543, D_sup_loss: 0.122, D_sup_acc: 97.11 Train acc: 96.922 Test acc: 97.040 \n",
      "step: 7059 | Train: G_Loss: 1.309, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.525, D_sup_loss: 0.119, D_sup_acc: 97.08 Train acc: 96.847 Test acc: 96.960 \n",
      "step: 7060 | Train: G_Loss: 1.159, D_unsup_loss_real: 0.674, D_unsup_loss_fake: 0.527, D_sup_loss: 0.120, D_sup_acc: 97.00 Train acc: 96.982 Test acc: 97.120 \n",
      "step: 7061 | Train: G_Loss: 1.171, D_unsup_loss_real: 0.475, D_unsup_loss_fake: 0.628, D_sup_loss: 0.114, D_sup_acc: 97.16 Train acc: 96.932 Test acc: 97.130 \n",
      "step: 7062 | Train: G_Loss: 1.160, D_unsup_loss_real: 0.535, D_unsup_loss_fake: 0.583, D_sup_loss: 0.120, D_sup_acc: 97.17 Train acc: 96.967 Test acc: 96.960 \n",
      "step: 7063 | Train: G_Loss: 1.348, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.605, D_sup_loss: 0.118, D_sup_acc: 97.00 Train acc: 97.000 Test acc: 97.140 \n",
      "step: 7064 | Train: G_Loss: 1.200, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.496, D_sup_loss: 0.116, D_sup_acc: 97.18 Train acc: 96.837 Test acc: 96.900 \n",
      "step: 7065 | Train: G_Loss: 1.293, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.634, D_sup_loss: 0.123, D_sup_acc: 96.94 Train acc: 96.893 Test acc: 97.140 \n",
      "step: 7066 | Train: G_Loss: 1.195, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.578, D_sup_loss: 0.121, D_sup_acc: 97.18 Train acc: 96.617 Test acc: 96.790 \n",
      "step: 7067 | Train: G_Loss: 1.268, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.587, D_sup_loss: 0.140, D_sup_acc: 96.83 Train acc: 96.997 Test acc: 97.100 \n",
      "step: 7068 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.573, D_sup_loss: 0.118, D_sup_acc: 97.14 Train acc: 97.015 Test acc: 97.160 \n",
      "step: 7069 | Train: G_Loss: 1.261, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.578, D_sup_loss: 0.119, D_sup_acc: 97.20 Train acc: 96.908 Test acc: 97.090 \n",
      "step: 7070 | Train: G_Loss: 1.205, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.593, D_sup_loss: 0.124, D_sup_acc: 97.13 Train acc: 96.965 Test acc: 97.080 \n",
      "step: 7071 | Train: G_Loss: 1.215, D_unsup_loss_real: 0.545, D_unsup_loss_fake: 0.557, D_sup_loss: 0.121, D_sup_acc: 97.12 Train acc: 97.023 Test acc: 97.140 \n",
      "step: 7072 | Train: G_Loss: 1.195, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.495, D_sup_loss: 0.118, D_sup_acc: 97.18 Train acc: 97.007 Test acc: 97.200 \n",
      "step: 7073 | Train: G_Loss: 1.197, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.634, D_sup_loss: 0.117, D_sup_acc: 97.24 Train acc: 97.067 Test acc: 97.270 \n",
      "step: 7074 | Train: G_Loss: 1.153, D_unsup_loss_real: 0.536, D_unsup_loss_fake: 0.565, D_sup_loss: 0.113, D_sup_acc: 97.30 Train acc: 96.950 Test acc: 97.190 \n",
      "step: 7075 | Train: G_Loss: 1.268, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.550, D_sup_loss: 0.119, D_sup_acc: 97.23 Train acc: 97.008 Test acc: 97.210 \n",
      "step: 7076 | Train: G_Loss: 1.185, D_unsup_loss_real: 0.646, D_unsup_loss_fake: 0.574, D_sup_loss: 0.119, D_sup_acc: 97.25 Train acc: 96.840 Test acc: 96.930 \n",
      "step: 7077 | Train: G_Loss: 1.284, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.557, D_sup_loss: 0.129, D_sup_acc: 96.97 Train acc: 97.018 Test acc: 97.060 \n",
      "step: 7078 | Train: G_Loss: 1.200, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.537, D_sup_loss: 0.117, D_sup_acc: 97.10 Train acc: 96.935 Test acc: 97.050 \n",
      "step: 7079 | Train: G_Loss: 1.305, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.607, D_sup_loss: 0.125, D_sup_acc: 97.09 Train acc: 97.117 Test acc: 97.230 \n",
      "step: 7080 | Train: G_Loss: 1.170, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.520, D_sup_loss: 0.114, D_sup_acc: 97.27 Train acc: 97.087 Test acc: 97.100 \n",
      "step: 7081 | Train: G_Loss: 1.108, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.588, D_sup_loss: 0.114, D_sup_acc: 97.14 Train acc: 96.980 Test acc: 97.010 \n",
      "step: 7082 | Train: G_Loss: 1.203, D_unsup_loss_real: 0.524, D_unsup_loss_fake: 0.592, D_sup_loss: 0.124, D_sup_acc: 97.05 Train acc: 96.902 Test acc: 96.960 \n",
      "step: 7083 | Train: G_Loss: 1.223, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.567, D_sup_loss: 0.129, D_sup_acc: 97.00 Train acc: 97.018 Test acc: 97.180 \n",
      "step: 7084 | Train: G_Loss: 1.246, D_unsup_loss_real: 0.508, D_unsup_loss_fake: 0.611, D_sup_loss: 0.117, D_sup_acc: 97.22 Train acc: 96.995 Test acc: 97.090 \n",
      "step: 7085 | Train: G_Loss: 1.306, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.576, D_sup_loss: 0.121, D_sup_acc: 97.13 Train acc: 97.100 Test acc: 97.210 \n",
      "step: 7086 | Train: G_Loss: 1.123, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.545, D_sup_loss: 0.113, D_sup_acc: 97.25 Train acc: 96.892 Test acc: 97.060 \n",
      "step: 7087 | Train: G_Loss: 1.181, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.513, D_sup_loss: 0.126, D_sup_acc: 97.10 Train acc: 97.045 Test acc: 97.190 \n",
      "step: 7088 | Train: G_Loss: 1.246, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.499, D_sup_loss: 0.118, D_sup_acc: 97.23 Train acc: 96.908 Test acc: 97.010 \n",
      "step: 7089 | Train: G_Loss: 1.177, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.532, D_sup_loss: 0.118, D_sup_acc: 97.05 Train acc: 96.725 Test acc: 96.980 \n",
      "step: 7090 | Train: G_Loss: 1.227, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.619, D_sup_loss: 0.135, D_sup_acc: 97.02 Train acc: 97.107 Test acc: 97.150 \n",
      "step: 7091 | Train: G_Loss: 1.280, D_unsup_loss_real: 0.537, D_unsup_loss_fake: 0.627, D_sup_loss: 0.113, D_sup_acc: 97.19 Train acc: 96.972 Test acc: 97.120 \n",
      "step: 7092 | Train: G_Loss: 1.432, D_unsup_loss_real: 0.535, D_unsup_loss_fake: 0.618, D_sup_loss: 0.120, D_sup_acc: 97.16 Train acc: 96.902 Test acc: 97.100 \n",
      "step: 7093 | Train: G_Loss: 1.319, D_unsup_loss_real: 0.663, D_unsup_loss_fake: 0.530, D_sup_loss: 0.116, D_sup_acc: 97.14 Train acc: 96.033 Test acc: 96.190 \n",
      "step: 7094 | Train: G_Loss: 1.271, D_unsup_loss_real: 0.787, D_unsup_loss_fake: 0.630, D_sup_loss: 0.186, D_sup_acc: 96.24 Train acc: 97.047 Test acc: 97.160 \n",
      "step: 7095 | Train: G_Loss: 1.123, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.571, D_sup_loss: 0.112, D_sup_acc: 97.20 Train acc: 96.843 Test acc: 97.010 \n",
      "step: 7096 | Train: G_Loss: 1.172, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.556, D_sup_loss: 0.123, D_sup_acc: 97.05 Train acc: 97.090 Test acc: 97.240 \n",
      "step: 7097 | Train: G_Loss: 1.210, D_unsup_loss_real: 0.525, D_unsup_loss_fake: 0.662, D_sup_loss: 0.110, D_sup_acc: 97.27 Train acc: 97.035 Test acc: 97.250 \n",
      "step: 7098 | Train: G_Loss: 1.093, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.660, D_sup_loss: 0.114, D_sup_acc: 97.28 Train acc: 96.920 Test acc: 97.120 \n",
      "step: 7099 | Train: G_Loss: 1.023, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.516, D_sup_loss: 0.125, D_sup_acc: 97.16 Train acc: 96.737 Test acc: 97.040 \n",
      "step: 7100 | Train: G_Loss: 1.268, D_unsup_loss_real: 0.503, D_unsup_loss_fake: 0.604, D_sup_loss: 0.129, D_sup_acc: 97.08 Train acc: 97.020 Test acc: 97.240 \n",
      "Train Classifier Accuracy: 97.020%\n",
      "\n",
      "Test Classifier Accuracy: 97.240%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_7100.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_7100.h5\n",
      "step: 7101 | Train: G_Loss: 1.259, D_unsup_loss_real: 0.513, D_unsup_loss_fake: 0.671, D_sup_loss: 0.117, D_sup_acc: 97.27 Train acc: 97.095 Test acc: 97.340 \n",
      "step: 7102 | Train: G_Loss: 1.133, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.470, D_sup_loss: 0.112, D_sup_acc: 97.37 Train acc: 97.057 Test acc: 97.260 \n",
      "step: 7103 | Train: G_Loss: 1.130, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.632, D_sup_loss: 0.115, D_sup_acc: 97.29 Train acc: 97.027 Test acc: 97.260 \n",
      "step: 7104 | Train: G_Loss: 1.328, D_unsup_loss_real: 0.541, D_unsup_loss_fake: 0.537, D_sup_loss: 0.120, D_sup_acc: 97.29 Train acc: 97.075 Test acc: 97.290 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 7105 | Train: G_Loss: 1.188, D_unsup_loss_real: 0.508, D_unsup_loss_fake: 0.585, D_sup_loss: 0.112, D_sup_acc: 97.32 Train acc: 97.023 Test acc: 97.250 \n",
      "step: 7106 | Train: G_Loss: 1.325, D_unsup_loss_real: 0.495, D_unsup_loss_fake: 0.618, D_sup_loss: 0.116, D_sup_acc: 97.28 Train acc: 97.050 Test acc: 97.200 \n",
      "step: 7107 | Train: G_Loss: 1.232, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.556, D_sup_loss: 0.116, D_sup_acc: 97.24 Train acc: 96.813 Test acc: 97.060 \n",
      "step: 7108 | Train: G_Loss: 1.312, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.548, D_sup_loss: 0.132, D_sup_acc: 97.10 Train acc: 96.942 Test acc: 97.110 \n",
      "step: 7109 | Train: G_Loss: 1.228, D_unsup_loss_real: 0.690, D_unsup_loss_fake: 0.580, D_sup_loss: 0.121, D_sup_acc: 97.15 Train acc: 96.795 Test acc: 97.090 \n",
      "step: 7110 | Train: G_Loss: 1.288, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.548, D_sup_loss: 0.135, D_sup_acc: 97.13 Train acc: 97.045 Test acc: 97.160 \n",
      "step: 7111 | Train: G_Loss: 1.117, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.544, D_sup_loss: 0.116, D_sup_acc: 97.20 Train acc: 96.830 Test acc: 96.960 \n",
      "step: 7112 | Train: G_Loss: 1.149, D_unsup_loss_real: 0.532, D_unsup_loss_fake: 0.531, D_sup_loss: 0.126, D_sup_acc: 97.00 Train acc: 96.888 Test acc: 97.050 \n",
      "step: 7113 | Train: G_Loss: 1.253, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.572, D_sup_loss: 0.125, D_sup_acc: 97.09 Train acc: 96.872 Test acc: 97.070 \n",
      "step: 7114 | Train: G_Loss: 1.122, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.550, D_sup_loss: 0.122, D_sup_acc: 97.11 Train acc: 96.712 Test acc: 96.920 \n",
      "step: 7115 | Train: G_Loss: 1.226, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.565, D_sup_loss: 0.133, D_sup_acc: 96.96 Train acc: 96.900 Test acc: 97.050 \n",
      "step: 7116 | Train: G_Loss: 1.204, D_unsup_loss_real: 0.514, D_unsup_loss_fake: 0.609, D_sup_loss: 0.121, D_sup_acc: 97.09 Train acc: 96.852 Test acc: 97.000 \n",
      "step: 7117 | Train: G_Loss: 1.228, D_unsup_loss_real: 0.458, D_unsup_loss_fake: 0.542, D_sup_loss: 0.121, D_sup_acc: 97.04 Train acc: 96.917 Test acc: 97.020 \n",
      "step: 7118 | Train: G_Loss: 1.197, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.531, D_sup_loss: 0.115, D_sup_acc: 97.06 Train acc: 96.790 Test acc: 96.930 \n",
      "step: 7119 | Train: G_Loss: 1.249, D_unsup_loss_real: 0.517, D_unsup_loss_fake: 0.573, D_sup_loss: 0.126, D_sup_acc: 96.97 Train acc: 96.907 Test acc: 97.120 \n",
      "step: 7120 | Train: G_Loss: 1.307, D_unsup_loss_real: 0.506, D_unsup_loss_fake: 0.617, D_sup_loss: 0.117, D_sup_acc: 97.16 Train acc: 96.992 Test acc: 97.120 \n",
      "step: 7121 | Train: G_Loss: 1.171, D_unsup_loss_real: 0.654, D_unsup_loss_fake: 0.547, D_sup_loss: 0.116, D_sup_acc: 97.16 Train acc: 96.963 Test acc: 97.140 \n",
      "step: 7122 | Train: G_Loss: 1.199, D_unsup_loss_real: 0.652, D_unsup_loss_fake: 0.563, D_sup_loss: 0.119, D_sup_acc: 97.18 Train acc: 96.910 Test acc: 97.140 \n",
      "step: 7123 | Train: G_Loss: 1.249, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.606, D_sup_loss: 0.125, D_sup_acc: 97.18 Train acc: 96.820 Test acc: 97.080 \n",
      "step: 7124 | Train: G_Loss: 1.187, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.476, D_sup_loss: 0.124, D_sup_acc: 97.12 Train acc: 96.788 Test acc: 96.970 \n",
      "step: 7125 | Train: G_Loss: 1.316, D_unsup_loss_real: 0.471, D_unsup_loss_fake: 0.565, D_sup_loss: 0.125, D_sup_acc: 97.01 Train acc: 96.647 Test acc: 96.830 \n",
      "step: 7126 | Train: G_Loss: 1.378, D_unsup_loss_real: 0.674, D_unsup_loss_fake: 0.582, D_sup_loss: 0.133, D_sup_acc: 96.87 Train acc: 96.845 Test acc: 97.040 \n",
      "step: 7127 | Train: G_Loss: 1.285, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.560, D_sup_loss: 0.121, D_sup_acc: 97.08 Train acc: 96.977 Test acc: 97.140 \n",
      "step: 7128 | Train: G_Loss: 1.259, D_unsup_loss_real: 0.514, D_unsup_loss_fake: 0.548, D_sup_loss: 0.116, D_sup_acc: 97.18 Train acc: 96.997 Test acc: 97.200 \n",
      "step: 7129 | Train: G_Loss: 1.232, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.528, D_sup_loss: 0.115, D_sup_acc: 97.24 Train acc: 97.055 Test acc: 97.250 \n",
      "step: 7130 | Train: G_Loss: 1.175, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.571, D_sup_loss: 0.114, D_sup_acc: 97.28 Train acc: 96.988 Test acc: 97.240 \n",
      "step: 7131 | Train: G_Loss: 1.230, D_unsup_loss_real: 0.530, D_unsup_loss_fake: 0.548, D_sup_loss: 0.119, D_sup_acc: 97.27 Train acc: 96.797 Test acc: 97.000 \n",
      "step: 7132 | Train: G_Loss: 1.264, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.529, D_sup_loss: 0.127, D_sup_acc: 97.04 Train acc: 97.063 Test acc: 97.320 \n",
      "step: 7133 | Train: G_Loss: 1.191, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.563, D_sup_loss: 0.110, D_sup_acc: 97.35 Train acc: 96.993 Test acc: 97.240 \n",
      "step: 7134 | Train: G_Loss: 1.243, D_unsup_loss_real: 0.538, D_unsup_loss_fake: 0.579, D_sup_loss: 0.119, D_sup_acc: 97.27 Train acc: 96.927 Test acc: 97.170 \n",
      "step: 7135 | Train: G_Loss: 1.268, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.537, D_sup_loss: 0.118, D_sup_acc: 97.21 Train acc: 96.827 Test acc: 97.120 \n",
      "step: 7136 | Train: G_Loss: 1.260, D_unsup_loss_real: 0.518, D_unsup_loss_fake: 0.586, D_sup_loss: 0.122, D_sup_acc: 97.16 Train acc: 96.907 Test acc: 97.100 \n",
      "step: 7137 | Train: G_Loss: 1.220, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.541, D_sup_loss: 0.118, D_sup_acc: 97.14 Train acc: 96.932 Test acc: 97.170 \n",
      "step: 7138 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.546, D_unsup_loss_fake: 0.566, D_sup_loss: 0.117, D_sup_acc: 97.21 Train acc: 96.870 Test acc: 97.070 \n",
      "step: 7139 | Train: G_Loss: 1.328, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.516, D_sup_loss: 0.121, D_sup_acc: 97.11 Train acc: 97.047 Test acc: 97.250 \n",
      "step: 7140 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.616, D_sup_loss: 0.108, D_sup_acc: 97.28 Train acc: 96.752 Test acc: 96.900 \n",
      "step: 7141 | Train: G_Loss: 1.265, D_unsup_loss_real: 0.544, D_unsup_loss_fake: 0.595, D_sup_loss: 0.130, D_sup_acc: 96.94 Train acc: 96.912 Test acc: 97.160 \n",
      "step: 7142 | Train: G_Loss: 1.176, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.628, D_sup_loss: 0.117, D_sup_acc: 97.20 Train acc: 96.875 Test acc: 97.060 \n",
      "step: 7143 | Train: G_Loss: 1.408, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.583, D_sup_loss: 0.123, D_sup_acc: 97.10 Train acc: 97.102 Test acc: 97.220 \n",
      "step: 7144 | Train: G_Loss: 1.301, D_unsup_loss_real: 0.663, D_unsup_loss_fake: 0.502, D_sup_loss: 0.109, D_sup_acc: 97.26 Train acc: 96.652 Test acc: 96.900 \n",
      "step: 7145 | Train: G_Loss: 1.305, D_unsup_loss_real: 0.677, D_unsup_loss_fake: 0.681, D_sup_loss: 0.143, D_sup_acc: 96.94 Train acc: 97.160 Test acc: 97.410 \n",
      "step: 7146 | Train: G_Loss: 1.066, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.566, D_sup_loss: 0.105, D_sup_acc: 97.44 Train acc: 97.088 Test acc: 97.190 \n",
      "step: 7147 | Train: G_Loss: 1.210, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.637, D_sup_loss: 0.114, D_sup_acc: 97.23 Train acc: 96.912 Test acc: 97.100 \n",
      "step: 7148 | Train: G_Loss: 1.160, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.587, D_sup_loss: 0.123, D_sup_acc: 97.14 Train acc: 96.900 Test acc: 97.120 \n",
      "step: 7149 | Train: G_Loss: 1.222, D_unsup_loss_real: 0.524, D_unsup_loss_fake: 0.583, D_sup_loss: 0.123, D_sup_acc: 97.16 Train acc: 96.910 Test acc: 97.070 \n",
      "step: 7150 | Train: G_Loss: 1.164, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.568, D_sup_loss: 0.118, D_sup_acc: 97.11 Train acc: 96.745 Test acc: 96.940 \n",
      "step: 7151 | Train: G_Loss: 1.220, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.565, D_sup_loss: 0.129, D_sup_acc: 96.98 Train acc: 96.777 Test acc: 96.930 \n",
      "step: 7152 | Train: G_Loss: 1.168, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.562, D_sup_loss: 0.127, D_sup_acc: 96.97 Train acc: 96.678 Test acc: 96.810 \n",
      "step: 7153 | Train: G_Loss: 1.281, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.584, D_sup_loss: 0.131, D_sup_acc: 96.85 Train acc: 96.950 Test acc: 97.190 \n",
      "step: 7154 | Train: G_Loss: 1.246, D_unsup_loss_real: 0.541, D_unsup_loss_fake: 0.509, D_sup_loss: 0.115, D_sup_acc: 97.23 Train acc: 96.880 Test acc: 96.990 \n",
      "step: 7155 | Train: G_Loss: 1.229, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.614, D_sup_loss: 0.122, D_sup_acc: 97.03 Train acc: 96.810 Test acc: 97.020 \n",
      "step: 7156 | Train: G_Loss: 1.215, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.525, D_sup_loss: 0.128, D_sup_acc: 97.06 Train acc: 96.907 Test acc: 97.040 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 7157 | Train: G_Loss: 1.268, D_unsup_loss_real: 0.504, D_unsup_loss_fake: 0.541, D_sup_loss: 0.117, D_sup_acc: 97.08 Train acc: 96.905 Test acc: 97.050 \n",
      "step: 7158 | Train: G_Loss: 1.186, D_unsup_loss_real: 0.523, D_unsup_loss_fake: 0.581, D_sup_loss: 0.115, D_sup_acc: 97.09 Train acc: 96.875 Test acc: 97.030 \n",
      "step: 7159 | Train: G_Loss: 1.210, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.594, D_sup_loss: 0.121, D_sup_acc: 97.07 Train acc: 96.920 Test acc: 97.040 \n",
      "step: 7160 | Train: G_Loss: 1.195, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.587, D_sup_loss: 0.120, D_sup_acc: 97.08 Train acc: 96.673 Test acc: 96.840 \n",
      "step: 7161 | Train: G_Loss: 1.159, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.613, D_sup_loss: 0.134, D_sup_acc: 96.88 Train acc: 96.780 Test acc: 96.920 \n",
      "step: 7162 | Train: G_Loss: 1.232, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.527, D_sup_loss: 0.126, D_sup_acc: 96.96 Train acc: 96.748 Test acc: 96.930 \n",
      "step: 7163 | Train: G_Loss: 1.205, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.578, D_sup_loss: 0.126, D_sup_acc: 96.97 Train acc: 96.905 Test acc: 97.120 \n",
      "step: 7164 | Train: G_Loss: 1.209, D_unsup_loss_real: 0.656, D_unsup_loss_fake: 0.504, D_sup_loss: 0.111, D_sup_acc: 97.16 Train acc: 95.870 Test acc: 96.180 \n",
      "step: 7165 | Train: G_Loss: 1.190, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.608, D_sup_loss: 0.183, D_sup_acc: 96.23 Train acc: 96.788 Test acc: 97.050 \n",
      "step: 7166 | Train: G_Loss: 1.215, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.557, D_sup_loss: 0.123, D_sup_acc: 97.09 Train acc: 96.805 Test acc: 97.090 \n",
      "step: 7167 | Train: G_Loss: 1.160, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.579, D_sup_loss: 0.120, D_sup_acc: 97.13 Train acc: 96.670 Test acc: 96.890 \n",
      "step: 7168 | Train: G_Loss: 1.204, D_unsup_loss_real: 0.545, D_unsup_loss_fake: 0.586, D_sup_loss: 0.131, D_sup_acc: 96.93 Train acc: 96.890 Test acc: 97.090 \n",
      "step: 7169 | Train: G_Loss: 1.309, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.637, D_sup_loss: 0.121, D_sup_acc: 97.13 Train acc: 96.807 Test acc: 97.010 \n",
      "step: 7170 | Train: G_Loss: 1.159, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.598, D_sup_loss: 0.126, D_sup_acc: 97.05 Train acc: 96.862 Test acc: 97.040 \n",
      "step: 7171 | Train: G_Loss: 1.210, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.563, D_sup_loss: 0.126, D_sup_acc: 97.08 Train acc: 96.747 Test acc: 97.010 \n",
      "step: 7172 | Train: G_Loss: 1.229, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.551, D_sup_loss: 0.131, D_sup_acc: 97.05 Train acc: 96.755 Test acc: 96.960 \n",
      "step: 7173 | Train: G_Loss: 1.312, D_unsup_loss_real: 0.533, D_unsup_loss_fake: 0.558, D_sup_loss: 0.131, D_sup_acc: 97.00 Train acc: 96.862 Test acc: 97.070 \n",
      "step: 7174 | Train: G_Loss: 1.096, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.544, D_sup_loss: 0.121, D_sup_acc: 97.11 Train acc: 96.845 Test acc: 97.080 \n",
      "step: 7175 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.619, D_sup_loss: 0.120, D_sup_acc: 97.12 Train acc: 96.723 Test acc: 96.980 \n",
      "step: 7176 | Train: G_Loss: 1.172, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.558, D_sup_loss: 0.134, D_sup_acc: 97.02 Train acc: 96.970 Test acc: 97.220 \n",
      "step: 7177 | Train: G_Loss: 1.187, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.537, D_sup_loss: 0.119, D_sup_acc: 97.26 Train acc: 97.025 Test acc: 97.190 \n",
      "step: 7178 | Train: G_Loss: 1.210, D_unsup_loss_real: 0.526, D_unsup_loss_fake: 0.584, D_sup_loss: 0.114, D_sup_acc: 97.23 Train acc: 96.970 Test acc: 97.190 \n",
      "step: 7179 | Train: G_Loss: 1.191, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.535, D_sup_loss: 0.118, D_sup_acc: 97.23 Train acc: 96.980 Test acc: 97.320 \n",
      "step: 7180 | Train: G_Loss: 1.281, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.633, D_sup_loss: 0.118, D_sup_acc: 97.35 Train acc: 96.625 Test acc: 96.720 \n",
      "step: 7181 | Train: G_Loss: 1.354, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.568, D_sup_loss: 0.142, D_sup_acc: 96.76 Train acc: 97.008 Test acc: 97.190 \n",
      "step: 7182 | Train: G_Loss: 1.172, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.600, D_sup_loss: 0.115, D_sup_acc: 97.23 Train acc: 97.007 Test acc: 97.270 \n",
      "step: 7183 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.518, D_unsup_loss_fake: 0.556, D_sup_loss: 0.119, D_sup_acc: 97.30 Train acc: 96.885 Test acc: 97.010 \n",
      "step: 7184 | Train: G_Loss: 1.166, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.615, D_sup_loss: 0.123, D_sup_acc: 97.05 Train acc: 96.987 Test acc: 97.200 \n",
      "step: 7185 | Train: G_Loss: 1.217, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.565, D_sup_loss: 0.120, D_sup_acc: 97.24 Train acc: 97.010 Test acc: 97.160 \n",
      "step: 7186 | Train: G_Loss: 1.174, D_unsup_loss_real: 0.532, D_unsup_loss_fake: 0.567, D_sup_loss: 0.120, D_sup_acc: 97.20 Train acc: 96.830 Test acc: 97.010 \n",
      "step: 7187 | Train: G_Loss: 1.212, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.590, D_sup_loss: 0.130, D_sup_acc: 97.05 Train acc: 96.982 Test acc: 97.070 \n",
      "step: 7188 | Train: G_Loss: 1.264, D_unsup_loss_real: 0.509, D_unsup_loss_fake: 0.625, D_sup_loss: 0.119, D_sup_acc: 97.11 Train acc: 96.932 Test acc: 97.100 \n",
      "step: 7189 | Train: G_Loss: 1.206, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.547, D_sup_loss: 0.123, D_sup_acc: 97.14 Train acc: 97.055 Test acc: 97.160 \n",
      "step: 7190 | Train: G_Loss: 1.228, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.551, D_sup_loss: 0.118, D_sup_acc: 97.20 Train acc: 96.940 Test acc: 97.060 \n",
      "step: 7191 | Train: G_Loss: 1.179, D_unsup_loss_real: 0.545, D_unsup_loss_fake: 0.514, D_sup_loss: 0.123, D_sup_acc: 97.10 Train acc: 96.982 Test acc: 97.050 \n",
      "step: 7192 | Train: G_Loss: 1.254, D_unsup_loss_real: 0.546, D_unsup_loss_fake: 0.520, D_sup_loss: 0.120, D_sup_acc: 97.09 Train acc: 96.983 Test acc: 97.160 \n",
      "step: 7193 | Train: G_Loss: 1.239, D_unsup_loss_real: 0.541, D_unsup_loss_fake: 0.606, D_sup_loss: 0.114, D_sup_acc: 97.20 Train acc: 96.863 Test acc: 97.000 \n",
      "step: 7194 | Train: G_Loss: 1.208, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.552, D_sup_loss: 0.125, D_sup_acc: 97.04 Train acc: 96.767 Test acc: 96.910 \n",
      "step: 7195 | Train: G_Loss: 1.269, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.598, D_sup_loss: 0.129, D_sup_acc: 96.95 Train acc: 96.735 Test acc: 96.900 \n",
      "step: 7196 | Train: G_Loss: 1.323, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.545, D_sup_loss: 0.134, D_sup_acc: 96.94 Train acc: 97.003 Test acc: 97.100 \n",
      "step: 7197 | Train: G_Loss: 1.265, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.571, D_sup_loss: 0.120, D_sup_acc: 97.14 Train acc: 96.915 Test acc: 96.990 \n",
      "step: 7198 | Train: G_Loss: 1.284, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.607, D_sup_loss: 0.121, D_sup_acc: 97.03 Train acc: 96.683 Test acc: 96.860 \n",
      "step: 7199 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.563, D_sup_loss: 0.133, D_sup_acc: 96.90 Train acc: 96.987 Test acc: 97.120 \n",
      "step: 7200 | Train: G_Loss: 1.189, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.542, D_sup_loss: 0.117, D_sup_acc: 97.16 Train acc: 96.977 Test acc: 97.170 \n",
      "Train Classifier Accuracy: 96.977%\n",
      "\n",
      "Test Classifier Accuracy: 97.170%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_7200.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_7200.h5\n",
      "step: 7201 | Train: G_Loss: 1.349, D_unsup_loss_real: 0.550, D_unsup_loss_fake: 0.561, D_sup_loss: 0.117, D_sup_acc: 97.21 Train acc: 97.050 Test acc: 97.290 \n",
      "step: 7202 | Train: G_Loss: 1.141, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.629, D_sup_loss: 0.114, D_sup_acc: 97.32 Train acc: 97.037 Test acc: 97.330 \n",
      "step: 7203 | Train: G_Loss: 1.112, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.530, D_sup_loss: 0.116, D_sup_acc: 97.36 Train acc: 97.018 Test acc: 97.210 \n",
      "step: 7204 | Train: G_Loss: 1.244, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.587, D_sup_loss: 0.120, D_sup_acc: 97.25 Train acc: 96.817 Test acc: 96.960 \n",
      "step: 7205 | Train: G_Loss: 1.234, D_unsup_loss_real: 0.531, D_unsup_loss_fake: 0.597, D_sup_loss: 0.127, D_sup_acc: 97.00 Train acc: 96.918 Test acc: 97.030 \n",
      "step: 7206 | Train: G_Loss: 1.206, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.547, D_sup_loss: 0.119, D_sup_acc: 97.07 Train acc: 96.787 Test acc: 96.960 \n",
      "step: 7207 | Train: G_Loss: 1.205, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.602, D_sup_loss: 0.132, D_sup_acc: 97.00 Train acc: 96.903 Test acc: 97.090 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 7208 | Train: G_Loss: 1.284, D_unsup_loss_real: 0.526, D_unsup_loss_fake: 0.607, D_sup_loss: 0.122, D_sup_acc: 97.13 Train acc: 96.765 Test acc: 96.970 \n",
      "step: 7209 | Train: G_Loss: 1.371, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.492, D_sup_loss: 0.132, D_sup_acc: 97.01 Train acc: 96.992 Test acc: 97.220 \n",
      "step: 7210 | Train: G_Loss: 1.241, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.633, D_sup_loss: 0.113, D_sup_acc: 97.26 Train acc: 96.845 Test acc: 97.030 \n",
      "step: 7211 | Train: G_Loss: 1.260, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.589, D_sup_loss: 0.127, D_sup_acc: 97.07 Train acc: 96.902 Test acc: 97.120 \n",
      "step: 7212 | Train: G_Loss: 1.232, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.561, D_sup_loss: 0.121, D_sup_acc: 97.16 Train acc: 97.013 Test acc: 97.240 \n",
      "step: 7213 | Train: G_Loss: 1.195, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.652, D_sup_loss: 0.116, D_sup_acc: 97.27 Train acc: 97.032 Test acc: 97.250 \n",
      "step: 7214 | Train: G_Loss: 1.223, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.575, D_sup_loss: 0.118, D_sup_acc: 97.28 Train acc: 97.032 Test acc: 97.290 \n",
      "step: 7215 | Train: G_Loss: 1.283, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.571, D_sup_loss: 0.116, D_sup_acc: 97.32 Train acc: 96.870 Test acc: 97.100 \n",
      "step: 7216 | Train: G_Loss: 1.154, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.557, D_sup_loss: 0.129, D_sup_acc: 97.14 Train acc: 96.908 Test acc: 97.120 \n",
      "step: 7217 | Train: G_Loss: 1.304, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.547, D_sup_loss: 0.127, D_sup_acc: 97.16 Train acc: 97.075 Test acc: 97.220 \n",
      "step: 7218 | Train: G_Loss: 1.186, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.559, D_sup_loss: 0.114, D_sup_acc: 97.26 Train acc: 96.860 Test acc: 97.120 \n",
      "step: 7219 | Train: G_Loss: 1.222, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.583, D_sup_loss: 0.129, D_sup_acc: 97.16 Train acc: 96.780 Test acc: 97.010 \n",
      "step: 7220 | Train: G_Loss: 1.179, D_unsup_loss_real: 0.535, D_unsup_loss_fake: 0.594, D_sup_loss: 0.134, D_sup_acc: 97.05 Train acc: 97.025 Test acc: 97.220 \n",
      "step: 7221 | Train: G_Loss: 1.170, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.539, D_sup_loss: 0.117, D_sup_acc: 97.26 Train acc: 96.872 Test acc: 97.080 \n",
      "step: 7222 | Train: G_Loss: 1.167, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.547, D_sup_loss: 0.126, D_sup_acc: 97.12 Train acc: 96.973 Test acc: 97.190 \n",
      "step: 7223 | Train: G_Loss: 1.200, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.634, D_sup_loss: 0.121, D_sup_acc: 97.23 Train acc: 96.982 Test acc: 97.250 \n",
      "step: 7224 | Train: G_Loss: 1.238, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.530, D_sup_loss: 0.122, D_sup_acc: 97.28 Train acc: 97.012 Test acc: 97.240 \n",
      "step: 7225 | Train: G_Loss: 1.364, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.576, D_sup_loss: 0.117, D_sup_acc: 97.27 Train acc: 96.957 Test acc: 97.210 \n",
      "step: 7226 | Train: G_Loss: 1.279, D_unsup_loss_real: 0.513, D_unsup_loss_fake: 0.574, D_sup_loss: 0.121, D_sup_acc: 97.25 Train acc: 96.817 Test acc: 97.050 \n",
      "step: 7227 | Train: G_Loss: 1.320, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.558, D_sup_loss: 0.131, D_sup_acc: 97.09 Train acc: 97.065 Test acc: 97.290 \n",
      "step: 7228 | Train: G_Loss: 1.237, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.625, D_sup_loss: 0.111, D_sup_acc: 97.32 Train acc: 96.690 Test acc: 96.940 \n",
      "step: 7229 | Train: G_Loss: 1.278, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.607, D_sup_loss: 0.136, D_sup_acc: 96.98 Train acc: 96.965 Test acc: 97.190 \n",
      "step: 7230 | Train: G_Loss: 1.224, D_unsup_loss_real: 0.644, D_unsup_loss_fake: 0.605, D_sup_loss: 0.121, D_sup_acc: 97.23 Train acc: 96.795 Test acc: 96.920 \n",
      "step: 7231 | Train: G_Loss: 1.262, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.574, D_sup_loss: 0.128, D_sup_acc: 96.96 Train acc: 96.823 Test acc: 97.140 \n",
      "step: 7232 | Train: G_Loss: 1.212, D_unsup_loss_real: 0.537, D_unsup_loss_fake: 0.555, D_sup_loss: 0.130, D_sup_acc: 97.18 Train acc: 97.038 Test acc: 97.340 \n",
      "step: 7233 | Train: G_Loss: 1.158, D_unsup_loss_real: 0.550, D_unsup_loss_fake: 0.556, D_sup_loss: 0.114, D_sup_acc: 97.37 Train acc: 97.002 Test acc: 97.220 \n",
      "step: 7234 | Train: G_Loss: 1.215, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.596, D_sup_loss: 0.119, D_sup_acc: 97.26 Train acc: 96.845 Test acc: 97.010 \n",
      "step: 7235 | Train: G_Loss: 1.184, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.601, D_sup_loss: 0.125, D_sup_acc: 97.05 Train acc: 96.895 Test acc: 97.010 \n",
      "step: 7236 | Train: G_Loss: 1.171, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.580, D_sup_loss: 0.122, D_sup_acc: 97.05 Train acc: 96.892 Test acc: 96.970 \n",
      "step: 7237 | Train: G_Loss: 1.244, D_unsup_loss_real: 0.521, D_unsup_loss_fake: 0.520, D_sup_loss: 0.126, D_sup_acc: 97.01 Train acc: 96.935 Test acc: 97.040 \n",
      "step: 7238 | Train: G_Loss: 1.260, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.564, D_sup_loss: 0.124, D_sup_acc: 97.08 Train acc: 96.868 Test acc: 97.070 \n",
      "step: 7239 | Train: G_Loss: 1.295, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.545, D_sup_loss: 0.129, D_sup_acc: 97.11 Train acc: 97.010 Test acc: 97.200 \n",
      "step: 7240 | Train: G_Loss: 1.194, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.606, D_sup_loss: 0.118, D_sup_acc: 97.24 Train acc: 97.007 Test acc: 97.170 \n",
      "step: 7241 | Train: G_Loss: 1.235, D_unsup_loss_real: 0.491, D_unsup_loss_fake: 0.558, D_sup_loss: 0.119, D_sup_acc: 97.21 Train acc: 97.007 Test acc: 97.210 \n",
      "step: 7242 | Train: G_Loss: 1.127, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.556, D_sup_loss: 0.120, D_sup_acc: 97.25 Train acc: 96.875 Test acc: 97.090 \n",
      "step: 7243 | Train: G_Loss: 1.237, D_unsup_loss_real: 0.523, D_unsup_loss_fake: 0.578, D_sup_loss: 0.131, D_sup_acc: 97.13 Train acc: 96.963 Test acc: 97.090 \n",
      "step: 7244 | Train: G_Loss: 1.343, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.548, D_sup_loss: 0.123, D_sup_acc: 97.13 Train acc: 97.067 Test acc: 97.290 \n",
      "step: 7245 | Train: G_Loss: 1.382, D_unsup_loss_real: 0.718, D_unsup_loss_fake: 0.561, D_sup_loss: 0.111, D_sup_acc: 97.32 Train acc: 95.947 Test acc: 96.200 \n",
      "step: 7246 | Train: G_Loss: 1.102, D_unsup_loss_real: 0.794, D_unsup_loss_fake: 0.626, D_sup_loss: 0.186, D_sup_acc: 96.25 Train acc: 96.975 Test acc: 97.210 \n",
      "step: 7247 | Train: G_Loss: 1.180, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.599, D_sup_loss: 0.113, D_sup_acc: 97.25 Train acc: 96.978 Test acc: 97.170 \n",
      "step: 7248 | Train: G_Loss: 1.264, D_unsup_loss_real: 0.485, D_unsup_loss_fake: 0.569, D_sup_loss: 0.118, D_sup_acc: 97.21 Train acc: 97.045 Test acc: 97.250 \n",
      "step: 7249 | Train: G_Loss: 1.163, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.564, D_sup_loss: 0.112, D_sup_acc: 97.28 Train acc: 96.895 Test acc: 97.060 \n",
      "step: 7250 | Train: G_Loss: 1.220, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.559, D_sup_loss: 0.125, D_sup_acc: 97.10 Train acc: 96.742 Test acc: 96.870 \n",
      "step: 7251 | Train: G_Loss: 1.166, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.636, D_sup_loss: 0.130, D_sup_acc: 96.91 Train acc: 97.000 Test acc: 97.150 \n",
      "step: 7252 | Train: G_Loss: 1.253, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.615, D_sup_loss: 0.119, D_sup_acc: 97.19 Train acc: 97.023 Test acc: 97.200 \n",
      "step: 7253 | Train: G_Loss: 1.236, D_unsup_loss_real: 0.477, D_unsup_loss_fake: 0.629, D_sup_loss: 0.116, D_sup_acc: 97.24 Train acc: 96.920 Test acc: 97.150 \n",
      "step: 7254 | Train: G_Loss: 1.195, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.529, D_sup_loss: 0.122, D_sup_acc: 97.19 Train acc: 96.977 Test acc: 97.160 \n",
      "step: 7255 | Train: G_Loss: 1.178, D_unsup_loss_real: 0.535, D_unsup_loss_fake: 0.594, D_sup_loss: 0.120, D_sup_acc: 97.20 Train acc: 96.783 Test acc: 96.940 \n",
      "step: 7256 | Train: G_Loss: 1.181, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.579, D_sup_loss: 0.129, D_sup_acc: 96.98 Train acc: 96.978 Test acc: 97.130 \n",
      "step: 7257 | Train: G_Loss: 1.153, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.582, D_sup_loss: 0.119, D_sup_acc: 97.17 Train acc: 96.842 Test acc: 96.980 \n",
      "step: 7258 | Train: G_Loss: 1.244, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.590, D_sup_loss: 0.128, D_sup_acc: 97.02 Train acc: 97.003 Test acc: 97.220 \n",
      "step: 7259 | Train: G_Loss: 1.207, D_unsup_loss_real: 0.502, D_unsup_loss_fake: 0.608, D_sup_loss: 0.116, D_sup_acc: 97.26 Train acc: 96.883 Test acc: 97.100 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 7260 | Train: G_Loss: 1.219, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.546, D_sup_loss: 0.123, D_sup_acc: 97.14 Train acc: 96.800 Test acc: 97.020 \n",
      "step: 7261 | Train: G_Loss: 1.213, D_unsup_loss_real: 0.546, D_unsup_loss_fake: 0.573, D_sup_loss: 0.130, D_sup_acc: 97.06 Train acc: 96.762 Test acc: 97.030 \n",
      "step: 7262 | Train: G_Loss: 1.322, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.613, D_sup_loss: 0.126, D_sup_acc: 97.07 Train acc: 96.720 Test acc: 97.030 \n",
      "step: 7263 | Train: G_Loss: 1.181, D_unsup_loss_real: 0.525, D_unsup_loss_fake: 0.586, D_sup_loss: 0.131, D_sup_acc: 97.07 Train acc: 96.938 Test acc: 97.160 \n",
      "step: 7264 | Train: G_Loss: 1.292, D_unsup_loss_real: 0.526, D_unsup_loss_fake: 0.568, D_sup_loss: 0.121, D_sup_acc: 97.20 Train acc: 97.038 Test acc: 97.300 \n",
      "step: 7265 | Train: G_Loss: 1.189, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.551, D_sup_loss: 0.113, D_sup_acc: 97.33 Train acc: 96.790 Test acc: 97.100 \n",
      "step: 7266 | Train: G_Loss: 1.273, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.523, D_sup_loss: 0.130, D_sup_acc: 97.14 Train acc: 96.970 Test acc: 97.210 \n",
      "step: 7267 | Train: G_Loss: 1.180, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.560, D_sup_loss: 0.118, D_sup_acc: 97.25 Train acc: 96.663 Test acc: 96.880 \n",
      "step: 7268 | Train: G_Loss: 1.186, D_unsup_loss_real: 0.475, D_unsup_loss_fake: 0.547, D_sup_loss: 0.134, D_sup_acc: 96.92 Train acc: 96.878 Test acc: 97.140 \n",
      "step: 7269 | Train: G_Loss: 1.249, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.609, D_sup_loss: 0.120, D_sup_acc: 97.18 Train acc: 96.480 Test acc: 96.690 \n",
      "step: 7270 | Train: G_Loss: 1.098, D_unsup_loss_real: 0.662, D_unsup_loss_fake: 0.659, D_sup_loss: 0.143, D_sup_acc: 96.73 Train acc: 96.943 Test acc: 97.140 \n",
      "step: 7271 | Train: G_Loss: 1.399, D_unsup_loss_real: 0.499, D_unsup_loss_fake: 0.611, D_sup_loss: 0.120, D_sup_acc: 97.18 Train acc: 97.083 Test acc: 97.290 \n",
      "step: 7272 | Train: G_Loss: 1.266, D_unsup_loss_real: 0.700, D_unsup_loss_fake: 0.535, D_sup_loss: 0.109, D_sup_acc: 97.32 Train acc: 96.390 Test acc: 96.640 \n",
      "step: 7273 | Train: G_Loss: 1.228, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.608, D_sup_loss: 0.157, D_sup_acc: 96.68 Train acc: 96.835 Test acc: 97.070 \n",
      "step: 7274 | Train: G_Loss: 1.342, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.548, D_sup_loss: 0.123, D_sup_acc: 97.11 Train acc: 96.898 Test acc: 97.200 \n",
      "step: 7275 | Train: G_Loss: 1.326, D_unsup_loss_real: 0.488, D_unsup_loss_fake: 0.588, D_sup_loss: 0.115, D_sup_acc: 97.24 Train acc: 96.770 Test acc: 96.920 \n",
      "step: 7276 | Train: G_Loss: 1.345, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.567, D_sup_loss: 0.124, D_sup_acc: 96.96 Train acc: 96.675 Test acc: 96.930 \n",
      "step: 7277 | Train: G_Loss: 1.327, D_unsup_loss_real: 0.517, D_unsup_loss_fake: 0.562, D_sup_loss: 0.126, D_sup_acc: 96.97 Train acc: 96.697 Test acc: 96.880 \n",
      "step: 7278 | Train: G_Loss: 1.258, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.589, D_sup_loss: 0.127, D_sup_acc: 96.92 Train acc: 96.848 Test acc: 97.050 \n",
      "step: 7279 | Train: G_Loss: 1.163, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.641, D_sup_loss: 0.118, D_sup_acc: 97.09 Train acc: 96.558 Test acc: 96.800 \n",
      "step: 7280 | Train: G_Loss: 1.218, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.602, D_sup_loss: 0.136, D_sup_acc: 96.84 Train acc: 96.875 Test acc: 97.100 \n",
      "step: 7281 | Train: G_Loss: 1.318, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.510, D_sup_loss: 0.122, D_sup_acc: 97.14 Train acc: 96.927 Test acc: 97.130 \n",
      "step: 7282 | Train: G_Loss: 1.228, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.540, D_sup_loss: 0.114, D_sup_acc: 97.17 Train acc: 96.447 Test acc: 96.630 \n",
      "step: 7283 | Train: G_Loss: 1.195, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.546, D_sup_loss: 0.147, D_sup_acc: 96.67 Train acc: 96.775 Test acc: 96.920 \n",
      "step: 7284 | Train: G_Loss: 1.185, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.616, D_sup_loss: 0.128, D_sup_acc: 96.96 Train acc: 96.853 Test acc: 97.100 \n",
      "step: 7285 | Train: G_Loss: 1.233, D_unsup_loss_real: 0.531, D_unsup_loss_fake: 0.519, D_sup_loss: 0.120, D_sup_acc: 97.14 Train acc: 96.593 Test acc: 96.750 \n",
      "step: 7286 | Train: G_Loss: 1.137, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.569, D_sup_loss: 0.131, D_sup_acc: 96.79 Train acc: 96.862 Test acc: 97.100 \n",
      "step: 7287 | Train: G_Loss: 1.115, D_unsup_loss_real: 0.536, D_unsup_loss_fake: 0.535, D_sup_loss: 0.121, D_sup_acc: 97.14 Train acc: 96.913 Test acc: 97.170 \n",
      "step: 7288 | Train: G_Loss: 1.222, D_unsup_loss_real: 0.499, D_unsup_loss_fake: 0.558, D_sup_loss: 0.116, D_sup_acc: 97.21 Train acc: 96.758 Test acc: 97.070 \n",
      "step: 7289 | Train: G_Loss: 1.101, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.560, D_sup_loss: 0.120, D_sup_acc: 97.11 Train acc: 96.682 Test acc: 96.920 \n",
      "step: 7290 | Train: G_Loss: 1.180, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.570, D_sup_loss: 0.128, D_sup_acc: 96.96 Train acc: 96.783 Test acc: 96.960 \n",
      "step: 7291 | Train: G_Loss: 1.193, D_unsup_loss_real: 0.656, D_unsup_loss_fake: 0.550, D_sup_loss: 0.123, D_sup_acc: 97.00 Train acc: 96.880 Test acc: 97.060 \n",
      "step: 7292 | Train: G_Loss: 1.282, D_unsup_loss_real: 0.497, D_unsup_loss_fake: 0.653, D_sup_loss: 0.118, D_sup_acc: 97.10 Train acc: 96.842 Test acc: 97.010 \n",
      "step: 7293 | Train: G_Loss: 1.263, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.479, D_sup_loss: 0.120, D_sup_acc: 97.05 Train acc: 96.970 Test acc: 97.270 \n",
      "step: 7294 | Train: G_Loss: 1.288, D_unsup_loss_real: 0.537, D_unsup_loss_fake: 0.532, D_sup_loss: 0.114, D_sup_acc: 97.30 Train acc: 96.993 Test acc: 97.270 \n",
      "step: 7295 | Train: G_Loss: 1.219, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.607, D_sup_loss: 0.115, D_sup_acc: 97.30 Train acc: 96.680 Test acc: 96.890 \n",
      "step: 7296 | Train: G_Loss: 1.250, D_unsup_loss_real: 0.546, D_unsup_loss_fake: 0.661, D_sup_loss: 0.130, D_sup_acc: 96.93 Train acc: 96.975 Test acc: 97.170 \n",
      "step: 7297 | Train: G_Loss: 1.139, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.605, D_sup_loss: 0.116, D_sup_acc: 97.21 Train acc: 96.790 Test acc: 97.010 \n",
      "step: 7298 | Train: G_Loss: 1.226, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.571, D_sup_loss: 0.129, D_sup_acc: 97.05 Train acc: 96.910 Test acc: 97.050 \n",
      "step: 7299 | Train: G_Loss: 1.195, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.618, D_sup_loss: 0.120, D_sup_acc: 97.09 Train acc: 96.780 Test acc: 96.940 \n",
      "step: 7300 | Train: G_Loss: 1.220, D_unsup_loss_real: 0.533, D_unsup_loss_fake: 0.528, D_sup_loss: 0.127, D_sup_acc: 96.98 Train acc: 96.923 Test acc: 97.030 \n",
      "Train Classifier Accuracy: 96.923%\n",
      "\n",
      "Test Classifier Accuracy: 97.030%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_7300.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_7300.h5\n",
      "step: 7301 | Train: G_Loss: 1.270, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.590, D_sup_loss: 0.121, D_sup_acc: 97.07 Train acc: 96.773 Test acc: 97.000 \n",
      "step: 7302 | Train: G_Loss: 1.146, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.557, D_sup_loss: 0.126, D_sup_acc: 97.04 Train acc: 96.825 Test acc: 97.110 \n",
      "step: 7303 | Train: G_Loss: 1.243, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.540, D_sup_loss: 0.125, D_sup_acc: 97.15 Train acc: 96.838 Test acc: 97.080 \n",
      "step: 7304 | Train: G_Loss: 1.228, D_unsup_loss_real: 0.524, D_unsup_loss_fake: 0.546, D_sup_loss: 0.122, D_sup_acc: 97.12 Train acc: 96.898 Test acc: 97.150 \n",
      "step: 7305 | Train: G_Loss: 1.244, D_unsup_loss_real: 0.662, D_unsup_loss_fake: 0.522, D_sup_loss: 0.118, D_sup_acc: 97.19 Train acc: 96.690 Test acc: 96.820 \n",
      "step: 7306 | Train: G_Loss: 1.338, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.565, D_sup_loss: 0.131, D_sup_acc: 96.86 Train acc: 96.780 Test acc: 97.110 \n",
      "step: 7307 | Train: G_Loss: 1.393, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.560, D_sup_loss: 0.124, D_sup_acc: 97.15 Train acc: 96.802 Test acc: 97.080 \n",
      "step: 7308 | Train: G_Loss: 1.154, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.523, D_sup_loss: 0.120, D_sup_acc: 97.12 Train acc: 96.813 Test acc: 97.110 \n",
      "step: 7309 | Train: G_Loss: 1.223, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.595, D_sup_loss: 0.123, D_sup_acc: 97.15 Train acc: 96.812 Test acc: 97.220 \n",
      "step: 7310 | Train: G_Loss: 1.140, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.579, D_sup_loss: 0.121, D_sup_acc: 97.26 Train acc: 96.865 Test acc: 97.180 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 7311 | Train: G_Loss: 1.202, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.577, D_sup_loss: 0.118, D_sup_acc: 97.22 Train acc: 96.822 Test acc: 97.160 \n",
      "step: 7312 | Train: G_Loss: 1.368, D_unsup_loss_real: 0.532, D_unsup_loss_fake: 0.544, D_sup_loss: 0.121, D_sup_acc: 97.20 Train acc: 96.900 Test acc: 97.160 \n",
      "step: 7313 | Train: G_Loss: 1.231, D_unsup_loss_real: 0.642, D_unsup_loss_fake: 0.562, D_sup_loss: 0.118, D_sup_acc: 97.20 Train acc: 96.838 Test acc: 97.170 \n",
      "step: 7314 | Train: G_Loss: 1.177, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.624, D_sup_loss: 0.121, D_sup_acc: 97.21 Train acc: 96.637 Test acc: 96.910 \n",
      "step: 7315 | Train: G_Loss: 1.230, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.531, D_sup_loss: 0.132, D_sup_acc: 96.95 Train acc: 96.975 Test acc: 97.130 \n",
      "step: 7316 | Train: G_Loss: 1.177, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.569, D_sup_loss: 0.114, D_sup_acc: 97.17 Train acc: 96.673 Test acc: 96.880 \n",
      "step: 7317 | Train: G_Loss: 1.223, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.597, D_sup_loss: 0.132, D_sup_acc: 96.92 Train acc: 96.907 Test acc: 97.080 \n",
      "step: 7318 | Train: G_Loss: 1.271, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.536, D_sup_loss: 0.120, D_sup_acc: 97.12 Train acc: 96.875 Test acc: 97.130 \n",
      "step: 7319 | Train: G_Loss: 1.227, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.591, D_sup_loss: 0.119, D_sup_acc: 97.17 Train acc: 97.000 Test acc: 97.220 \n",
      "step: 7320 | Train: G_Loss: 1.172, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.566, D_sup_loss: 0.117, D_sup_acc: 97.26 Train acc: 96.692 Test acc: 96.930 \n",
      "step: 7321 | Train: G_Loss: 1.208, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.571, D_sup_loss: 0.132, D_sup_acc: 96.97 Train acc: 96.712 Test acc: 96.910 \n",
      "step: 7322 | Train: G_Loss: 1.234, D_unsup_loss_real: 0.523, D_unsup_loss_fake: 0.606, D_sup_loss: 0.127, D_sup_acc: 96.95 Train acc: 96.825 Test acc: 97.050 \n",
      "step: 7323 | Train: G_Loss: 1.218, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.602, D_sup_loss: 0.124, D_sup_acc: 97.09 Train acc: 96.890 Test acc: 97.080 \n",
      "step: 7324 | Train: G_Loss: 1.268, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.517, D_sup_loss: 0.120, D_sup_acc: 97.12 Train acc: 96.818 Test acc: 97.070 \n",
      "step: 7325 | Train: G_Loss: 1.209, D_unsup_loss_real: 0.499, D_unsup_loss_fake: 0.605, D_sup_loss: 0.123, D_sup_acc: 97.11 Train acc: 96.702 Test acc: 97.060 \n",
      "step: 7326 | Train: G_Loss: 1.128, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.627, D_sup_loss: 0.126, D_sup_acc: 97.10 Train acc: 96.587 Test acc: 96.830 \n",
      "step: 7327 | Train: G_Loss: 1.234, D_unsup_loss_real: 0.527, D_unsup_loss_fake: 0.517, D_sup_loss: 0.135, D_sup_acc: 96.87 Train acc: 96.648 Test acc: 97.000 \n",
      "step: 7328 | Train: G_Loss: 1.186, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.575, D_sup_loss: 0.129, D_sup_acc: 97.04 Train acc: 96.875 Test acc: 97.170 \n",
      "step: 7329 | Train: G_Loss: 1.389, D_unsup_loss_real: 0.541, D_unsup_loss_fake: 0.566, D_sup_loss: 0.120, D_sup_acc: 97.21 Train acc: 96.853 Test acc: 97.200 \n",
      "step: 7330 | Train: G_Loss: 1.221, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.581, D_sup_loss: 0.120, D_sup_acc: 97.24 Train acc: 96.522 Test acc: 96.760 \n",
      "step: 7331 | Train: G_Loss: 1.245, D_unsup_loss_real: 0.646, D_unsup_loss_fake: 0.593, D_sup_loss: 0.146, D_sup_acc: 96.80 Train acc: 96.837 Test acc: 97.130 \n",
      "step: 7332 | Train: G_Loss: 1.181, D_unsup_loss_real: 0.507, D_unsup_loss_fake: 0.525, D_sup_loss: 0.121, D_sup_acc: 97.17 Train acc: 96.843 Test acc: 97.160 \n",
      "step: 7333 | Train: G_Loss: 1.463, D_unsup_loss_real: 0.511, D_unsup_loss_fake: 0.584, D_sup_loss: 0.121, D_sup_acc: 97.20 Train acc: 96.895 Test acc: 97.200 \n",
      "step: 7334 | Train: G_Loss: 1.343, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.516, D_sup_loss: 0.114, D_sup_acc: 97.24 Train acc: 96.385 Test acc: 96.730 \n",
      "step: 7335 | Train: G_Loss: 1.364, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.508, D_sup_loss: 0.151, D_sup_acc: 96.77 Train acc: 96.845 Test acc: 97.180 \n",
      "step: 7336 | Train: G_Loss: 1.154, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.526, D_sup_loss: 0.120, D_sup_acc: 97.22 Train acc: 96.793 Test acc: 97.080 \n",
      "step: 7337 | Train: G_Loss: 1.264, D_unsup_loss_real: 0.542, D_unsup_loss_fake: 0.549, D_sup_loss: 0.122, D_sup_acc: 97.12 Train acc: 96.890 Test acc: 97.180 \n",
      "step: 7338 | Train: G_Loss: 1.258, D_unsup_loss_real: 0.510, D_unsup_loss_fake: 0.615, D_sup_loss: 0.116, D_sup_acc: 97.22 Train acc: 96.902 Test acc: 97.240 \n",
      "step: 7339 | Train: G_Loss: 1.251, D_unsup_loss_real: 0.502, D_unsup_loss_fake: 0.530, D_sup_loss: 0.114, D_sup_acc: 97.27 Train acc: 96.990 Test acc: 97.260 \n",
      "step: 7340 | Train: G_Loss: 1.238, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.607, D_sup_loss: 0.107, D_sup_acc: 97.29 Train acc: 96.703 Test acc: 96.950 \n",
      "step: 7341 | Train: G_Loss: 1.196, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.581, D_sup_loss: 0.130, D_sup_acc: 96.99 Train acc: 96.885 Test acc: 97.100 \n",
      "step: 7342 | Train: G_Loss: 1.252, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.586, D_sup_loss: 0.119, D_sup_acc: 97.14 Train acc: 96.832 Test acc: 97.080 \n",
      "step: 7343 | Train: G_Loss: 1.204, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.576, D_sup_loss: 0.117, D_sup_acc: 97.12 Train acc: 96.563 Test acc: 96.770 \n",
      "step: 7344 | Train: G_Loss: 1.279, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.510, D_sup_loss: 0.138, D_sup_acc: 96.81 Train acc: 96.825 Test acc: 97.070 \n",
      "step: 7345 | Train: G_Loss: 1.325, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.580, D_sup_loss: 0.120, D_sup_acc: 97.11 Train acc: 96.963 Test acc: 97.210 \n",
      "step: 7346 | Train: G_Loss: 1.261, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.618, D_sup_loss: 0.111, D_sup_acc: 97.25 Train acc: 96.752 Test acc: 97.020 \n",
      "step: 7347 | Train: G_Loss: 1.312, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.559, D_sup_loss: 0.128, D_sup_acc: 97.06 Train acc: 96.847 Test acc: 97.120 \n",
      "step: 7348 | Train: G_Loss: 1.191, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.548, D_sup_loss: 0.118, D_sup_acc: 97.16 Train acc: 96.607 Test acc: 96.820 \n",
      "step: 7349 | Train: G_Loss: 1.029, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.636, D_sup_loss: 0.134, D_sup_acc: 96.86 Train acc: 96.637 Test acc: 96.810 \n",
      "step: 7350 | Train: G_Loss: 1.206, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.597, D_sup_loss: 0.133, D_sup_acc: 96.85 Train acc: 96.660 Test acc: 97.000 \n",
      "step: 7351 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.520, D_sup_loss: 0.128, D_sup_acc: 97.04 Train acc: 96.720 Test acc: 96.930 \n",
      "step: 7352 | Train: G_Loss: 1.232, D_unsup_loss_real: 0.526, D_unsup_loss_fake: 0.559, D_sup_loss: 0.122, D_sup_acc: 96.97 Train acc: 96.283 Test acc: 96.460 \n",
      "step: 7353 | Train: G_Loss: 1.179, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.553, D_sup_loss: 0.144, D_sup_acc: 96.50 Train acc: 96.643 Test acc: 97.000 \n",
      "step: 7354 | Train: G_Loss: 1.327, D_unsup_loss_real: 0.535, D_unsup_loss_fake: 0.562, D_sup_loss: 0.123, D_sup_acc: 97.04 Train acc: 96.803 Test acc: 97.060 \n",
      "step: 7355 | Train: G_Loss: 1.253, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.586, D_sup_loss: 0.118, D_sup_acc: 97.10 Train acc: 96.648 Test acc: 96.930 \n",
      "step: 7356 | Train: G_Loss: 1.223, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.575, D_sup_loss: 0.132, D_sup_acc: 96.97 Train acc: 96.595 Test acc: 96.830 \n",
      "step: 7357 | Train: G_Loss: 1.266, D_unsup_loss_real: 0.536, D_unsup_loss_fake: 0.650, D_sup_loss: 0.137, D_sup_acc: 96.87 Train acc: 96.760 Test acc: 97.000 \n",
      "step: 7358 | Train: G_Loss: 1.172, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.602, D_sup_loss: 0.120, D_sup_acc: 97.04 Train acc: 96.782 Test acc: 96.950 \n",
      "step: 7359 | Train: G_Loss: 1.265, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.569, D_sup_loss: 0.124, D_sup_acc: 96.99 Train acc: 96.838 Test acc: 97.160 \n",
      "step: 7360 | Train: G_Loss: 1.224, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.601, D_sup_loss: 0.119, D_sup_acc: 97.20 Train acc: 96.780 Test acc: 97.150 \n",
      "step: 7361 | Train: G_Loss: 1.271, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.544, D_sup_loss: 0.126, D_sup_acc: 97.19 Train acc: 96.580 Test acc: 96.850 \n",
      "step: 7362 | Train: G_Loss: 1.198, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.629, D_sup_loss: 0.141, D_sup_acc: 96.89 Train acc: 96.590 Test acc: 96.860 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 7363 | Train: G_Loss: 1.377, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.544, D_sup_loss: 0.138, D_sup_acc: 96.90 Train acc: 96.775 Test acc: 96.990 \n",
      "step: 7364 | Train: G_Loss: 1.168, D_unsup_loss_real: 0.546, D_unsup_loss_fake: 0.613, D_sup_loss: 0.123, D_sup_acc: 97.03 Train acc: 96.482 Test acc: 96.800 \n",
      "step: 7365 | Train: G_Loss: 1.215, D_unsup_loss_real: 0.531, D_unsup_loss_fake: 0.542, D_sup_loss: 0.139, D_sup_acc: 96.84 Train acc: 96.325 Test acc: 96.610 \n",
      "step: 7366 | Train: G_Loss: 1.188, D_unsup_loss_real: 0.687, D_unsup_loss_fake: 0.483, D_sup_loss: 0.147, D_sup_acc: 96.65 Train acc: 96.668 Test acc: 97.030 \n",
      "step: 7367 | Train: G_Loss: 1.242, D_unsup_loss_real: 0.642, D_unsup_loss_fake: 0.561, D_sup_loss: 0.126, D_sup_acc: 97.07 Train acc: 96.743 Test acc: 97.000 \n",
      "step: 7368 | Train: G_Loss: 1.155, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.588, D_sup_loss: 0.125, D_sup_acc: 97.04 Train acc: 96.657 Test acc: 96.930 \n",
      "step: 7369 | Train: G_Loss: 1.210, D_unsup_loss_real: 0.544, D_unsup_loss_fake: 0.621, D_sup_loss: 0.135, D_sup_acc: 96.97 Train acc: 96.648 Test acc: 96.910 \n",
      "step: 7370 | Train: G_Loss: 1.236, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.569, D_sup_loss: 0.135, D_sup_acc: 96.95 Train acc: 96.478 Test acc: 96.720 \n",
      "step: 7371 | Train: G_Loss: 1.205, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.571, D_sup_loss: 0.146, D_sup_acc: 96.76 Train acc: 96.797 Test acc: 96.980 \n",
      "step: 7372 | Train: G_Loss: 1.314, D_unsup_loss_real: 0.498, D_unsup_loss_fake: 0.516, D_sup_loss: 0.126, D_sup_acc: 97.02 Train acc: 96.843 Test acc: 97.070 \n",
      "step: 7373 | Train: G_Loss: 1.174, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.545, D_sup_loss: 0.122, D_sup_acc: 97.11 Train acc: 96.813 Test acc: 97.060 \n",
      "step: 7374 | Train: G_Loss: 1.273, D_unsup_loss_real: 0.502, D_unsup_loss_fake: 0.592, D_sup_loss: 0.123, D_sup_acc: 97.10 Train acc: 96.552 Test acc: 96.740 \n",
      "step: 7375 | Train: G_Loss: 1.155, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.580, D_sup_loss: 0.138, D_sup_acc: 96.78 Train acc: 96.573 Test acc: 96.810 \n",
      "step: 7376 | Train: G_Loss: 1.157, D_unsup_loss_real: 0.701, D_unsup_loss_fake: 0.516, D_sup_loss: 0.140, D_sup_acc: 96.85 Train acc: 96.672 Test acc: 96.900 \n",
      "step: 7377 | Train: G_Loss: 1.165, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.602, D_sup_loss: 0.127, D_sup_acc: 96.94 Train acc: 96.555 Test acc: 96.640 \n",
      "step: 7378 | Train: G_Loss: 1.213, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.617, D_sup_loss: 0.139, D_sup_acc: 96.68 Train acc: 96.700 Test acc: 96.870 \n",
      "step: 7379 | Train: G_Loss: 1.202, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.644, D_sup_loss: 0.134, D_sup_acc: 96.91 Train acc: 96.722 Test acc: 96.880 \n",
      "step: 7380 | Train: G_Loss: 1.216, D_unsup_loss_real: 0.473, D_unsup_loss_fake: 0.575, D_sup_loss: 0.131, D_sup_acc: 96.92 Train acc: 96.933 Test acc: 97.110 \n",
      "step: 7381 | Train: G_Loss: 1.150, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.590, D_sup_loss: 0.120, D_sup_acc: 97.15 Train acc: 96.510 Test acc: 96.710 \n",
      "step: 7382 | Train: G_Loss: 1.189, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.555, D_sup_loss: 0.145, D_sup_acc: 96.75 Train acc: 96.708 Test acc: 96.860 \n",
      "step: 7383 | Train: G_Loss: 1.304, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.539, D_sup_loss: 0.133, D_sup_acc: 96.90 Train acc: 96.787 Test acc: 96.970 \n",
      "step: 7384 | Train: G_Loss: 1.130, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.595, D_sup_loss: 0.123, D_sup_acc: 97.01 Train acc: 96.758 Test acc: 96.880 \n",
      "step: 7385 | Train: G_Loss: 1.217, D_unsup_loss_real: 0.538, D_unsup_loss_fake: 0.511, D_sup_loss: 0.131, D_sup_acc: 96.92 Train acc: 96.733 Test acc: 96.870 \n",
      "step: 7386 | Train: G_Loss: 1.154, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.528, D_sup_loss: 0.130, D_sup_acc: 96.91 Train acc: 96.637 Test acc: 96.850 \n",
      "step: 7387 | Train: G_Loss: 1.221, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.588, D_sup_loss: 0.134, D_sup_acc: 96.89 Train acc: 96.580 Test acc: 96.770 \n",
      "step: 7388 | Train: G_Loss: 1.301, D_unsup_loss_real: 0.550, D_unsup_loss_fake: 0.589, D_sup_loss: 0.137, D_sup_acc: 96.81 Train acc: 96.967 Test acc: 97.170 \n",
      "step: 7389 | Train: G_Loss: 1.228, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.571, D_sup_loss: 0.116, D_sup_acc: 97.21 Train acc: 96.792 Test acc: 97.030 \n",
      "step: 7390 | Train: G_Loss: 1.375, D_unsup_loss_real: 0.529, D_unsup_loss_fake: 0.607, D_sup_loss: 0.126, D_sup_acc: 97.07 Train acc: 96.965 Test acc: 97.200 \n",
      "step: 7391 | Train: G_Loss: 1.120, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.564, D_sup_loss: 0.110, D_sup_acc: 97.24 Train acc: 96.688 Test acc: 96.950 \n",
      "step: 7392 | Train: G_Loss: 1.286, D_unsup_loss_real: 0.522, D_unsup_loss_fake: 0.584, D_sup_loss: 0.131, D_sup_acc: 96.99 Train acc: 96.977 Test acc: 97.190 \n",
      "step: 7393 | Train: G_Loss: 1.135, D_unsup_loss_real: 0.512, D_unsup_loss_fake: 0.574, D_sup_loss: 0.112, D_sup_acc: 97.23 Train acc: 96.998 Test acc: 97.170 \n",
      "step: 7394 | Train: G_Loss: 1.217, D_unsup_loss_real: 0.534, D_unsup_loss_fake: 0.621, D_sup_loss: 0.116, D_sup_acc: 97.21 Train acc: 97.043 Test acc: 97.260 \n",
      "step: 7395 | Train: G_Loss: 1.276, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.584, D_sup_loss: 0.112, D_sup_acc: 97.29 Train acc: 97.030 Test acc: 97.250 \n",
      "step: 7396 | Train: G_Loss: 1.264, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.585, D_sup_loss: 0.113, D_sup_acc: 97.28 Train acc: 96.930 Test acc: 97.200 \n",
      "step: 7397 | Train: G_Loss: 1.178, D_unsup_loss_real: 0.507, D_unsup_loss_fake: 0.619, D_sup_loss: 0.118, D_sup_acc: 97.24 Train acc: 96.913 Test acc: 97.070 \n",
      "step: 7398 | Train: G_Loss: 1.202, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.589, D_sup_loss: 0.122, D_sup_acc: 97.11 Train acc: 97.050 Test acc: 97.320 \n",
      "step: 7399 | Train: G_Loss: 1.223, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.618, D_sup_loss: 0.109, D_sup_acc: 97.35 Train acc: 96.837 Test acc: 97.060 \n",
      "step: 7400 | Train: G_Loss: 1.219, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.636, D_sup_loss: 0.127, D_sup_acc: 97.10 Train acc: 96.878 Test acc: 97.050 \n",
      "Train Classifier Accuracy: 96.878%\n",
      "\n",
      "Test Classifier Accuracy: 97.050%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_7400.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_7400.h5\n",
      "step: 7401 | Train: G_Loss: 1.268, D_unsup_loss_real: 0.668, D_unsup_loss_fake: 0.569, D_sup_loss: 0.120, D_sup_acc: 97.09 Train acc: 96.975 Test acc: 97.120 \n",
      "step: 7402 | Train: G_Loss: 1.207, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.497, D_sup_loss: 0.120, D_sup_acc: 97.16 Train acc: 96.910 Test acc: 97.120 \n",
      "step: 7403 | Train: G_Loss: 1.218, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.530, D_sup_loss: 0.120, D_sup_acc: 97.16 Train acc: 96.947 Test acc: 97.230 \n",
      "step: 7404 | Train: G_Loss: 1.338, D_unsup_loss_real: 0.496, D_unsup_loss_fake: 0.596, D_sup_loss: 0.121, D_sup_acc: 97.27 Train acc: 96.990 Test acc: 97.120 \n",
      "step: 7405 | Train: G_Loss: 1.246, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.519, D_sup_loss: 0.115, D_sup_acc: 97.16 Train acc: 96.783 Test acc: 97.020 \n",
      "step: 7406 | Train: G_Loss: 1.344, D_unsup_loss_real: 0.635, D_unsup_loss_fake: 0.597, D_sup_loss: 0.131, D_sup_acc: 97.06 Train acc: 96.880 Test acc: 97.080 \n",
      "step: 7407 | Train: G_Loss: 1.292, D_unsup_loss_real: 0.645, D_unsup_loss_fake: 0.573, D_sup_loss: 0.120, D_sup_acc: 97.12 Train acc: 96.428 Test acc: 96.570 \n",
      "step: 7408 | Train: G_Loss: 1.272, D_unsup_loss_real: 0.744, D_unsup_loss_fake: 0.540, D_sup_loss: 0.156, D_sup_acc: 96.61 Train acc: 97.013 Test acc: 97.230 \n",
      "step: 7409 | Train: G_Loss: 1.125, D_unsup_loss_real: 0.541, D_unsup_loss_fake: 0.582, D_sup_loss: 0.110, D_sup_acc: 97.27 Train acc: 96.902 Test acc: 97.040 \n",
      "step: 7410 | Train: G_Loss: 1.225, D_unsup_loss_real: 0.513, D_unsup_loss_fake: 0.576, D_sup_loss: 0.119, D_sup_acc: 97.08 Train acc: 96.568 Test acc: 96.750 \n",
      "step: 7411 | Train: G_Loss: 1.188, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.534, D_sup_loss: 0.134, D_sup_acc: 96.79 Train acc: 96.775 Test acc: 97.000 \n",
      "step: 7412 | Train: G_Loss: 1.287, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.632, D_sup_loss: 0.124, D_sup_acc: 97.04 Train acc: 96.788 Test acc: 97.030 \n",
      "step: 7413 | Train: G_Loss: 1.149, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.592, D_sup_loss: 0.129, D_sup_acc: 97.07 Train acc: 96.727 Test acc: 96.940 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 7414 | Train: G_Loss: 1.280, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.537, D_sup_loss: 0.134, D_sup_acc: 96.98 Train acc: 96.852 Test acc: 97.030 \n",
      "step: 7415 | Train: G_Loss: 1.170, D_unsup_loss_real: 0.541, D_unsup_loss_fake: 0.514, D_sup_loss: 0.125, D_sup_acc: 97.07 Train acc: 96.905 Test acc: 97.120 \n",
      "step: 7416 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.550, D_sup_loss: 0.120, D_sup_acc: 97.16 Train acc: 96.747 Test acc: 96.950 \n",
      "step: 7417 | Train: G_Loss: 1.185, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.595, D_sup_loss: 0.132, D_sup_acc: 96.99 Train acc: 96.823 Test acc: 97.040 \n",
      "step: 7418 | Train: G_Loss: 1.245, D_unsup_loss_real: 0.504, D_unsup_loss_fake: 0.499, D_sup_loss: 0.128, D_sup_acc: 97.08 Train acc: 96.940 Test acc: 97.220 \n",
      "step: 7419 | Train: G_Loss: 1.241, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.623, D_sup_loss: 0.114, D_sup_acc: 97.26 Train acc: 96.887 Test acc: 97.120 \n",
      "step: 7420 | Train: G_Loss: 1.308, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.599, D_sup_loss: 0.122, D_sup_acc: 97.16 Train acc: 96.895 Test acc: 97.060 \n",
      "step: 7421 | Train: G_Loss: 1.302, D_unsup_loss_real: 0.489, D_unsup_loss_fake: 0.638, D_sup_loss: 0.122, D_sup_acc: 97.10 Train acc: 96.770 Test acc: 96.940 \n",
      "step: 7422 | Train: G_Loss: 1.197, D_unsup_loss_real: 0.659, D_unsup_loss_fake: 0.493, D_sup_loss: 0.125, D_sup_acc: 96.98 Train acc: 96.963 Test acc: 97.250 \n",
      "step: 7423 | Train: G_Loss: 1.175, D_unsup_loss_real: 0.656, D_unsup_loss_fake: 0.598, D_sup_loss: 0.114, D_sup_acc: 97.28 Train acc: 96.942 Test acc: 97.070 \n",
      "step: 7424 | Train: G_Loss: 1.191, D_unsup_loss_real: 0.541, D_unsup_loss_fake: 0.595, D_sup_loss: 0.123, D_sup_acc: 97.11 Train acc: 96.748 Test acc: 96.910 \n",
      "step: 7425 | Train: G_Loss: 1.136, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.558, D_sup_loss: 0.128, D_sup_acc: 96.95 Train acc: 96.430 Test acc: 96.580 \n",
      "step: 7426 | Train: G_Loss: 1.097, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.609, D_sup_loss: 0.138, D_sup_acc: 96.62 Train acc: 96.607 Test acc: 96.830 \n",
      "step: 7427 | Train: G_Loss: 1.222, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.595, D_sup_loss: 0.139, D_sup_acc: 96.87 Train acc: 96.805 Test acc: 97.080 \n",
      "step: 7428 | Train: G_Loss: 1.288, D_unsup_loss_real: 0.541, D_unsup_loss_fake: 0.614, D_sup_loss: 0.125, D_sup_acc: 97.12 Train acc: 96.857 Test acc: 97.080 \n",
      "step: 7429 | Train: G_Loss: 1.228, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.542, D_sup_loss: 0.125, D_sup_acc: 97.12 Train acc: 97.020 Test acc: 97.300 \n",
      "step: 7430 | Train: G_Loss: 1.214, D_unsup_loss_real: 0.654, D_unsup_loss_fake: 0.563, D_sup_loss: 0.113, D_sup_acc: 97.33 Train acc: 96.908 Test acc: 97.100 \n",
      "step: 7431 | Train: G_Loss: 1.180, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.602, D_sup_loss: 0.128, D_sup_acc: 97.14 Train acc: 97.043 Test acc: 97.250 \n",
      "step: 7432 | Train: G_Loss: 1.197, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.559, D_sup_loss: 0.115, D_sup_acc: 97.28 Train acc: 96.983 Test acc: 97.190 \n",
      "step: 7433 | Train: G_Loss: 1.241, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.594, D_sup_loss: 0.119, D_sup_acc: 97.23 Train acc: 96.862 Test acc: 97.020 \n",
      "step: 7434 | Train: G_Loss: 1.280, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.510, D_sup_loss: 0.126, D_sup_acc: 97.06 Train acc: 97.037 Test acc: 97.300 \n",
      "step: 7435 | Train: G_Loss: 1.163, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.556, D_sup_loss: 0.113, D_sup_acc: 97.33 Train acc: 96.783 Test acc: 97.100 \n",
      "step: 7436 | Train: G_Loss: 1.078, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.610, D_sup_loss: 0.130, D_sup_acc: 97.14 Train acc: 96.802 Test acc: 97.130 \n",
      "step: 7437 | Train: G_Loss: 1.129, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.556, D_sup_loss: 0.126, D_sup_acc: 97.17 Train acc: 96.965 Test acc: 97.160 \n",
      "step: 7438 | Train: G_Loss: 1.160, D_unsup_loss_real: 0.536, D_unsup_loss_fake: 0.533, D_sup_loss: 0.117, D_sup_acc: 97.20 Train acc: 96.925 Test acc: 97.160 \n",
      "step: 7439 | Train: G_Loss: 1.162, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.590, D_sup_loss: 0.120, D_sup_acc: 97.20 Train acc: 96.515 Test acc: 96.690 \n",
      "step: 7440 | Train: G_Loss: 1.163, D_unsup_loss_real: 0.644, D_unsup_loss_fake: 0.534, D_sup_loss: 0.144, D_sup_acc: 96.73 Train acc: 97.103 Test acc: 97.350 \n",
      "step: 7441 | Train: G_Loss: 1.108, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.594, D_sup_loss: 0.112, D_sup_acc: 97.38 Train acc: 96.585 Test acc: 96.870 \n",
      "step: 7442 | Train: G_Loss: 1.153, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.578, D_sup_loss: 0.139, D_sup_acc: 96.91 Train acc: 96.957 Test acc: 97.230 \n",
      "step: 7443 | Train: G_Loss: 1.180, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.570, D_sup_loss: 0.117, D_sup_acc: 97.27 Train acc: 96.887 Test acc: 97.050 \n",
      "step: 7444 | Train: G_Loss: 1.163, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.527, D_sup_loss: 0.123, D_sup_acc: 97.09 Train acc: 97.037 Test acc: 97.190 \n",
      "step: 7445 | Train: G_Loss: 1.208, D_unsup_loss_real: 0.517, D_unsup_loss_fake: 0.614, D_sup_loss: 0.118, D_sup_acc: 97.23 Train acc: 96.817 Test acc: 96.960 \n",
      "step: 7446 | Train: G_Loss: 1.263, D_unsup_loss_real: 0.504, D_unsup_loss_fake: 0.582, D_sup_loss: 0.127, D_sup_acc: 97.00 Train acc: 97.017 Test acc: 97.250 \n",
      "step: 7447 | Train: G_Loss: 1.143, D_unsup_loss_real: 0.546, D_unsup_loss_fake: 0.577, D_sup_loss: 0.116, D_sup_acc: 97.28 Train acc: 97.060 Test acc: 97.330 \n",
      "step: 7448 | Train: G_Loss: 1.092, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.534, D_sup_loss: 0.114, D_sup_acc: 97.36 Train acc: 96.897 Test acc: 97.160 \n",
      "step: 7449 | Train: G_Loss: 1.256, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.611, D_sup_loss: 0.122, D_sup_acc: 97.20 Train acc: 96.853 Test acc: 97.170 \n",
      "step: 7450 | Train: G_Loss: 1.245, D_unsup_loss_real: 0.511, D_unsup_loss_fake: 0.598, D_sup_loss: 0.124, D_sup_acc: 97.21 Train acc: 96.970 Test acc: 97.230 \n",
      "step: 7451 | Train: G_Loss: 1.305, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.620, D_sup_loss: 0.117, D_sup_acc: 97.27 Train acc: 96.385 Test acc: 96.520 \n",
      "step: 7452 | Train: G_Loss: 1.197, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.596, D_sup_loss: 0.142, D_sup_acc: 96.56 Train acc: 96.633 Test acc: 96.760 \n",
      "step: 7453 | Train: G_Loss: 1.291, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.572, D_sup_loss: 0.135, D_sup_acc: 96.80 Train acc: 96.863 Test acc: 97.130 \n",
      "step: 7454 | Train: G_Loss: 1.283, D_unsup_loss_real: 0.536, D_unsup_loss_fake: 0.502, D_sup_loss: 0.121, D_sup_acc: 97.17 Train acc: 96.797 Test acc: 97.090 \n",
      "step: 7455 | Train: G_Loss: 1.135, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.556, D_sup_loss: 0.125, D_sup_acc: 97.13 Train acc: 96.562 Test acc: 96.760 \n",
      "step: 7456 | Train: G_Loss: 1.201, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.558, D_sup_loss: 0.137, D_sup_acc: 96.80 Train acc: 96.675 Test acc: 96.870 \n",
      "step: 7457 | Train: G_Loss: 1.174, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.510, D_sup_loss: 0.131, D_sup_acc: 96.91 Train acc: 96.805 Test acc: 97.070 \n",
      "step: 7458 | Train: G_Loss: 1.228, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.608, D_sup_loss: 0.123, D_sup_acc: 97.11 Train acc: 96.850 Test acc: 97.120 \n",
      "step: 7459 | Train: G_Loss: 1.081, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.557, D_sup_loss: 0.119, D_sup_acc: 97.16 Train acc: 96.703 Test acc: 96.890 \n",
      "step: 7460 | Train: G_Loss: 1.281, D_unsup_loss_real: 0.493, D_unsup_loss_fake: 0.598, D_sup_loss: 0.131, D_sup_acc: 96.93 Train acc: 96.720 Test acc: 97.010 \n",
      "step: 7461 | Train: G_Loss: 1.253, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.615, D_sup_loss: 0.125, D_sup_acc: 97.05 Train acc: 96.968 Test acc: 97.210 \n",
      "step: 7462 | Train: G_Loss: 1.207, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.574, D_sup_loss: 0.119, D_sup_acc: 97.25 Train acc: 97.115 Test acc: 97.320 \n",
      "step: 7463 | Train: G_Loss: 1.211, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.551, D_sup_loss: 0.112, D_sup_acc: 97.35 Train acc: 96.945 Test acc: 97.160 \n",
      "step: 7464 | Train: G_Loss: 1.263, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.550, D_sup_loss: 0.115, D_sup_acc: 97.20 Train acc: 97.002 Test acc: 97.250 \n",
      "step: 7465 | Train: G_Loss: 1.212, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.481, D_sup_loss: 0.117, D_sup_acc: 97.28 Train acc: 97.090 Test acc: 97.340 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 7466 | Train: G_Loss: 1.200, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.577, D_sup_loss: 0.110, D_sup_acc: 97.37 Train acc: 96.850 Test acc: 97.050 \n",
      "step: 7467 | Train: G_Loss: 1.244, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.559, D_sup_loss: 0.123, D_sup_acc: 97.09 Train acc: 97.022 Test acc: 97.260 \n",
      "step: 7468 | Train: G_Loss: 1.344, D_unsup_loss_real: 0.542, D_unsup_loss_fake: 0.583, D_sup_loss: 0.114, D_sup_acc: 97.29 Train acc: 97.068 Test acc: 97.220 \n",
      "step: 7469 | Train: G_Loss: 1.260, D_unsup_loss_real: 0.656, D_unsup_loss_fake: 0.556, D_sup_loss: 0.110, D_sup_acc: 97.26 Train acc: 96.907 Test acc: 97.100 \n",
      "step: 7470 | Train: G_Loss: 1.233, D_unsup_loss_real: 0.538, D_unsup_loss_fake: 0.554, D_sup_loss: 0.121, D_sup_acc: 97.14 Train acc: 97.013 Test acc: 97.190 \n",
      "step: 7471 | Train: G_Loss: 1.147, D_unsup_loss_real: 0.537, D_unsup_loss_fake: 0.540, D_sup_loss: 0.113, D_sup_acc: 97.23 Train acc: 96.853 Test acc: 97.090 \n",
      "step: 7472 | Train: G_Loss: 1.374, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.592, D_sup_loss: 0.121, D_sup_acc: 97.13 Train acc: 97.010 Test acc: 97.160 \n",
      "step: 7473 | Train: G_Loss: 1.246, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.561, D_sup_loss: 0.111, D_sup_acc: 97.20 Train acc: 96.767 Test acc: 96.910 \n",
      "step: 7474 | Train: G_Loss: 1.296, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.569, D_sup_loss: 0.130, D_sup_acc: 96.95 Train acc: 97.082 Test acc: 97.220 \n",
      "step: 7475 | Train: G_Loss: 1.117, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.545, D_sup_loss: 0.112, D_sup_acc: 97.26 Train acc: 96.937 Test acc: 97.120 \n",
      "step: 7476 | Train: G_Loss: 1.261, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.601, D_sup_loss: 0.120, D_sup_acc: 97.16 Train acc: 97.022 Test acc: 97.130 \n",
      "step: 7477 | Train: G_Loss: 1.216, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.646, D_sup_loss: 0.114, D_sup_acc: 97.17 Train acc: 96.990 Test acc: 97.230 \n",
      "step: 7478 | Train: G_Loss: 1.184, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.563, D_sup_loss: 0.120, D_sup_acc: 97.27 Train acc: 96.885 Test acc: 97.180 \n",
      "step: 7479 | Train: G_Loss: 1.252, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.550, D_sup_loss: 0.129, D_sup_acc: 97.22 Train acc: 97.048 Test acc: 97.350 \n",
      "step: 7480 | Train: G_Loss: 1.272, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.507, D_sup_loss: 0.116, D_sup_acc: 97.38 Train acc: 96.967 Test acc: 97.250 \n",
      "step: 7481 | Train: G_Loss: 1.311, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.545, D_sup_loss: 0.120, D_sup_acc: 97.28 Train acc: 96.922 Test acc: 97.310 \n",
      "step: 7482 | Train: G_Loss: 1.031, D_unsup_loss_real: 0.523, D_unsup_loss_fake: 0.534, D_sup_loss: 0.120, D_sup_acc: 97.34 Train acc: 96.767 Test acc: 96.960 \n",
      "step: 7483 | Train: G_Loss: 1.206, D_unsup_loss_real: 0.522, D_unsup_loss_fake: 0.559, D_sup_loss: 0.125, D_sup_acc: 97.00 Train acc: 96.833 Test acc: 97.150 \n",
      "step: 7484 | Train: G_Loss: 1.277, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.521, D_sup_loss: 0.124, D_sup_acc: 97.19 Train acc: 96.997 Test acc: 97.300 \n",
      "step: 7485 | Train: G_Loss: 1.218, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.581, D_sup_loss: 0.112, D_sup_acc: 97.33 Train acc: 96.757 Test acc: 97.010 \n",
      "step: 7486 | Train: G_Loss: 1.243, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.561, D_sup_loss: 0.137, D_sup_acc: 97.05 Train acc: 96.755 Test acc: 96.890 \n",
      "step: 7487 | Train: G_Loss: 1.273, D_unsup_loss_real: 0.529, D_unsup_loss_fake: 0.560, D_sup_loss: 0.128, D_sup_acc: 96.93 Train acc: 97.030 Test acc: 97.230 \n",
      "step: 7488 | Train: G_Loss: 1.250, D_unsup_loss_real: 0.537, D_unsup_loss_fake: 0.563, D_sup_loss: 0.114, D_sup_acc: 97.27 Train acc: 96.965 Test acc: 97.170 \n",
      "step: 7489 | Train: G_Loss: 1.206, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.566, D_sup_loss: 0.116, D_sup_acc: 97.21 Train acc: 96.953 Test acc: 97.210 \n",
      "step: 7490 | Train: G_Loss: 1.133, D_unsup_loss_real: 0.546, D_unsup_loss_fake: 0.589, D_sup_loss: 0.119, D_sup_acc: 97.25 Train acc: 97.065 Test acc: 97.340 \n",
      "step: 7491 | Train: G_Loss: 1.200, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.613, D_sup_loss: 0.114, D_sup_acc: 97.37 Train acc: 97.020 Test acc: 97.230 \n",
      "step: 7492 | Train: G_Loss: 1.133, D_unsup_loss_real: 0.533, D_unsup_loss_fake: 0.640, D_sup_loss: 0.115, D_sup_acc: 97.27 Train acc: 96.825 Test acc: 97.050 \n",
      "step: 7493 | Train: G_Loss: 1.269, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.545, D_sup_loss: 0.127, D_sup_acc: 97.09 Train acc: 96.510 Test acc: 96.790 \n",
      "step: 7494 | Train: G_Loss: 1.221, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.555, D_sup_loss: 0.137, D_sup_acc: 96.83 Train acc: 96.688 Test acc: 96.920 \n",
      "step: 7495 | Train: G_Loss: 1.246, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.590, D_sup_loss: 0.129, D_sup_acc: 96.96 Train acc: 96.925 Test acc: 97.130 \n",
      "step: 7496 | Train: G_Loss: 1.314, D_unsup_loss_real: 0.532, D_unsup_loss_fake: 0.499, D_sup_loss: 0.119, D_sup_acc: 97.17 Train acc: 96.823 Test acc: 97.150 \n",
      "step: 7497 | Train: G_Loss: 1.281, D_unsup_loss_real: 0.487, D_unsup_loss_fake: 0.596, D_sup_loss: 0.126, D_sup_acc: 97.19 Train acc: 96.797 Test acc: 97.100 \n",
      "step: 7498 | Train: G_Loss: 1.254, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.555, D_sup_loss: 0.128, D_sup_acc: 97.14 Train acc: 96.950 Test acc: 97.200 \n",
      "step: 7499 | Train: G_Loss: 1.234, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.544, D_sup_loss: 0.116, D_sup_acc: 97.24 Train acc: 96.863 Test acc: 97.090 \n",
      "step: 7500 | Train: G_Loss: 1.191, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.559, D_sup_loss: 0.127, D_sup_acc: 97.13 Train acc: 96.940 Test acc: 97.180 \n",
      "Train Classifier Accuracy: 96.940%\n",
      "\n",
      "Test Classifier Accuracy: 97.180%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_7500.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_7500.h5\n",
      "step: 7501 | Train: G_Loss: 1.141, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.604, D_sup_loss: 0.119, D_sup_acc: 97.22 Train acc: 96.560 Test acc: 96.670 \n",
      "step: 7502 | Train: G_Loss: 1.276, D_unsup_loss_real: 0.509, D_unsup_loss_fake: 0.572, D_sup_loss: 0.140, D_sup_acc: 96.71 Train acc: 97.050 Test acc: 97.290 \n",
      "step: 7503 | Train: G_Loss: 1.157, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.587, D_sup_loss: 0.111, D_sup_acc: 97.32 Train acc: 96.715 Test acc: 96.960 \n",
      "step: 7504 | Train: G_Loss: 1.213, D_unsup_loss_real: 0.476, D_unsup_loss_fake: 0.550, D_sup_loss: 0.132, D_sup_acc: 97.00 Train acc: 96.932 Test acc: 97.160 \n",
      "step: 7505 | Train: G_Loss: 1.297, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.545, D_sup_loss: 0.117, D_sup_acc: 97.20 Train acc: 96.827 Test acc: 97.080 \n",
      "step: 7506 | Train: G_Loss: 1.213, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.613, D_sup_loss: 0.121, D_sup_acc: 97.12 Train acc: 97.013 Test acc: 97.280 \n",
      "step: 7507 | Train: G_Loss: 1.222, D_unsup_loss_real: 0.677, D_unsup_loss_fake: 0.591, D_sup_loss: 0.111, D_sup_acc: 97.31 Train acc: 96.818 Test acc: 97.070 \n",
      "step: 7508 | Train: G_Loss: 1.219, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.583, D_sup_loss: 0.127, D_sup_acc: 97.11 Train acc: 96.883 Test acc: 97.130 \n",
      "step: 7509 | Train: G_Loss: 1.346, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.537, D_sup_loss: 0.121, D_sup_acc: 97.17 Train acc: 96.838 Test acc: 97.110 \n",
      "step: 7510 | Train: G_Loss: 1.201, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.572, D_sup_loss: 0.121, D_sup_acc: 97.15 Train acc: 97.023 Test acc: 97.200 \n",
      "step: 7511 | Train: G_Loss: 1.279, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.617, D_sup_loss: 0.115, D_sup_acc: 97.24 Train acc: 97.000 Test acc: 97.170 \n",
      "step: 7512 | Train: G_Loss: 1.373, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.509, D_sup_loss: 0.117, D_sup_acc: 97.21 Train acc: 97.050 Test acc: 97.280 \n",
      "step: 7513 | Train: G_Loss: 1.149, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.533, D_sup_loss: 0.112, D_sup_acc: 97.31 Train acc: 96.575 Test acc: 96.910 \n",
      "step: 7514 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.556, D_sup_loss: 0.139, D_sup_acc: 96.95 Train acc: 96.710 Test acc: 96.960 \n",
      "step: 7515 | Train: G_Loss: 1.249, D_unsup_loss_real: 0.528, D_unsup_loss_fake: 0.603, D_sup_loss: 0.129, D_sup_acc: 97.00 Train acc: 96.968 Test acc: 97.200 \n",
      "step: 7516 | Train: G_Loss: 1.120, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.573, D_sup_loss: 0.117, D_sup_acc: 97.24 Train acc: 96.768 Test acc: 96.990 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 7517 | Train: G_Loss: 1.300, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.615, D_sup_loss: 0.130, D_sup_acc: 97.03 Train acc: 96.882 Test acc: 97.140 \n",
      "step: 7518 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.628, D_sup_loss: 0.125, D_sup_acc: 97.18 Train acc: 96.698 Test acc: 96.950 \n",
      "step: 7519 | Train: G_Loss: 1.239, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.480, D_sup_loss: 0.137, D_sup_acc: 96.99 Train acc: 97.140 Test acc: 97.390 \n",
      "step: 7520 | Train: G_Loss: 1.301, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.502, D_sup_loss: 0.110, D_sup_acc: 97.42 Train acc: 97.075 Test acc: 97.260 \n",
      "step: 7521 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.530, D_unsup_loss_fake: 0.573, D_sup_loss: 0.115, D_sup_acc: 97.29 Train acc: 96.963 Test acc: 97.190 \n",
      "step: 7522 | Train: G_Loss: 1.201, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.609, D_sup_loss: 0.117, D_sup_acc: 97.23 Train acc: 97.020 Test acc: 97.270 \n",
      "step: 7523 | Train: G_Loss: 1.210, D_unsup_loss_real: 0.653, D_unsup_loss_fake: 0.636, D_sup_loss: 0.114, D_sup_acc: 97.30 Train acc: 96.693 Test acc: 97.000 \n",
      "step: 7524 | Train: G_Loss: 1.308, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.534, D_sup_loss: 0.137, D_sup_acc: 97.04 Train acc: 97.120 Test acc: 97.290 \n",
      "step: 7525 | Train: G_Loss: 1.177, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.570, D_sup_loss: 0.111, D_sup_acc: 97.32 Train acc: 96.662 Test acc: 96.980 \n",
      "step: 7526 | Train: G_Loss: 1.272, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.643, D_sup_loss: 0.132, D_sup_acc: 97.02 Train acc: 96.643 Test acc: 96.980 \n",
      "step: 7527 | Train: G_Loss: 1.224, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.552, D_sup_loss: 0.131, D_sup_acc: 97.02 Train acc: 96.843 Test acc: 97.170 \n",
      "step: 7528 | Train: G_Loss: 1.154, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.581, D_sup_loss: 0.124, D_sup_acc: 97.21 Train acc: 96.955 Test acc: 97.240 \n",
      "step: 7529 | Train: G_Loss: 1.242, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.584, D_sup_loss: 0.119, D_sup_acc: 97.27 Train acc: 96.920 Test acc: 97.190 \n",
      "step: 7530 | Train: G_Loss: 1.227, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.630, D_sup_loss: 0.119, D_sup_acc: 97.23 Train acc: 96.982 Test acc: 97.330 \n",
      "step: 7531 | Train: G_Loss: 1.146, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.545, D_sup_loss: 0.120, D_sup_acc: 97.36 Train acc: 96.887 Test acc: 97.290 \n",
      "step: 7532 | Train: G_Loss: 1.231, D_unsup_loss_real: 0.545, D_unsup_loss_fake: 0.553, D_sup_loss: 0.124, D_sup_acc: 97.32 Train acc: 97.070 Test acc: 97.320 \n",
      "step: 7533 | Train: G_Loss: 1.321, D_unsup_loss_real: 0.514, D_unsup_loss_fake: 0.520, D_sup_loss: 0.110, D_sup_acc: 97.35 Train acc: 97.075 Test acc: 97.370 \n",
      "step: 7534 | Train: G_Loss: 1.237, D_unsup_loss_real: 0.520, D_unsup_loss_fake: 0.583, D_sup_loss: 0.112, D_sup_acc: 97.40 Train acc: 96.847 Test acc: 97.170 \n",
      "step: 7535 | Train: G_Loss: 1.205, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.506, D_sup_loss: 0.121, D_sup_acc: 97.21 Train acc: 96.958 Test acc: 97.250 \n",
      "step: 7536 | Train: G_Loss: 1.247, D_unsup_loss_real: 0.542, D_unsup_loss_fake: 0.600, D_sup_loss: 0.114, D_sup_acc: 97.28 Train acc: 96.918 Test acc: 97.240 \n",
      "step: 7537 | Train: G_Loss: 1.351, D_unsup_loss_real: 0.526, D_unsup_loss_fake: 0.584, D_sup_loss: 0.116, D_sup_acc: 97.27 Train acc: 96.975 Test acc: 97.210 \n",
      "step: 7538 | Train: G_Loss: 1.292, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.600, D_sup_loss: 0.116, D_sup_acc: 97.25 Train acc: 96.715 Test acc: 97.140 \n",
      "step: 7539 | Train: G_Loss: 1.140, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.620, D_sup_loss: 0.129, D_sup_acc: 97.18 Train acc: 96.810 Test acc: 97.160 \n",
      "step: 7540 | Train: G_Loss: 1.225, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.474, D_sup_loss: 0.129, D_sup_acc: 97.20 Train acc: 96.995 Test acc: 97.210 \n",
      "step: 7541 | Train: G_Loss: 1.211, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.542, D_sup_loss: 0.116, D_sup_acc: 97.25 Train acc: 96.795 Test acc: 97.220 \n",
      "step: 7542 | Train: G_Loss: 1.368, D_unsup_loss_real: 0.546, D_unsup_loss_fake: 0.668, D_sup_loss: 0.126, D_sup_acc: 97.26 Train acc: 97.112 Test acc: 97.360 \n",
      "step: 7543 | Train: G_Loss: 1.249, D_unsup_loss_real: 0.523, D_unsup_loss_fake: 0.554, D_sup_loss: 0.110, D_sup_acc: 97.39 Train acc: 96.982 Test acc: 97.220 \n",
      "step: 7544 | Train: G_Loss: 1.195, D_unsup_loss_real: 0.656, D_unsup_loss_fake: 0.575, D_sup_loss: 0.115, D_sup_acc: 97.26 Train acc: 96.490 Test acc: 96.860 \n",
      "step: 7545 | Train: G_Loss: 1.204, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.528, D_sup_loss: 0.137, D_sup_acc: 96.90 Train acc: 97.035 Test acc: 97.220 \n",
      "step: 7546 | Train: G_Loss: 1.228, D_unsup_loss_real: 0.517, D_unsup_loss_fake: 0.591, D_sup_loss: 0.111, D_sup_acc: 97.26 Train acc: 96.602 Test acc: 96.980 \n",
      "step: 7547 | Train: G_Loss: 1.299, D_unsup_loss_real: 0.534, D_unsup_loss_fake: 0.582, D_sup_loss: 0.136, D_sup_acc: 97.02 Train acc: 96.930 Test acc: 97.130 \n",
      "step: 7548 | Train: G_Loss: 1.180, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.558, D_sup_loss: 0.118, D_sup_acc: 97.17 Train acc: 96.755 Test acc: 97.130 \n",
      "step: 7549 | Train: G_Loss: 1.288, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.545, D_sup_loss: 0.127, D_sup_acc: 97.17 Train acc: 97.060 Test acc: 97.290 \n",
      "step: 7550 | Train: G_Loss: 1.193, D_unsup_loss_real: 0.722, D_unsup_loss_fake: 0.609, D_sup_loss: 0.110, D_sup_acc: 97.32 Train acc: 96.035 Test acc: 96.340 \n",
      "step: 7551 | Train: G_Loss: 1.361, D_unsup_loss_real: 0.710, D_unsup_loss_fake: 0.586, D_sup_loss: 0.167, D_sup_acc: 96.39 Train acc: 96.860 Test acc: 97.160 \n",
      "step: 7552 | Train: G_Loss: 1.129, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.615, D_sup_loss: 0.123, D_sup_acc: 97.20 Train acc: 96.602 Test acc: 96.910 \n",
      "step: 7553 | Train: G_Loss: 1.285, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.617, D_sup_loss: 0.134, D_sup_acc: 96.95 Train acc: 96.712 Test acc: 96.980 \n",
      "step: 7554 | Train: G_Loss: 1.246, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.537, D_sup_loss: 0.129, D_sup_acc: 97.02 Train acc: 96.903 Test acc: 97.110 \n",
      "step: 7555 | Train: G_Loss: 1.299, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.546, D_sup_loss: 0.122, D_sup_acc: 97.15 Train acc: 97.000 Test acc: 97.220 \n",
      "step: 7556 | Train: G_Loss: 1.121, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.597, D_sup_loss: 0.115, D_sup_acc: 97.26 Train acc: 96.847 Test acc: 97.030 \n",
      "step: 7557 | Train: G_Loss: 1.219, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.634, D_sup_loss: 0.128, D_sup_acc: 97.07 Train acc: 96.917 Test acc: 97.090 \n",
      "step: 7558 | Train: G_Loss: 1.245, D_unsup_loss_real: 0.521, D_unsup_loss_fake: 0.475, D_sup_loss: 0.123, D_sup_acc: 97.13 Train acc: 97.042 Test acc: 97.150 \n",
      "step: 7559 | Train: G_Loss: 1.199, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.593, D_sup_loss: 0.114, D_sup_acc: 97.19 Train acc: 96.862 Test acc: 97.110 \n",
      "step: 7560 | Train: G_Loss: 1.167, D_unsup_loss_real: 0.631, D_unsup_loss_fake: 0.533, D_sup_loss: 0.130, D_sup_acc: 97.15 Train acc: 96.833 Test acc: 97.010 \n",
      "step: 7561 | Train: G_Loss: 1.171, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.633, D_sup_loss: 0.132, D_sup_acc: 97.05 Train acc: 97.060 Test acc: 97.300 \n",
      "step: 7562 | Train: G_Loss: 1.213, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.594, D_sup_loss: 0.115, D_sup_acc: 97.33 Train acc: 96.865 Test acc: 97.170 \n",
      "step: 7563 | Train: G_Loss: 1.160, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.653, D_sup_loss: 0.126, D_sup_acc: 97.21 Train acc: 96.723 Test acc: 96.930 \n",
      "step: 7564 | Train: G_Loss: 1.313, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.578, D_sup_loss: 0.133, D_sup_acc: 96.97 Train acc: 96.968 Test acc: 97.180 \n",
      "step: 7565 | Train: G_Loss: 1.219, D_unsup_loss_real: 0.534, D_unsup_loss_fake: 0.622, D_sup_loss: 0.119, D_sup_acc: 97.22 Train acc: 96.610 Test acc: 96.820 \n",
      "step: 7566 | Train: G_Loss: 1.180, D_unsup_loss_real: 0.664, D_unsup_loss_fake: 0.540, D_sup_loss: 0.138, D_sup_acc: 96.86 Train acc: 96.442 Test acc: 96.660 \n",
      "step: 7567 | Train: G_Loss: 1.139, D_unsup_loss_real: 0.485, D_unsup_loss_fake: 0.529, D_sup_loss: 0.147, D_sup_acc: 96.70 Train acc: 96.933 Test acc: 97.150 \n",
      "step: 7568 | Train: G_Loss: 1.267, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.567, D_sup_loss: 0.120, D_sup_acc: 97.19 Train acc: 96.993 Test acc: 97.080 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 7569 | Train: G_Loss: 1.204, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.572, D_sup_loss: 0.117, D_sup_acc: 97.12 Train acc: 96.815 Test acc: 97.070 \n",
      "step: 7570 | Train: G_Loss: 1.168, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.600, D_sup_loss: 0.130, D_sup_acc: 97.11 Train acc: 96.958 Test acc: 97.100 \n",
      "step: 7571 | Train: G_Loss: 1.176, D_unsup_loss_real: 0.529, D_unsup_loss_fake: 0.561, D_sup_loss: 0.122, D_sup_acc: 97.14 Train acc: 97.023 Test acc: 97.190 \n",
      "step: 7572 | Train: G_Loss: 1.193, D_unsup_loss_real: 0.538, D_unsup_loss_fake: 0.572, D_sup_loss: 0.118, D_sup_acc: 97.23 Train acc: 96.880 Test acc: 97.060 \n",
      "step: 7573 | Train: G_Loss: 1.306, D_unsup_loss_real: 0.550, D_unsup_loss_fake: 0.547, D_sup_loss: 0.124, D_sup_acc: 97.10 Train acc: 96.718 Test acc: 96.930 \n",
      "step: 7574 | Train: G_Loss: 1.151, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.623, D_sup_loss: 0.128, D_sup_acc: 96.97 Train acc: 96.632 Test acc: 96.730 \n",
      "step: 7575 | Train: G_Loss: 1.281, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.495, D_sup_loss: 0.138, D_sup_acc: 96.77 Train acc: 96.878 Test acc: 96.970 \n",
      "step: 7576 | Train: G_Loss: 1.300, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.626, D_sup_loss: 0.123, D_sup_acc: 97.01 Train acc: 96.943 Test acc: 96.990 \n",
      "step: 7577 | Train: G_Loss: 1.212, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.645, D_sup_loss: 0.120, D_sup_acc: 97.03 Train acc: 96.972 Test acc: 96.980 \n",
      "step: 7578 | Train: G_Loss: 1.245, D_unsup_loss_real: 0.515, D_unsup_loss_fake: 0.496, D_sup_loss: 0.120, D_sup_acc: 97.02 Train acc: 97.000 Test acc: 97.060 \n",
      "step: 7579 | Train: G_Loss: 1.253, D_unsup_loss_real: 0.544, D_unsup_loss_fake: 0.511, D_sup_loss: 0.117, D_sup_acc: 97.10 Train acc: 97.077 Test acc: 97.160 \n",
      "step: 7580 | Train: G_Loss: 1.203, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.571, D_sup_loss: 0.113, D_sup_acc: 97.20 Train acc: 96.860 Test acc: 97.080 \n",
      "step: 7581 | Train: G_Loss: 1.166, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.583, D_sup_loss: 0.127, D_sup_acc: 97.12 Train acc: 96.903 Test acc: 97.050 \n",
      "step: 7582 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.540, D_sup_loss: 0.122, D_sup_acc: 97.09 Train acc: 96.950 Test acc: 97.150 \n",
      "step: 7583 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.526, D_unsup_loss_fake: 0.568, D_sup_loss: 0.121, D_sup_acc: 97.19 Train acc: 96.997 Test acc: 97.110 \n",
      "step: 7584 | Train: G_Loss: 1.319, D_unsup_loss_real: 0.535, D_unsup_loss_fake: 0.603, D_sup_loss: 0.120, D_sup_acc: 97.15 Train acc: 96.873 Test acc: 97.060 \n",
      "step: 7585 | Train: G_Loss: 1.241, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.578, D_sup_loss: 0.125, D_sup_acc: 97.10 Train acc: 96.855 Test acc: 97.010 \n",
      "step: 7586 | Train: G_Loss: 1.232, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.558, D_sup_loss: 0.126, D_sup_acc: 97.05 Train acc: 96.715 Test acc: 96.950 \n",
      "step: 7587 | Train: G_Loss: 1.165, D_unsup_loss_real: 0.527, D_unsup_loss_fake: 0.540, D_sup_loss: 0.129, D_sup_acc: 96.99 Train acc: 97.037 Test acc: 97.160 \n",
      "step: 7588 | Train: G_Loss: 1.227, D_unsup_loss_real: 0.538, D_unsup_loss_fake: 0.548, D_sup_loss: 0.109, D_sup_acc: 97.20 Train acc: 96.855 Test acc: 97.090 \n",
      "step: 7589 | Train: G_Loss: 1.182, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.638, D_sup_loss: 0.125, D_sup_acc: 97.13 Train acc: 97.000 Test acc: 97.150 \n",
      "step: 7590 | Train: G_Loss: 1.116, D_unsup_loss_real: 0.660, D_unsup_loss_fake: 0.565, D_sup_loss: 0.119, D_sup_acc: 97.19 Train acc: 96.863 Test acc: 97.040 \n",
      "step: 7591 | Train: G_Loss: 1.311, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.581, D_sup_loss: 0.126, D_sup_acc: 97.08 Train acc: 96.802 Test acc: 96.940 \n",
      "step: 7592 | Train: G_Loss: 1.155, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.577, D_sup_loss: 0.124, D_sup_acc: 96.98 Train acc: 96.913 Test acc: 97.040 \n",
      "step: 7593 | Train: G_Loss: 1.238, D_unsup_loss_real: 0.531, D_unsup_loss_fake: 0.530, D_sup_loss: 0.120, D_sup_acc: 97.08 Train acc: 96.852 Test acc: 97.070 \n",
      "step: 7594 | Train: G_Loss: 1.154, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.534, D_sup_loss: 0.124, D_sup_acc: 97.11 Train acc: 96.845 Test acc: 97.040 \n",
      "step: 7595 | Train: G_Loss: 1.157, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.563, D_sup_loss: 0.124, D_sup_acc: 97.08 Train acc: 97.093 Test acc: 97.130 \n",
      "step: 7596 | Train: G_Loss: 1.490, D_unsup_loss_real: 0.447, D_unsup_loss_fake: 0.520, D_sup_loss: 0.110, D_sup_acc: 97.17 Train acc: 96.968 Test acc: 97.070 \n",
      "step: 7597 | Train: G_Loss: 1.503, D_unsup_loss_real: 0.710, D_unsup_loss_fake: 0.489, D_sup_loss: 0.109, D_sup_acc: 97.11 Train acc: 95.565 Test acc: 95.830 \n",
      "step: 7598 | Train: G_Loss: 1.225, D_unsup_loss_real: 0.755, D_unsup_loss_fake: 0.574, D_sup_loss: 0.203, D_sup_acc: 95.88 Train acc: 96.808 Test acc: 97.070 \n",
      "step: 7599 | Train: G_Loss: 1.204, D_unsup_loss_real: 0.518, D_unsup_loss_fake: 0.562, D_sup_loss: 0.120, D_sup_acc: 97.11 Train acc: 96.988 Test acc: 97.200 \n",
      "step: 7600 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.642, D_sup_loss: 0.113, D_sup_acc: 97.24 Train acc: 96.982 Test acc: 97.130 \n",
      "Train Classifier Accuracy: 96.982%\n",
      "\n",
      "Test Classifier Accuracy: 97.130%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_7600.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_7600.h5\n",
      "step: 7601 | Train: G_Loss: 1.193, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.604, D_sup_loss: 0.114, D_sup_acc: 97.17 Train acc: 96.955 Test acc: 97.010 \n",
      "step: 7602 | Train: G_Loss: 1.295, D_unsup_loss_real: 0.531, D_unsup_loss_fake: 0.602, D_sup_loss: 0.117, D_sup_acc: 97.05 Train acc: 97.022 Test acc: 97.130 \n",
      "step: 7603 | Train: G_Loss: 1.165, D_unsup_loss_real: 0.516, D_unsup_loss_fake: 0.611, D_sup_loss: 0.113, D_sup_acc: 97.17 Train acc: 96.895 Test acc: 97.030 \n",
      "step: 7604 | Train: G_Loss: 1.163, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.545, D_sup_loss: 0.121, D_sup_acc: 97.07 Train acc: 97.043 Test acc: 97.160 \n",
      "step: 7605 | Train: G_Loss: 1.298, D_unsup_loss_real: 0.503, D_unsup_loss_fake: 0.562, D_sup_loss: 0.113, D_sup_acc: 97.20 Train acc: 96.940 Test acc: 97.090 \n",
      "step: 7606 | Train: G_Loss: 1.195, D_unsup_loss_real: 0.498, D_unsup_loss_fake: 0.618, D_sup_loss: 0.116, D_sup_acc: 97.13 Train acc: 96.752 Test acc: 96.900 \n",
      "step: 7607 | Train: G_Loss: 1.271, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.587, D_sup_loss: 0.127, D_sup_acc: 96.94 Train acc: 96.907 Test acc: 97.030 \n",
      "step: 7608 | Train: G_Loss: 1.225, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.539, D_sup_loss: 0.118, D_sup_acc: 97.07 Train acc: 96.880 Test acc: 97.040 \n",
      "step: 7609 | Train: G_Loss: 1.208, D_unsup_loss_real: 0.531, D_unsup_loss_fake: 0.548, D_sup_loss: 0.119, D_sup_acc: 97.08 Train acc: 96.895 Test acc: 97.130 \n",
      "step: 7610 | Train: G_Loss: 1.173, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.521, D_sup_loss: 0.120, D_sup_acc: 97.17 Train acc: 96.867 Test acc: 97.100 \n",
      "step: 7611 | Train: G_Loss: 1.188, D_unsup_loss_real: 0.504, D_unsup_loss_fake: 0.611, D_sup_loss: 0.122, D_sup_acc: 97.14 Train acc: 96.758 Test acc: 96.910 \n",
      "step: 7612 | Train: G_Loss: 1.191, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.600, D_sup_loss: 0.124, D_sup_acc: 96.95 Train acc: 96.568 Test acc: 96.800 \n",
      "step: 7613 | Train: G_Loss: 1.296, D_unsup_loss_real: 0.550, D_unsup_loss_fake: 0.531, D_sup_loss: 0.132, D_sup_acc: 96.84 Train acc: 96.163 Test acc: 96.370 \n",
      "step: 7614 | Train: G_Loss: 1.252, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.545, D_sup_loss: 0.146, D_sup_acc: 96.42 Train acc: 96.700 Test acc: 96.960 \n",
      "step: 7615 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.530, D_unsup_loss_fake: 0.518, D_sup_loss: 0.127, D_sup_acc: 97.00 Train acc: 96.633 Test acc: 96.820 \n",
      "step: 7616 | Train: G_Loss: 1.238, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.569, D_sup_loss: 0.128, D_sup_acc: 96.86 Train acc: 96.788 Test acc: 96.960 \n",
      "step: 7617 | Train: G_Loss: 1.216, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.621, D_sup_loss: 0.121, D_sup_acc: 97.00 Train acc: 96.707 Test acc: 96.870 \n",
      "step: 7618 | Train: G_Loss: 1.188, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.586, D_sup_loss: 0.128, D_sup_acc: 96.91 Train acc: 96.743 Test acc: 96.920 \n",
      "step: 7619 | Train: G_Loss: 1.265, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.582, D_sup_loss: 0.123, D_sup_acc: 96.96 Train acc: 96.830 Test acc: 97.010 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 7620 | Train: G_Loss: 1.496, D_unsup_loss_real: 0.546, D_unsup_loss_fake: 0.518, D_sup_loss: 0.122, D_sup_acc: 97.05 Train acc: 96.992 Test acc: 97.090 \n",
      "step: 7621 | Train: G_Loss: 1.242, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.537, D_sup_loss: 0.111, D_sup_acc: 97.13 Train acc: 96.457 Test acc: 96.670 \n",
      "step: 7622 | Train: G_Loss: 1.310, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.540, D_sup_loss: 0.145, D_sup_acc: 96.71 Train acc: 96.907 Test acc: 97.110 \n",
      "step: 7623 | Train: G_Loss: 1.193, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.538, D_sup_loss: 0.119, D_sup_acc: 97.15 Train acc: 97.020 Test acc: 97.200 \n",
      "step: 7624 | Train: G_Loss: 1.111, D_unsup_loss_real: 0.537, D_unsup_loss_fake: 0.577, D_sup_loss: 0.110, D_sup_acc: 97.24 Train acc: 96.617 Test acc: 96.820 \n",
      "step: 7625 | Train: G_Loss: 1.079, D_unsup_loss_real: 0.522, D_unsup_loss_fake: 0.596, D_sup_loss: 0.134, D_sup_acc: 96.86 Train acc: 96.877 Test acc: 97.060 \n",
      "step: 7626 | Train: G_Loss: 1.444, D_unsup_loss_real: 0.525, D_unsup_loss_fake: 0.640, D_sup_loss: 0.119, D_sup_acc: 97.10 Train acc: 96.960 Test acc: 97.040 \n",
      "step: 7627 | Train: G_Loss: 1.449, D_unsup_loss_real: 0.802, D_unsup_loss_fake: 0.527, D_sup_loss: 0.109, D_sup_acc: 97.08 Train acc: 95.197 Test acc: 95.280 \n",
      "step: 7628 | Train: G_Loss: 1.149, D_unsup_loss_real: 0.924, D_unsup_loss_fake: 0.742, D_sup_loss: 0.223, D_sup_acc: 95.34 Train acc: 96.798 Test acc: 97.030 \n",
      "step: 7629 | Train: G_Loss: 1.249, D_unsup_loss_real: 0.631, D_unsup_loss_fake: 0.629, D_sup_loss: 0.125, D_sup_acc: 97.07 Train acc: 96.968 Test acc: 97.110 \n",
      "step: 7630 | Train: G_Loss: 1.169, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.631, D_sup_loss: 0.119, D_sup_acc: 97.15 Train acc: 96.758 Test acc: 96.950 \n",
      "step: 7631 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.516, D_unsup_loss_fake: 0.616, D_sup_loss: 0.125, D_sup_acc: 96.99 Train acc: 96.865 Test acc: 97.040 \n",
      "step: 7632 | Train: G_Loss: 1.232, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.597, D_sup_loss: 0.121, D_sup_acc: 97.08 Train acc: 96.735 Test acc: 97.020 \n",
      "step: 7633 | Train: G_Loss: 1.239, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.560, D_sup_loss: 0.126, D_sup_acc: 97.06 Train acc: 96.775 Test acc: 97.080 \n",
      "step: 7634 | Train: G_Loss: 1.176, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.552, D_sup_loss: 0.121, D_sup_acc: 97.12 Train acc: 96.650 Test acc: 96.970 \n",
      "step: 7635 | Train: G_Loss: 1.177, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.543, D_sup_loss: 0.128, D_sup_acc: 97.01 Train acc: 96.763 Test acc: 97.120 \n",
      "step: 7636 | Train: G_Loss: 1.215, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.605, D_sup_loss: 0.119, D_sup_acc: 97.16 Train acc: 96.665 Test acc: 96.910 \n",
      "step: 7637 | Train: G_Loss: 1.150, D_unsup_loss_real: 0.528, D_unsup_loss_fake: 0.504, D_sup_loss: 0.128, D_sup_acc: 96.95 Train acc: 96.860 Test acc: 97.140 \n",
      "step: 7638 | Train: G_Loss: 1.117, D_unsup_loss_real: 0.472, D_unsup_loss_fake: 0.548, D_sup_loss: 0.116, D_sup_acc: 97.18 Train acc: 96.798 Test acc: 96.950 \n",
      "step: 7639 | Train: G_Loss: 1.229, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.557, D_sup_loss: 0.122, D_sup_acc: 96.99 Train acc: 96.782 Test acc: 96.950 \n",
      "step: 7640 | Train: G_Loss: 1.174, D_unsup_loss_real: 0.510, D_unsup_loss_fake: 0.633, D_sup_loss: 0.122, D_sup_acc: 96.99 Train acc: 96.442 Test acc: 96.750 \n",
      "step: 7641 | Train: G_Loss: 1.332, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.586, D_sup_loss: 0.139, D_sup_acc: 96.79 Train acc: 96.887 Test acc: 97.030 \n",
      "step: 7642 | Train: G_Loss: 1.314, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.557, D_sup_loss: 0.118, D_sup_acc: 97.07 Train acc: 96.830 Test acc: 97.040 \n",
      "step: 7643 | Train: G_Loss: 1.279, D_unsup_loss_real: 0.535, D_unsup_loss_fake: 0.595, D_sup_loss: 0.118, D_sup_acc: 97.08 Train acc: 96.903 Test acc: 97.040 \n",
      "step: 7644 | Train: G_Loss: 1.312, D_unsup_loss_real: 0.646, D_unsup_loss_fake: 0.605, D_sup_loss: 0.116, D_sup_acc: 97.08 Train acc: 96.778 Test acc: 96.940 \n",
      "step: 7645 | Train: G_Loss: 1.343, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.515, D_sup_loss: 0.126, D_sup_acc: 96.98 Train acc: 96.998 Test acc: 97.160 \n",
      "step: 7646 | Train: G_Loss: 1.333, D_unsup_loss_real: 0.511, D_unsup_loss_fake: 0.557, D_sup_loss: 0.114, D_sup_acc: 97.20 Train acc: 96.903 Test acc: 97.090 \n",
      "step: 7647 | Train: G_Loss: 1.156, D_unsup_loss_real: 0.550, D_unsup_loss_fake: 0.574, D_sup_loss: 0.118, D_sup_acc: 97.13 Train acc: 96.540 Test acc: 96.770 \n",
      "step: 7648 | Train: G_Loss: 1.228, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.542, D_sup_loss: 0.136, D_sup_acc: 96.81 Train acc: 96.668 Test acc: 96.790 \n",
      "step: 7649 | Train: G_Loss: 1.188, D_unsup_loss_real: 0.533, D_unsup_loss_fake: 0.542, D_sup_loss: 0.130, D_sup_acc: 96.83 Train acc: 96.450 Test acc: 96.670 \n",
      "step: 7650 | Train: G_Loss: 1.186, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.529, D_sup_loss: 0.140, D_sup_acc: 96.71 Train acc: 96.830 Test acc: 96.980 \n",
      "step: 7651 | Train: G_Loss: 1.188, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.534, D_sup_loss: 0.122, D_sup_acc: 97.02 Train acc: 96.838 Test acc: 96.920 \n",
      "step: 7652 | Train: G_Loss: 1.294, D_unsup_loss_real: 0.500, D_unsup_loss_fake: 0.552, D_sup_loss: 0.121, D_sup_acc: 96.96 Train acc: 96.827 Test acc: 97.030 \n",
      "step: 7653 | Train: G_Loss: 1.191, D_unsup_loss_real: 0.533, D_unsup_loss_fake: 0.558, D_sup_loss: 0.120, D_sup_acc: 97.07 Train acc: 96.818 Test acc: 96.920 \n",
      "step: 7654 | Train: G_Loss: 1.300, D_unsup_loss_real: 0.516, D_unsup_loss_fake: 0.583, D_sup_loss: 0.119, D_sup_acc: 96.96 Train acc: 96.820 Test acc: 96.940 \n",
      "step: 7655 | Train: G_Loss: 1.237, D_unsup_loss_real: 0.514, D_unsup_loss_fake: 0.598, D_sup_loss: 0.121, D_sup_acc: 96.98 Train acc: 96.710 Test acc: 96.850 \n",
      "step: 7656 | Train: G_Loss: 1.263, D_unsup_loss_real: 0.651, D_unsup_loss_fake: 0.550, D_sup_loss: 0.127, D_sup_acc: 96.89 Train acc: 96.833 Test acc: 97.000 \n",
      "step: 7657 | Train: G_Loss: 1.287, D_unsup_loss_real: 0.510, D_unsup_loss_fake: 0.563, D_sup_loss: 0.123, D_sup_acc: 97.04 Train acc: 96.918 Test acc: 97.060 \n",
      "step: 7658 | Train: G_Loss: 1.244, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.511, D_sup_loss: 0.118, D_sup_acc: 97.10 Train acc: 96.603 Test acc: 96.790 \n",
      "step: 7659 | Train: G_Loss: 1.196, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.603, D_sup_loss: 0.131, D_sup_acc: 96.83 Train acc: 96.500 Test acc: 96.780 \n",
      "step: 7660 | Train: G_Loss: 1.318, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.583, D_sup_loss: 0.135, D_sup_acc: 96.82 Train acc: 96.843 Test acc: 97.020 \n",
      "step: 7661 | Train: G_Loss: 1.241, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.536, D_sup_loss: 0.119, D_sup_acc: 97.06 Train acc: 96.785 Test acc: 96.910 \n",
      "step: 7662 | Train: G_Loss: 1.095, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.543, D_sup_loss: 0.115, D_sup_acc: 96.95 Train acc: 96.560 Test acc: 96.780 \n",
      "step: 7663 | Train: G_Loss: 1.259, D_unsup_loss_real: 0.524, D_unsup_loss_fake: 0.535, D_sup_loss: 0.138, D_sup_acc: 96.82 Train acc: 96.912 Test acc: 97.150 \n",
      "step: 7664 | Train: G_Loss: 1.299, D_unsup_loss_real: 0.507, D_unsup_loss_fake: 0.542, D_sup_loss: 0.115, D_sup_acc: 97.19 Train acc: 96.992 Test acc: 97.180 \n",
      "step: 7665 | Train: G_Loss: 1.217, D_unsup_loss_real: 0.532, D_unsup_loss_fake: 0.543, D_sup_loss: 0.110, D_sup_acc: 97.22 Train acc: 96.650 Test acc: 96.830 \n",
      "step: 7666 | Train: G_Loss: 1.314, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.605, D_sup_loss: 0.135, D_sup_acc: 96.87 Train acc: 96.662 Test acc: 96.950 \n",
      "step: 7667 | Train: G_Loss: 1.281, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.649, D_sup_loss: 0.126, D_sup_acc: 96.99 Train acc: 96.605 Test acc: 96.770 \n",
      "step: 7668 | Train: G_Loss: 1.243, D_unsup_loss_real: 0.517, D_unsup_loss_fake: 0.593, D_sup_loss: 0.124, D_sup_acc: 96.81 Train acc: 96.412 Test acc: 96.650 \n",
      "step: 7669 | Train: G_Loss: 1.238, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.558, D_sup_loss: 0.144, D_sup_acc: 96.69 Train acc: 96.630 Test acc: 96.830 \n",
      "step: 7670 | Train: G_Loss: 1.291, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.604, D_sup_loss: 0.134, D_sup_acc: 96.87 Train acc: 96.670 Test acc: 96.870 \n",
      "step: 7671 | Train: G_Loss: 1.321, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.558, D_sup_loss: 0.132, D_sup_acc: 96.91 Train acc: 96.737 Test acc: 96.950 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 7672 | Train: G_Loss: 1.284, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.575, D_sup_loss: 0.122, D_sup_acc: 96.99 Train acc: 96.633 Test acc: 96.830 \n",
      "step: 7673 | Train: G_Loss: 1.209, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.576, D_sup_loss: 0.131, D_sup_acc: 96.87 Train acc: 96.667 Test acc: 96.770 \n",
      "step: 7674 | Train: G_Loss: 1.216, D_unsup_loss_real: 0.533, D_unsup_loss_fake: 0.650, D_sup_loss: 0.125, D_sup_acc: 96.81 Train acc: 96.728 Test acc: 96.880 \n",
      "step: 7675 | Train: G_Loss: 1.291, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.588, D_sup_loss: 0.123, D_sup_acc: 96.92 Train acc: 96.642 Test acc: 96.780 \n",
      "step: 7676 | Train: G_Loss: 1.289, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.509, D_sup_loss: 0.127, D_sup_acc: 96.82 Train acc: 96.693 Test acc: 96.890 \n",
      "step: 7677 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.512, D_sup_loss: 0.129, D_sup_acc: 96.93 Train acc: 96.792 Test acc: 97.030 \n",
      "step: 7678 | Train: G_Loss: 1.226, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.585, D_sup_loss: 0.123, D_sup_acc: 97.07 Train acc: 96.702 Test acc: 96.870 \n",
      "step: 7679 | Train: G_Loss: 1.125, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.567, D_sup_loss: 0.128, D_sup_acc: 96.91 Train acc: 96.792 Test acc: 97.030 \n",
      "step: 7680 | Train: G_Loss: 1.143, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.555, D_sup_loss: 0.121, D_sup_acc: 97.07 Train acc: 96.983 Test acc: 97.090 \n",
      "step: 7681 | Train: G_Loss: 1.223, D_unsup_loss_real: 0.496, D_unsup_loss_fake: 0.553, D_sup_loss: 0.113, D_sup_acc: 97.13 Train acc: 96.822 Test acc: 96.850 \n",
      "step: 7682 | Train: G_Loss: 1.254, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.605, D_sup_loss: 0.121, D_sup_acc: 96.89 Train acc: 96.652 Test acc: 96.830 \n",
      "step: 7683 | Train: G_Loss: 1.210, D_unsup_loss_real: 0.514, D_unsup_loss_fake: 0.520, D_sup_loss: 0.132, D_sup_acc: 96.87 Train acc: 96.743 Test acc: 96.940 \n",
      "step: 7684 | Train: G_Loss: 1.300, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.581, D_sup_loss: 0.126, D_sup_acc: 96.98 Train acc: 96.818 Test acc: 96.990 \n",
      "step: 7685 | Train: G_Loss: 1.215, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.612, D_sup_loss: 0.119, D_sup_acc: 97.03 Train acc: 96.615 Test acc: 96.850 \n",
      "step: 7686 | Train: G_Loss: 1.324, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.608, D_sup_loss: 0.134, D_sup_acc: 96.89 Train acc: 96.570 Test acc: 96.800 \n",
      "step: 7687 | Train: G_Loss: 1.280, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.612, D_sup_loss: 0.133, D_sup_acc: 96.84 Train acc: 96.683 Test acc: 96.840 \n",
      "step: 7688 | Train: G_Loss: 1.296, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.582, D_sup_loss: 0.129, D_sup_acc: 96.88 Train acc: 96.693 Test acc: 96.850 \n",
      "step: 7689 | Train: G_Loss: 1.185, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.590, D_sup_loss: 0.126, D_sup_acc: 96.89 Train acc: 95.973 Test acc: 96.210 \n",
      "step: 7690 | Train: G_Loss: 1.212, D_unsup_loss_real: 0.546, D_unsup_loss_fake: 0.525, D_sup_loss: 0.161, D_sup_acc: 96.26 Train acc: 96.528 Test acc: 96.750 \n",
      "step: 7691 | Train: G_Loss: 1.182, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.586, D_sup_loss: 0.134, D_sup_acc: 96.79 Train acc: 96.530 Test acc: 96.730 \n",
      "step: 7692 | Train: G_Loss: 1.134, D_unsup_loss_real: 0.516, D_unsup_loss_fake: 0.514, D_sup_loss: 0.134, D_sup_acc: 96.77 Train acc: 96.587 Test acc: 96.750 \n",
      "step: 7693 | Train: G_Loss: 1.201, D_unsup_loss_real: 0.530, D_unsup_loss_fake: 0.627, D_sup_loss: 0.128, D_sup_acc: 96.79 Train acc: 96.385 Test acc: 96.590 \n",
      "step: 7694 | Train: G_Loss: 1.178, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.541, D_sup_loss: 0.140, D_sup_acc: 96.63 Train acc: 96.673 Test acc: 96.820 \n",
      "step: 7695 | Train: G_Loss: 1.143, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.649, D_sup_loss: 0.125, D_sup_acc: 96.86 Train acc: 96.863 Test acc: 97.070 \n",
      "step: 7696 | Train: G_Loss: 1.206, D_unsup_loss_real: 0.530, D_unsup_loss_fake: 0.611, D_sup_loss: 0.120, D_sup_acc: 97.11 Train acc: 96.828 Test acc: 97.050 \n",
      "step: 7697 | Train: G_Loss: 1.218, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.612, D_sup_loss: 0.120, D_sup_acc: 97.09 Train acc: 96.778 Test acc: 96.900 \n",
      "step: 7698 | Train: G_Loss: 1.350, D_unsup_loss_real: 0.515, D_unsup_loss_fake: 0.605, D_sup_loss: 0.120, D_sup_acc: 96.94 Train acc: 96.882 Test acc: 96.990 \n",
      "step: 7699 | Train: G_Loss: 1.190, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.617, D_sup_loss: 0.117, D_sup_acc: 97.03 Train acc: 96.872 Test acc: 96.990 \n",
      "step: 7700 | Train: G_Loss: 1.245, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.573, D_sup_loss: 0.120, D_sup_acc: 97.03 Train acc: 96.698 Test acc: 97.010 \n",
      "Train Classifier Accuracy: 96.698%\n",
      "\n",
      "Test Classifier Accuracy: 97.010%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_7700.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_7700.h5\n",
      "step: 7701 | Train: G_Loss: 1.248, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.645, D_sup_loss: 0.124, D_sup_acc: 97.05 Train acc: 96.558 Test acc: 96.890 \n",
      "step: 7702 | Train: G_Loss: 1.287, D_unsup_loss_real: 0.534, D_unsup_loss_fake: 0.495, D_sup_loss: 0.136, D_sup_acc: 96.93 Train acc: 96.708 Test acc: 97.010 \n",
      "step: 7703 | Train: G_Loss: 1.228, D_unsup_loss_real: 0.513, D_unsup_loss_fake: 0.578, D_sup_loss: 0.124, D_sup_acc: 97.05 Train acc: 96.768 Test acc: 97.070 \n",
      "step: 7704 | Train: G_Loss: 1.208, D_unsup_loss_real: 0.516, D_unsup_loss_fake: 0.537, D_sup_loss: 0.124, D_sup_acc: 97.11 Train acc: 96.737 Test acc: 97.020 \n",
      "step: 7705 | Train: G_Loss: 1.248, D_unsup_loss_real: 0.679, D_unsup_loss_fake: 0.565, D_sup_loss: 0.124, D_sup_acc: 97.06 Train acc: 96.877 Test acc: 97.140 \n",
      "step: 7706 | Train: G_Loss: 1.306, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.545, D_sup_loss: 0.119, D_sup_acc: 97.18 Train acc: 96.552 Test acc: 96.840 \n",
      "step: 7707 | Train: G_Loss: 1.221, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.511, D_sup_loss: 0.135, D_sup_acc: 96.88 Train acc: 96.892 Test acc: 97.080 \n",
      "step: 7708 | Train: G_Loss: 1.208, D_unsup_loss_real: 0.513, D_unsup_loss_fake: 0.530, D_sup_loss: 0.117, D_sup_acc: 97.12 Train acc: 96.858 Test acc: 96.990 \n",
      "step: 7709 | Train: G_Loss: 1.282, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.632, D_sup_loss: 0.118, D_sup_acc: 97.03 Train acc: 96.915 Test acc: 97.150 \n",
      "step: 7710 | Train: G_Loss: 1.239, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.570, D_sup_loss: 0.118, D_sup_acc: 97.19 Train acc: 96.933 Test acc: 97.150 \n",
      "step: 7711 | Train: G_Loss: 1.478, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.635, D_sup_loss: 0.118, D_sup_acc: 97.19 Train acc: 96.860 Test acc: 96.980 \n",
      "step: 7712 | Train: G_Loss: 1.478, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.479, D_sup_loss: 0.114, D_sup_acc: 97.02 Train acc: 96.000 Test acc: 96.370 \n",
      "step: 7713 | Train: G_Loss: 1.166, D_unsup_loss_real: 0.818, D_unsup_loss_fake: 0.599, D_sup_loss: 0.177, D_sup_acc: 96.42 Train acc: 96.592 Test acc: 96.870 \n",
      "step: 7714 | Train: G_Loss: 1.114, D_unsup_loss_real: 0.541, D_unsup_loss_fake: 0.561, D_sup_loss: 0.132, D_sup_acc: 96.91 Train acc: 96.855 Test acc: 97.040 \n",
      "step: 7715 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.564, D_sup_loss: 0.115, D_sup_acc: 97.08 Train acc: 96.942 Test acc: 97.100 \n",
      "step: 7716 | Train: G_Loss: 1.322, D_unsup_loss_real: 0.460, D_unsup_loss_fake: 0.644, D_sup_loss: 0.115, D_sup_acc: 97.14 Train acc: 96.847 Test acc: 97.050 \n",
      "step: 7717 | Train: G_Loss: 1.182, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.506, D_sup_loss: 0.120, D_sup_acc: 97.09 Train acc: 96.892 Test acc: 97.050 \n",
      "step: 7718 | Train: G_Loss: 1.234, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.581, D_sup_loss: 0.119, D_sup_acc: 97.09 Train acc: 96.903 Test acc: 97.080 \n",
      "step: 7719 | Train: G_Loss: 1.289, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.602, D_sup_loss: 0.118, D_sup_acc: 97.12 Train acc: 96.910 Test acc: 97.120 \n",
      "step: 7720 | Train: G_Loss: 1.216, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.587, D_sup_loss: 0.116, D_sup_acc: 97.16 Train acc: 96.665 Test acc: 96.880 \n",
      "step: 7721 | Train: G_Loss: 1.238, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.540, D_sup_loss: 0.132, D_sup_acc: 96.92 Train acc: 96.897 Test acc: 97.090 \n",
      "step: 7722 | Train: G_Loss: 1.161, D_unsup_loss_real: 0.515, D_unsup_loss_fake: 0.579, D_sup_loss: 0.119, D_sup_acc: 97.13 Train acc: 96.602 Test acc: 96.770 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 7723 | Train: G_Loss: 1.258, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.552, D_sup_loss: 0.134, D_sup_acc: 96.81 Train acc: 97.045 Test acc: 97.090 \n",
      "step: 7724 | Train: G_Loss: 1.216, D_unsup_loss_real: 0.528, D_unsup_loss_fake: 0.545, D_sup_loss: 0.113, D_sup_acc: 97.13 Train acc: 96.820 Test acc: 96.910 \n",
      "step: 7725 | Train: G_Loss: 1.312, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.573, D_sup_loss: 0.126, D_sup_acc: 96.95 Train acc: 96.858 Test acc: 97.070 \n",
      "step: 7726 | Train: G_Loss: 1.177, D_unsup_loss_real: 0.541, D_unsup_loss_fake: 0.497, D_sup_loss: 0.120, D_sup_acc: 97.11 Train acc: 96.522 Test acc: 96.720 \n",
      "step: 7727 | Train: G_Loss: 1.210, D_unsup_loss_real: 0.658, D_unsup_loss_fake: 0.548, D_sup_loss: 0.137, D_sup_acc: 96.76 Train acc: 96.728 Test acc: 96.800 \n",
      "step: 7728 | Train: G_Loss: 1.180, D_unsup_loss_real: 0.500, D_unsup_loss_fake: 0.625, D_sup_loss: 0.131, D_sup_acc: 96.84 Train acc: 96.927 Test acc: 97.030 \n",
      "step: 7729 | Train: G_Loss: 1.214, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.613, D_sup_loss: 0.116, D_sup_acc: 97.07 Train acc: 96.845 Test acc: 96.900 \n",
      "step: 7730 | Train: G_Loss: 1.215, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.617, D_sup_loss: 0.123, D_sup_acc: 96.94 Train acc: 96.857 Test acc: 96.960 \n",
      "step: 7731 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.632, D_sup_loss: 0.122, D_sup_acc: 97.00 Train acc: 96.607 Test acc: 96.730 \n",
      "step: 7732 | Train: G_Loss: 1.361, D_unsup_loss_real: 0.524, D_unsup_loss_fake: 0.495, D_sup_loss: 0.139, D_sup_acc: 96.77 Train acc: 97.028 Test acc: 97.150 \n",
      "step: 7733 | Train: G_Loss: 1.224, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.583, D_sup_loss: 0.109, D_sup_acc: 97.19 Train acc: 96.608 Test acc: 96.740 \n",
      "step: 7734 | Train: G_Loss: 1.108, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.552, D_sup_loss: 0.139, D_sup_acc: 96.78 Train acc: 96.767 Test acc: 96.950 \n",
      "step: 7735 | Train: G_Loss: 1.283, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.605, D_sup_loss: 0.127, D_sup_acc: 96.99 Train acc: 97.038 Test acc: 97.050 \n",
      "step: 7736 | Train: G_Loss: 1.247, D_unsup_loss_real: 0.628, D_unsup_loss_fake: 0.665, D_sup_loss: 0.111, D_sup_acc: 97.09 Train acc: 96.723 Test acc: 96.950 \n",
      "step: 7737 | Train: G_Loss: 1.228, D_unsup_loss_real: 0.511, D_unsup_loss_fake: 0.523, D_sup_loss: 0.131, D_sup_acc: 96.99 Train acc: 96.978 Test acc: 97.160 \n",
      "step: 7738 | Train: G_Loss: 1.185, D_unsup_loss_real: 0.517, D_unsup_loss_fake: 0.542, D_sup_loss: 0.114, D_sup_acc: 97.20 Train acc: 96.817 Test acc: 97.030 \n",
      "step: 7739 | Train: G_Loss: 1.187, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.616, D_sup_loss: 0.123, D_sup_acc: 97.07 Train acc: 96.860 Test acc: 97.090 \n",
      "step: 7740 | Train: G_Loss: 1.238, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.544, D_sup_loss: 0.123, D_sup_acc: 97.13 Train acc: 97.058 Test acc: 97.270 \n",
      "step: 7741 | Train: G_Loss: 1.243, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.595, D_sup_loss: 0.112, D_sup_acc: 97.30 Train acc: 97.010 Test acc: 97.120 \n",
      "step: 7742 | Train: G_Loss: 1.273, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.576, D_sup_loss: 0.116, D_sup_acc: 97.16 Train acc: 97.023 Test acc: 97.270 \n",
      "step: 7743 | Train: G_Loss: 1.271, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.557, D_sup_loss: 0.114, D_sup_acc: 97.30 Train acc: 97.003 Test acc: 97.300 \n",
      "step: 7744 | Train: G_Loss: 1.297, D_unsup_loss_real: 0.533, D_unsup_loss_fake: 0.561, D_sup_loss: 0.117, D_sup_acc: 97.33 Train acc: 96.828 Test acc: 97.040 \n",
      "step: 7745 | Train: G_Loss: 1.189, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.529, D_sup_loss: 0.126, D_sup_acc: 97.08 Train acc: 96.827 Test acc: 97.120 \n",
      "step: 7746 | Train: G_Loss: 1.269, D_unsup_loss_real: 0.496, D_unsup_loss_fake: 0.580, D_sup_loss: 0.124, D_sup_acc: 97.16 Train acc: 96.820 Test acc: 97.120 \n",
      "step: 7747 | Train: G_Loss: 1.251, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.499, D_sup_loss: 0.124, D_sup_acc: 97.16 Train acc: 96.763 Test acc: 96.920 \n",
      "step: 7748 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.492, D_sup_loss: 0.127, D_sup_acc: 96.96 Train acc: 96.892 Test acc: 97.120 \n",
      "step: 7749 | Train: G_Loss: 1.116, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.575, D_sup_loss: 0.118, D_sup_acc: 97.16 Train acc: 96.878 Test acc: 97.150 \n",
      "step: 7750 | Train: G_Loss: 1.178, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.536, D_sup_loss: 0.119, D_sup_acc: 97.19 Train acc: 96.742 Test acc: 96.990 \n",
      "step: 7751 | Train: G_Loss: 1.294, D_unsup_loss_real: 0.505, D_unsup_loss_fake: 0.502, D_sup_loss: 0.126, D_sup_acc: 97.03 Train acc: 96.707 Test acc: 97.040 \n",
      "step: 7752 | Train: G_Loss: 1.288, D_unsup_loss_real: 0.490, D_unsup_loss_fake: 0.548, D_sup_loss: 0.125, D_sup_acc: 97.08 Train acc: 96.740 Test acc: 96.940 \n",
      "step: 7753 | Train: G_Loss: 1.273, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.664, D_sup_loss: 0.123, D_sup_acc: 96.98 Train acc: 96.785 Test acc: 97.000 \n",
      "step: 7754 | Train: G_Loss: 1.163, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.543, D_sup_loss: 0.119, D_sup_acc: 97.04 Train acc: 96.597 Test acc: 96.790 \n",
      "step: 7755 | Train: G_Loss: 1.160, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.560, D_sup_loss: 0.141, D_sup_acc: 96.83 Train acc: 96.577 Test acc: 96.770 \n",
      "step: 7756 | Train: G_Loss: 1.283, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.549, D_sup_loss: 0.133, D_sup_acc: 96.81 Train acc: 96.288 Test acc: 96.580 \n",
      "step: 7757 | Train: G_Loss: 1.331, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.603, D_sup_loss: 0.142, D_sup_acc: 96.62 Train acc: 96.867 Test acc: 97.040 \n",
      "step: 7758 | Train: G_Loss: 1.171, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.547, D_sup_loss: 0.116, D_sup_acc: 97.08 Train acc: 96.612 Test acc: 96.820 \n",
      "step: 7759 | Train: G_Loss: 1.279, D_unsup_loss_real: 0.668, D_unsup_loss_fake: 0.628, D_sup_loss: 0.131, D_sup_acc: 96.86 Train acc: 96.877 Test acc: 97.130 \n",
      "step: 7760 | Train: G_Loss: 1.286, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.579, D_sup_loss: 0.119, D_sup_acc: 97.17 Train acc: 96.980 Test acc: 97.110 \n",
      "step: 7761 | Train: G_Loss: 1.194, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.563, D_sup_loss: 0.116, D_sup_acc: 97.15 Train acc: 96.947 Test acc: 97.140 \n",
      "step: 7762 | Train: G_Loss: 1.157, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.535, D_sup_loss: 0.119, D_sup_acc: 97.18 Train acc: 96.970 Test acc: 97.070 \n",
      "step: 7763 | Train: G_Loss: 1.215, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.514, D_sup_loss: 0.117, D_sup_acc: 97.11 Train acc: 96.642 Test acc: 96.860 \n",
      "step: 7764 | Train: G_Loss: 1.280, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.588, D_sup_loss: 0.135, D_sup_acc: 96.90 Train acc: 97.050 Test acc: 97.200 \n",
      "step: 7765 | Train: G_Loss: 1.195, D_unsup_loss_real: 0.497, D_unsup_loss_fake: 0.552, D_sup_loss: 0.111, D_sup_acc: 97.24 Train acc: 96.915 Test acc: 97.170 \n",
      "step: 7766 | Train: G_Loss: 1.167, D_unsup_loss_real: 0.509, D_unsup_loss_fake: 0.569, D_sup_loss: 0.119, D_sup_acc: 97.21 Train acc: 96.897 Test acc: 97.170 \n",
      "step: 7767 | Train: G_Loss: 1.245, D_unsup_loss_real: 0.542, D_unsup_loss_fake: 0.578, D_sup_loss: 0.120, D_sup_acc: 97.21 Train acc: 96.867 Test acc: 97.120 \n",
      "step: 7768 | Train: G_Loss: 1.252, D_unsup_loss_real: 0.516, D_unsup_loss_fake: 0.542, D_sup_loss: 0.125, D_sup_acc: 97.16 Train acc: 96.880 Test acc: 97.110 \n",
      "step: 7769 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.586, D_sup_loss: 0.119, D_sup_acc: 97.15 Train acc: 96.722 Test acc: 97.120 \n",
      "step: 7770 | Train: G_Loss: 1.280, D_unsup_loss_real: 0.535, D_unsup_loss_fake: 0.596, D_sup_loss: 0.130, D_sup_acc: 97.16 Train acc: 96.878 Test acc: 97.170 \n",
      "step: 7771 | Train: G_Loss: 1.238, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.516, D_sup_loss: 0.121, D_sup_acc: 97.21 Train acc: 97.058 Test acc: 97.180 \n",
      "step: 7772 | Train: G_Loss: 1.328, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.580, D_sup_loss: 0.111, D_sup_acc: 97.22 Train acc: 97.012 Test acc: 97.170 \n",
      "step: 7773 | Train: G_Loss: 1.210, D_unsup_loss_real: 0.525, D_unsup_loss_fake: 0.570, D_sup_loss: 0.115, D_sup_acc: 97.21 Train acc: 96.798 Test acc: 96.960 \n",
      "step: 7774 | Train: G_Loss: 1.351, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.511, D_sup_loss: 0.130, D_sup_acc: 97.00 Train acc: 96.890 Test acc: 97.070 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 7775 | Train: G_Loss: 1.164, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.535, D_sup_loss: 0.119, D_sup_acc: 97.11 Train acc: 96.487 Test acc: 96.700 \n",
      "step: 7776 | Train: G_Loss: 1.163, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.619, D_sup_loss: 0.134, D_sup_acc: 96.74 Train acc: 96.720 Test acc: 96.890 \n",
      "step: 7777 | Train: G_Loss: 1.235, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.594, D_sup_loss: 0.134, D_sup_acc: 96.93 Train acc: 96.990 Test acc: 97.200 \n",
      "step: 7778 | Train: G_Loss: 1.257, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.535, D_sup_loss: 0.114, D_sup_acc: 97.24 Train acc: 96.883 Test acc: 97.140 \n",
      "step: 7779 | Train: G_Loss: 1.172, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.569, D_sup_loss: 0.120, D_sup_acc: 97.18 Train acc: 96.972 Test acc: 97.210 \n",
      "step: 7780 | Train: G_Loss: 1.327, D_unsup_loss_real: 0.500, D_unsup_loss_fake: 0.589, D_sup_loss: 0.118, D_sup_acc: 97.25 Train acc: 97.033 Test acc: 97.250 \n",
      "step: 7781 | Train: G_Loss: 1.180, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.552, D_sup_loss: 0.113, D_sup_acc: 97.28 Train acc: 96.803 Test acc: 97.080 \n",
      "step: 7782 | Train: G_Loss: 1.219, D_unsup_loss_real: 0.652, D_unsup_loss_fake: 0.615, D_sup_loss: 0.123, D_sup_acc: 97.12 Train acc: 96.722 Test acc: 96.990 \n",
      "step: 7783 | Train: G_Loss: 1.220, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.554, D_sup_loss: 0.131, D_sup_acc: 97.03 Train acc: 96.938 Test acc: 97.170 \n",
      "step: 7784 | Train: G_Loss: 1.201, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.565, D_sup_loss: 0.121, D_sup_acc: 97.21 Train acc: 97.095 Test acc: 97.220 \n",
      "step: 7785 | Train: G_Loss: 1.168, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.611, D_sup_loss: 0.112, D_sup_acc: 97.26 Train acc: 96.737 Test acc: 96.910 \n",
      "step: 7786 | Train: G_Loss: 1.234, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.620, D_sup_loss: 0.133, D_sup_acc: 96.95 Train acc: 96.873 Test acc: 97.110 \n",
      "step: 7787 | Train: G_Loss: 1.227, D_unsup_loss_real: 0.639, D_unsup_loss_fake: 0.537, D_sup_loss: 0.125, D_sup_acc: 97.15 Train acc: 96.915 Test acc: 97.160 \n",
      "step: 7788 | Train: G_Loss: 1.147, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.610, D_sup_loss: 0.121, D_sup_acc: 97.20 Train acc: 96.705 Test acc: 97.040 \n",
      "step: 7789 | Train: G_Loss: 1.293, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.566, D_sup_loss: 0.132, D_sup_acc: 97.08 Train acc: 96.980 Test acc: 97.110 \n",
      "step: 7790 | Train: G_Loss: 1.231, D_unsup_loss_real: 0.511, D_unsup_loss_fake: 0.625, D_sup_loss: 0.118, D_sup_acc: 97.15 Train acc: 97.053 Test acc: 97.220 \n",
      "step: 7791 | Train: G_Loss: 1.213, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.535, D_sup_loss: 0.115, D_sup_acc: 97.26 Train acc: 97.080 Test acc: 97.210 \n",
      "step: 7792 | Train: G_Loss: 1.203, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.549, D_sup_loss: 0.114, D_sup_acc: 97.25 Train acc: 97.003 Test acc: 97.170 \n",
      "step: 7793 | Train: G_Loss: 1.156, D_unsup_loss_real: 0.481, D_unsup_loss_fake: 0.591, D_sup_loss: 0.118, D_sup_acc: 97.21 Train acc: 96.992 Test acc: 97.140 \n",
      "step: 7794 | Train: G_Loss: 1.241, D_unsup_loss_real: 0.538, D_unsup_loss_fake: 0.560, D_sup_loss: 0.116, D_sup_acc: 97.18 Train acc: 96.887 Test acc: 97.050 \n",
      "step: 7795 | Train: G_Loss: 1.192, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.548, D_sup_loss: 0.123, D_sup_acc: 97.09 Train acc: 97.067 Test acc: 97.240 \n",
      "step: 7796 | Train: G_Loss: 1.326, D_unsup_loss_real: 0.503, D_unsup_loss_fake: 0.558, D_sup_loss: 0.110, D_sup_acc: 97.27 Train acc: 97.030 Test acc: 97.300 \n",
      "step: 7797 | Train: G_Loss: 1.173, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.556, D_sup_loss: 0.114, D_sup_acc: 97.33 Train acc: 96.965 Test acc: 97.190 \n",
      "step: 7798 | Train: G_Loss: 1.225, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.553, D_sup_loss: 0.117, D_sup_acc: 97.23 Train acc: 96.865 Test acc: 97.130 \n",
      "step: 7799 | Train: G_Loss: 1.342, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.602, D_sup_loss: 0.119, D_sup_acc: 97.17 Train acc: 96.920 Test acc: 97.190 \n",
      "step: 7800 | Train: G_Loss: 1.315, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.624, D_sup_loss: 0.117, D_sup_acc: 97.23 Train acc: 97.097 Test acc: 97.240 \n",
      "Train Classifier Accuracy: 97.097%\n",
      "\n",
      "Test Classifier Accuracy: 97.240%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_7800.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_7800.h5\n",
      "step: 7801 | Train: G_Loss: 1.237, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.543, D_sup_loss: 0.107, D_sup_acc: 97.27 Train acc: 96.632 Test acc: 96.960 \n",
      "step: 7802 | Train: G_Loss: 1.346, D_unsup_loss_real: 0.526, D_unsup_loss_fake: 0.587, D_sup_loss: 0.141, D_sup_acc: 97.00 Train acc: 97.005 Test acc: 97.270 \n",
      "step: 7803 | Train: G_Loss: 1.194, D_unsup_loss_real: 0.545, D_unsup_loss_fake: 0.531, D_sup_loss: 0.114, D_sup_acc: 97.30 Train acc: 96.937 Test acc: 97.190 \n",
      "step: 7804 | Train: G_Loss: 1.248, D_unsup_loss_real: 0.513, D_unsup_loss_fake: 0.513, D_sup_loss: 0.117, D_sup_acc: 97.23 Train acc: 97.023 Test acc: 97.330 \n",
      "step: 7805 | Train: G_Loss: 1.239, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.495, D_sup_loss: 0.111, D_sup_acc: 97.36 Train acc: 96.858 Test acc: 97.190 \n",
      "step: 7806 | Train: G_Loss: 1.223, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.589, D_sup_loss: 0.120, D_sup_acc: 97.23 Train acc: 97.062 Test acc: 97.300 \n",
      "step: 7807 | Train: G_Loss: 1.175, D_unsup_loss_real: 0.507, D_unsup_loss_fake: 0.555, D_sup_loss: 0.109, D_sup_acc: 97.33 Train acc: 96.997 Test acc: 97.230 \n",
      "step: 7808 | Train: G_Loss: 1.176, D_unsup_loss_real: 0.521, D_unsup_loss_fake: 0.592, D_sup_loss: 0.112, D_sup_acc: 97.27 Train acc: 96.743 Test acc: 97.050 \n",
      "step: 7809 | Train: G_Loss: 1.332, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.576, D_sup_loss: 0.127, D_sup_acc: 97.09 Train acc: 96.942 Test acc: 97.170 \n",
      "step: 7810 | Train: G_Loss: 1.271, D_unsup_loss_real: 0.531, D_unsup_loss_fake: 0.524, D_sup_loss: 0.114, D_sup_acc: 97.21 Train acc: 97.037 Test acc: 97.210 \n",
      "step: 7811 | Train: G_Loss: 1.276, D_unsup_loss_real: 0.534, D_unsup_loss_fake: 0.505, D_sup_loss: 0.112, D_sup_acc: 97.25 Train acc: 96.983 Test acc: 97.170 \n",
      "step: 7812 | Train: G_Loss: 1.310, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.514, D_sup_loss: 0.116, D_sup_acc: 97.21 Train acc: 97.040 Test acc: 97.250 \n",
      "step: 7813 | Train: G_Loss: 1.285, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.528, D_sup_loss: 0.111, D_sup_acc: 97.28 Train acc: 96.892 Test acc: 97.110 \n",
      "step: 7814 | Train: G_Loss: 1.495, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.663, D_sup_loss: 0.124, D_sup_acc: 97.15 Train acc: 97.155 Test acc: 97.210 \n",
      "step: 7815 | Train: G_Loss: 1.135, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.560, D_sup_loss: 0.104, D_sup_acc: 97.25 Train acc: 97.035 Test acc: 97.190 \n",
      "step: 7816 | Train: G_Loss: 1.260, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.569, D_sup_loss: 0.119, D_sup_acc: 97.23 Train acc: 97.077 Test acc: 97.200 \n",
      "step: 7817 | Train: G_Loss: 1.168, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.616, D_sup_loss: 0.115, D_sup_acc: 97.24 Train acc: 97.058 Test acc: 97.250 \n",
      "step: 7818 | Train: G_Loss: 1.216, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.559, D_sup_loss: 0.118, D_sup_acc: 97.28 Train acc: 96.890 Test acc: 97.090 \n",
      "step: 7819 | Train: G_Loss: 1.263, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.521, D_sup_loss: 0.122, D_sup_acc: 97.13 Train acc: 97.102 Test acc: 97.160 \n",
      "step: 7820 | Train: G_Loss: 1.242, D_unsup_loss_real: 0.520, D_unsup_loss_fake: 0.587, D_sup_loss: 0.113, D_sup_acc: 97.20 Train acc: 97.103 Test acc: 97.250 \n",
      "step: 7821 | Train: G_Loss: 1.243, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.494, D_sup_loss: 0.115, D_sup_acc: 97.28 Train acc: 97.167 Test acc: 97.270 \n",
      "step: 7822 | Train: G_Loss: 1.287, D_unsup_loss_real: 0.509, D_unsup_loss_fake: 0.556, D_sup_loss: 0.105, D_sup_acc: 97.30 Train acc: 96.970 Test acc: 97.100 \n",
      "step: 7823 | Train: G_Loss: 1.307, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.605, D_sup_loss: 0.118, D_sup_acc: 97.14 Train acc: 97.065 Test acc: 97.210 \n",
      "step: 7824 | Train: G_Loss: 1.163, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.588, D_sup_loss: 0.111, D_sup_acc: 97.25 Train acc: 96.855 Test acc: 97.110 \n",
      "step: 7825 | Train: G_Loss: 1.037, D_unsup_loss_real: 0.536, D_unsup_loss_fake: 0.565, D_sup_loss: 0.126, D_sup_acc: 97.15 Train acc: 96.870 Test acc: 97.110 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 7826 | Train: G_Loss: 1.276, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.604, D_sup_loss: 0.125, D_sup_acc: 97.15 Train acc: 97.073 Test acc: 97.300 \n",
      "step: 7827 | Train: G_Loss: 1.223, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.598, D_sup_loss: 0.113, D_sup_acc: 97.33 Train acc: 96.948 Test acc: 97.280 \n",
      "step: 7828 | Train: G_Loss: 1.151, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.579, D_sup_loss: 0.121, D_sup_acc: 97.31 Train acc: 97.162 Test acc: 97.360 \n",
      "step: 7829 | Train: G_Loss: 1.307, D_unsup_loss_real: 0.512, D_unsup_loss_fake: 0.547, D_sup_loss: 0.105, D_sup_acc: 97.39 Train acc: 97.132 Test acc: 97.380 \n",
      "step: 7830 | Train: G_Loss: 1.324, D_unsup_loss_real: 0.526, D_unsup_loss_fake: 0.502, D_sup_loss: 0.109, D_sup_acc: 97.41 Train acc: 96.943 Test acc: 97.260 \n",
      "step: 7831 | Train: G_Loss: 1.265, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.584, D_sup_loss: 0.118, D_sup_acc: 97.29 Train acc: 96.993 Test acc: 97.190 \n",
      "step: 7832 | Train: G_Loss: 1.400, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.568, D_sup_loss: 0.116, D_sup_acc: 97.23 Train acc: 97.088 Test acc: 97.270 \n",
      "step: 7833 | Train: G_Loss: 1.251, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.531, D_sup_loss: 0.107, D_sup_acc: 97.30 Train acc: 97.007 Test acc: 97.260 \n",
      "step: 7834 | Train: G_Loss: 1.280, D_unsup_loss_real: 0.515, D_unsup_loss_fake: 0.565, D_sup_loss: 0.117, D_sup_acc: 97.29 Train acc: 96.692 Test acc: 96.990 \n",
      "step: 7835 | Train: G_Loss: 1.255, D_unsup_loss_real: 0.507, D_unsup_loss_fake: 0.627, D_sup_loss: 0.132, D_sup_acc: 97.03 Train acc: 97.020 Test acc: 97.260 \n",
      "step: 7836 | Train: G_Loss: 1.182, D_unsup_loss_real: 0.544, D_unsup_loss_fake: 0.491, D_sup_loss: 0.113, D_sup_acc: 97.29 Train acc: 97.097 Test acc: 97.300 \n",
      "step: 7837 | Train: G_Loss: 1.155, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.540, D_sup_loss: 0.107, D_sup_acc: 97.33 Train acc: 97.125 Test acc: 97.470 \n",
      "step: 7838 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.588, D_sup_loss: 0.107, D_sup_acc: 97.50 Train acc: 97.018 Test acc: 97.260 \n",
      "step: 7839 | Train: G_Loss: 1.308, D_unsup_loss_real: 0.683, D_unsup_loss_fake: 0.585, D_sup_loss: 0.117, D_sup_acc: 97.29 Train acc: 97.080 Test acc: 97.340 \n",
      "step: 7840 | Train: G_Loss: 1.231, D_unsup_loss_real: 0.491, D_unsup_loss_fake: 0.567, D_sup_loss: 0.111, D_sup_acc: 97.37 Train acc: 97.012 Test acc: 97.220 \n",
      "step: 7841 | Train: G_Loss: 1.240, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.524, D_sup_loss: 0.114, D_sup_acc: 97.26 Train acc: 96.850 Test acc: 97.010 \n",
      "step: 7842 | Train: G_Loss: 1.261, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.615, D_sup_loss: 0.124, D_sup_acc: 97.05 Train acc: 97.017 Test acc: 97.320 \n",
      "step: 7843 | Train: G_Loss: 1.467, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.593, D_sup_loss: 0.115, D_sup_acc: 97.35 Train acc: 97.007 Test acc: 97.310 \n",
      "step: 7844 | Train: G_Loss: 1.343, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.533, D_sup_loss: 0.113, D_sup_acc: 97.34 Train acc: 96.938 Test acc: 97.200 \n",
      "step: 7845 | Train: G_Loss: 1.275, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.572, D_sup_loss: 0.119, D_sup_acc: 97.24 Train acc: 96.952 Test acc: 97.160 \n",
      "step: 7846 | Train: G_Loss: 1.429, D_unsup_loss_real: 0.536, D_unsup_loss_fake: 0.533, D_sup_loss: 0.118, D_sup_acc: 97.20 Train acc: 97.115 Test acc: 97.230 \n",
      "step: 7847 | Train: G_Loss: 1.277, D_unsup_loss_real: 0.532, D_unsup_loss_fake: 0.529, D_sup_loss: 0.106, D_sup_acc: 97.27 Train acc: 97.032 Test acc: 97.140 \n",
      "step: 7848 | Train: G_Loss: 1.263, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.558, D_sup_loss: 0.114, D_sup_acc: 97.18 Train acc: 96.852 Test acc: 97.060 \n",
      "step: 7849 | Train: G_Loss: 1.327, D_unsup_loss_real: 0.518, D_unsup_loss_fake: 0.510, D_sup_loss: 0.123, D_sup_acc: 97.10 Train acc: 97.015 Test acc: 97.260 \n",
      "step: 7850 | Train: G_Loss: 1.273, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.638, D_sup_loss: 0.111, D_sup_acc: 97.29 Train acc: 96.823 Test acc: 97.040 \n",
      "step: 7851 | Train: G_Loss: 1.200, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.569, D_sup_loss: 0.123, D_sup_acc: 97.08 Train acc: 97.032 Test acc: 97.310 \n",
      "step: 7852 | Train: G_Loss: 1.268, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.514, D_sup_loss: 0.111, D_sup_acc: 97.34 Train acc: 97.023 Test acc: 97.260 \n",
      "step: 7853 | Train: G_Loss: 1.312, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.580, D_sup_loss: 0.115, D_sup_acc: 97.29 Train acc: 96.938 Test acc: 97.220 \n",
      "step: 7854 | Train: G_Loss: 1.203, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.590, D_sup_loss: 0.115, D_sup_acc: 97.26 Train acc: 96.567 Test acc: 96.700 \n",
      "step: 7855 | Train: G_Loss: 1.241, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.504, D_sup_loss: 0.141, D_sup_acc: 96.74 Train acc: 96.893 Test acc: 97.120 \n",
      "step: 7856 | Train: G_Loss: 1.133, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.524, D_sup_loss: 0.120, D_sup_acc: 97.16 Train acc: 96.827 Test acc: 96.960 \n",
      "step: 7857 | Train: G_Loss: 1.304, D_unsup_loss_real: 0.510, D_unsup_loss_fake: 0.599, D_sup_loss: 0.126, D_sup_acc: 97.00 Train acc: 96.810 Test acc: 97.130 \n",
      "step: 7858 | Train: G_Loss: 1.171, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.612, D_sup_loss: 0.123, D_sup_acc: 97.17 Train acc: 96.713 Test acc: 96.900 \n",
      "step: 7859 | Train: G_Loss: 1.255, D_unsup_loss_real: 0.648, D_unsup_loss_fake: 0.563, D_sup_loss: 0.130, D_sup_acc: 96.94 Train acc: 96.890 Test acc: 97.190 \n",
      "step: 7860 | Train: G_Loss: 1.090, D_unsup_loss_real: 0.532, D_unsup_loss_fake: 0.488, D_sup_loss: 0.117, D_sup_acc: 97.23 Train acc: 96.888 Test acc: 97.230 \n",
      "step: 7861 | Train: G_Loss: 1.308, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.572, D_sup_loss: 0.121, D_sup_acc: 97.27 Train acc: 96.737 Test acc: 97.080 \n",
      "step: 7862 | Train: G_Loss: 1.274, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.607, D_sup_loss: 0.123, D_sup_acc: 97.12 Train acc: 96.847 Test acc: 97.110 \n",
      "step: 7863 | Train: G_Loss: 1.245, D_unsup_loss_real: 0.509, D_unsup_loss_fake: 0.549, D_sup_loss: 0.121, D_sup_acc: 97.15 Train acc: 96.892 Test acc: 97.210 \n",
      "step: 7864 | Train: G_Loss: 1.268, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.574, D_sup_loss: 0.117, D_sup_acc: 97.25 Train acc: 96.647 Test acc: 96.910 \n",
      "step: 7865 | Train: G_Loss: 1.209, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.562, D_sup_loss: 0.126, D_sup_acc: 96.95 Train acc: 96.628 Test acc: 96.860 \n",
      "step: 7866 | Train: G_Loss: 1.321, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.591, D_sup_loss: 0.135, D_sup_acc: 96.90 Train acc: 96.878 Test acc: 97.230 \n",
      "step: 7867 | Train: G_Loss: 1.246, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.628, D_sup_loss: 0.115, D_sup_acc: 97.27 Train acc: 96.975 Test acc: 97.200 \n",
      "step: 7868 | Train: G_Loss: 1.297, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.573, D_sup_loss: 0.113, D_sup_acc: 97.24 Train acc: 96.888 Test acc: 97.170 \n",
      "step: 7869 | Train: G_Loss: 1.299, D_unsup_loss_real: 0.515, D_unsup_loss_fake: 0.525, D_sup_loss: 0.117, D_sup_acc: 97.21 Train acc: 96.855 Test acc: 97.200 \n",
      "step: 7870 | Train: G_Loss: 1.214, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.560, D_sup_loss: 0.119, D_sup_acc: 97.24 Train acc: 97.090 Test acc: 97.350 \n",
      "step: 7871 | Train: G_Loss: 1.161, D_unsup_loss_real: 0.506, D_unsup_loss_fake: 0.591, D_sup_loss: 0.109, D_sup_acc: 97.38 Train acc: 96.952 Test acc: 97.280 \n",
      "step: 7872 | Train: G_Loss: 1.255, D_unsup_loss_real: 0.518, D_unsup_loss_fake: 0.567, D_sup_loss: 0.115, D_sup_acc: 97.31 Train acc: 96.757 Test acc: 96.960 \n",
      "step: 7873 | Train: G_Loss: 1.366, D_unsup_loss_real: 0.527, D_unsup_loss_fake: 0.493, D_sup_loss: 0.130, D_sup_acc: 97.00 Train acc: 96.878 Test acc: 97.070 \n",
      "step: 7874 | Train: G_Loss: 1.201, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.551, D_sup_loss: 0.116, D_sup_acc: 97.11 Train acc: 96.733 Test acc: 96.840 \n",
      "step: 7875 | Train: G_Loss: 1.278, D_unsup_loss_real: 0.464, D_unsup_loss_fake: 0.544, D_sup_loss: 0.122, D_sup_acc: 96.88 Train acc: 96.748 Test acc: 96.830 \n",
      "step: 7876 | Train: G_Loss: 1.338, D_unsup_loss_real: 0.501, D_unsup_loss_fake: 0.510, D_sup_loss: 0.120, D_sup_acc: 96.87 Train acc: 96.643 Test acc: 96.890 \n",
      "step: 7877 | Train: G_Loss: 1.349, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.529, D_sup_loss: 0.127, D_sup_acc: 96.93 Train acc: 96.970 Test acc: 97.260 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 7878 | Train: G_Loss: 1.204, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.562, D_sup_loss: 0.111, D_sup_acc: 97.29 Train acc: 96.913 Test acc: 97.210 \n",
      "step: 7879 | Train: G_Loss: 1.228, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.540, D_sup_loss: 0.116, D_sup_acc: 97.25 Train acc: 97.005 Test acc: 97.200 \n",
      "step: 7880 | Train: G_Loss: 1.338, D_unsup_loss_real: 0.541, D_unsup_loss_fake: 0.599, D_sup_loss: 0.112, D_sup_acc: 97.24 Train acc: 96.835 Test acc: 97.050 \n",
      "step: 7881 | Train: G_Loss: 1.381, D_unsup_loss_real: 0.650, D_unsup_loss_fake: 0.554, D_sup_loss: 0.114, D_sup_acc: 97.09 Train acc: 94.365 Test acc: 94.350 \n",
      "step: 7882 | Train: G_Loss: 1.190, D_unsup_loss_real: 0.808, D_unsup_loss_fake: 0.643, D_sup_loss: 0.247, D_sup_acc: 94.42 Train acc: 96.928 Test acc: 97.150 \n",
      "step: 7883 | Train: G_Loss: 1.149, D_unsup_loss_real: 0.546, D_unsup_loss_fake: 0.575, D_sup_loss: 0.116, D_sup_acc: 97.19 Train acc: 96.823 Test acc: 97.050 \n",
      "step: 7884 | Train: G_Loss: 1.176, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.681, D_sup_loss: 0.120, D_sup_acc: 97.09 Train acc: 96.963 Test acc: 97.200 \n",
      "step: 7885 | Train: G_Loss: 1.172, D_unsup_loss_real: 0.495, D_unsup_loss_fake: 0.632, D_sup_loss: 0.116, D_sup_acc: 97.24 Train acc: 97.012 Test acc: 97.170 \n",
      "step: 7886 | Train: G_Loss: 1.246, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.548, D_sup_loss: 0.113, D_sup_acc: 97.21 Train acc: 96.837 Test acc: 97.090 \n",
      "step: 7887 | Train: G_Loss: 1.258, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.621, D_sup_loss: 0.119, D_sup_acc: 97.13 Train acc: 97.050 Test acc: 97.220 \n",
      "step: 7888 | Train: G_Loss: 1.132, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.544, D_sup_loss: 0.111, D_sup_acc: 97.26 Train acc: 96.665 Test acc: 96.930 \n",
      "step: 7889 | Train: G_Loss: 1.090, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.552, D_sup_loss: 0.133, D_sup_acc: 96.97 Train acc: 96.663 Test acc: 96.990 \n",
      "step: 7890 | Train: G_Loss: 1.325, D_unsup_loss_real: 0.546, D_unsup_loss_fake: 0.546, D_sup_loss: 0.129, D_sup_acc: 97.03 Train acc: 96.822 Test acc: 97.120 \n",
      "step: 7891 | Train: G_Loss: 1.259, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.607, D_sup_loss: 0.119, D_sup_acc: 97.16 Train acc: 96.587 Test acc: 96.910 \n",
      "step: 7892 | Train: G_Loss: 1.261, D_unsup_loss_real: 0.533, D_unsup_loss_fake: 0.598, D_sup_loss: 0.133, D_sup_acc: 96.95 Train acc: 96.835 Test acc: 97.080 \n",
      "step: 7893 | Train: G_Loss: 1.269, D_unsup_loss_real: 0.503, D_unsup_loss_fake: 0.545, D_sup_loss: 0.121, D_sup_acc: 97.12 Train acc: 96.915 Test acc: 97.140 \n",
      "step: 7894 | Train: G_Loss: 1.250, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.575, D_sup_loss: 0.113, D_sup_acc: 97.18 Train acc: 96.842 Test acc: 97.130 \n",
      "step: 7895 | Train: G_Loss: 1.266, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.583, D_sup_loss: 0.118, D_sup_acc: 97.17 Train acc: 96.708 Test acc: 96.900 \n",
      "step: 7896 | Train: G_Loss: 1.272, D_unsup_loss_real: 0.544, D_unsup_loss_fake: 0.584, D_sup_loss: 0.123, D_sup_acc: 96.94 Train acc: 96.647 Test acc: 96.900 \n",
      "step: 7897 | Train: G_Loss: 1.236, D_unsup_loss_real: 0.538, D_unsup_loss_fake: 0.559, D_sup_loss: 0.127, D_sup_acc: 96.94 Train acc: 96.632 Test acc: 96.910 \n",
      "step: 7898 | Train: G_Loss: 1.254, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.657, D_sup_loss: 0.125, D_sup_acc: 96.95 Train acc: 96.937 Test acc: 97.070 \n",
      "step: 7899 | Train: G_Loss: 1.245, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.571, D_sup_loss: 0.114, D_sup_acc: 97.11 Train acc: 96.760 Test acc: 96.970 \n",
      "step: 7900 | Train: G_Loss: 1.332, D_unsup_loss_real: 0.518, D_unsup_loss_fake: 0.488, D_sup_loss: 0.122, D_sup_acc: 97.01 Train acc: 97.070 Test acc: 97.270 \n",
      "Train Classifier Accuracy: 97.070%\n",
      "\n",
      "Test Classifier Accuracy: 97.270%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_7900.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_7900.h5\n",
      "step: 7901 | Train: G_Loss: 1.316, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.573, D_sup_loss: 0.104, D_sup_acc: 97.30 Train acc: 96.633 Test acc: 96.970 \n",
      "step: 7902 | Train: G_Loss: 1.244, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.527, D_sup_loss: 0.133, D_sup_acc: 97.01 Train acc: 96.762 Test acc: 96.950 \n",
      "step: 7903 | Train: G_Loss: 1.119, D_unsup_loss_real: 0.546, D_unsup_loss_fake: 0.542, D_sup_loss: 0.126, D_sup_acc: 96.99 Train acc: 96.965 Test acc: 97.130 \n",
      "step: 7904 | Train: G_Loss: 1.311, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.606, D_sup_loss: 0.119, D_sup_acc: 97.17 Train acc: 96.850 Test acc: 97.070 \n",
      "step: 7905 | Train: G_Loss: 1.232, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.584, D_sup_loss: 0.121, D_sup_acc: 97.11 Train acc: 96.652 Test acc: 96.920 \n",
      "step: 7906 | Train: G_Loss: 1.194, D_unsup_loss_real: 0.498, D_unsup_loss_fake: 0.543, D_sup_loss: 0.134, D_sup_acc: 96.96 Train acc: 96.985 Test acc: 97.110 \n",
      "step: 7907 | Train: G_Loss: 1.257, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.563, D_sup_loss: 0.116, D_sup_acc: 97.15 Train acc: 96.970 Test acc: 97.140 \n",
      "step: 7908 | Train: G_Loss: 1.228, D_unsup_loss_real: 0.537, D_unsup_loss_fake: 0.611, D_sup_loss: 0.117, D_sup_acc: 97.18 Train acc: 97.013 Test acc: 97.190 \n",
      "step: 7909 | Train: G_Loss: 1.362, D_unsup_loss_real: 0.519, D_unsup_loss_fake: 0.519, D_sup_loss: 0.114, D_sup_acc: 97.23 Train acc: 96.998 Test acc: 97.220 \n",
      "step: 7910 | Train: G_Loss: 1.361, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.526, D_sup_loss: 0.114, D_sup_acc: 97.26 Train acc: 97.007 Test acc: 97.140 \n",
      "step: 7911 | Train: G_Loss: 1.172, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.511, D_sup_loss: 0.111, D_sup_acc: 97.18 Train acc: 97.055 Test acc: 97.150 \n",
      "step: 7912 | Train: G_Loss: 1.205, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.587, D_sup_loss: 0.114, D_sup_acc: 97.19 Train acc: 96.845 Test acc: 97.000 \n",
      "step: 7913 | Train: G_Loss: 1.244, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.615, D_sup_loss: 0.125, D_sup_acc: 97.04 Train acc: 96.862 Test acc: 96.900 \n",
      "step: 7914 | Train: G_Loss: 1.160, D_unsup_loss_real: 0.522, D_unsup_loss_fake: 0.616, D_sup_loss: 0.124, D_sup_acc: 96.94 Train acc: 96.922 Test acc: 97.010 \n",
      "step: 7915 | Train: G_Loss: 1.235, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.533, D_sup_loss: 0.120, D_sup_acc: 97.05 Train acc: 96.443 Test acc: 96.660 \n",
      "step: 7916 | Train: G_Loss: 1.266, D_unsup_loss_real: 0.704, D_unsup_loss_fake: 0.578, D_sup_loss: 0.143, D_sup_acc: 96.70 Train acc: 96.870 Test acc: 96.990 \n",
      "step: 7917 | Train: G_Loss: 1.293, D_unsup_loss_real: 0.509, D_unsup_loss_fake: 0.604, D_sup_loss: 0.123, D_sup_acc: 97.03 Train acc: 96.742 Test acc: 96.970 \n",
      "step: 7918 | Train: G_Loss: 1.315, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.613, D_sup_loss: 0.127, D_sup_acc: 97.01 Train acc: 96.953 Test acc: 97.100 \n",
      "step: 7919 | Train: G_Loss: 1.234, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.593, D_sup_loss: 0.118, D_sup_acc: 97.14 Train acc: 96.697 Test acc: 97.000 \n",
      "step: 7920 | Train: G_Loss: 1.238, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.534, D_sup_loss: 0.132, D_sup_acc: 97.04 Train acc: 96.782 Test acc: 97.010 \n",
      "step: 7921 | Train: G_Loss: 1.206, D_unsup_loss_real: 0.501, D_unsup_loss_fake: 0.564, D_sup_loss: 0.124, D_sup_acc: 97.05 Train acc: 96.780 Test acc: 97.060 \n",
      "step: 7922 | Train: G_Loss: 1.251, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.583, D_sup_loss: 0.126, D_sup_acc: 97.10 Train acc: 97.045 Test acc: 97.180 \n",
      "step: 7923 | Train: G_Loss: 1.160, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.560, D_sup_loss: 0.114, D_sup_acc: 97.22 Train acc: 96.878 Test acc: 97.020 \n",
      "step: 7924 | Train: G_Loss: 1.223, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.520, D_sup_loss: 0.121, D_sup_acc: 97.06 Train acc: 97.018 Test acc: 97.160 \n",
      "step: 7925 | Train: G_Loss: 1.359, D_unsup_loss_real: 0.529, D_unsup_loss_fake: 0.538, D_sup_loss: 0.115, D_sup_acc: 97.20 Train acc: 97.055 Test acc: 97.210 \n",
      "step: 7926 | Train: G_Loss: 1.178, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.534, D_sup_loss: 0.111, D_sup_acc: 97.25 Train acc: 96.855 Test acc: 97.140 \n",
      "step: 7927 | Train: G_Loss: 1.250, D_unsup_loss_real: 0.504, D_unsup_loss_fake: 0.568, D_sup_loss: 0.120, D_sup_acc: 97.18 Train acc: 96.960 Test acc: 97.250 \n",
      "step: 7928 | Train: G_Loss: 1.467, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.565, D_sup_loss: 0.116, D_sup_acc: 97.28 Train acc: 97.045 Test acc: 97.240 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 7929 | Train: G_Loss: 1.250, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.590, D_sup_loss: 0.107, D_sup_acc: 97.27 Train acc: 96.695 Test acc: 96.920 \n",
      "step: 7930 | Train: G_Loss: 1.231, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.559, D_sup_loss: 0.132, D_sup_acc: 96.96 Train acc: 97.072 Test acc: 97.280 \n",
      "step: 7931 | Train: G_Loss: 1.167, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.537, D_sup_loss: 0.110, D_sup_acc: 97.31 Train acc: 97.028 Test acc: 97.250 \n",
      "step: 7932 | Train: G_Loss: 1.169, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.567, D_sup_loss: 0.113, D_sup_acc: 97.28 Train acc: 96.903 Test acc: 97.230 \n",
      "step: 7933 | Train: G_Loss: 1.248, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.535, D_sup_loss: 0.115, D_sup_acc: 97.27 Train acc: 96.913 Test acc: 97.250 \n",
      "step: 7934 | Train: G_Loss: 1.217, D_unsup_loss_real: 0.480, D_unsup_loss_fake: 0.576, D_sup_loss: 0.120, D_sup_acc: 97.28 Train acc: 97.048 Test acc: 97.310 \n",
      "step: 7935 | Train: G_Loss: 1.178, D_unsup_loss_real: 0.504, D_unsup_loss_fake: 0.512, D_sup_loss: 0.112, D_sup_acc: 97.34 Train acc: 96.990 Test acc: 97.270 \n",
      "step: 7936 | Train: G_Loss: 1.139, D_unsup_loss_real: 0.527, D_unsup_loss_fake: 0.534, D_sup_loss: 0.111, D_sup_acc: 97.30 Train acc: 96.845 Test acc: 97.110 \n",
      "step: 7937 | Train: G_Loss: 1.309, D_unsup_loss_real: 0.508, D_unsup_loss_fake: 0.641, D_sup_loss: 0.119, D_sup_acc: 97.15 Train acc: 97.147 Test acc: 97.330 \n",
      "step: 7938 | Train: G_Loss: 1.158, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.546, D_sup_loss: 0.104, D_sup_acc: 97.36 Train acc: 96.998 Test acc: 97.110 \n",
      "step: 7939 | Train: G_Loss: 1.285, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.615, D_sup_loss: 0.111, D_sup_acc: 97.15 Train acc: 96.902 Test acc: 97.150 \n",
      "step: 7940 | Train: G_Loss: 1.417, D_unsup_loss_real: 0.491, D_unsup_loss_fake: 0.601, D_sup_loss: 0.118, D_sup_acc: 97.19 Train acc: 97.052 Test acc: 97.260 \n",
      "step: 7941 | Train: G_Loss: 1.303, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.629, D_sup_loss: 0.108, D_sup_acc: 97.29 Train acc: 97.032 Test acc: 97.280 \n",
      "step: 7942 | Train: G_Loss: 1.227, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.564, D_sup_loss: 0.112, D_sup_acc: 97.31 Train acc: 97.030 Test acc: 97.210 \n",
      "step: 7943 | Train: G_Loss: 1.214, D_unsup_loss_real: 0.482, D_unsup_loss_fake: 0.536, D_sup_loss: 0.108, D_sup_acc: 97.25 Train acc: 97.127 Test acc: 97.350 \n",
      "step: 7944 | Train: G_Loss: 1.339, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.546, D_sup_loss: 0.107, D_sup_acc: 97.38 Train acc: 97.068 Test acc: 97.250 \n",
      "step: 7945 | Train: G_Loss: 1.283, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.556, D_sup_loss: 0.106, D_sup_acc: 97.28 Train acc: 96.712 Test acc: 97.040 \n",
      "step: 7946 | Train: G_Loss: 1.188, D_unsup_loss_real: 0.505, D_unsup_loss_fake: 0.584, D_sup_loss: 0.134, D_sup_acc: 97.08 Train acc: 97.023 Test acc: 97.220 \n",
      "step: 7947 | Train: G_Loss: 1.317, D_unsup_loss_real: 0.538, D_unsup_loss_fake: 0.531, D_sup_loss: 0.113, D_sup_acc: 97.26 Train acc: 97.003 Test acc: 97.250 \n",
      "step: 7948 | Train: G_Loss: 1.226, D_unsup_loss_real: 0.542, D_unsup_loss_fake: 0.619, D_sup_loss: 0.114, D_sup_acc: 97.28 Train acc: 96.890 Test acc: 97.110 \n",
      "step: 7949 | Train: G_Loss: 1.207, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.535, D_sup_loss: 0.119, D_sup_acc: 97.15 Train acc: 96.575 Test acc: 96.800 \n",
      "step: 7950 | Train: G_Loss: 1.324, D_unsup_loss_real: 0.546, D_unsup_loss_fake: 0.556, D_sup_loss: 0.136, D_sup_acc: 96.84 Train acc: 96.897 Test acc: 97.020 \n",
      "step: 7951 | Train: G_Loss: 1.437, D_unsup_loss_real: 0.480, D_unsup_loss_fake: 0.518, D_sup_loss: 0.117, D_sup_acc: 97.06 Train acc: 96.802 Test acc: 96.990 \n",
      "step: 7952 | Train: G_Loss: 1.174, D_unsup_loss_real: 0.530, D_unsup_loss_fake: 0.499, D_sup_loss: 0.119, D_sup_acc: 97.03 Train acc: 96.672 Test acc: 97.000 \n",
      "step: 7953 | Train: G_Loss: 1.531, D_unsup_loss_real: 0.653, D_unsup_loss_fake: 0.532, D_sup_loss: 0.127, D_sup_acc: 97.04 Train acc: 97.040 Test acc: 97.090 \n",
      "step: 7954 | Train: G_Loss: 1.149, D_unsup_loss_real: 0.657, D_unsup_loss_fake: 0.587, D_sup_loss: 0.104, D_sup_acc: 97.13 Train acc: 96.537 Test acc: 96.770 \n",
      "step: 7955 | Train: G_Loss: 1.087, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.540, D_sup_loss: 0.147, D_sup_acc: 96.81 Train acc: 96.958 Test acc: 97.350 \n",
      "step: 7956 | Train: G_Loss: 1.322, D_unsup_loss_real: 0.504, D_unsup_loss_fake: 0.537, D_sup_loss: 0.114, D_sup_acc: 97.38 Train acc: 97.077 Test acc: 97.250 \n",
      "step: 7957 | Train: G_Loss: 1.233, D_unsup_loss_real: 0.542, D_unsup_loss_fake: 0.576, D_sup_loss: 0.104, D_sup_acc: 97.28 Train acc: 96.680 Test acc: 97.060 \n",
      "step: 7958 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.563, D_sup_loss: 0.123, D_sup_acc: 97.10 Train acc: 96.742 Test acc: 97.150 \n",
      "step: 7959 | Train: G_Loss: 1.277, D_unsup_loss_real: 0.550, D_unsup_loss_fake: 0.532, D_sup_loss: 0.118, D_sup_acc: 97.19 Train acc: 97.075 Test acc: 97.350 \n",
      "step: 7960 | Train: G_Loss: 1.247, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.667, D_sup_loss: 0.104, D_sup_acc: 97.38 Train acc: 96.970 Test acc: 97.250 \n",
      "step: 7961 | Train: G_Loss: 1.302, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.559, D_sup_loss: 0.117, D_sup_acc: 97.28 Train acc: 97.035 Test acc: 97.270 \n",
      "step: 7962 | Train: G_Loss: 1.143, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.602, D_sup_loss: 0.106, D_sup_acc: 97.30 Train acc: 96.797 Test acc: 97.130 \n",
      "step: 7963 | Train: G_Loss: 1.173, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.562, D_sup_loss: 0.122, D_sup_acc: 97.17 Train acc: 96.855 Test acc: 97.260 \n",
      "step: 7964 | Train: G_Loss: 1.347, D_unsup_loss_real: 0.533, D_unsup_loss_fake: 0.633, D_sup_loss: 0.118, D_sup_acc: 97.29 Train acc: 96.930 Test acc: 97.170 \n",
      "step: 7965 | Train: G_Loss: 1.250, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.544, D_sup_loss: 0.113, D_sup_acc: 97.21 Train acc: 96.828 Test acc: 97.180 \n",
      "step: 7966 | Train: G_Loss: 1.330, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.608, D_sup_loss: 0.121, D_sup_acc: 97.22 Train acc: 97.025 Test acc: 97.310 \n",
      "step: 7967 | Train: G_Loss: 1.297, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.533, D_sup_loss: 0.109, D_sup_acc: 97.34 Train acc: 96.805 Test acc: 97.040 \n",
      "step: 7968 | Train: G_Loss: 1.284, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.543, D_sup_loss: 0.120, D_sup_acc: 97.08 Train acc: 97.075 Test acc: 97.350 \n",
      "step: 7969 | Train: G_Loss: 1.234, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.576, D_sup_loss: 0.105, D_sup_acc: 97.38 Train acc: 96.940 Test acc: 97.270 \n",
      "step: 7970 | Train: G_Loss: 1.218, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.642, D_sup_loss: 0.113, D_sup_acc: 97.30 Train acc: 97.015 Test acc: 97.240 \n",
      "step: 7971 | Train: G_Loss: 1.225, D_unsup_loss_real: 0.535, D_unsup_loss_fake: 0.525, D_sup_loss: 0.110, D_sup_acc: 97.27 Train acc: 97.007 Test acc: 97.270 \n",
      "step: 7972 | Train: G_Loss: 1.186, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.610, D_sup_loss: 0.109, D_sup_acc: 97.30 Train acc: 96.972 Test acc: 97.230 \n",
      "step: 7973 | Train: G_Loss: 1.280, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.559, D_sup_loss: 0.112, D_sup_acc: 97.27 Train acc: 96.875 Test acc: 97.130 \n",
      "step: 7974 | Train: G_Loss: 1.310, D_unsup_loss_real: 0.550, D_unsup_loss_fake: 0.677, D_sup_loss: 0.115, D_sup_acc: 97.17 Train acc: 97.033 Test acc: 97.340 \n",
      "step: 7975 | Train: G_Loss: 1.330, D_unsup_loss_real: 0.537, D_unsup_loss_fake: 0.567, D_sup_loss: 0.106, D_sup_acc: 97.37 Train acc: 97.040 Test acc: 97.260 \n",
      "step: 7976 | Train: G_Loss: 1.223, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.609, D_sup_loss: 0.109, D_sup_acc: 97.29 Train acc: 96.778 Test acc: 97.070 \n",
      "step: 7977 | Train: G_Loss: 1.522, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.468, D_sup_loss: 0.124, D_sup_acc: 97.11 Train acc: 96.878 Test acc: 97.150 \n",
      "step: 7978 | Train: G_Loss: 1.212, D_unsup_loss_real: 0.678, D_unsup_loss_fake: 0.587, D_sup_loss: 0.117, D_sup_acc: 97.19 Train acc: 96.908 Test acc: 97.140 \n",
      "step: 7979 | Train: G_Loss: 1.170, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.586, D_sup_loss: 0.119, D_sup_acc: 97.18 Train acc: 96.595 Test acc: 96.870 \n",
      "step: 7980 | Train: G_Loss: 1.414, D_unsup_loss_real: 0.534, D_unsup_loss_fake: 0.533, D_sup_loss: 0.133, D_sup_acc: 96.91 Train acc: 96.992 Test acc: 97.270 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 7981 | Train: G_Loss: 1.261, D_unsup_loss_real: 0.517, D_unsup_loss_fake: 0.554, D_sup_loss: 0.108, D_sup_acc: 97.30 Train acc: 96.843 Test acc: 97.050 \n",
      "step: 7982 | Train: G_Loss: 1.310, D_unsup_loss_real: 0.493, D_unsup_loss_fake: 0.557, D_sup_loss: 0.125, D_sup_acc: 97.09 Train acc: 96.962 Test acc: 97.100 \n",
      "step: 7983 | Train: G_Loss: 1.236, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.520, D_sup_loss: 0.114, D_sup_acc: 97.14 Train acc: 96.918 Test acc: 97.090 \n",
      "step: 7984 | Train: G_Loss: 1.278, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.617, D_sup_loss: 0.117, D_sup_acc: 97.13 Train acc: 96.820 Test acc: 97.110 \n",
      "step: 7985 | Train: G_Loss: 1.229, D_unsup_loss_real: 0.529, D_unsup_loss_fake: 0.544, D_sup_loss: 0.123, D_sup_acc: 97.15 Train acc: 96.850 Test acc: 97.160 \n",
      "step: 7986 | Train: G_Loss: 1.323, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.512, D_sup_loss: 0.120, D_sup_acc: 97.20 Train acc: 97.087 Test acc: 97.220 \n",
      "step: 7987 | Train: G_Loss: 1.167, D_unsup_loss_real: 0.528, D_unsup_loss_fake: 0.524, D_sup_loss: 0.108, D_sup_acc: 97.26 Train acc: 96.600 Test acc: 96.930 \n",
      "step: 7988 | Train: G_Loss: 1.378, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.574, D_sup_loss: 0.131, D_sup_acc: 96.97 Train acc: 96.702 Test acc: 96.900 \n",
      "step: 7989 | Train: G_Loss: 1.175, D_unsup_loss_real: 0.517, D_unsup_loss_fake: 0.554, D_sup_loss: 0.126, D_sup_acc: 96.94 Train acc: 96.925 Test acc: 97.120 \n",
      "step: 7990 | Train: G_Loss: 1.274, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.628, D_sup_loss: 0.116, D_sup_acc: 97.16 Train acc: 97.057 Test acc: 97.260 \n",
      "step: 7991 | Train: G_Loss: 1.214, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.529, D_sup_loss: 0.107, D_sup_acc: 97.29 Train acc: 96.880 Test acc: 97.060 \n",
      "step: 7992 | Train: G_Loss: 1.277, D_unsup_loss_real: 0.542, D_unsup_loss_fake: 0.554, D_sup_loss: 0.124, D_sup_acc: 97.10 Train acc: 97.013 Test acc: 97.230 \n",
      "step: 7993 | Train: G_Loss: 1.332, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.576, D_sup_loss: 0.112, D_sup_acc: 97.27 Train acc: 96.848 Test acc: 97.090 \n",
      "step: 7994 | Train: G_Loss: 1.233, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.514, D_sup_loss: 0.119, D_sup_acc: 97.13 Train acc: 96.957 Test acc: 97.260 \n",
      "step: 7995 | Train: G_Loss: 1.296, D_unsup_loss_real: 0.514, D_unsup_loss_fake: 0.654, D_sup_loss: 0.116, D_sup_acc: 97.29 Train acc: 96.775 Test acc: 97.010 \n",
      "step: 7996 | Train: G_Loss: 1.274, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.521, D_sup_loss: 0.126, D_sup_acc: 97.05 Train acc: 96.780 Test acc: 97.010 \n",
      "step: 7997 | Train: G_Loss: 1.216, D_unsup_loss_real: 0.530, D_unsup_loss_fake: 0.540, D_sup_loss: 0.125, D_sup_acc: 97.05 Train acc: 96.972 Test acc: 97.160 \n",
      "step: 7998 | Train: G_Loss: 1.163, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.506, D_sup_loss: 0.113, D_sup_acc: 97.20 Train acc: 96.802 Test acc: 97.040 \n",
      "step: 7999 | Train: G_Loss: 1.420, D_unsup_loss_real: 0.492, D_unsup_loss_fake: 0.532, D_sup_loss: 0.122, D_sup_acc: 97.08 Train acc: 97.027 Test acc: 97.240 \n",
      "step: 8000 | Train: G_Loss: 1.307, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.588, D_sup_loss: 0.108, D_sup_acc: 97.27 Train acc: 96.213 Test acc: 96.560 \n",
      "Train Classifier Accuracy: 96.213%\n",
      "\n",
      "Test Classifier Accuracy: 96.560%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_8000.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_8000.h5\n",
      "step: 8001 | Train: G_Loss: 1.231, D_unsup_loss_real: 0.664, D_unsup_loss_fake: 0.580, D_sup_loss: 0.153, D_sup_acc: 96.60 Train acc: 96.738 Test acc: 96.970 \n",
      "step: 8002 | Train: G_Loss: 1.211, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.529, D_sup_loss: 0.125, D_sup_acc: 97.01 Train acc: 96.960 Test acc: 97.210 \n",
      "step: 8003 | Train: G_Loss: 1.192, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.572, D_sup_loss: 0.114, D_sup_acc: 97.25 Train acc: 96.822 Test acc: 97.100 \n",
      "step: 8004 | Train: G_Loss: 1.291, D_unsup_loss_real: 0.496, D_unsup_loss_fake: 0.610, D_sup_loss: 0.121, D_sup_acc: 97.14 Train acc: 96.915 Test acc: 97.130 \n",
      "step: 8005 | Train: G_Loss: 1.249, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.576, D_sup_loss: 0.116, D_sup_acc: 97.17 Train acc: 96.880 Test acc: 97.160 \n",
      "step: 8006 | Train: G_Loss: 1.264, D_unsup_loss_real: 0.518, D_unsup_loss_fake: 0.503, D_sup_loss: 0.118, D_sup_acc: 97.20 Train acc: 96.937 Test acc: 97.210 \n",
      "step: 8007 | Train: G_Loss: 1.196, D_unsup_loss_real: 0.497, D_unsup_loss_fake: 0.523, D_sup_loss: 0.116, D_sup_acc: 97.25 Train acc: 96.417 Test acc: 96.590 \n",
      "step: 8008 | Train: G_Loss: 1.248, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.549, D_sup_loss: 0.138, D_sup_acc: 96.63 Train acc: 96.947 Test acc: 97.080 \n",
      "step: 8009 | Train: G_Loss: 1.302, D_unsup_loss_real: 0.518, D_unsup_loss_fake: 0.554, D_sup_loss: 0.116, D_sup_acc: 97.12 Train acc: 96.903 Test acc: 97.040 \n",
      "step: 8010 | Train: G_Loss: 1.255, D_unsup_loss_real: 0.541, D_unsup_loss_fake: 0.588, D_sup_loss: 0.116, D_sup_acc: 97.08 Train acc: 96.960 Test acc: 97.220 \n",
      "step: 8011 | Train: G_Loss: 1.131, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.565, D_sup_loss: 0.116, D_sup_acc: 97.26 Train acc: 97.020 Test acc: 97.290 \n",
      "step: 8012 | Train: G_Loss: 1.229, D_unsup_loss_real: 0.503, D_unsup_loss_fake: 0.675, D_sup_loss: 0.110, D_sup_acc: 97.32 Train acc: 96.863 Test acc: 97.080 \n",
      "step: 8013 | Train: G_Loss: 1.330, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.579, D_sup_loss: 0.118, D_sup_acc: 97.12 Train acc: 96.863 Test acc: 97.090 \n",
      "step: 8014 | Train: G_Loss: 1.210, D_unsup_loss_real: 0.516, D_unsup_loss_fake: 0.515, D_sup_loss: 0.118, D_sup_acc: 97.13 Train acc: 96.867 Test acc: 97.030 \n",
      "step: 8015 | Train: G_Loss: 1.343, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.482, D_sup_loss: 0.124, D_sup_acc: 97.07 Train acc: 97.027 Test acc: 97.240 \n",
      "step: 8016 | Train: G_Loss: 1.295, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.550, D_sup_loss: 0.112, D_sup_acc: 97.27 Train acc: 96.832 Test acc: 97.160 \n",
      "step: 8017 | Train: G_Loss: 1.252, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.521, D_sup_loss: 0.123, D_sup_acc: 97.20 Train acc: 96.867 Test acc: 97.170 \n",
      "step: 8018 | Train: G_Loss: 1.284, D_unsup_loss_real: 0.628, D_unsup_loss_fake: 0.555, D_sup_loss: 0.119, D_sup_acc: 97.21 Train acc: 96.810 Test acc: 97.160 \n",
      "step: 8019 | Train: G_Loss: 1.292, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.544, D_sup_loss: 0.128, D_sup_acc: 97.20 Train acc: 96.802 Test acc: 97.170 \n",
      "step: 8020 | Train: G_Loss: 1.361, D_unsup_loss_real: 0.635, D_unsup_loss_fake: 0.579, D_sup_loss: 0.124, D_sup_acc: 97.21 Train acc: 96.942 Test acc: 97.250 \n",
      "step: 8021 | Train: G_Loss: 1.224, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.521, D_sup_loss: 0.115, D_sup_acc: 97.28 Train acc: 96.982 Test acc: 97.220 \n",
      "step: 8022 | Train: G_Loss: 1.231, D_unsup_loss_real: 0.508, D_unsup_loss_fake: 0.542, D_sup_loss: 0.116, D_sup_acc: 97.26 Train acc: 96.875 Test acc: 97.250 \n",
      "step: 8023 | Train: G_Loss: 1.281, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.536, D_sup_loss: 0.118, D_sup_acc: 97.28 Train acc: 97.010 Test acc: 97.310 \n",
      "step: 8024 | Train: G_Loss: 1.232, D_unsup_loss_real: 0.526, D_unsup_loss_fake: 0.594, D_sup_loss: 0.112, D_sup_acc: 97.34 Train acc: 96.955 Test acc: 97.210 \n",
      "step: 8025 | Train: G_Loss: 1.159, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.570, D_sup_loss: 0.117, D_sup_acc: 97.25 Train acc: 96.857 Test acc: 97.080 \n",
      "step: 8026 | Train: G_Loss: 1.198, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.605, D_sup_loss: 0.121, D_sup_acc: 97.12 Train acc: 96.895 Test acc: 97.200 \n",
      "step: 8027 | Train: G_Loss: 1.359, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.554, D_sup_loss: 0.120, D_sup_acc: 97.24 Train acc: 96.940 Test acc: 97.160 \n",
      "step: 8028 | Train: G_Loss: 1.235, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.561, D_sup_loss: 0.117, D_sup_acc: 97.20 Train acc: 96.865 Test acc: 97.170 \n",
      "step: 8029 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.483, D_sup_loss: 0.122, D_sup_acc: 97.21 Train acc: 96.785 Test acc: 97.090 \n",
      "step: 8030 | Train: G_Loss: 1.272, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.534, D_sup_loss: 0.125, D_sup_acc: 97.13 Train acc: 96.850 Test acc: 97.180 \n",
      "step: 8031 | Train: G_Loss: 1.314, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.583, D_sup_loss: 0.117, D_sup_acc: 97.22 Train acc: 96.720 Test acc: 97.040 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 8032 | Train: G_Loss: 1.192, D_unsup_loss_real: 0.531, D_unsup_loss_fake: 0.571, D_sup_loss: 0.125, D_sup_acc: 97.08 Train acc: 96.753 Test acc: 96.990 \n",
      "step: 8033 | Train: G_Loss: 1.168, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.523, D_sup_loss: 0.125, D_sup_acc: 97.03 Train acc: 96.598 Test acc: 96.750 \n",
      "step: 8034 | Train: G_Loss: 1.172, D_unsup_loss_real: 0.550, D_unsup_loss_fake: 0.596, D_sup_loss: 0.132, D_sup_acc: 96.79 Train acc: 96.658 Test acc: 96.940 \n",
      "step: 8035 | Train: G_Loss: 1.180, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.530, D_sup_loss: 0.127, D_sup_acc: 96.98 Train acc: 96.855 Test acc: 97.160 \n",
      "step: 8036 | Train: G_Loss: 1.217, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.537, D_sup_loss: 0.119, D_sup_acc: 97.20 Train acc: 96.928 Test acc: 97.350 \n",
      "step: 8037 | Train: G_Loss: 1.184, D_unsup_loss_real: 0.490, D_unsup_loss_fake: 0.588, D_sup_loss: 0.114, D_sup_acc: 97.38 Train acc: 96.472 Test acc: 96.860 \n",
      "step: 8038 | Train: G_Loss: 1.305, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.581, D_sup_loss: 0.138, D_sup_acc: 96.90 Train acc: 96.543 Test acc: 96.840 \n",
      "step: 8039 | Train: G_Loss: 1.193, D_unsup_loss_real: 0.518, D_unsup_loss_fake: 0.564, D_sup_loss: 0.134, D_sup_acc: 96.88 Train acc: 96.902 Test acc: 97.170 \n",
      "step: 8040 | Train: G_Loss: 1.353, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.577, D_sup_loss: 0.118, D_sup_acc: 97.21 Train acc: 96.962 Test acc: 97.150 \n",
      "step: 8041 | Train: G_Loss: 1.183, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.569, D_sup_loss: 0.112, D_sup_acc: 97.19 Train acc: 96.390 Test acc: 96.630 \n",
      "step: 8042 | Train: G_Loss: 1.363, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.605, D_sup_loss: 0.146, D_sup_acc: 96.67 Train acc: 96.832 Test acc: 97.080 \n",
      "step: 8043 | Train: G_Loss: 1.195, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.545, D_sup_loss: 0.119, D_sup_acc: 97.12 Train acc: 96.863 Test acc: 97.180 \n",
      "step: 8044 | Train: G_Loss: 1.227, D_unsup_loss_real: 0.486, D_unsup_loss_fake: 0.592, D_sup_loss: 0.118, D_sup_acc: 97.22 Train acc: 96.890 Test acc: 97.150 \n",
      "step: 8045 | Train: G_Loss: 1.164, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.580, D_sup_loss: 0.119, D_sup_acc: 97.19 Train acc: 96.808 Test acc: 97.110 \n",
      "step: 8046 | Train: G_Loss: 1.439, D_unsup_loss_real: 0.538, D_unsup_loss_fake: 0.570, D_sup_loss: 0.123, D_sup_acc: 97.15 Train acc: 96.983 Test acc: 97.170 \n",
      "step: 8047 | Train: G_Loss: 1.186, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.536, D_sup_loss: 0.111, D_sup_acc: 97.21 Train acc: 96.572 Test acc: 96.820 \n",
      "step: 8048 | Train: G_Loss: 1.319, D_unsup_loss_real: 0.644, D_unsup_loss_fake: 0.584, D_sup_loss: 0.145, D_sup_acc: 96.86 Train acc: 97.008 Test acc: 97.230 \n",
      "step: 8049 | Train: G_Loss: 1.194, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.563, D_sup_loss: 0.113, D_sup_acc: 97.27 Train acc: 97.018 Test acc: 97.370 \n",
      "step: 8050 | Train: G_Loss: 1.350, D_unsup_loss_real: 0.506, D_unsup_loss_fake: 0.566, D_sup_loss: 0.113, D_sup_acc: 97.40 Train acc: 96.988 Test acc: 97.250 \n",
      "step: 8051 | Train: G_Loss: 1.321, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.525, D_sup_loss: 0.115, D_sup_acc: 97.28 Train acc: 96.920 Test acc: 97.190 \n",
      "step: 8052 | Train: G_Loss: 1.291, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.488, D_sup_loss: 0.118, D_sup_acc: 97.23 Train acc: 96.885 Test acc: 97.160 \n",
      "step: 8053 | Train: G_Loss: 1.215, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.642, D_sup_loss: 0.118, D_sup_acc: 97.20 Train acc: 96.962 Test acc: 97.240 \n",
      "step: 8054 | Train: G_Loss: 1.214, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.540, D_sup_loss: 0.115, D_sup_acc: 97.27 Train acc: 96.872 Test acc: 97.080 \n",
      "step: 8055 | Train: G_Loss: 1.335, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.584, D_sup_loss: 0.121, D_sup_acc: 97.12 Train acc: 97.093 Test acc: 97.290 \n",
      "step: 8056 | Train: G_Loss: 1.253, D_unsup_loss_real: 0.512, D_unsup_loss_fake: 0.526, D_sup_loss: 0.110, D_sup_acc: 97.32 Train acc: 96.950 Test acc: 97.150 \n",
      "step: 8057 | Train: G_Loss: 1.178, D_unsup_loss_real: 0.517, D_unsup_loss_fake: 0.593, D_sup_loss: 0.118, D_sup_acc: 97.19 Train acc: 96.753 Test acc: 97.050 \n",
      "step: 8058 | Train: G_Loss: 1.332, D_unsup_loss_real: 0.545, D_unsup_loss_fake: 0.514, D_sup_loss: 0.129, D_sup_acc: 97.09 Train acc: 97.012 Test acc: 97.170 \n",
      "step: 8059 | Train: G_Loss: 1.299, D_unsup_loss_real: 0.669, D_unsup_loss_fake: 0.570, D_sup_loss: 0.111, D_sup_acc: 97.21 Train acc: 96.835 Test acc: 97.250 \n",
      "step: 8060 | Train: G_Loss: 1.331, D_unsup_loss_real: 0.500, D_unsup_loss_fake: 0.467, D_sup_loss: 0.123, D_sup_acc: 97.28 Train acc: 96.968 Test acc: 97.320 \n",
      "step: 8061 | Train: G_Loss: 1.207, D_unsup_loss_real: 0.535, D_unsup_loss_fake: 0.557, D_sup_loss: 0.111, D_sup_acc: 97.35 Train acc: 96.902 Test acc: 97.340 \n",
      "step: 8062 | Train: G_Loss: 1.240, D_unsup_loss_real: 0.481, D_unsup_loss_fake: 0.589, D_sup_loss: 0.114, D_sup_acc: 97.37 Train acc: 96.863 Test acc: 97.250 \n",
      "step: 8063 | Train: G_Loss: 1.154, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.529, D_sup_loss: 0.115, D_sup_acc: 97.28 Train acc: 96.717 Test acc: 96.970 \n",
      "step: 8064 | Train: G_Loss: 1.263, D_unsup_loss_real: 0.536, D_unsup_loss_fake: 0.567, D_sup_loss: 0.124, D_sup_acc: 97.01 Train acc: 96.970 Test acc: 97.240 \n",
      "step: 8065 | Train: G_Loss: 1.301, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.527, D_sup_loss: 0.110, D_sup_acc: 97.27 Train acc: 96.880 Test acc: 97.110 \n",
      "step: 8066 | Train: G_Loss: 1.348, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.506, D_sup_loss: 0.115, D_sup_acc: 97.15 Train acc: 97.018 Test acc: 97.340 \n",
      "step: 8067 | Train: G_Loss: 1.190, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.548, D_sup_loss: 0.109, D_sup_acc: 97.37 Train acc: 96.982 Test acc: 97.210 \n",
      "step: 8068 | Train: G_Loss: 1.261, D_unsup_loss_real: 0.480, D_unsup_loss_fake: 0.537, D_sup_loss: 0.111, D_sup_acc: 97.25 Train acc: 96.797 Test acc: 97.040 \n",
      "step: 8069 | Train: G_Loss: 1.250, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.562, D_sup_loss: 0.120, D_sup_acc: 97.08 Train acc: 97.057 Test acc: 97.140 \n",
      "step: 8070 | Train: G_Loss: 1.241, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.577, D_sup_loss: 0.107, D_sup_acc: 97.18 Train acc: 96.802 Test acc: 97.070 \n",
      "step: 8071 | Train: G_Loss: 1.209, D_unsup_loss_real: 0.669, D_unsup_loss_fake: 0.555, D_sup_loss: 0.125, D_sup_acc: 97.11 Train acc: 97.108 Test acc: 97.270 \n",
      "step: 8072 | Train: G_Loss: 1.176, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.546, D_sup_loss: 0.108, D_sup_acc: 97.30 Train acc: 97.025 Test acc: 97.290 \n",
      "step: 8073 | Train: G_Loss: 1.302, D_unsup_loss_real: 0.493, D_unsup_loss_fake: 0.585, D_sup_loss: 0.112, D_sup_acc: 97.32 Train acc: 97.082 Test acc: 97.260 \n",
      "step: 8074 | Train: G_Loss: 1.158, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.623, D_sup_loss: 0.109, D_sup_acc: 97.29 Train acc: 96.743 Test acc: 97.040 \n",
      "step: 8075 | Train: G_Loss: 1.230, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.578, D_sup_loss: 0.123, D_sup_acc: 97.08 Train acc: 97.032 Test acc: 97.280 \n",
      "step: 8076 | Train: G_Loss: 1.276, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.658, D_sup_loss: 0.110, D_sup_acc: 97.31 Train acc: 96.840 Test acc: 97.050 \n",
      "step: 8077 | Train: G_Loss: 1.392, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.514, D_sup_loss: 0.123, D_sup_acc: 97.09 Train acc: 96.993 Test acc: 97.200 \n",
      "step: 8078 | Train: G_Loss: 1.203, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.602, D_sup_loss: 0.110, D_sup_acc: 97.24 Train acc: 96.863 Test acc: 97.140 \n",
      "step: 8079 | Train: G_Loss: 1.288, D_unsup_loss_real: 0.545, D_unsup_loss_fake: 0.550, D_sup_loss: 0.121, D_sup_acc: 97.18 Train acc: 96.878 Test acc: 97.160 \n",
      "step: 8080 | Train: G_Loss: 1.186, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.527, D_sup_loss: 0.120, D_sup_acc: 97.20 Train acc: 96.830 Test acc: 97.130 \n",
      "step: 8081 | Train: G_Loss: 1.142, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.610, D_sup_loss: 0.118, D_sup_acc: 97.17 Train acc: 96.932 Test acc: 97.190 \n",
      "step: 8082 | Train: G_Loss: 1.343, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.563, D_sup_loss: 0.117, D_sup_acc: 97.23 Train acc: 96.683 Test acc: 97.040 \n",
      "step: 8083 | Train: G_Loss: 1.322, D_unsup_loss_real: 0.545, D_unsup_loss_fake: 0.557, D_sup_loss: 0.124, D_sup_acc: 97.08 Train acc: 96.865 Test acc: 97.130 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 8084 | Train: G_Loss: 1.197, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.586, D_sup_loss: 0.116, D_sup_acc: 97.17 Train acc: 96.823 Test acc: 97.100 \n",
      "step: 8085 | Train: G_Loss: 1.283, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.557, D_sup_loss: 0.122, D_sup_acc: 97.14 Train acc: 96.903 Test acc: 97.170 \n",
      "step: 8086 | Train: G_Loss: 1.238, D_unsup_loss_real: 0.512, D_unsup_loss_fake: 0.591, D_sup_loss: 0.118, D_sup_acc: 97.21 Train acc: 96.832 Test acc: 97.180 \n",
      "step: 8087 | Train: G_Loss: 1.243, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.537, D_sup_loss: 0.120, D_sup_acc: 97.22 Train acc: 96.743 Test acc: 97.120 \n",
      "step: 8088 | Train: G_Loss: 1.248, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.564, D_sup_loss: 0.122, D_sup_acc: 97.16 Train acc: 96.725 Test acc: 97.020 \n",
      "step: 8089 | Train: G_Loss: 1.236, D_unsup_loss_real: 0.520, D_unsup_loss_fake: 0.575, D_sup_loss: 0.123, D_sup_acc: 97.06 Train acc: 96.520 Test acc: 96.910 \n",
      "step: 8090 | Train: G_Loss: 1.252, D_unsup_loss_real: 0.497, D_unsup_loss_fake: 0.530, D_sup_loss: 0.132, D_sup_acc: 96.95 Train acc: 96.555 Test acc: 96.770 \n",
      "step: 8091 | Train: G_Loss: 1.187, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.550, D_sup_loss: 0.130, D_sup_acc: 96.81 Train acc: 96.633 Test acc: 96.930 \n",
      "step: 8092 | Train: G_Loss: 1.278, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.562, D_sup_loss: 0.126, D_sup_acc: 96.97 Train acc: 96.662 Test acc: 96.990 \n",
      "step: 8093 | Train: G_Loss: 1.220, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.562, D_sup_loss: 0.120, D_sup_acc: 97.03 Train acc: 96.758 Test acc: 96.960 \n",
      "step: 8094 | Train: G_Loss: 1.304, D_unsup_loss_real: 0.528, D_unsup_loss_fake: 0.611, D_sup_loss: 0.121, D_sup_acc: 97.00 Train acc: 96.458 Test acc: 96.720 \n",
      "step: 8095 | Train: G_Loss: 1.279, D_unsup_loss_real: 0.550, D_unsup_loss_fake: 0.538, D_sup_loss: 0.135, D_sup_acc: 96.76 Train acc: 96.783 Test acc: 97.010 \n",
      "step: 8096 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.578, D_sup_loss: 0.121, D_sup_acc: 97.05 Train acc: 96.777 Test acc: 96.960 \n",
      "step: 8097 | Train: G_Loss: 1.219, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.584, D_sup_loss: 0.124, D_sup_acc: 97.00 Train acc: 96.613 Test acc: 96.870 \n",
      "step: 8098 | Train: G_Loss: 1.196, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.526, D_sup_loss: 0.129, D_sup_acc: 96.91 Train acc: 96.963 Test acc: 97.100 \n",
      "step: 8099 | Train: G_Loss: 1.449, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.592, D_sup_loss: 0.114, D_sup_acc: 97.14 Train acc: 96.915 Test acc: 97.100 \n",
      "step: 8100 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.587, D_sup_loss: 0.115, D_sup_acc: 97.14 Train acc: 96.478 Test acc: 96.780 \n",
      "Train Classifier Accuracy: 96.478%\n",
      "\n",
      "Test Classifier Accuracy: 96.780%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_8100.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_8100.h5\n",
      "step: 8101 | Train: G_Loss: 1.360, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.527, D_sup_loss: 0.140, D_sup_acc: 96.82 Train acc: 96.863 Test acc: 97.060 \n",
      "step: 8102 | Train: G_Loss: 1.265, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.678, D_sup_loss: 0.116, D_sup_acc: 97.10 Train acc: 96.637 Test acc: 96.880 \n",
      "step: 8103 | Train: G_Loss: 1.268, D_unsup_loss_real: 0.491, D_unsup_loss_fake: 0.531, D_sup_loss: 0.133, D_sup_acc: 96.92 Train acc: 96.923 Test acc: 97.090 \n",
      "step: 8104 | Train: G_Loss: 1.337, D_unsup_loss_real: 0.523, D_unsup_loss_fake: 0.580, D_sup_loss: 0.116, D_sup_acc: 97.13 Train acc: 96.848 Test acc: 97.060 \n",
      "step: 8105 | Train: G_Loss: 1.147, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.521, D_sup_loss: 0.118, D_sup_acc: 97.10 Train acc: 96.610 Test acc: 96.960 \n",
      "step: 8106 | Train: G_Loss: 1.388, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.550, D_sup_loss: 0.133, D_sup_acc: 97.00 Train acc: 96.615 Test acc: 96.850 \n",
      "step: 8107 | Train: G_Loss: 1.242, D_unsup_loss_real: 0.534, D_unsup_loss_fake: 0.543, D_sup_loss: 0.128, D_sup_acc: 96.89 Train acc: 96.670 Test acc: 96.980 \n",
      "step: 8108 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.546, D_unsup_loss_fake: 0.597, D_sup_loss: 0.123, D_sup_acc: 97.02 Train acc: 96.567 Test acc: 96.870 \n",
      "step: 8109 | Train: G_Loss: 1.199, D_unsup_loss_real: 0.509, D_unsup_loss_fake: 0.579, D_sup_loss: 0.132, D_sup_acc: 96.91 Train acc: 96.923 Test acc: 97.310 \n",
      "step: 8110 | Train: G_Loss: 1.212, D_unsup_loss_real: 0.631, D_unsup_loss_fake: 0.588, D_sup_loss: 0.118, D_sup_acc: 97.34 Train acc: 96.883 Test acc: 97.210 \n",
      "step: 8111 | Train: G_Loss: 1.222, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.569, D_sup_loss: 0.120, D_sup_acc: 97.25 Train acc: 96.932 Test acc: 97.290 \n",
      "step: 8112 | Train: G_Loss: 1.377, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.566, D_sup_loss: 0.118, D_sup_acc: 97.32 Train acc: 96.877 Test acc: 97.200 \n",
      "step: 8113 | Train: G_Loss: 1.254, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.599, D_sup_loss: 0.115, D_sup_acc: 97.24 Train acc: 96.572 Test acc: 97.000 \n",
      "step: 8114 | Train: G_Loss: 1.243, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.564, D_sup_loss: 0.134, D_sup_acc: 97.04 Train acc: 96.940 Test acc: 97.350 \n",
      "step: 8115 | Train: G_Loss: 1.288, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.493, D_sup_loss: 0.116, D_sup_acc: 97.38 Train acc: 97.005 Test acc: 97.360 \n",
      "step: 8116 | Train: G_Loss: 1.047, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.564, D_sup_loss: 0.111, D_sup_acc: 97.39 Train acc: 96.663 Test acc: 97.050 \n",
      "step: 8117 | Train: G_Loss: 1.323, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.525, D_sup_loss: 0.130, D_sup_acc: 97.09 Train acc: 97.010 Test acc: 97.360 \n",
      "step: 8118 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.520, D_sup_loss: 0.109, D_sup_acc: 97.39 Train acc: 96.937 Test acc: 97.300 \n",
      "step: 8119 | Train: G_Loss: 1.296, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.588, D_sup_loss: 0.115, D_sup_acc: 97.33 Train acc: 97.052 Test acc: 97.250 \n",
      "step: 8120 | Train: G_Loss: 1.242, D_unsup_loss_real: 0.507, D_unsup_loss_fake: 0.581, D_sup_loss: 0.105, D_sup_acc: 97.28 Train acc: 96.657 Test acc: 97.030 \n",
      "step: 8121 | Train: G_Loss: 1.239, D_unsup_loss_real: 0.509, D_unsup_loss_fake: 0.596, D_sup_loss: 0.125, D_sup_acc: 97.07 Train acc: 96.893 Test acc: 97.210 \n",
      "step: 8122 | Train: G_Loss: 1.308, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.641, D_sup_loss: 0.116, D_sup_acc: 97.25 Train acc: 96.923 Test acc: 97.300 \n",
      "step: 8123 | Train: G_Loss: 1.252, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.562, D_sup_loss: 0.115, D_sup_acc: 97.33 Train acc: 96.745 Test acc: 97.070 \n",
      "step: 8124 | Train: G_Loss: 1.342, D_unsup_loss_real: 0.515, D_unsup_loss_fake: 0.566, D_sup_loss: 0.122, D_sup_acc: 97.11 Train acc: 96.818 Test acc: 97.190 \n",
      "step: 8125 | Train: G_Loss: 1.226, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.509, D_sup_loss: 0.118, D_sup_acc: 97.23 Train acc: 96.980 Test acc: 97.250 \n",
      "step: 8126 | Train: G_Loss: 1.151, D_unsup_loss_real: 0.676, D_unsup_loss_fake: 0.538, D_sup_loss: 0.111, D_sup_acc: 97.28 Train acc: 96.915 Test acc: 97.310 \n",
      "step: 8127 | Train: G_Loss: 1.217, D_unsup_loss_real: 0.519, D_unsup_loss_fake: 0.559, D_sup_loss: 0.117, D_sup_acc: 97.34 Train acc: 96.955 Test acc: 97.270 \n",
      "step: 8128 | Train: G_Loss: 1.320, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.560, D_sup_loss: 0.114, D_sup_acc: 97.30 Train acc: 96.948 Test acc: 97.200 \n",
      "step: 8129 | Train: G_Loss: 1.315, D_unsup_loss_real: 0.538, D_unsup_loss_fake: 0.617, D_sup_loss: 0.116, D_sup_acc: 97.24 Train acc: 97.022 Test acc: 97.300 \n",
      "step: 8130 | Train: G_Loss: 1.268, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.545, D_sup_loss: 0.107, D_sup_acc: 97.33 Train acc: 96.802 Test acc: 97.140 \n",
      "step: 8131 | Train: G_Loss: 1.241, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.571, D_sup_loss: 0.130, D_sup_acc: 97.18 Train acc: 96.930 Test acc: 97.200 \n",
      "step: 8132 | Train: G_Loss: 1.363, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.588, D_sup_loss: 0.116, D_sup_acc: 97.24 Train acc: 96.920 Test acc: 97.090 \n",
      "step: 8133 | Train: G_Loss: 1.287, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.593, D_sup_loss: 0.116, D_sup_acc: 97.13 Train acc: 96.917 Test acc: 97.090 \n",
      "step: 8134 | Train: G_Loss: 1.308, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.582, D_sup_loss: 0.114, D_sup_acc: 97.13 Train acc: 96.825 Test acc: 97.170 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 8135 | Train: G_Loss: 1.152, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.524, D_sup_loss: 0.118, D_sup_acc: 97.21 Train acc: 96.845 Test acc: 97.120 \n",
      "step: 8136 | Train: G_Loss: 1.271, D_unsup_loss_real: 0.544, D_unsup_loss_fake: 0.571, D_sup_loss: 0.121, D_sup_acc: 97.16 Train acc: 96.995 Test acc: 97.250 \n",
      "step: 8137 | Train: G_Loss: 1.364, D_unsup_loss_real: 0.522, D_unsup_loss_fake: 0.588, D_sup_loss: 0.114, D_sup_acc: 97.28 Train acc: 97.093 Test acc: 97.300 \n",
      "step: 8138 | Train: G_Loss: 1.247, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.484, D_sup_loss: 0.110, D_sup_acc: 97.33 Train acc: 96.962 Test acc: 97.240 \n",
      "step: 8139 | Train: G_Loss: 1.275, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.637, D_sup_loss: 0.116, D_sup_acc: 97.27 Train acc: 97.008 Test acc: 97.290 \n",
      "step: 8140 | Train: G_Loss: 1.171, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.584, D_sup_loss: 0.114, D_sup_acc: 97.32 Train acc: 96.848 Test acc: 97.170 \n",
      "step: 8141 | Train: G_Loss: 1.199, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.683, D_sup_loss: 0.121, D_sup_acc: 97.21 Train acc: 96.463 Test acc: 96.770 \n",
      "step: 8142 | Train: G_Loss: 1.267, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.618, D_sup_loss: 0.135, D_sup_acc: 96.81 Train acc: 96.167 Test acc: 96.370 \n",
      "step: 8143 | Train: G_Loss: 1.338, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.537, D_sup_loss: 0.149, D_sup_acc: 96.42 Train acc: 96.755 Test acc: 97.020 \n",
      "step: 8144 | Train: G_Loss: 1.208, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.546, D_sup_loss: 0.125, D_sup_acc: 97.06 Train acc: 96.730 Test acc: 97.030 \n",
      "step: 8145 | Train: G_Loss: 1.427, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.578, D_sup_loss: 0.128, D_sup_acc: 97.07 Train acc: 96.967 Test acc: 97.290 \n",
      "step: 8146 | Train: G_Loss: 1.181, D_unsup_loss_real: 0.631, D_unsup_loss_fake: 0.577, D_sup_loss: 0.110, D_sup_acc: 97.32 Train acc: 96.555 Test acc: 96.940 \n",
      "step: 8147 | Train: G_Loss: 1.208, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.623, D_sup_loss: 0.131, D_sup_acc: 96.98 Train acc: 96.690 Test acc: 96.990 \n",
      "step: 8148 | Train: G_Loss: 1.245, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.557, D_sup_loss: 0.121, D_sup_acc: 97.03 Train acc: 96.815 Test acc: 97.080 \n",
      "step: 8149 | Train: G_Loss: 1.172, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.612, D_sup_loss: 0.118, D_sup_acc: 97.12 Train acc: 96.830 Test acc: 97.040 \n",
      "step: 8150 | Train: G_Loss: 1.205, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.527, D_sup_loss: 0.117, D_sup_acc: 97.08 Train acc: 96.697 Test acc: 97.060 \n",
      "step: 8151 | Train: G_Loss: 1.229, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.616, D_sup_loss: 0.125, D_sup_acc: 97.10 Train acc: 96.750 Test acc: 97.100 \n",
      "step: 8152 | Train: G_Loss: 1.320, D_unsup_loss_real: 0.441, D_unsup_loss_fake: 0.584, D_sup_loss: 0.124, D_sup_acc: 97.14 Train acc: 96.850 Test acc: 97.180 \n",
      "step: 8153 | Train: G_Loss: 1.350, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.530, D_sup_loss: 0.118, D_sup_acc: 97.22 Train acc: 96.915 Test acc: 97.190 \n",
      "step: 8154 | Train: G_Loss: 1.281, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.566, D_sup_loss: 0.115, D_sup_acc: 97.23 Train acc: 96.810 Test acc: 97.110 \n",
      "step: 8155 | Train: G_Loss: 1.261, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.537, D_sup_loss: 0.121, D_sup_acc: 97.15 Train acc: 96.860 Test acc: 97.130 \n",
      "step: 8156 | Train: G_Loss: 1.262, D_unsup_loss_real: 0.507, D_unsup_loss_fake: 0.574, D_sup_loss: 0.118, D_sup_acc: 97.17 Train acc: 96.353 Test acc: 96.680 \n",
      "step: 8157 | Train: G_Loss: 1.299, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.611, D_sup_loss: 0.141, D_sup_acc: 96.72 Train acc: 96.748 Test acc: 96.980 \n",
      "step: 8158 | Train: G_Loss: 1.237, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.636, D_sup_loss: 0.122, D_sup_acc: 97.02 Train acc: 96.013 Test acc: 96.330 \n",
      "step: 8159 | Train: G_Loss: 1.284, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.510, D_sup_loss: 0.161, D_sup_acc: 96.38 Train acc: 96.447 Test acc: 96.860 \n",
      "step: 8160 | Train: G_Loss: 1.245, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.554, D_sup_loss: 0.134, D_sup_acc: 96.90 Train acc: 96.608 Test acc: 96.830 \n",
      "step: 8161 | Train: G_Loss: 1.370, D_unsup_loss_real: 0.502, D_unsup_loss_fake: 0.485, D_sup_loss: 0.131, D_sup_acc: 96.87 Train acc: 96.913 Test acc: 97.200 \n",
      "step: 8162 | Train: G_Loss: 1.200, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.511, D_sup_loss: 0.114, D_sup_acc: 97.24 Train acc: 96.818 Test acc: 97.010 \n",
      "step: 8163 | Train: G_Loss: 1.153, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.575, D_sup_loss: 0.123, D_sup_acc: 97.05 Train acc: 96.767 Test acc: 96.930 \n",
      "step: 8164 | Train: G_Loss: 1.235, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.565, D_sup_loss: 0.124, D_sup_acc: 96.97 Train acc: 96.757 Test acc: 96.880 \n",
      "step: 8165 | Train: G_Loss: 1.195, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.588, D_sup_loss: 0.125, D_sup_acc: 96.92 Train acc: 96.900 Test acc: 97.040 \n",
      "step: 8166 | Train: G_Loss: 1.266, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.540, D_sup_loss: 0.114, D_sup_acc: 97.08 Train acc: 96.953 Test acc: 97.160 \n",
      "step: 8167 | Train: G_Loss: 1.139, D_unsup_loss_real: 0.522, D_unsup_loss_fake: 0.541, D_sup_loss: 0.112, D_sup_acc: 97.20 Train acc: 96.653 Test acc: 96.910 \n",
      "step: 8168 | Train: G_Loss: 1.255, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.566, D_sup_loss: 0.132, D_sup_acc: 96.95 Train acc: 96.755 Test acc: 97.060 \n",
      "step: 8169 | Train: G_Loss: 1.312, D_unsup_loss_real: 0.515, D_unsup_loss_fake: 0.633, D_sup_loss: 0.126, D_sup_acc: 97.10 Train acc: 96.732 Test acc: 96.920 \n",
      "step: 8170 | Train: G_Loss: 1.207, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.572, D_sup_loss: 0.128, D_sup_acc: 96.96 Train acc: 96.707 Test acc: 96.940 \n",
      "step: 8171 | Train: G_Loss: 1.225, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.529, D_sup_loss: 0.123, D_sup_acc: 96.98 Train acc: 96.740 Test acc: 96.960 \n",
      "step: 8172 | Train: G_Loss: 1.280, D_unsup_loss_real: 0.533, D_unsup_loss_fake: 0.583, D_sup_loss: 0.128, D_sup_acc: 97.00 Train acc: 96.830 Test acc: 97.030 \n",
      "step: 8173 | Train: G_Loss: 1.269, D_unsup_loss_real: 0.544, D_unsup_loss_fake: 0.591, D_sup_loss: 0.120, D_sup_acc: 97.07 Train acc: 96.835 Test acc: 97.020 \n",
      "step: 8174 | Train: G_Loss: 1.288, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.578, D_sup_loss: 0.122, D_sup_acc: 97.06 Train acc: 96.673 Test acc: 96.810 \n",
      "step: 8175 | Train: G_Loss: 1.187, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.556, D_sup_loss: 0.125, D_sup_acc: 96.85 Train acc: 96.752 Test acc: 96.860 \n",
      "step: 8176 | Train: G_Loss: 1.210, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.624, D_sup_loss: 0.122, D_sup_acc: 96.90 Train acc: 96.577 Test acc: 96.870 \n",
      "step: 8177 | Train: G_Loss: 1.333, D_unsup_loss_real: 0.501, D_unsup_loss_fake: 0.607, D_sup_loss: 0.133, D_sup_acc: 96.91 Train acc: 96.827 Test acc: 97.060 \n",
      "step: 8178 | Train: G_Loss: 1.471, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.493, D_sup_loss: 0.118, D_sup_acc: 97.10 Train acc: 96.587 Test acc: 96.970 \n",
      "step: 8179 | Train: G_Loss: 1.360, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.485, D_sup_loss: 0.127, D_sup_acc: 97.01 Train acc: 96.600 Test acc: 96.980 \n",
      "step: 8180 | Train: G_Loss: 1.310, D_unsup_loss_real: 0.512, D_unsup_loss_fake: 0.571, D_sup_loss: 0.128, D_sup_acc: 97.02 Train acc: 96.687 Test acc: 96.860 \n",
      "step: 8181 | Train: G_Loss: 1.260, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.550, D_sup_loss: 0.124, D_sup_acc: 96.90 Train acc: 96.558 Test acc: 96.720 \n",
      "step: 8182 | Train: G_Loss: 1.216, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.597, D_sup_loss: 0.126, D_sup_acc: 96.76 Train acc: 96.373 Test acc: 96.630 \n",
      "step: 8183 | Train: G_Loss: 1.314, D_unsup_loss_real: 0.508, D_unsup_loss_fake: 0.609, D_sup_loss: 0.139, D_sup_acc: 96.67 Train acc: 96.728 Test acc: 96.940 \n",
      "step: 8184 | Train: G_Loss: 1.352, D_unsup_loss_real: 0.520, D_unsup_loss_fake: 0.523, D_sup_loss: 0.123, D_sup_acc: 96.98 Train acc: 96.815 Test acc: 97.030 \n",
      "step: 8185 | Train: G_Loss: 1.233, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.458, D_sup_loss: 0.113, D_sup_acc: 97.07 Train acc: 96.453 Test acc: 96.770 \n",
      "step: 8186 | Train: G_Loss: 1.274, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.597, D_sup_loss: 0.142, D_sup_acc: 96.81 Train acc: 96.825 Test acc: 97.020 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 8187 | Train: G_Loss: 1.317, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.505, D_sup_loss: 0.121, D_sup_acc: 97.06 Train acc: 96.890 Test acc: 97.180 \n",
      "step: 8188 | Train: G_Loss: 1.228, D_unsup_loss_real: 0.525, D_unsup_loss_fake: 0.588, D_sup_loss: 0.116, D_sup_acc: 97.22 Train acc: 96.918 Test acc: 97.240 \n",
      "step: 8189 | Train: G_Loss: 1.243, D_unsup_loss_real: 0.532, D_unsup_loss_fake: 0.553, D_sup_loss: 0.115, D_sup_acc: 97.27 Train acc: 96.883 Test acc: 97.180 \n",
      "step: 8190 | Train: G_Loss: 1.312, D_unsup_loss_real: 0.489, D_unsup_loss_fake: 0.507, D_sup_loss: 0.118, D_sup_acc: 97.22 Train acc: 97.063 Test acc: 97.230 \n",
      "step: 8191 | Train: G_Loss: 1.238, D_unsup_loss_real: 0.503, D_unsup_loss_fake: 0.546, D_sup_loss: 0.108, D_sup_acc: 97.27 Train acc: 96.903 Test acc: 97.080 \n",
      "step: 8192 | Train: G_Loss: 1.185, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.620, D_sup_loss: 0.121, D_sup_acc: 97.12 Train acc: 96.925 Test acc: 97.070 \n",
      "step: 8193 | Train: G_Loss: 1.161, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.546, D_sup_loss: 0.118, D_sup_acc: 97.11 Train acc: 97.040 Test acc: 97.140 \n",
      "step: 8194 | Train: G_Loss: 1.222, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.652, D_sup_loss: 0.115, D_sup_acc: 97.18 Train acc: 97.037 Test acc: 97.280 \n",
      "step: 8195 | Train: G_Loss: 1.212, D_unsup_loss_real: 0.481, D_unsup_loss_fake: 0.565, D_sup_loss: 0.117, D_sup_acc: 97.31 Train acc: 96.952 Test acc: 97.160 \n",
      "step: 8196 | Train: G_Loss: 1.341, D_unsup_loss_real: 0.465, D_unsup_loss_fake: 0.615, D_sup_loss: 0.119, D_sup_acc: 97.20 Train acc: 96.988 Test acc: 97.150 \n",
      "step: 8197 | Train: G_Loss: 1.150, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.546, D_sup_loss: 0.113, D_sup_acc: 97.19 Train acc: 96.863 Test acc: 97.090 \n",
      "step: 8198 | Train: G_Loss: 1.329, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.559, D_sup_loss: 0.125, D_sup_acc: 97.13 Train acc: 96.827 Test acc: 97.150 \n",
      "step: 8199 | Train: G_Loss: 1.269, D_unsup_loss_real: 0.465, D_unsup_loss_fake: 0.509, D_sup_loss: 0.124, D_sup_acc: 97.19 Train acc: 97.020 Test acc: 97.250 \n",
      "step: 8200 | Train: G_Loss: 1.320, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.582, D_sup_loss: 0.113, D_sup_acc: 97.28 Train acc: 96.633 Test acc: 96.890 \n",
      "Train Classifier Accuracy: 96.633%\n",
      "\n",
      "Test Classifier Accuracy: 96.890%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_8200.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_8200.h5\n",
      "step: 8201 | Train: G_Loss: 1.266, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.487, D_sup_loss: 0.133, D_sup_acc: 96.93 Train acc: 96.908 Test acc: 97.160 \n",
      "step: 8202 | Train: G_Loss: 1.188, D_unsup_loss_real: 0.522, D_unsup_loss_fake: 0.551, D_sup_loss: 0.119, D_sup_acc: 97.20 Train acc: 96.920 Test acc: 97.180 \n",
      "step: 8203 | Train: G_Loss: 1.315, D_unsup_loss_real: 0.536, D_unsup_loss_fake: 0.528, D_sup_loss: 0.118, D_sup_acc: 97.22 Train acc: 96.793 Test acc: 97.000 \n",
      "step: 8204 | Train: G_Loss: 1.245, D_unsup_loss_real: 0.542, D_unsup_loss_fake: 0.543, D_sup_loss: 0.123, D_sup_acc: 97.04 Train acc: 96.918 Test acc: 97.150 \n",
      "step: 8205 | Train: G_Loss: 1.234, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.574, D_sup_loss: 0.117, D_sup_acc: 97.19 Train acc: 96.782 Test acc: 97.050 \n",
      "step: 8206 | Train: G_Loss: 1.157, D_unsup_loss_real: 0.653, D_unsup_loss_fake: 0.560, D_sup_loss: 0.127, D_sup_acc: 97.09 Train acc: 96.820 Test acc: 97.060 \n",
      "step: 8207 | Train: G_Loss: 1.332, D_unsup_loss_real: 0.519, D_unsup_loss_fake: 0.556, D_sup_loss: 0.122, D_sup_acc: 97.10 Train acc: 96.967 Test acc: 97.130 \n",
      "step: 8208 | Train: G_Loss: 1.240, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.513, D_sup_loss: 0.113, D_sup_acc: 97.17 Train acc: 96.803 Test acc: 97.110 \n",
      "step: 8209 | Train: G_Loss: 1.244, D_unsup_loss_real: 0.542, D_unsup_loss_fake: 0.627, D_sup_loss: 0.125, D_sup_acc: 97.15 Train acc: 96.532 Test acc: 96.750 \n",
      "step: 8210 | Train: G_Loss: 1.193, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.554, D_sup_loss: 0.135, D_sup_acc: 96.79 Train acc: 96.548 Test acc: 96.840 \n",
      "step: 8211 | Train: G_Loss: 1.214, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.570, D_sup_loss: 0.131, D_sup_acc: 96.88 Train acc: 96.582 Test acc: 96.750 \n",
      "step: 8212 | Train: G_Loss: 1.179, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.592, D_sup_loss: 0.134, D_sup_acc: 96.79 Train acc: 96.737 Test acc: 97.020 \n",
      "step: 8213 | Train: G_Loss: 1.124, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.582, D_sup_loss: 0.125, D_sup_acc: 97.06 Train acc: 96.772 Test acc: 97.110 \n",
      "step: 8214 | Train: G_Loss: 1.380, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.501, D_sup_loss: 0.122, D_sup_acc: 97.15 Train acc: 96.822 Test acc: 97.180 \n",
      "step: 8215 | Train: G_Loss: 1.267, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.523, D_sup_loss: 0.120, D_sup_acc: 97.22 Train acc: 96.835 Test acc: 97.030 \n",
      "step: 8216 | Train: G_Loss: 1.189, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.437, D_sup_loss: 0.123, D_sup_acc: 97.07 Train acc: 97.033 Test acc: 97.290 \n",
      "step: 8217 | Train: G_Loss: 1.287, D_unsup_loss_real: 0.490, D_unsup_loss_fake: 0.642, D_sup_loss: 0.112, D_sup_acc: 97.32 Train acc: 96.792 Test acc: 97.110 \n",
      "step: 8218 | Train: G_Loss: 1.200, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.560, D_sup_loss: 0.120, D_sup_acc: 97.15 Train acc: 96.957 Test acc: 97.240 \n",
      "step: 8219 | Train: G_Loss: 1.171, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.507, D_sup_loss: 0.114, D_sup_acc: 97.27 Train acc: 96.833 Test acc: 97.070 \n",
      "step: 8220 | Train: G_Loss: 1.168, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.538, D_sup_loss: 0.120, D_sup_acc: 97.11 Train acc: 96.653 Test acc: 96.890 \n",
      "step: 8221 | Train: G_Loss: 1.295, D_unsup_loss_real: 0.508, D_unsup_loss_fake: 0.563, D_sup_loss: 0.129, D_sup_acc: 96.93 Train acc: 96.670 Test acc: 97.050 \n",
      "step: 8222 | Train: G_Loss: 1.291, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.545, D_sup_loss: 0.128, D_sup_acc: 97.09 Train acc: 96.762 Test acc: 97.150 \n",
      "step: 8223 | Train: G_Loss: 1.197, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.566, D_sup_loss: 0.124, D_sup_acc: 97.19 Train acc: 96.813 Test acc: 97.030 \n",
      "step: 8224 | Train: G_Loss: 1.257, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.503, D_sup_loss: 0.124, D_sup_acc: 97.07 Train acc: 97.077 Test acc: 97.260 \n",
      "step: 8225 | Train: G_Loss: 1.206, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.538, D_sup_loss: 0.112, D_sup_acc: 97.29 Train acc: 96.727 Test acc: 96.980 \n",
      "step: 8226 | Train: G_Loss: 1.264, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.523, D_sup_loss: 0.131, D_sup_acc: 97.02 Train acc: 96.950 Test acc: 97.120 \n",
      "step: 8227 | Train: G_Loss: 1.205, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.570, D_sup_loss: 0.119, D_sup_acc: 97.16 Train acc: 97.008 Test acc: 97.210 \n",
      "step: 8228 | Train: G_Loss: 1.223, D_unsup_loss_real: 0.475, D_unsup_loss_fake: 0.570, D_sup_loss: 0.115, D_sup_acc: 97.25 Train acc: 97.018 Test acc: 97.190 \n",
      "step: 8229 | Train: G_Loss: 1.177, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.541, D_sup_loss: 0.113, D_sup_acc: 97.23 Train acc: 96.818 Test acc: 97.050 \n",
      "step: 8230 | Train: G_Loss: 1.117, D_unsup_loss_real: 0.636, D_unsup_loss_fake: 0.534, D_sup_loss: 0.123, D_sup_acc: 97.09 Train acc: 96.902 Test acc: 97.220 \n",
      "step: 8231 | Train: G_Loss: 1.187, D_unsup_loss_real: 0.534, D_unsup_loss_fake: 0.588, D_sup_loss: 0.117, D_sup_acc: 97.26 Train acc: 96.892 Test acc: 97.240 \n",
      "step: 8232 | Train: G_Loss: 1.291, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.655, D_sup_loss: 0.119, D_sup_acc: 97.27 Train acc: 97.092 Test acc: 97.270 \n",
      "step: 8233 | Train: G_Loss: 1.444, D_unsup_loss_real: 0.536, D_unsup_loss_fake: 0.565, D_sup_loss: 0.114, D_sup_acc: 97.30 Train acc: 97.208 Test acc: 97.310 \n",
      "step: 8234 | Train: G_Loss: 1.327, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.561, D_sup_loss: 0.106, D_sup_acc: 97.34 Train acc: 96.882 Test acc: 97.130 \n",
      "step: 8235 | Train: G_Loss: 1.310, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.547, D_sup_loss: 0.127, D_sup_acc: 97.17 Train acc: 97.050 Test acc: 97.270 \n",
      "step: 8236 | Train: G_Loss: 1.180, D_unsup_loss_real: 0.536, D_unsup_loss_fake: 0.581, D_sup_loss: 0.114, D_sup_acc: 97.30 Train acc: 96.982 Test acc: 97.180 \n",
      "step: 8237 | Train: G_Loss: 1.260, D_unsup_loss_real: 0.535, D_unsup_loss_fake: 0.589, D_sup_loss: 0.114, D_sup_acc: 97.22 Train acc: 96.965 Test acc: 97.170 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 8238 | Train: G_Loss: 1.307, D_unsup_loss_real: 0.532, D_unsup_loss_fake: 0.540, D_sup_loss: 0.119, D_sup_acc: 97.21 Train acc: 96.975 Test acc: 97.230 \n",
      "step: 8239 | Train: G_Loss: 1.242, D_unsup_loss_real: 0.658, D_unsup_loss_fake: 0.565, D_sup_loss: 0.116, D_sup_acc: 97.27 Train acc: 96.783 Test acc: 97.050 \n",
      "step: 8240 | Train: G_Loss: 1.320, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.583, D_sup_loss: 0.134, D_sup_acc: 97.09 Train acc: 96.982 Test acc: 97.280 \n",
      "step: 8241 | Train: G_Loss: 1.237, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.599, D_sup_loss: 0.118, D_sup_acc: 97.31 Train acc: 96.942 Test acc: 97.210 \n",
      "step: 8242 | Train: G_Loss: 1.214, D_unsup_loss_real: 0.462, D_unsup_loss_fake: 0.470, D_sup_loss: 0.121, D_sup_acc: 97.25 Train acc: 97.013 Test acc: 97.120 \n",
      "step: 8243 | Train: G_Loss: 1.289, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.532, D_sup_loss: 0.112, D_sup_acc: 97.16 Train acc: 96.800 Test acc: 97.160 \n",
      "step: 8244 | Train: G_Loss: 1.267, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.582, D_sup_loss: 0.129, D_sup_acc: 97.20 Train acc: 96.975 Test acc: 97.300 \n",
      "step: 8245 | Train: G_Loss: 1.185, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.558, D_sup_loss: 0.114, D_sup_acc: 97.33 Train acc: 96.885 Test acc: 97.200 \n",
      "step: 8246 | Train: G_Loss: 1.213, D_unsup_loss_real: 0.546, D_unsup_loss_fake: 0.573, D_sup_loss: 0.117, D_sup_acc: 97.24 Train acc: 96.790 Test acc: 97.110 \n",
      "step: 8247 | Train: G_Loss: 1.257, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.574, D_sup_loss: 0.124, D_sup_acc: 97.15 Train acc: 96.928 Test acc: 97.260 \n",
      "step: 8248 | Train: G_Loss: 1.335, D_unsup_loss_real: 0.534, D_unsup_loss_fake: 0.566, D_sup_loss: 0.116, D_sup_acc: 97.29 Train acc: 96.645 Test acc: 96.960 \n",
      "step: 8249 | Train: G_Loss: 1.364, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.612, D_sup_loss: 0.133, D_sup_acc: 97.00 Train acc: 96.353 Test acc: 96.630 \n",
      "step: 8250 | Train: G_Loss: 1.309, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.590, D_sup_loss: 0.143, D_sup_acc: 96.67 Train acc: 96.685 Test acc: 97.080 \n",
      "step: 8251 | Train: G_Loss: 1.335, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.527, D_sup_loss: 0.131, D_sup_acc: 97.12 Train acc: 96.882 Test acc: 97.240 \n",
      "step: 8252 | Train: G_Loss: 1.280, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.501, D_sup_loss: 0.119, D_sup_acc: 97.27 Train acc: 96.893 Test acc: 97.330 \n",
      "step: 8253 | Train: G_Loss: 1.206, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.617, D_sup_loss: 0.119, D_sup_acc: 97.36 Train acc: 96.830 Test acc: 97.250 \n",
      "step: 8254 | Train: G_Loss: 1.193, D_unsup_loss_real: 0.691, D_unsup_loss_fake: 0.575, D_sup_loss: 0.124, D_sup_acc: 97.28 Train acc: 96.812 Test acc: 97.220 \n",
      "step: 8255 | Train: G_Loss: 1.348, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.499, D_sup_loss: 0.126, D_sup_acc: 97.26 Train acc: 96.883 Test acc: 97.180 \n",
      "step: 8256 | Train: G_Loss: 1.266, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.572, D_sup_loss: 0.118, D_sup_acc: 97.22 Train acc: 96.640 Test acc: 97.040 \n",
      "step: 8257 | Train: G_Loss: 1.228, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.587, D_sup_loss: 0.135, D_sup_acc: 97.08 Train acc: 96.558 Test acc: 96.880 \n",
      "step: 8258 | Train: G_Loss: 1.208, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.524, D_sup_loss: 0.131, D_sup_acc: 96.92 Train acc: 96.690 Test acc: 96.950 \n",
      "step: 8259 | Train: G_Loss: 1.366, D_unsup_loss_real: 0.500, D_unsup_loss_fake: 0.629, D_sup_loss: 0.126, D_sup_acc: 96.99 Train acc: 96.668 Test acc: 96.930 \n",
      "step: 8260 | Train: G_Loss: 1.282, D_unsup_loss_real: 0.508, D_unsup_loss_fake: 0.572, D_sup_loss: 0.127, D_sup_acc: 96.97 Train acc: 96.667 Test acc: 96.960 \n",
      "step: 8261 | Train: G_Loss: 1.367, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.624, D_sup_loss: 0.127, D_sup_acc: 97.00 Train acc: 96.663 Test acc: 96.890 \n",
      "step: 8262 | Train: G_Loss: 1.421, D_unsup_loss_real: 0.497, D_unsup_loss_fake: 0.521, D_sup_loss: 0.128, D_sup_acc: 96.93 Train acc: 96.330 Test acc: 96.670 \n",
      "step: 8263 | Train: G_Loss: 1.214, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.472, D_sup_loss: 0.140, D_sup_acc: 96.71 Train acc: 96.415 Test acc: 96.720 \n",
      "step: 8264 | Train: G_Loss: 1.291, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.532, D_sup_loss: 0.143, D_sup_acc: 96.76 Train acc: 96.778 Test acc: 97.090 \n",
      "step: 8265 | Train: G_Loss: 1.333, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.520, D_sup_loss: 0.127, D_sup_acc: 97.13 Train acc: 97.043 Test acc: 97.280 \n",
      "step: 8266 | Train: G_Loss: 1.248, D_unsup_loss_real: 0.464, D_unsup_loss_fake: 0.574, D_sup_loss: 0.111, D_sup_acc: 97.31 Train acc: 97.085 Test acc: 97.290 \n",
      "step: 8267 | Train: G_Loss: 1.180, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.532, D_sup_loss: 0.110, D_sup_acc: 97.32 Train acc: 96.637 Test acc: 97.000 \n",
      "step: 8268 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.533, D_sup_loss: 0.128, D_sup_acc: 97.04 Train acc: 96.750 Test acc: 97.110 \n",
      "step: 8269 | Train: G_Loss: 1.268, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.535, D_sup_loss: 0.128, D_sup_acc: 97.15 Train acc: 96.980 Test acc: 97.300 \n",
      "step: 8270 | Train: G_Loss: 1.243, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.586, D_sup_loss: 0.116, D_sup_acc: 97.33 Train acc: 96.880 Test acc: 97.210 \n",
      "step: 8271 | Train: G_Loss: 1.201, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.581, D_sup_loss: 0.121, D_sup_acc: 97.25 Train acc: 96.798 Test acc: 97.200 \n",
      "step: 8272 | Train: G_Loss: 1.288, D_unsup_loss_real: 0.524, D_unsup_loss_fake: 0.558, D_sup_loss: 0.122, D_sup_acc: 97.24 Train acc: 96.938 Test acc: 97.320 \n",
      "step: 8273 | Train: G_Loss: 1.226, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.487, D_sup_loss: 0.114, D_sup_acc: 97.35 Train acc: 96.602 Test acc: 96.960 \n",
      "step: 8274 | Train: G_Loss: 1.241, D_unsup_loss_real: 0.550, D_unsup_loss_fake: 0.613, D_sup_loss: 0.130, D_sup_acc: 97.00 Train acc: 96.510 Test acc: 96.960 \n",
      "step: 8275 | Train: G_Loss: 1.282, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.578, D_sup_loss: 0.129, D_sup_acc: 97.00 Train acc: 96.872 Test acc: 97.170 \n",
      "step: 8276 | Train: G_Loss: 1.225, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.553, D_sup_loss: 0.116, D_sup_acc: 97.21 Train acc: 96.850 Test acc: 97.170 \n",
      "step: 8277 | Train: G_Loss: 1.169, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.545, D_sup_loss: 0.118, D_sup_acc: 97.21 Train acc: 96.847 Test acc: 97.150 \n",
      "step: 8278 | Train: G_Loss: 1.347, D_unsup_loss_real: 0.472, D_unsup_loss_fake: 0.621, D_sup_loss: 0.121, D_sup_acc: 97.19 Train acc: 96.962 Test acc: 97.300 \n",
      "step: 8279 | Train: G_Loss: 1.342, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.506, D_sup_loss: 0.112, D_sup_acc: 97.33 Train acc: 96.957 Test acc: 97.300 \n",
      "step: 8280 | Train: G_Loss: 1.181, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.484, D_sup_loss: 0.115, D_sup_acc: 97.33 Train acc: 96.915 Test acc: 97.180 \n",
      "step: 8281 | Train: G_Loss: 1.290, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.538, D_sup_loss: 0.118, D_sup_acc: 97.22 Train acc: 97.003 Test acc: 97.260 \n",
      "step: 8282 | Train: G_Loss: 1.289, D_unsup_loss_real: 0.542, D_unsup_loss_fake: 0.568, D_sup_loss: 0.115, D_sup_acc: 97.29 Train acc: 97.035 Test acc: 97.300 \n",
      "step: 8283 | Train: G_Loss: 1.221, D_unsup_loss_real: 0.503, D_unsup_loss_fake: 0.549, D_sup_loss: 0.110, D_sup_acc: 97.33 Train acc: 96.933 Test acc: 97.270 \n",
      "step: 8284 | Train: G_Loss: 1.213, D_unsup_loss_real: 0.542, D_unsup_loss_fake: 0.548, D_sup_loss: 0.118, D_sup_acc: 97.30 Train acc: 96.873 Test acc: 97.240 \n",
      "step: 8285 | Train: G_Loss: 1.206, D_unsup_loss_real: 0.531, D_unsup_loss_fake: 0.533, D_sup_loss: 0.116, D_sup_acc: 97.27 Train acc: 96.692 Test acc: 97.010 \n",
      "step: 8286 | Train: G_Loss: 1.352, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.505, D_sup_loss: 0.127, D_sup_acc: 97.05 Train acc: 96.828 Test acc: 97.140 \n",
      "step: 8287 | Train: G_Loss: 1.232, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.550, D_sup_loss: 0.118, D_sup_acc: 97.18 Train acc: 96.318 Test acc: 96.690 \n",
      "step: 8288 | Train: G_Loss: 1.285, D_unsup_loss_real: 0.512, D_unsup_loss_fake: 0.620, D_sup_loss: 0.139, D_sup_acc: 96.73 Train acc: 96.365 Test acc: 96.630 \n",
      "step: 8289 | Train: G_Loss: 1.357, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.478, D_sup_loss: 0.139, D_sup_acc: 96.67 Train acc: 96.908 Test acc: 97.200 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 8290 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.514, D_sup_loss: 0.114, D_sup_acc: 97.24 Train acc: 96.658 Test acc: 96.980 \n",
      "step: 8291 | Train: G_Loss: 1.373, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.527, D_sup_loss: 0.127, D_sup_acc: 97.02 Train acc: 96.815 Test acc: 97.130 \n",
      "step: 8292 | Train: G_Loss: 1.372, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.540, D_sup_loss: 0.121, D_sup_acc: 97.17 Train acc: 96.560 Test acc: 96.960 \n",
      "step: 8293 | Train: G_Loss: 1.303, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.618, D_sup_loss: 0.134, D_sup_acc: 97.00 Train acc: 96.815 Test acc: 97.090 \n",
      "step: 8294 | Train: G_Loss: 1.280, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.530, D_sup_loss: 0.122, D_sup_acc: 97.13 Train acc: 96.335 Test acc: 96.760 \n",
      "step: 8295 | Train: G_Loss: 1.272, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.553, D_sup_loss: 0.138, D_sup_acc: 96.80 Train acc: 96.980 Test acc: 97.260 \n",
      "step: 8296 | Train: G_Loss: 1.210, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.552, D_sup_loss: 0.112, D_sup_acc: 97.29 Train acc: 96.987 Test acc: 97.150 \n",
      "step: 8297 | Train: G_Loss: 1.299, D_unsup_loss_real: 0.513, D_unsup_loss_fake: 0.612, D_sup_loss: 0.112, D_sup_acc: 97.19 Train acc: 96.977 Test acc: 97.230 \n",
      "step: 8298 | Train: G_Loss: 1.141, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.574, D_sup_loss: 0.111, D_sup_acc: 97.27 Train acc: 97.003 Test acc: 97.220 \n",
      "step: 8299 | Train: G_Loss: 1.214, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.604, D_sup_loss: 0.110, D_sup_acc: 97.26 Train acc: 96.633 Test acc: 97.050 \n",
      "step: 8300 | Train: G_Loss: 1.360, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.602, D_sup_loss: 0.129, D_sup_acc: 97.09 Train acc: 96.768 Test acc: 97.130 \n",
      "Train Classifier Accuracy: 96.768%\n",
      "\n",
      "Test Classifier Accuracy: 97.130%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_8300.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_8300.h5\n",
      "step: 8301 | Train: G_Loss: 1.238, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.495, D_sup_loss: 0.118, D_sup_acc: 97.17 Train acc: 96.917 Test acc: 97.180 \n",
      "step: 8302 | Train: G_Loss: 1.300, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.550, D_sup_loss: 0.115, D_sup_acc: 97.22 Train acc: 96.800 Test acc: 97.130 \n",
      "step: 8303 | Train: G_Loss: 1.217, D_unsup_loss_real: 0.512, D_unsup_loss_fake: 0.496, D_sup_loss: 0.116, D_sup_acc: 97.17 Train acc: 96.455 Test acc: 96.790 \n",
      "step: 8304 | Train: G_Loss: 1.337, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.585, D_sup_loss: 0.132, D_sup_acc: 96.83 Train acc: 96.543 Test acc: 96.930 \n",
      "step: 8305 | Train: G_Loss: 1.277, D_unsup_loss_real: 0.529, D_unsup_loss_fake: 0.496, D_sup_loss: 0.130, D_sup_acc: 96.97 Train acc: 96.882 Test acc: 97.210 \n",
      "step: 8306 | Train: G_Loss: 1.284, D_unsup_loss_real: 0.528, D_unsup_loss_fake: 0.564, D_sup_loss: 0.111, D_sup_acc: 97.25 Train acc: 97.010 Test acc: 97.400 \n",
      "step: 8307 | Train: G_Loss: 1.174, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.521, D_sup_loss: 0.109, D_sup_acc: 97.43 Train acc: 96.838 Test acc: 97.340 \n",
      "step: 8308 | Train: G_Loss: 1.215, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.538, D_sup_loss: 0.117, D_sup_acc: 97.37 Train acc: 96.713 Test acc: 97.070 \n",
      "step: 8309 | Train: G_Loss: 1.261, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.522, D_sup_loss: 0.125, D_sup_acc: 97.11 Train acc: 96.765 Test acc: 97.160 \n",
      "step: 8310 | Train: G_Loss: 1.245, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.604, D_sup_loss: 0.118, D_sup_acc: 97.20 Train acc: 97.107 Test acc: 97.370 \n",
      "step: 8311 | Train: G_Loss: 1.245, D_unsup_loss_real: 0.515, D_unsup_loss_fake: 0.585, D_sup_loss: 0.109, D_sup_acc: 97.40 Train acc: 96.952 Test acc: 97.210 \n",
      "step: 8312 | Train: G_Loss: 1.287, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.582, D_sup_loss: 0.116, D_sup_acc: 97.25 Train acc: 97.098 Test acc: 97.330 \n",
      "step: 8313 | Train: G_Loss: 1.249, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.645, D_sup_loss: 0.108, D_sup_acc: 97.36 Train acc: 96.765 Test acc: 97.170 \n",
      "step: 8314 | Train: G_Loss: 1.594, D_unsup_loss_real: 0.516, D_unsup_loss_fake: 0.533, D_sup_loss: 0.120, D_sup_acc: 97.21 Train acc: 97.135 Test acc: 97.360 \n",
      "step: 8315 | Train: G_Loss: 1.274, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.577, D_sup_loss: 0.102, D_sup_acc: 97.39 Train acc: 96.490 Test acc: 96.820 \n",
      "step: 8316 | Train: G_Loss: 1.353, D_unsup_loss_real: 0.685, D_unsup_loss_fake: 0.591, D_sup_loss: 0.143, D_sup_acc: 96.86 Train acc: 97.003 Test acc: 97.340 \n",
      "step: 8317 | Train: G_Loss: 1.279, D_unsup_loss_real: 0.509, D_unsup_loss_fake: 0.665, D_sup_loss: 0.106, D_sup_acc: 97.37 Train acc: 96.708 Test acc: 97.080 \n",
      "step: 8318 | Train: G_Loss: 1.192, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.554, D_sup_loss: 0.125, D_sup_acc: 97.12 Train acc: 97.112 Test acc: 97.470 \n",
      "step: 8319 | Train: G_Loss: 1.182, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.591, D_sup_loss: 0.105, D_sup_acc: 97.50 Train acc: 97.110 Test acc: 97.460 \n",
      "step: 8320 | Train: G_Loss: 1.213, D_unsup_loss_real: 0.513, D_unsup_loss_fake: 0.575, D_sup_loss: 0.106, D_sup_acc: 97.49 Train acc: 96.832 Test acc: 97.180 \n",
      "step: 8321 | Train: G_Loss: 1.319, D_unsup_loss_real: 0.541, D_unsup_loss_fake: 0.641, D_sup_loss: 0.119, D_sup_acc: 97.22 Train acc: 96.895 Test acc: 97.290 \n",
      "step: 8322 | Train: G_Loss: 1.207, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.535, D_sup_loss: 0.118, D_sup_acc: 97.32 Train acc: 97.147 Test acc: 97.470 \n",
      "step: 8323 | Train: G_Loss: 1.208, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.602, D_sup_loss: 0.108, D_sup_acc: 97.50 Train acc: 96.797 Test acc: 97.180 \n",
      "step: 8324 | Train: G_Loss: 1.324, D_unsup_loss_real: 0.507, D_unsup_loss_fake: 0.551, D_sup_loss: 0.124, D_sup_acc: 97.22 Train acc: 97.038 Test acc: 97.260 \n",
      "step: 8325 | Train: G_Loss: 1.321, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.579, D_sup_loss: 0.109, D_sup_acc: 97.29 Train acc: 96.955 Test acc: 97.280 \n",
      "step: 8326 | Train: G_Loss: 1.223, D_unsup_loss_real: 0.502, D_unsup_loss_fake: 0.553, D_sup_loss: 0.115, D_sup_acc: 97.31 Train acc: 96.952 Test acc: 97.190 \n",
      "step: 8327 | Train: G_Loss: 1.183, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.541, D_sup_loss: 0.110, D_sup_acc: 97.23 Train acc: 96.782 Test acc: 97.040 \n",
      "step: 8328 | Train: G_Loss: 1.117, D_unsup_loss_real: 0.508, D_unsup_loss_fake: 0.632, D_sup_loss: 0.123, D_sup_acc: 97.08 Train acc: 96.693 Test acc: 96.990 \n",
      "step: 8329 | Train: G_Loss: 1.247, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.661, D_sup_loss: 0.128, D_sup_acc: 97.03 Train acc: 96.957 Test acc: 97.280 \n",
      "step: 8330 | Train: G_Loss: 1.252, D_unsup_loss_real: 0.545, D_unsup_loss_fake: 0.590, D_sup_loss: 0.117, D_sup_acc: 97.31 Train acc: 96.572 Test acc: 96.880 \n",
      "step: 8331 | Train: G_Loss: 1.287, D_unsup_loss_real: 0.459, D_unsup_loss_fake: 0.489, D_sup_loss: 0.130, D_sup_acc: 96.92 Train acc: 97.008 Test acc: 97.240 \n",
      "step: 8332 | Train: G_Loss: 1.311, D_unsup_loss_real: 0.541, D_unsup_loss_fake: 0.486, D_sup_loss: 0.111, D_sup_acc: 97.27 Train acc: 97.007 Test acc: 97.190 \n",
      "step: 8333 | Train: G_Loss: 1.337, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.551, D_sup_loss: 0.114, D_sup_acc: 97.23 Train acc: 96.998 Test acc: 97.220 \n",
      "step: 8334 | Train: G_Loss: 1.293, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.524, D_sup_loss: 0.114, D_sup_acc: 97.26 Train acc: 96.968 Test acc: 97.230 \n",
      "step: 8335 | Train: G_Loss: 1.330, D_unsup_loss_real: 0.494, D_unsup_loss_fake: 0.484, D_sup_loss: 0.115, D_sup_acc: 97.27 Train acc: 97.043 Test acc: 97.300 \n",
      "step: 8336 | Train: G_Loss: 1.125, D_unsup_loss_real: 0.503, D_unsup_loss_fake: 0.509, D_sup_loss: 0.108, D_sup_acc: 97.33 Train acc: 96.908 Test acc: 97.240 \n",
      "step: 8337 | Train: G_Loss: 1.244, D_unsup_loss_real: 0.524, D_unsup_loss_fake: 0.556, D_sup_loss: 0.116, D_sup_acc: 97.27 Train acc: 96.907 Test acc: 97.160 \n",
      "step: 8338 | Train: G_Loss: 1.160, D_unsup_loss_real: 0.480, D_unsup_loss_fake: 0.490, D_sup_loss: 0.119, D_sup_acc: 97.20 Train acc: 96.935 Test acc: 97.260 \n",
      "step: 8339 | Train: G_Loss: 1.260, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.555, D_sup_loss: 0.114, D_sup_acc: 97.29 Train acc: 96.950 Test acc: 97.280 \n",
      "step: 8340 | Train: G_Loss: 1.239, D_unsup_loss_real: 0.506, D_unsup_loss_fake: 0.539, D_sup_loss: 0.112, D_sup_acc: 97.31 Train acc: 96.837 Test acc: 97.170 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 8341 | Train: G_Loss: 1.259, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.582, D_sup_loss: 0.115, D_sup_acc: 97.21 Train acc: 96.930 Test acc: 97.170 \n",
      "step: 8342 | Train: G_Loss: 1.331, D_unsup_loss_real: 0.536, D_unsup_loss_fake: 0.561, D_sup_loss: 0.114, D_sup_acc: 97.21 Train acc: 97.098 Test acc: 97.340 \n",
      "step: 8343 | Train: G_Loss: 1.149, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.557, D_sup_loss: 0.105, D_sup_acc: 97.37 Train acc: 96.658 Test acc: 96.980 \n",
      "step: 8344 | Train: G_Loss: 1.206, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.630, D_sup_loss: 0.125, D_sup_acc: 97.02 Train acc: 97.053 Test acc: 97.250 \n",
      "step: 8345 | Train: G_Loss: 1.380, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.478, D_sup_loss: 0.109, D_sup_acc: 97.28 Train acc: 96.962 Test acc: 97.240 \n",
      "step: 8346 | Train: G_Loss: 1.279, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.557, D_sup_loss: 0.115, D_sup_acc: 97.27 Train acc: 96.918 Test acc: 97.240 \n",
      "step: 8347 | Train: G_Loss: 1.290, D_unsup_loss_real: 0.542, D_unsup_loss_fake: 0.538, D_sup_loss: 0.118, D_sup_acc: 97.27 Train acc: 96.812 Test acc: 97.110 \n",
      "step: 8348 | Train: G_Loss: 1.162, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.605, D_sup_loss: 0.118, D_sup_acc: 97.15 Train acc: 96.857 Test acc: 97.160 \n",
      "step: 8349 | Train: G_Loss: 1.160, D_unsup_loss_real: 0.544, D_unsup_loss_fake: 0.592, D_sup_loss: 0.120, D_sup_acc: 97.20 Train acc: 96.955 Test acc: 97.240 \n",
      "step: 8350 | Train: G_Loss: 1.340, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.552, D_sup_loss: 0.110, D_sup_acc: 97.27 Train acc: 96.898 Test acc: 97.270 \n",
      "step: 8351 | Train: G_Loss: 1.175, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.576, D_sup_loss: 0.115, D_sup_acc: 97.30 Train acc: 96.922 Test acc: 97.300 \n",
      "step: 8352 | Train: G_Loss: 1.406, D_unsup_loss_real: 0.533, D_unsup_loss_fake: 0.584, D_sup_loss: 0.114, D_sup_acc: 97.33 Train acc: 97.150 Test acc: 97.420 \n",
      "step: 8353 | Train: G_Loss: 1.319, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.623, D_sup_loss: 0.101, D_sup_acc: 97.45 Train acc: 96.690 Test acc: 96.980 \n",
      "step: 8354 | Train: G_Loss: 1.401, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.566, D_sup_loss: 0.133, D_sup_acc: 97.02 Train acc: 96.878 Test acc: 97.180 \n",
      "step: 8355 | Train: G_Loss: 1.277, D_unsup_loss_real: 0.522, D_unsup_loss_fake: 0.574, D_sup_loss: 0.115, D_sup_acc: 97.22 Train acc: 96.847 Test acc: 97.140 \n",
      "step: 8356 | Train: G_Loss: 1.351, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.516, D_sup_loss: 0.121, D_sup_acc: 97.18 Train acc: 97.018 Test acc: 97.240 \n",
      "step: 8357 | Train: G_Loss: 1.316, D_unsup_loss_real: 0.639, D_unsup_loss_fake: 0.562, D_sup_loss: 0.113, D_sup_acc: 97.27 Train acc: 96.735 Test acc: 97.040 \n",
      "step: 8358 | Train: G_Loss: 1.289, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.606, D_sup_loss: 0.128, D_sup_acc: 97.08 Train acc: 96.940 Test acc: 97.240 \n",
      "step: 8359 | Train: G_Loss: 1.203, D_unsup_loss_real: 0.663, D_unsup_loss_fake: 0.573, D_sup_loss: 0.112, D_sup_acc: 97.27 Train acc: 96.557 Test acc: 96.830 \n",
      "step: 8360 | Train: G_Loss: 1.269, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.533, D_sup_loss: 0.134, D_sup_acc: 96.87 Train acc: 96.832 Test acc: 97.150 \n",
      "step: 8361 | Train: G_Loss: 1.366, D_unsup_loss_real: 0.478, D_unsup_loss_fake: 0.632, D_sup_loss: 0.118, D_sup_acc: 97.19 Train acc: 96.855 Test acc: 97.330 \n",
      "step: 8362 | Train: G_Loss: 1.243, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.517, D_sup_loss: 0.111, D_sup_acc: 97.36 Train acc: 96.542 Test acc: 96.920 \n",
      "step: 8363 | Train: G_Loss: 1.252, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.535, D_sup_loss: 0.136, D_sup_acc: 96.96 Train acc: 97.075 Test acc: 97.440 \n",
      "step: 8364 | Train: G_Loss: 1.273, D_unsup_loss_real: 0.521, D_unsup_loss_fake: 0.544, D_sup_loss: 0.105, D_sup_acc: 97.47 Train acc: 96.773 Test acc: 97.060 \n",
      "step: 8365 | Train: G_Loss: 1.314, D_unsup_loss_real: 0.494, D_unsup_loss_fake: 0.592, D_sup_loss: 0.120, D_sup_acc: 97.10 Train acc: 97.003 Test acc: 97.400 \n",
      "step: 8366 | Train: G_Loss: 1.248, D_unsup_loss_real: 0.684, D_unsup_loss_fake: 0.551, D_sup_loss: 0.109, D_sup_acc: 97.43 Train acc: 96.875 Test acc: 97.240 \n",
      "step: 8367 | Train: G_Loss: 1.326, D_unsup_loss_real: 0.505, D_unsup_loss_fake: 0.591, D_sup_loss: 0.120, D_sup_acc: 97.27 Train acc: 97.022 Test acc: 97.410 \n",
      "step: 8368 | Train: G_Loss: 1.308, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.558, D_sup_loss: 0.110, D_sup_acc: 97.44 Train acc: 96.997 Test acc: 97.350 \n",
      "step: 8369 | Train: G_Loss: 1.234, D_unsup_loss_real: 0.506, D_unsup_loss_fake: 0.641, D_sup_loss: 0.110, D_sup_acc: 97.38 Train acc: 96.988 Test acc: 97.300 \n",
      "step: 8370 | Train: G_Loss: 1.286, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.563, D_sup_loss: 0.112, D_sup_acc: 97.33 Train acc: 97.058 Test acc: 97.440 \n",
      "step: 8371 | Train: G_Loss: 1.264, D_unsup_loss_real: 0.525, D_unsup_loss_fake: 0.540, D_sup_loss: 0.107, D_sup_acc: 97.47 Train acc: 96.858 Test acc: 97.170 \n",
      "step: 8372 | Train: G_Loss: 1.269, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.602, D_sup_loss: 0.115, D_sup_acc: 97.21 Train acc: 96.920 Test acc: 97.210 \n",
      "step: 8373 | Train: G_Loss: 1.269, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.558, D_sup_loss: 0.111, D_sup_acc: 97.25 Train acc: 96.793 Test acc: 97.090 \n",
      "step: 8374 | Train: G_Loss: 1.281, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.567, D_sup_loss: 0.121, D_sup_acc: 97.13 Train acc: 96.638 Test acc: 96.850 \n",
      "step: 8375 | Train: G_Loss: 1.357, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.640, D_sup_loss: 0.127, D_sup_acc: 96.89 Train acc: 96.833 Test acc: 97.050 \n",
      "step: 8376 | Train: G_Loss: 1.270, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.465, D_sup_loss: 0.117, D_sup_acc: 97.09 Train acc: 96.477 Test acc: 96.700 \n",
      "step: 8377 | Train: G_Loss: 1.257, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.569, D_sup_loss: 0.138, D_sup_acc: 96.74 Train acc: 96.938 Test acc: 97.270 \n",
      "step: 8378 | Train: G_Loss: 1.173, D_unsup_loss_real: 0.532, D_unsup_loss_fake: 0.615, D_sup_loss: 0.111, D_sup_acc: 97.30 Train acc: 96.743 Test acc: 96.970 \n",
      "step: 8379 | Train: G_Loss: 1.251, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.576, D_sup_loss: 0.122, D_sup_acc: 97.01 Train acc: 96.663 Test acc: 96.880 \n",
      "step: 8380 | Train: G_Loss: 1.377, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.579, D_sup_loss: 0.127, D_sup_acc: 96.92 Train acc: 96.792 Test acc: 97.190 \n",
      "step: 8381 | Train: G_Loss: 1.249, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.559, D_sup_loss: 0.116, D_sup_acc: 97.23 Train acc: 96.488 Test acc: 96.750 \n",
      "step: 8382 | Train: G_Loss: 1.284, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.562, D_sup_loss: 0.134, D_sup_acc: 96.79 Train acc: 96.607 Test acc: 96.850 \n",
      "step: 8383 | Train: G_Loss: 1.262, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.565, D_sup_loss: 0.131, D_sup_acc: 96.89 Train acc: 96.827 Test acc: 97.110 \n",
      "step: 8384 | Train: G_Loss: 1.320, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.566, D_sup_loss: 0.117, D_sup_acc: 97.15 Train acc: 96.982 Test acc: 97.350 \n",
      "step: 8385 | Train: G_Loss: 1.279, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.545, D_sup_loss: 0.112, D_sup_acc: 97.38 Train acc: 96.992 Test acc: 97.280 \n",
      "step: 8386 | Train: G_Loss: 1.259, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.588, D_sup_loss: 0.113, D_sup_acc: 97.31 Train acc: 96.840 Test acc: 97.140 \n",
      "step: 8387 | Train: G_Loss: 1.236, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.583, D_sup_loss: 0.122, D_sup_acc: 97.18 Train acc: 97.015 Test acc: 97.270 \n",
      "step: 8388 | Train: G_Loss: 1.182, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.602, D_sup_loss: 0.109, D_sup_acc: 97.30 Train acc: 96.782 Test acc: 97.060 \n",
      "step: 8389 | Train: G_Loss: 1.541, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.512, D_sup_loss: 0.122, D_sup_acc: 97.10 Train acc: 96.982 Test acc: 97.190 \n",
      "step: 8390 | Train: G_Loss: 1.498, D_unsup_loss_real: 0.690, D_unsup_loss_fake: 0.488, D_sup_loss: 0.106, D_sup_acc: 97.23 Train acc: 95.312 Test acc: 95.510 \n",
      "step: 8391 | Train: G_Loss: 1.221, D_unsup_loss_real: 0.815, D_unsup_loss_fake: 0.674, D_sup_loss: 0.205, D_sup_acc: 95.57 Train acc: 96.685 Test acc: 96.910 \n",
      "step: 8392 | Train: G_Loss: 1.299, D_unsup_loss_real: 0.493, D_unsup_loss_fake: 0.628, D_sup_loss: 0.122, D_sup_acc: 96.95 Train acc: 97.045 Test acc: 97.340 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 8393 | Train: G_Loss: 1.216, D_unsup_loss_real: 0.488, D_unsup_loss_fake: 0.652, D_sup_loss: 0.108, D_sup_acc: 97.37 Train acc: 96.970 Test acc: 97.240 \n",
      "step: 8394 | Train: G_Loss: 1.288, D_unsup_loss_real: 0.529, D_unsup_loss_fake: 0.546, D_sup_loss: 0.112, D_sup_acc: 97.27 Train acc: 96.962 Test acc: 97.140 \n",
      "step: 8395 | Train: G_Loss: 1.220, D_unsup_loss_real: 0.521, D_unsup_loss_fake: 0.581, D_sup_loss: 0.111, D_sup_acc: 97.18 Train acc: 96.732 Test acc: 97.140 \n",
      "step: 8396 | Train: G_Loss: 1.173, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.506, D_sup_loss: 0.123, D_sup_acc: 97.18 Train acc: 96.830 Test acc: 97.240 \n",
      "step: 8397 | Train: G_Loss: 1.210, D_unsup_loss_real: 0.496, D_unsup_loss_fake: 0.511, D_sup_loss: 0.118, D_sup_acc: 97.27 Train acc: 96.898 Test acc: 97.230 \n",
      "step: 8398 | Train: G_Loss: 1.205, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.567, D_sup_loss: 0.112, D_sup_acc: 97.27 Train acc: 96.843 Test acc: 97.150 \n",
      "step: 8399 | Train: G_Loss: 1.281, D_unsup_loss_real: 0.500, D_unsup_loss_fake: 0.555, D_sup_loss: 0.119, D_sup_acc: 97.19 Train acc: 96.900 Test acc: 97.160 \n",
      "step: 8400 | Train: G_Loss: 1.248, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.571, D_sup_loss: 0.112, D_sup_acc: 97.20 Train acc: 96.785 Test acc: 97.090 \n",
      "Train Classifier Accuracy: 96.785%\n",
      "\n",
      "Test Classifier Accuracy: 97.090%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_8400.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_8400.h5\n",
      "step: 8401 | Train: G_Loss: 1.240, D_unsup_loss_real: 0.535, D_unsup_loss_fake: 0.585, D_sup_loss: 0.120, D_sup_acc: 97.13 Train acc: 96.828 Test acc: 97.150 \n",
      "step: 8402 | Train: G_Loss: 1.342, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.537, D_sup_loss: 0.118, D_sup_acc: 97.19 Train acc: 96.677 Test acc: 96.980 \n",
      "step: 8403 | Train: G_Loss: 1.290, D_unsup_loss_real: 0.523, D_unsup_loss_fake: 0.508, D_sup_loss: 0.123, D_sup_acc: 97.02 Train acc: 96.972 Test acc: 97.280 \n",
      "step: 8404 | Train: G_Loss: 1.234, D_unsup_loss_real: 0.542, D_unsup_loss_fake: 0.636, D_sup_loss: 0.110, D_sup_acc: 97.31 Train acc: 96.978 Test acc: 97.290 \n",
      "step: 8405 | Train: G_Loss: 1.203, D_unsup_loss_real: 0.484, D_unsup_loss_fake: 0.575, D_sup_loss: 0.109, D_sup_acc: 97.32 Train acc: 96.900 Test acc: 97.200 \n",
      "step: 8406 | Train: G_Loss: 1.323, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.565, D_sup_loss: 0.115, D_sup_acc: 97.24 Train acc: 96.813 Test acc: 97.160 \n",
      "step: 8407 | Train: G_Loss: 1.379, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.538, D_sup_loss: 0.118, D_sup_acc: 97.20 Train acc: 96.772 Test acc: 97.160 \n",
      "step: 8408 | Train: G_Loss: 1.194, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.528, D_sup_loss: 0.120, D_sup_acc: 97.20 Train acc: 96.683 Test acc: 97.010 \n",
      "step: 8409 | Train: G_Loss: 1.327, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.616, D_sup_loss: 0.125, D_sup_acc: 97.05 Train acc: 96.787 Test acc: 97.130 \n",
      "step: 8410 | Train: G_Loss: 1.398, D_unsup_loss_real: 0.515, D_unsup_loss_fake: 0.565, D_sup_loss: 0.123, D_sup_acc: 97.17 Train acc: 96.533 Test acc: 96.920 \n",
      "step: 8411 | Train: G_Loss: 1.345, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.562, D_sup_loss: 0.127, D_sup_acc: 96.96 Train acc: 96.448 Test acc: 96.910 \n",
      "step: 8412 | Train: G_Loss: 1.205, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.523, D_sup_loss: 0.136, D_sup_acc: 96.95 Train acc: 96.693 Test acc: 97.080 \n",
      "step: 8413 | Train: G_Loss: 1.371, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.500, D_sup_loss: 0.124, D_sup_acc: 97.12 Train acc: 96.407 Test acc: 96.840 \n",
      "step: 8414 | Train: G_Loss: 1.247, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.568, D_sup_loss: 0.134, D_sup_acc: 96.88 Train acc: 96.137 Test acc: 96.490 \n",
      "step: 8415 | Train: G_Loss: 1.436, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.597, D_sup_loss: 0.149, D_sup_acc: 96.53 Train acc: 96.700 Test acc: 97.010 \n",
      "step: 8416 | Train: G_Loss: 1.320, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.496, D_sup_loss: 0.123, D_sup_acc: 97.05 Train acc: 96.463 Test acc: 96.780 \n",
      "step: 8417 | Train: G_Loss: 1.273, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.548, D_sup_loss: 0.134, D_sup_acc: 96.82 Train acc: 96.925 Test acc: 97.240 \n",
      "step: 8418 | Train: G_Loss: 1.307, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.570, D_sup_loss: 0.114, D_sup_acc: 97.27 Train acc: 96.595 Test acc: 96.840 \n",
      "step: 8419 | Train: G_Loss: 1.235, D_unsup_loss_real: 0.534, D_unsup_loss_fake: 0.571, D_sup_loss: 0.130, D_sup_acc: 96.88 Train acc: 96.803 Test acc: 97.060 \n",
      "step: 8420 | Train: G_Loss: 1.284, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.575, D_sup_loss: 0.120, D_sup_acc: 97.10 Train acc: 96.698 Test acc: 97.030 \n",
      "step: 8421 | Train: G_Loss: 1.176, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.564, D_sup_loss: 0.128, D_sup_acc: 97.07 Train acc: 96.867 Test acc: 97.240 \n",
      "step: 8422 | Train: G_Loss: 1.192, D_unsup_loss_real: 0.518, D_unsup_loss_fake: 0.539, D_sup_loss: 0.118, D_sup_acc: 97.27 Train acc: 96.892 Test acc: 97.100 \n",
      "step: 8423 | Train: G_Loss: 1.226, D_unsup_loss_real: 0.538, D_unsup_loss_fake: 0.509, D_sup_loss: 0.115, D_sup_acc: 97.14 Train acc: 96.670 Test acc: 96.980 \n",
      "step: 8424 | Train: G_Loss: 1.196, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.555, D_sup_loss: 0.128, D_sup_acc: 97.02 Train acc: 96.833 Test acc: 97.120 \n",
      "step: 8425 | Train: G_Loss: 1.220, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.536, D_sup_loss: 0.116, D_sup_acc: 97.16 Train acc: 96.812 Test acc: 97.120 \n",
      "step: 8426 | Train: G_Loss: 1.312, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.509, D_sup_loss: 0.118, D_sup_acc: 97.16 Train acc: 96.650 Test acc: 96.980 \n",
      "step: 8427 | Train: G_Loss: 1.320, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.552, D_sup_loss: 0.127, D_sup_acc: 97.02 Train acc: 96.760 Test acc: 97.110 \n",
      "step: 8428 | Train: G_Loss: 1.155, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.563, D_sup_loss: 0.119, D_sup_acc: 97.15 Train acc: 96.502 Test acc: 96.870 \n",
      "step: 8429 | Train: G_Loss: 1.126, D_unsup_loss_real: 0.513, D_unsup_loss_fake: 0.561, D_sup_loss: 0.130, D_sup_acc: 96.91 Train acc: 96.790 Test acc: 97.130 \n",
      "step: 8430 | Train: G_Loss: 1.348, D_unsup_loss_real: 0.532, D_unsup_loss_fake: 0.545, D_sup_loss: 0.117, D_sup_acc: 97.17 Train acc: 96.608 Test acc: 96.960 \n",
      "step: 8431 | Train: G_Loss: 1.394, D_unsup_loss_real: 0.500, D_unsup_loss_fake: 0.619, D_sup_loss: 0.122, D_sup_acc: 97.00 Train acc: 96.883 Test acc: 97.190 \n",
      "step: 8432 | Train: G_Loss: 1.186, D_unsup_loss_real: 0.528, D_unsup_loss_fake: 0.502, D_sup_loss: 0.110, D_sup_acc: 97.23 Train acc: 96.687 Test acc: 97.140 \n",
      "step: 8433 | Train: G_Loss: 1.238, D_unsup_loss_real: 0.535, D_unsup_loss_fake: 0.541, D_sup_loss: 0.126, D_sup_acc: 97.18 Train acc: 96.955 Test acc: 97.340 \n",
      "step: 8434 | Train: G_Loss: 1.406, D_unsup_loss_real: 0.535, D_unsup_loss_fake: 0.512, D_sup_loss: 0.109, D_sup_acc: 97.37 Train acc: 96.957 Test acc: 97.220 \n",
      "step: 8435 | Train: G_Loss: 1.298, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.525, D_sup_loss: 0.104, D_sup_acc: 97.26 Train acc: 96.813 Test acc: 97.240 \n",
      "step: 8436 | Train: G_Loss: 1.219, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.536, D_sup_loss: 0.122, D_sup_acc: 97.27 Train acc: 97.015 Test acc: 97.420 \n",
      "step: 8437 | Train: G_Loss: 1.291, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.619, D_sup_loss: 0.108, D_sup_acc: 97.45 Train acc: 96.688 Test acc: 97.090 \n",
      "step: 8438 | Train: G_Loss: 1.160, D_unsup_loss_real: 0.526, D_unsup_loss_fake: 0.627, D_sup_loss: 0.121, D_sup_acc: 97.13 Train acc: 96.680 Test acc: 97.090 \n",
      "step: 8439 | Train: G_Loss: 1.207, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.548, D_sup_loss: 0.127, D_sup_acc: 97.13 Train acc: 97.000 Test acc: 97.350 \n",
      "step: 8440 | Train: G_Loss: 1.247, D_unsup_loss_real: 0.503, D_unsup_loss_fake: 0.544, D_sup_loss: 0.108, D_sup_acc: 97.38 Train acc: 96.770 Test acc: 97.190 \n",
      "step: 8441 | Train: G_Loss: 1.295, D_unsup_loss_real: 0.538, D_unsup_loss_fake: 0.567, D_sup_loss: 0.116, D_sup_acc: 97.23 Train acc: 96.960 Test acc: 97.250 \n",
      "step: 8442 | Train: G_Loss: 1.280, D_unsup_loss_real: 0.629, D_unsup_loss_fake: 0.513, D_sup_loss: 0.108, D_sup_acc: 97.28 Train acc: 96.688 Test acc: 97.090 \n",
      "step: 8443 | Train: G_Loss: 1.213, D_unsup_loss_real: 0.544, D_unsup_loss_fake: 0.594, D_sup_loss: 0.122, D_sup_acc: 97.13 Train acc: 96.878 Test acc: 97.260 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 8444 | Train: G_Loss: 1.297, D_unsup_loss_real: 0.506, D_unsup_loss_fake: 0.537, D_sup_loss: 0.112, D_sup_acc: 97.29 Train acc: 96.887 Test acc: 97.240 \n",
      "step: 8445 | Train: G_Loss: 1.310, D_unsup_loss_real: 0.511, D_unsup_loss_fake: 0.552, D_sup_loss: 0.110, D_sup_acc: 97.27 Train acc: 96.638 Test acc: 97.060 \n",
      "step: 8446 | Train: G_Loss: 1.327, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.612, D_sup_loss: 0.123, D_sup_acc: 97.10 Train acc: 97.057 Test acc: 97.410 \n",
      "step: 8447 | Train: G_Loss: 1.406, D_unsup_loss_real: 0.528, D_unsup_loss_fake: 0.550, D_sup_loss: 0.106, D_sup_acc: 97.44 Train acc: 96.830 Test acc: 97.240 \n",
      "step: 8448 | Train: G_Loss: 1.269, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.598, D_sup_loss: 0.113, D_sup_acc: 97.27 Train acc: 96.733 Test acc: 97.040 \n",
      "step: 8449 | Train: G_Loss: 1.317, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.488, D_sup_loss: 0.118, D_sup_acc: 97.08 Train acc: 96.928 Test acc: 97.120 \n",
      "step: 8450 | Train: G_Loss: 1.319, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.536, D_sup_loss: 0.110, D_sup_acc: 97.16 Train acc: 96.408 Test acc: 96.790 \n",
      "step: 8451 | Train: G_Loss: 1.302, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.605, D_sup_loss: 0.132, D_sup_acc: 96.83 Train acc: 96.512 Test acc: 96.740 \n",
      "step: 8452 | Train: G_Loss: 1.343, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.504, D_sup_loss: 0.131, D_sup_acc: 96.78 Train acc: 96.713 Test acc: 97.110 \n",
      "step: 8453 | Train: G_Loss: 1.244, D_unsup_loss_real: 0.515, D_unsup_loss_fake: 0.541, D_sup_loss: 0.117, D_sup_acc: 97.15 Train acc: 96.748 Test acc: 97.100 \n",
      "step: 8454 | Train: G_Loss: 1.224, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.574, D_sup_loss: 0.116, D_sup_acc: 97.14 Train acc: 96.735 Test acc: 97.050 \n",
      "step: 8455 | Train: G_Loss: 1.311, D_unsup_loss_real: 0.483, D_unsup_loss_fake: 0.493, D_sup_loss: 0.121, D_sup_acc: 97.09 Train acc: 96.988 Test acc: 97.320 \n",
      "step: 8456 | Train: G_Loss: 1.228, D_unsup_loss_real: 0.632, D_unsup_loss_fake: 0.544, D_sup_loss: 0.108, D_sup_acc: 97.35 Train acc: 96.900 Test acc: 97.250 \n",
      "step: 8457 | Train: G_Loss: 1.230, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.611, D_sup_loss: 0.116, D_sup_acc: 97.28 Train acc: 96.977 Test acc: 97.330 \n",
      "step: 8458 | Train: G_Loss: 1.219, D_unsup_loss_real: 0.505, D_unsup_loss_fake: 0.527, D_sup_loss: 0.107, D_sup_acc: 97.36 Train acc: 96.677 Test acc: 96.970 \n",
      "step: 8459 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.518, D_sup_loss: 0.124, D_sup_acc: 97.01 Train acc: 96.955 Test acc: 97.320 \n",
      "step: 8460 | Train: G_Loss: 1.257, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.563, D_sup_loss: 0.111, D_sup_acc: 97.35 Train acc: 96.423 Test acc: 96.820 \n",
      "step: 8461 | Train: G_Loss: 1.289, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.558, D_sup_loss: 0.130, D_sup_acc: 96.86 Train acc: 96.920 Test acc: 97.380 \n",
      "step: 8462 | Train: G_Loss: 1.264, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.637, D_sup_loss: 0.109, D_sup_acc: 97.41 Train acc: 96.698 Test acc: 97.190 \n",
      "step: 8463 | Train: G_Loss: 1.238, D_unsup_loss_real: 0.465, D_unsup_loss_fake: 0.635, D_sup_loss: 0.123, D_sup_acc: 97.23 Train acc: 96.867 Test acc: 97.320 \n",
      "step: 8464 | Train: G_Loss: 1.256, D_unsup_loss_real: 0.528, D_unsup_loss_fake: 0.511, D_sup_loss: 0.112, D_sup_acc: 97.35 Train acc: 96.983 Test acc: 97.360 \n",
      "step: 8465 | Train: G_Loss: 1.250, D_unsup_loss_real: 0.470, D_unsup_loss_fake: 0.591, D_sup_loss: 0.112, D_sup_acc: 97.39 Train acc: 96.853 Test acc: 97.300 \n",
      "step: 8466 | Train: G_Loss: 1.411, D_unsup_loss_real: 0.544, D_unsup_loss_fake: 0.550, D_sup_loss: 0.116, D_sup_acc: 97.33 Train acc: 96.982 Test acc: 97.380 \n",
      "step: 8467 | Train: G_Loss: 1.146, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.505, D_sup_loss: 0.110, D_sup_acc: 97.41 Train acc: 96.895 Test acc: 97.320 \n",
      "step: 8468 | Train: G_Loss: 1.244, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.548, D_sup_loss: 0.118, D_sup_acc: 97.35 Train acc: 96.757 Test acc: 97.230 \n",
      "step: 8469 | Train: G_Loss: 1.278, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.600, D_sup_loss: 0.125, D_sup_acc: 97.27 Train acc: 96.975 Test acc: 97.370 \n",
      "step: 8470 | Train: G_Loss: 1.262, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.527, D_sup_loss: 0.114, D_sup_acc: 97.40 Train acc: 96.938 Test acc: 97.210 \n",
      "step: 8471 | Train: G_Loss: 1.300, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.561, D_sup_loss: 0.116, D_sup_acc: 97.25 Train acc: 97.075 Test acc: 97.380 \n",
      "step: 8472 | Train: G_Loss: 1.335, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.501, D_sup_loss: 0.107, D_sup_acc: 97.41 Train acc: 97.002 Test acc: 97.240 \n",
      "step: 8473 | Train: G_Loss: 1.293, D_unsup_loss_real: 0.488, D_unsup_loss_fake: 0.561, D_sup_loss: 0.113, D_sup_acc: 97.27 Train acc: 96.987 Test acc: 97.330 \n",
      "step: 8474 | Train: G_Loss: 1.312, D_unsup_loss_real: 0.482, D_unsup_loss_fake: 0.555, D_sup_loss: 0.110, D_sup_acc: 97.36 Train acc: 96.707 Test acc: 97.040 \n",
      "step: 8475 | Train: G_Loss: 1.189, D_unsup_loss_real: 0.522, D_unsup_loss_fake: 0.515, D_sup_loss: 0.130, D_sup_acc: 97.08 Train acc: 97.022 Test acc: 97.290 \n",
      "step: 8476 | Train: G_Loss: 1.177, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.572, D_sup_loss: 0.108, D_sup_acc: 97.32 Train acc: 96.683 Test acc: 97.010 \n",
      "step: 8477 | Train: G_Loss: 1.245, D_unsup_loss_real: 0.494, D_unsup_loss_fake: 0.591, D_sup_loss: 0.126, D_sup_acc: 97.05 Train acc: 96.882 Test acc: 97.270 \n",
      "step: 8478 | Train: G_Loss: 1.426, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.505, D_sup_loss: 0.116, D_sup_acc: 97.30 Train acc: 96.980 Test acc: 97.270 \n",
      "step: 8479 | Train: G_Loss: 1.265, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.517, D_sup_loss: 0.112, D_sup_acc: 97.30 Train acc: 96.878 Test acc: 97.210 \n",
      "step: 8480 | Train: G_Loss: 1.340, D_unsup_loss_real: 0.525, D_unsup_loss_fake: 0.548, D_sup_loss: 0.120, D_sup_acc: 97.25 Train acc: 96.888 Test acc: 97.230 \n",
      "step: 8481 | Train: G_Loss: 1.351, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.513, D_sup_loss: 0.116, D_sup_acc: 97.27 Train acc: 97.023 Test acc: 97.350 \n",
      "step: 8482 | Train: G_Loss: 1.339, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.592, D_sup_loss: 0.105, D_sup_acc: 97.38 Train acc: 96.957 Test acc: 97.290 \n",
      "step: 8483 | Train: G_Loss: 1.211, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.557, D_sup_loss: 0.110, D_sup_acc: 97.32 Train acc: 96.397 Test acc: 96.800 \n",
      "step: 8484 | Train: G_Loss: 1.371, D_unsup_loss_real: 0.530, D_unsup_loss_fake: 0.597, D_sup_loss: 0.137, D_sup_acc: 96.84 Train acc: 97.002 Test acc: 97.310 \n",
      "step: 8485 | Train: G_Loss: 1.305, D_unsup_loss_real: 0.494, D_unsup_loss_fake: 0.594, D_sup_loss: 0.105, D_sup_acc: 97.34 Train acc: 97.030 Test acc: 97.390 \n",
      "step: 8486 | Train: G_Loss: 1.321, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.514, D_sup_loss: 0.107, D_sup_acc: 97.42 Train acc: 96.668 Test acc: 96.950 \n",
      "step: 8487 | Train: G_Loss: 1.316, D_unsup_loss_real: 0.542, D_unsup_loss_fake: 0.557, D_sup_loss: 0.123, D_sup_acc: 96.99 Train acc: 96.960 Test acc: 97.280 \n",
      "step: 8488 | Train: G_Loss: 1.327, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.574, D_sup_loss: 0.109, D_sup_acc: 97.31 Train acc: 96.575 Test acc: 96.960 \n",
      "step: 8489 | Train: G_Loss: 1.328, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.505, D_sup_loss: 0.128, D_sup_acc: 97.00 Train acc: 96.945 Test acc: 97.300 \n",
      "step: 8490 | Train: G_Loss: 1.353, D_unsup_loss_real: 0.510, D_unsup_loss_fake: 0.487, D_sup_loss: 0.112, D_sup_acc: 97.33 Train acc: 96.923 Test acc: 97.110 \n",
      "step: 8491 | Train: G_Loss: 1.270, D_unsup_loss_real: 0.504, D_unsup_loss_fake: 0.605, D_sup_loss: 0.108, D_sup_acc: 97.15 Train acc: 96.642 Test acc: 97.080 \n",
      "step: 8492 | Train: G_Loss: 1.327, D_unsup_loss_real: 0.538, D_unsup_loss_fake: 0.503, D_sup_loss: 0.122, D_sup_acc: 97.12 Train acc: 96.802 Test acc: 97.170 \n",
      "step: 8493 | Train: G_Loss: 1.183, D_unsup_loss_real: 0.667, D_unsup_loss_fake: 0.545, D_sup_loss: 0.114, D_sup_acc: 97.21 Train acc: 96.518 Test acc: 96.790 \n",
      "step: 8494 | Train: G_Loss: 1.216, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.604, D_sup_loss: 0.135, D_sup_acc: 96.83 Train acc: 97.038 Test acc: 97.400 \n",
      "step: 8495 | Train: G_Loss: 1.163, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.557, D_sup_loss: 0.107, D_sup_acc: 97.43 Train acc: 96.978 Test acc: 97.370 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 8496 | Train: G_Loss: 1.244, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.531, D_sup_loss: 0.109, D_sup_acc: 97.40 Train acc: 96.708 Test acc: 97.120 \n",
      "step: 8497 | Train: G_Loss: 1.222, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.600, D_sup_loss: 0.123, D_sup_acc: 97.16 Train acc: 97.008 Test acc: 97.380 \n",
      "step: 8498 | Train: G_Loss: 1.212, D_unsup_loss_real: 0.523, D_unsup_loss_fake: 0.585, D_sup_loss: 0.108, D_sup_acc: 97.41 Train acc: 96.552 Test acc: 96.960 \n",
      "step: 8499 | Train: G_Loss: 1.218, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.541, D_sup_loss: 0.126, D_sup_acc: 97.00 Train acc: 96.533 Test acc: 96.790 \n",
      "step: 8500 | Train: G_Loss: 1.293, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.607, D_sup_loss: 0.128, D_sup_acc: 96.83 Train acc: 96.903 Test acc: 97.240 \n",
      "Train Classifier Accuracy: 96.903%\n",
      "\n",
      "Test Classifier Accuracy: 97.240%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_8500.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_8500.h5\n",
      "step: 8501 | Train: G_Loss: 1.302, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.573, D_sup_loss: 0.111, D_sup_acc: 97.27 Train acc: 96.748 Test acc: 97.070 \n",
      "step: 8502 | Train: G_Loss: 1.359, D_unsup_loss_real: 0.492, D_unsup_loss_fake: 0.583, D_sup_loss: 0.123, D_sup_acc: 97.11 Train acc: 96.642 Test acc: 96.990 \n",
      "step: 8503 | Train: G_Loss: 1.384, D_unsup_loss_real: 0.523, D_unsup_loss_fake: 0.575, D_sup_loss: 0.125, D_sup_acc: 97.03 Train acc: 96.698 Test acc: 97.080 \n",
      "step: 8504 | Train: G_Loss: 1.338, D_unsup_loss_real: 0.638, D_unsup_loss_fake: 0.526, D_sup_loss: 0.120, D_sup_acc: 97.12 Train acc: 96.810 Test acc: 97.150 \n",
      "step: 8505 | Train: G_Loss: 1.248, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.549, D_sup_loss: 0.123, D_sup_acc: 97.19 Train acc: 96.928 Test acc: 97.260 \n",
      "step: 8506 | Train: G_Loss: 1.211, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.635, D_sup_loss: 0.113, D_sup_acc: 97.29 Train acc: 97.115 Test acc: 97.340 \n",
      "step: 8507 | Train: G_Loss: 1.235, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.569, D_sup_loss: 0.101, D_sup_acc: 97.37 Train acc: 96.530 Test acc: 96.800 \n",
      "step: 8508 | Train: G_Loss: 1.291, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.500, D_sup_loss: 0.139, D_sup_acc: 96.84 Train acc: 97.130 Test acc: 97.430 \n",
      "step: 8509 | Train: G_Loss: 1.152, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.673, D_sup_loss: 0.103, D_sup_acc: 97.46 Train acc: 96.658 Test acc: 97.020 \n",
      "step: 8510 | Train: G_Loss: 1.218, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.591, D_sup_loss: 0.130, D_sup_acc: 97.06 Train acc: 96.933 Test acc: 97.290 \n",
      "step: 8511 | Train: G_Loss: 1.297, D_unsup_loss_real: 0.518, D_unsup_loss_fake: 0.549, D_sup_loss: 0.111, D_sup_acc: 97.32 Train acc: 96.830 Test acc: 97.190 \n",
      "step: 8512 | Train: G_Loss: 1.358, D_unsup_loss_real: 0.508, D_unsup_loss_fake: 0.594, D_sup_loss: 0.117, D_sup_acc: 97.23 Train acc: 96.973 Test acc: 97.370 \n",
      "step: 8513 | Train: G_Loss: 1.192, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.463, D_sup_loss: 0.110, D_sup_acc: 97.40 Train acc: 96.955 Test acc: 97.270 \n",
      "step: 8514 | Train: G_Loss: 1.265, D_unsup_loss_real: 0.534, D_unsup_loss_fake: 0.560, D_sup_loss: 0.111, D_sup_acc: 97.30 Train acc: 97.022 Test acc: 97.360 \n",
      "step: 8515 | Train: G_Loss: 1.218, D_unsup_loss_real: 0.506, D_unsup_loss_fake: 0.538, D_sup_loss: 0.110, D_sup_acc: 97.39 Train acc: 97.100 Test acc: 97.450 \n",
      "step: 8516 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.643, D_sup_loss: 0.105, D_sup_acc: 97.48 Train acc: 96.725 Test acc: 97.180 \n",
      "step: 8517 | Train: G_Loss: 1.193, D_unsup_loss_real: 0.531, D_unsup_loss_fake: 0.610, D_sup_loss: 0.124, D_sup_acc: 97.22 Train acc: 97.097 Test acc: 97.300 \n",
      "step: 8518 | Train: G_Loss: 1.410, D_unsup_loss_real: 0.525, D_unsup_loss_fake: 0.628, D_sup_loss: 0.108, D_sup_acc: 97.33 Train acc: 96.883 Test acc: 97.220 \n",
      "step: 8519 | Train: G_Loss: 1.241, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.539, D_sup_loss: 0.118, D_sup_acc: 97.26 Train acc: 96.958 Test acc: 97.160 \n",
      "step: 8520 | Train: G_Loss: 1.333, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.527, D_sup_loss: 0.118, D_sup_acc: 97.20 Train acc: 96.980 Test acc: 97.270 \n",
      "step: 8521 | Train: G_Loss: 1.260, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.521, D_sup_loss: 0.110, D_sup_acc: 97.30 Train acc: 96.890 Test acc: 97.240 \n",
      "step: 8522 | Train: G_Loss: 1.138, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.558, D_sup_loss: 0.116, D_sup_acc: 97.27 Train acc: 96.805 Test acc: 97.270 \n",
      "step: 8523 | Train: G_Loss: 1.359, D_unsup_loss_real: 0.536, D_unsup_loss_fake: 0.597, D_sup_loss: 0.123, D_sup_acc: 97.30 Train acc: 96.942 Test acc: 97.350 \n",
      "step: 8524 | Train: G_Loss: 1.257, D_unsup_loss_real: 0.546, D_unsup_loss_fake: 0.529, D_sup_loss: 0.114, D_sup_acc: 97.38 Train acc: 96.810 Test acc: 97.280 \n",
      "step: 8525 | Train: G_Loss: 1.197, D_unsup_loss_real: 0.521, D_unsup_loss_fake: 0.576, D_sup_loss: 0.116, D_sup_acc: 97.31 Train acc: 96.962 Test acc: 97.400 \n",
      "step: 8526 | Train: G_Loss: 1.271, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.638, D_sup_loss: 0.114, D_sup_acc: 97.43 Train acc: 96.898 Test acc: 97.370 \n",
      "step: 8527 | Train: G_Loss: 1.391, D_unsup_loss_real: 0.504, D_unsup_loss_fake: 0.605, D_sup_loss: 0.113, D_sup_acc: 97.40 Train acc: 97.048 Test acc: 97.350 \n",
      "step: 8528 | Train: G_Loss: 1.263, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.524, D_sup_loss: 0.108, D_sup_acc: 97.38 Train acc: 97.078 Test acc: 97.330 \n",
      "step: 8529 | Train: G_Loss: 1.244, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.528, D_sup_loss: 0.106, D_sup_acc: 97.36 Train acc: 97.040 Test acc: 97.320 \n",
      "step: 8530 | Train: G_Loss: 1.160, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.504, D_sup_loss: 0.111, D_sup_acc: 97.35 Train acc: 97.085 Test acc: 97.380 \n",
      "step: 8531 | Train: G_Loss: 1.282, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.576, D_sup_loss: 0.109, D_sup_acc: 97.41 Train acc: 96.918 Test acc: 97.300 \n",
      "step: 8532 | Train: G_Loss: 1.256, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.557, D_sup_loss: 0.114, D_sup_acc: 97.33 Train acc: 97.095 Test acc: 97.270 \n",
      "step: 8533 | Train: G_Loss: 1.331, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.599, D_sup_loss: 0.109, D_sup_acc: 97.30 Train acc: 96.657 Test acc: 97.030 \n",
      "step: 8534 | Train: G_Loss: 1.309, D_unsup_loss_real: 0.536, D_unsup_loss_fake: 0.576, D_sup_loss: 0.128, D_sup_acc: 97.07 Train acc: 96.970 Test acc: 97.320 \n",
      "step: 8535 | Train: G_Loss: 1.385, D_unsup_loss_real: 0.484, D_unsup_loss_fake: 0.492, D_sup_loss: 0.113, D_sup_acc: 97.35 Train acc: 96.868 Test acc: 97.220 \n",
      "step: 8536 | Train: G_Loss: 1.217, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.585, D_sup_loss: 0.112, D_sup_acc: 97.26 Train acc: 96.652 Test acc: 96.970 \n",
      "step: 8537 | Train: G_Loss: 1.251, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.431, D_sup_loss: 0.128, D_sup_acc: 97.01 Train acc: 96.997 Test acc: 97.270 \n",
      "step: 8538 | Train: G_Loss: 1.276, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.543, D_sup_loss: 0.107, D_sup_acc: 97.30 Train acc: 96.813 Test acc: 97.220 \n",
      "step: 8539 | Train: G_Loss: 1.208, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.637, D_sup_loss: 0.116, D_sup_acc: 97.26 Train acc: 96.748 Test acc: 97.160 \n",
      "step: 8540 | Train: G_Loss: 1.208, D_unsup_loss_real: 0.531, D_unsup_loss_fake: 0.531, D_sup_loss: 0.119, D_sup_acc: 97.20 Train acc: 96.660 Test acc: 97.010 \n",
      "step: 8541 | Train: G_Loss: 1.312, D_unsup_loss_real: 0.525, D_unsup_loss_fake: 0.548, D_sup_loss: 0.123, D_sup_acc: 97.05 Train acc: 97.025 Test acc: 97.200 \n",
      "step: 8542 | Train: G_Loss: 1.218, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.618, D_sup_loss: 0.106, D_sup_acc: 97.24 Train acc: 96.758 Test acc: 97.120 \n",
      "step: 8543 | Train: G_Loss: 1.344, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.585, D_sup_loss: 0.125, D_sup_acc: 97.16 Train acc: 96.975 Test acc: 97.250 \n",
      "step: 8544 | Train: G_Loss: 1.384, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.605, D_sup_loss: 0.112, D_sup_acc: 97.28 Train acc: 96.875 Test acc: 97.200 \n",
      "step: 8545 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.566, D_sup_loss: 0.114, D_sup_acc: 97.24 Train acc: 96.537 Test acc: 96.900 \n",
      "step: 8546 | Train: G_Loss: 1.189, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.467, D_sup_loss: 0.135, D_sup_acc: 96.94 Train acc: 96.840 Test acc: 97.180 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 8547 | Train: G_Loss: 1.248, D_unsup_loss_real: 0.493, D_unsup_loss_fake: 0.540, D_sup_loss: 0.119, D_sup_acc: 97.22 Train acc: 96.862 Test acc: 97.220 \n",
      "step: 8548 | Train: G_Loss: 1.256, D_unsup_loss_real: 0.531, D_unsup_loss_fake: 0.563, D_sup_loss: 0.113, D_sup_acc: 97.26 Train acc: 96.740 Test acc: 97.100 \n",
      "step: 8549 | Train: G_Loss: 1.189, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.596, D_sup_loss: 0.118, D_sup_acc: 97.14 Train acc: 97.025 Test acc: 97.330 \n",
      "step: 8550 | Train: G_Loss: 1.181, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.585, D_sup_loss: 0.107, D_sup_acc: 97.36 Train acc: 96.455 Test acc: 96.810 \n",
      "step: 8551 | Train: G_Loss: 1.344, D_unsup_loss_real: 0.532, D_unsup_loss_fake: 0.649, D_sup_loss: 0.133, D_sup_acc: 96.85 Train acc: 96.838 Test acc: 97.180 \n",
      "step: 8552 | Train: G_Loss: 1.198, D_unsup_loss_real: 0.536, D_unsup_loss_fake: 0.577, D_sup_loss: 0.115, D_sup_acc: 97.22 Train acc: 96.732 Test acc: 97.110 \n",
      "step: 8553 | Train: G_Loss: 1.383, D_unsup_loss_real: 0.525, D_unsup_loss_fake: 0.571, D_sup_loss: 0.126, D_sup_acc: 97.15 Train acc: 96.680 Test acc: 97.000 \n",
      "step: 8554 | Train: G_Loss: 1.435, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.542, D_sup_loss: 0.127, D_sup_acc: 97.04 Train acc: 96.893 Test acc: 97.240 \n",
      "step: 8555 | Train: G_Loss: 1.218, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.561, D_sup_loss: 0.115, D_sup_acc: 97.27 Train acc: 96.743 Test acc: 97.180 \n",
      "step: 8556 | Train: G_Loss: 1.350, D_unsup_loss_real: 0.504, D_unsup_loss_fake: 0.518, D_sup_loss: 0.124, D_sup_acc: 97.22 Train acc: 96.883 Test acc: 97.220 \n",
      "step: 8557 | Train: G_Loss: 1.314, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.503, D_sup_loss: 0.110, D_sup_acc: 97.26 Train acc: 96.472 Test acc: 96.970 \n",
      "step: 8558 | Train: G_Loss: 1.287, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.524, D_sup_loss: 0.141, D_sup_acc: 97.01 Train acc: 96.930 Test acc: 97.380 \n",
      "step: 8559 | Train: G_Loss: 1.374, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.553, D_sup_loss: 0.114, D_sup_acc: 97.41 Train acc: 96.973 Test acc: 97.290 \n",
      "step: 8560 | Train: G_Loss: 1.343, D_unsup_loss_real: 0.496, D_unsup_loss_fake: 0.647, D_sup_loss: 0.111, D_sup_acc: 97.32 Train acc: 96.608 Test acc: 96.930 \n",
      "step: 8561 | Train: G_Loss: 1.285, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.555, D_sup_loss: 0.130, D_sup_acc: 96.97 Train acc: 96.482 Test acc: 96.840 \n",
      "step: 8562 | Train: G_Loss: 1.200, D_unsup_loss_real: 0.494, D_unsup_loss_fake: 0.589, D_sup_loss: 0.129, D_sup_acc: 96.88 Train acc: 96.593 Test acc: 97.040 \n",
      "step: 8563 | Train: G_Loss: 1.298, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.637, D_sup_loss: 0.127, D_sup_acc: 97.08 Train acc: 96.772 Test acc: 97.220 \n",
      "step: 8564 | Train: G_Loss: 1.346, D_unsup_loss_real: 0.545, D_unsup_loss_fake: 0.569, D_sup_loss: 0.117, D_sup_acc: 97.26 Train acc: 96.747 Test acc: 97.150 \n",
      "step: 8565 | Train: G_Loss: 1.227, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.529, D_sup_loss: 0.124, D_sup_acc: 97.19 Train acc: 96.875 Test acc: 97.190 \n",
      "step: 8566 | Train: G_Loss: 1.324, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.580, D_sup_loss: 0.119, D_sup_acc: 97.23 Train acc: 96.920 Test acc: 97.300 \n",
      "step: 8567 | Train: G_Loss: 1.341, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.505, D_sup_loss: 0.114, D_sup_acc: 97.33 Train acc: 96.978 Test acc: 97.320 \n",
      "step: 8568 | Train: G_Loss: 1.162, D_unsup_loss_real: 0.529, D_unsup_loss_fake: 0.620, D_sup_loss: 0.115, D_sup_acc: 97.35 Train acc: 96.913 Test acc: 97.280 \n",
      "step: 8569 | Train: G_Loss: 1.192, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.634, D_sup_loss: 0.119, D_sup_acc: 97.31 Train acc: 96.690 Test acc: 97.040 \n",
      "step: 8570 | Train: G_Loss: 1.290, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.615, D_sup_loss: 0.128, D_sup_acc: 97.08 Train acc: 96.820 Test acc: 97.160 \n",
      "step: 8571 | Train: G_Loss: 1.258, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.501, D_sup_loss: 0.123, D_sup_acc: 97.20 Train acc: 96.740 Test acc: 97.110 \n",
      "step: 8572 | Train: G_Loss: 1.331, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.540, D_sup_loss: 0.123, D_sup_acc: 97.15 Train acc: 97.092 Test acc: 97.350 \n",
      "step: 8573 | Train: G_Loss: 1.147, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.508, D_sup_loss: 0.107, D_sup_acc: 97.38 Train acc: 96.833 Test acc: 97.140 \n",
      "step: 8574 | Train: G_Loss: 1.211, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.531, D_sup_loss: 0.123, D_sup_acc: 97.18 Train acc: 97.022 Test acc: 97.270 \n",
      "step: 8575 | Train: G_Loss: 1.194, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.555, D_sup_loss: 0.108, D_sup_acc: 97.30 Train acc: 96.765 Test acc: 97.070 \n",
      "step: 8576 | Train: G_Loss: 1.168, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.542, D_sup_loss: 0.125, D_sup_acc: 97.11 Train acc: 96.887 Test acc: 97.180 \n",
      "step: 8577 | Train: G_Loss: 1.209, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.581, D_sup_loss: 0.121, D_sup_acc: 97.22 Train acc: 96.883 Test acc: 97.270 \n",
      "step: 8578 | Train: G_Loss: 1.273, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.552, D_sup_loss: 0.118, D_sup_acc: 97.30 Train acc: 97.098 Test acc: 97.390 \n",
      "step: 8579 | Train: G_Loss: 1.322, D_unsup_loss_real: 0.490, D_unsup_loss_fake: 0.583, D_sup_loss: 0.107, D_sup_acc: 97.42 Train acc: 97.092 Test acc: 97.450 \n",
      "step: 8580 | Train: G_Loss: 1.203, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.578, D_sup_loss: 0.107, D_sup_acc: 97.48 Train acc: 96.708 Test acc: 97.030 \n",
      "step: 8581 | Train: G_Loss: 1.293, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.506, D_sup_loss: 0.129, D_sup_acc: 97.07 Train acc: 96.955 Test acc: 97.260 \n",
      "step: 8582 | Train: G_Loss: 1.214, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.508, D_sup_loss: 0.114, D_sup_acc: 97.29 Train acc: 96.985 Test acc: 97.330 \n",
      "step: 8583 | Train: G_Loss: 1.376, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.578, D_sup_loss: 0.116, D_sup_acc: 97.36 Train acc: 96.910 Test acc: 97.290 \n",
      "step: 8584 | Train: G_Loss: 1.306, D_unsup_loss_real: 0.472, D_unsup_loss_fake: 0.605, D_sup_loss: 0.115, D_sup_acc: 97.32 Train acc: 96.905 Test acc: 97.220 \n",
      "step: 8585 | Train: G_Loss: 1.259, D_unsup_loss_real: 0.506, D_unsup_loss_fake: 0.556, D_sup_loss: 0.113, D_sup_acc: 97.26 Train acc: 96.902 Test acc: 97.290 \n",
      "step: 8586 | Train: G_Loss: 1.249, D_unsup_loss_real: 0.528, D_unsup_loss_fake: 0.512, D_sup_loss: 0.115, D_sup_acc: 97.32 Train acc: 97.052 Test acc: 97.450 \n",
      "step: 8587 | Train: G_Loss: 1.366, D_unsup_loss_real: 0.503, D_unsup_loss_fake: 0.546, D_sup_loss: 0.106, D_sup_acc: 97.48 Train acc: 97.130 Test acc: 97.480 \n",
      "step: 8588 | Train: G_Loss: 1.376, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.548, D_sup_loss: 0.102, D_sup_acc: 97.51 Train acc: 97.137 Test acc: 97.410 \n",
      "step: 8589 | Train: G_Loss: 1.278, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.646, D_sup_loss: 0.103, D_sup_acc: 97.44 Train acc: 97.050 Test acc: 97.370 \n",
      "step: 8590 | Train: G_Loss: 1.319, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.593, D_sup_loss: 0.110, D_sup_acc: 97.40 Train acc: 96.953 Test acc: 97.320 \n",
      "step: 8591 | Train: G_Loss: 1.354, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.515, D_sup_loss: 0.117, D_sup_acc: 97.35 Train acc: 97.163 Test acc: 97.430 \n",
      "step: 8592 | Train: G_Loss: 1.277, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.514, D_sup_loss: 0.104, D_sup_acc: 97.46 Train acc: 96.740 Test acc: 97.040 \n",
      "step: 8593 | Train: G_Loss: 1.258, D_unsup_loss_real: 0.545, D_unsup_loss_fake: 0.500, D_sup_loss: 0.131, D_sup_acc: 97.08 Train acc: 97.203 Test acc: 97.420 \n",
      "step: 8594 | Train: G_Loss: 1.311, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.566, D_sup_loss: 0.102, D_sup_acc: 97.45 Train acc: 96.828 Test acc: 97.210 \n",
      "step: 8595 | Train: G_Loss: 1.250, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.544, D_sup_loss: 0.125, D_sup_acc: 97.25 Train acc: 96.803 Test acc: 97.200 \n",
      "step: 8596 | Train: G_Loss: 1.372, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.567, D_sup_loss: 0.122, D_sup_acc: 97.24 Train acc: 96.890 Test acc: 97.260 \n",
      "step: 8597 | Train: G_Loss: 1.260, D_unsup_loss_real: 0.550, D_unsup_loss_fake: 0.561, D_sup_loss: 0.113, D_sup_acc: 97.29 Train acc: 96.768 Test acc: 97.140 \n",
      "step: 8598 | Train: G_Loss: 1.248, D_unsup_loss_real: 0.542, D_unsup_loss_fake: 0.533, D_sup_loss: 0.125, D_sup_acc: 97.18 Train acc: 96.782 Test acc: 97.090 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 8599 | Train: G_Loss: 1.380, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.549, D_sup_loss: 0.120, D_sup_acc: 97.13 Train acc: 97.133 Test acc: 97.450 \n",
      "step: 8600 | Train: G_Loss: 1.266, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.587, D_sup_loss: 0.105, D_sup_acc: 97.48 Train acc: 96.972 Test acc: 97.330 \n",
      "Train Classifier Accuracy: 96.972%\n",
      "\n",
      "Test Classifier Accuracy: 97.330%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_8600.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_8600.h5\n",
      "step: 8601 | Train: G_Loss: 1.234, D_unsup_loss_real: 0.511, D_unsup_loss_fake: 0.540, D_sup_loss: 0.112, D_sup_acc: 97.36 Train acc: 96.848 Test acc: 97.150 \n",
      "step: 8602 | Train: G_Loss: 1.244, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.558, D_sup_loss: 0.121, D_sup_acc: 97.19 Train acc: 97.077 Test acc: 97.320 \n",
      "step: 8603 | Train: G_Loss: 1.291, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.546, D_sup_loss: 0.109, D_sup_acc: 97.35 Train acc: 96.808 Test acc: 97.160 \n",
      "step: 8604 | Train: G_Loss: 1.238, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.524, D_sup_loss: 0.120, D_sup_acc: 97.20 Train acc: 97.058 Test acc: 97.330 \n",
      "step: 8605 | Train: G_Loss: 1.193, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.565, D_sup_loss: 0.108, D_sup_acc: 97.36 Train acc: 97.085 Test acc: 97.380 \n",
      "step: 8606 | Train: G_Loss: 1.195, D_unsup_loss_real: 0.536, D_unsup_loss_fake: 0.563, D_sup_loss: 0.108, D_sup_acc: 97.41 Train acc: 96.813 Test acc: 97.100 \n",
      "step: 8607 | Train: G_Loss: 1.177, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.597, D_sup_loss: 0.123, D_sup_acc: 97.14 Train acc: 96.882 Test acc: 97.210 \n",
      "step: 8608 | Train: G_Loss: 1.334, D_unsup_loss_real: 0.512, D_unsup_loss_fake: 0.497, D_sup_loss: 0.118, D_sup_acc: 97.25 Train acc: 96.632 Test acc: 97.030 \n",
      "step: 8609 | Train: G_Loss: 1.395, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.504, D_sup_loss: 0.126, D_sup_acc: 97.07 Train acc: 97.160 Test acc: 97.390 \n",
      "step: 8610 | Train: G_Loss: 1.243, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.626, D_sup_loss: 0.101, D_sup_acc: 97.42 Train acc: 96.828 Test acc: 97.120 \n",
      "step: 8611 | Train: G_Loss: 1.298, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.545, D_sup_loss: 0.118, D_sup_acc: 97.16 Train acc: 97.010 Test acc: 97.290 \n",
      "step: 8612 | Train: G_Loss: 1.286, D_unsup_loss_real: 0.510, D_unsup_loss_fake: 0.568, D_sup_loss: 0.109, D_sup_acc: 97.32 Train acc: 96.928 Test acc: 97.400 \n",
      "step: 8613 | Train: G_Loss: 1.206, D_unsup_loss_real: 0.537, D_unsup_loss_fake: 0.580, D_sup_loss: 0.114, D_sup_acc: 97.43 Train acc: 96.997 Test acc: 97.450 \n",
      "step: 8614 | Train: G_Loss: 1.357, D_unsup_loss_real: 0.516, D_unsup_loss_fake: 0.540, D_sup_loss: 0.110, D_sup_acc: 97.48 Train acc: 97.028 Test acc: 97.430 \n",
      "step: 8615 | Train: G_Loss: 1.362, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.479, D_sup_loss: 0.106, D_sup_acc: 97.46 Train acc: 97.058 Test acc: 97.500 \n",
      "step: 8616 | Train: G_Loss: 1.241, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.547, D_sup_loss: 0.105, D_sup_acc: 97.53 Train acc: 96.900 Test acc: 97.360 \n",
      "step: 8617 | Train: G_Loss: 1.394, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.577, D_sup_loss: 0.114, D_sup_acc: 97.39 Train acc: 96.877 Test acc: 97.290 \n",
      "step: 8618 | Train: G_Loss: 1.321, D_unsup_loss_real: 0.534, D_unsup_loss_fake: 0.622, D_sup_loss: 0.111, D_sup_acc: 97.32 Train acc: 96.152 Test acc: 96.600 \n",
      "step: 8619 | Train: G_Loss: 1.255, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.556, D_sup_loss: 0.140, D_sup_acc: 96.64 Train acc: 96.820 Test acc: 97.220 \n",
      "step: 8620 | Train: G_Loss: 1.239, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.540, D_sup_loss: 0.116, D_sup_acc: 97.26 Train acc: 96.968 Test acc: 97.170 \n",
      "step: 8621 | Train: G_Loss: 1.335, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.549, D_sup_loss: 0.112, D_sup_acc: 97.21 Train acc: 97.197 Test acc: 97.410 \n",
      "step: 8622 | Train: G_Loss: 1.216, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.526, D_sup_loss: 0.100, D_sup_acc: 97.44 Train acc: 96.743 Test acc: 97.170 \n",
      "step: 8623 | Train: G_Loss: 1.345, D_unsup_loss_real: 0.515, D_unsup_loss_fake: 0.628, D_sup_loss: 0.122, D_sup_acc: 97.21 Train acc: 97.155 Test acc: 97.380 \n",
      "step: 8624 | Train: G_Loss: 1.403, D_unsup_loss_real: 0.546, D_unsup_loss_fake: 0.566, D_sup_loss: 0.102, D_sup_acc: 97.41 Train acc: 97.243 Test acc: 97.480 \n",
      "step: 8625 | Train: G_Loss: 1.295, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.584, D_sup_loss: 0.099, D_sup_acc: 97.51 Train acc: 96.905 Test acc: 97.330 \n",
      "step: 8626 | Train: G_Loss: 1.245, D_unsup_loss_real: 0.621, D_unsup_loss_fake: 0.537, D_sup_loss: 0.117, D_sup_acc: 97.36 Train acc: 97.062 Test acc: 97.440 \n",
      "step: 8627 | Train: G_Loss: 1.319, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.501, D_sup_loss: 0.107, D_sup_acc: 97.47 Train acc: 97.252 Test acc: 97.510 \n",
      "step: 8628 | Train: G_Loss: 1.339, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.612, D_sup_loss: 0.098, D_sup_acc: 97.54 Train acc: 97.055 Test acc: 97.420 \n",
      "step: 8629 | Train: G_Loss: 1.250, D_unsup_loss_real: 0.545, D_unsup_loss_fake: 0.567, D_sup_loss: 0.109, D_sup_acc: 97.45 Train acc: 97.070 Test acc: 97.420 \n",
      "step: 8630 | Train: G_Loss: 1.314, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.614, D_sup_loss: 0.108, D_sup_acc: 97.45 Train acc: 96.782 Test acc: 97.170 \n",
      "step: 8631 | Train: G_Loss: 1.284, D_unsup_loss_real: 0.519, D_unsup_loss_fake: 0.539, D_sup_loss: 0.119, D_sup_acc: 97.21 Train acc: 97.052 Test acc: 97.420 \n",
      "step: 8632 | Train: G_Loss: 1.372, D_unsup_loss_real: 0.630, D_unsup_loss_fake: 0.506, D_sup_loss: 0.107, D_sup_acc: 97.45 Train acc: 97.138 Test acc: 97.430 \n",
      "step: 8633 | Train: G_Loss: 1.227, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.516, D_sup_loss: 0.105, D_sup_acc: 97.46 Train acc: 97.078 Test acc: 97.410 \n",
      "step: 8634 | Train: G_Loss: 1.361, D_unsup_loss_real: 0.471, D_unsup_loss_fake: 0.544, D_sup_loss: 0.111, D_sup_acc: 97.44 Train acc: 97.165 Test acc: 97.520 \n",
      "step: 8635 | Train: G_Loss: 1.255, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.593, D_sup_loss: 0.102, D_sup_acc: 97.55 Train acc: 96.837 Test acc: 97.120 \n",
      "step: 8636 | Train: G_Loss: 1.348, D_unsup_loss_real: 0.535, D_unsup_loss_fake: 0.553, D_sup_loss: 0.120, D_sup_acc: 97.16 Train acc: 97.203 Test acc: 97.440 \n",
      "step: 8637 | Train: G_Loss: 1.191, D_unsup_loss_real: 0.522, D_unsup_loss_fake: 0.492, D_sup_loss: 0.100, D_sup_acc: 97.47 Train acc: 96.915 Test acc: 97.160 \n",
      "step: 8638 | Train: G_Loss: 1.133, D_unsup_loss_real: 0.501, D_unsup_loss_fake: 0.539, D_sup_loss: 0.115, D_sup_acc: 97.20 Train acc: 96.653 Test acc: 97.010 \n",
      "step: 8639 | Train: G_Loss: 1.304, D_unsup_loss_real: 0.486, D_unsup_loss_fake: 0.619, D_sup_loss: 0.124, D_sup_acc: 97.05 Train acc: 97.078 Test acc: 97.440 \n",
      "step: 8640 | Train: G_Loss: 1.280, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.576, D_sup_loss: 0.107, D_sup_acc: 97.47 Train acc: 96.923 Test acc: 97.320 \n",
      "step: 8641 | Train: G_Loss: 1.372, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.607, D_sup_loss: 0.113, D_sup_acc: 97.35 Train acc: 97.048 Test acc: 97.340 \n",
      "step: 8642 | Train: G_Loss: 1.187, D_unsup_loss_real: 0.524, D_unsup_loss_fake: 0.551, D_sup_loss: 0.107, D_sup_acc: 97.37 Train acc: 97.040 Test acc: 97.340 \n",
      "step: 8643 | Train: G_Loss: 1.352, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.489, D_sup_loss: 0.109, D_sup_acc: 97.37 Train acc: 97.070 Test acc: 97.410 \n",
      "step: 8644 | Train: G_Loss: 1.181, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.466, D_sup_loss: 0.108, D_sup_acc: 97.44 Train acc: 96.570 Test acc: 96.850 \n",
      "step: 8645 | Train: G_Loss: 1.372, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.582, D_sup_loss: 0.130, D_sup_acc: 96.89 Train acc: 96.838 Test acc: 97.260 \n",
      "step: 8646 | Train: G_Loss: 1.333, D_unsup_loss_real: 0.513, D_unsup_loss_fake: 0.574, D_sup_loss: 0.118, D_sup_acc: 97.29 Train acc: 96.932 Test acc: 97.260 \n",
      "step: 8647 | Train: G_Loss: 1.250, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.568, D_sup_loss: 0.115, D_sup_acc: 97.29 Train acc: 96.902 Test acc: 97.250 \n",
      "step: 8648 | Train: G_Loss: 1.212, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.587, D_sup_loss: 0.119, D_sup_acc: 97.28 Train acc: 97.075 Test acc: 97.480 \n",
      "step: 8649 | Train: G_Loss: 1.235, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.551, D_sup_loss: 0.108, D_sup_acc: 97.51 Train acc: 97.167 Test acc: 97.370 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 8650 | Train: G_Loss: 1.152, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.527, D_sup_loss: 0.105, D_sup_acc: 97.40 Train acc: 96.565 Test acc: 96.910 \n",
      "step: 8651 | Train: G_Loss: 1.233, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.614, D_sup_loss: 0.134, D_sup_acc: 96.95 Train acc: 97.117 Test acc: 97.420 \n",
      "step: 8652 | Train: G_Loss: 1.170, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.563, D_sup_loss: 0.108, D_sup_acc: 97.45 Train acc: 96.833 Test acc: 97.140 \n",
      "step: 8653 | Train: G_Loss: 1.266, D_unsup_loss_real: 0.655, D_unsup_loss_fake: 0.578, D_sup_loss: 0.124, D_sup_acc: 97.18 Train acc: 97.107 Test acc: 97.310 \n",
      "step: 8654 | Train: G_Loss: 1.343, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.630, D_sup_loss: 0.106, D_sup_acc: 97.34 Train acc: 96.900 Test acc: 97.230 \n",
      "step: 8655 | Train: G_Loss: 1.311, D_unsup_loss_real: 0.521, D_unsup_loss_fake: 0.617, D_sup_loss: 0.119, D_sup_acc: 97.27 Train acc: 96.998 Test acc: 97.330 \n",
      "step: 8656 | Train: G_Loss: 1.277, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.508, D_sup_loss: 0.111, D_sup_acc: 97.36 Train acc: 96.763 Test acc: 97.140 \n",
      "step: 8657 | Train: G_Loss: 1.334, D_unsup_loss_real: 0.514, D_unsup_loss_fake: 0.518, D_sup_loss: 0.123, D_sup_acc: 97.18 Train acc: 96.957 Test acc: 97.270 \n",
      "step: 8658 | Train: G_Loss: 1.214, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.597, D_sup_loss: 0.113, D_sup_acc: 97.30 Train acc: 96.880 Test acc: 97.150 \n",
      "step: 8659 | Train: G_Loss: 1.199, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.485, D_sup_loss: 0.120, D_sup_acc: 97.19 Train acc: 96.982 Test acc: 97.290 \n",
      "step: 8660 | Train: G_Loss: 1.218, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.587, D_sup_loss: 0.114, D_sup_acc: 97.32 Train acc: 96.890 Test acc: 97.150 \n",
      "step: 8661 | Train: G_Loss: 1.259, D_unsup_loss_real: 0.514, D_unsup_loss_fake: 0.541, D_sup_loss: 0.115, D_sup_acc: 97.19 Train acc: 96.897 Test acc: 97.200 \n",
      "step: 8662 | Train: G_Loss: 1.218, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.608, D_sup_loss: 0.116, D_sup_acc: 97.24 Train acc: 96.745 Test acc: 97.000 \n",
      "step: 8663 | Train: G_Loss: 1.215, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.574, D_sup_loss: 0.126, D_sup_acc: 97.04 Train acc: 96.898 Test acc: 97.190 \n",
      "step: 8664 | Train: G_Loss: 1.238, D_unsup_loss_real: 0.469, D_unsup_loss_fake: 0.447, D_sup_loss: 0.117, D_sup_acc: 97.23 Train acc: 97.012 Test acc: 97.280 \n",
      "step: 8665 | Train: G_Loss: 1.179, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.573, D_sup_loss: 0.108, D_sup_acc: 97.31 Train acc: 96.783 Test acc: 97.030 \n",
      "step: 8666 | Train: G_Loss: 1.148, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.564, D_sup_loss: 0.130, D_sup_acc: 97.07 Train acc: 96.920 Test acc: 97.250 \n",
      "step: 8667 | Train: G_Loss: 1.164, D_unsup_loss_real: 0.519, D_unsup_loss_fake: 0.575, D_sup_loss: 0.118, D_sup_acc: 97.28 Train acc: 96.648 Test acc: 96.990 \n",
      "step: 8668 | Train: G_Loss: 1.392, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.609, D_sup_loss: 0.130, D_sup_acc: 97.03 Train acc: 97.110 Test acc: 97.550 \n",
      "step: 8669 | Train: G_Loss: 1.262, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.497, D_sup_loss: 0.102, D_sup_acc: 97.58 Train acc: 96.707 Test acc: 97.080 \n",
      "step: 8670 | Train: G_Loss: 1.155, D_unsup_loss_real: 0.538, D_unsup_loss_fake: 0.497, D_sup_loss: 0.126, D_sup_acc: 97.12 Train acc: 96.940 Test acc: 97.280 \n",
      "step: 8671 | Train: G_Loss: 1.313, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.539, D_sup_loss: 0.112, D_sup_acc: 97.31 Train acc: 96.978 Test acc: 97.320 \n",
      "step: 8672 | Train: G_Loss: 1.387, D_unsup_loss_real: 0.510, D_unsup_loss_fake: 0.608, D_sup_loss: 0.115, D_sup_acc: 97.35 Train acc: 97.008 Test acc: 97.330 \n",
      "step: 8673 | Train: G_Loss: 1.259, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.564, D_sup_loss: 0.109, D_sup_acc: 97.36 Train acc: 96.753 Test acc: 97.110 \n",
      "step: 8674 | Train: G_Loss: 1.270, D_unsup_loss_real: 0.648, D_unsup_loss_fake: 0.584, D_sup_loss: 0.127, D_sup_acc: 97.15 Train acc: 97.027 Test acc: 97.320 \n",
      "step: 8675 | Train: G_Loss: 1.289, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.556, D_sup_loss: 0.111, D_sup_acc: 97.35 Train acc: 96.743 Test acc: 97.050 \n",
      "step: 8676 | Train: G_Loss: 1.202, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.632, D_sup_loss: 0.122, D_sup_acc: 97.09 Train acc: 96.942 Test acc: 96.980 \n",
      "step: 8677 | Train: G_Loss: 1.298, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.534, D_sup_loss: 0.117, D_sup_acc: 97.02 Train acc: 96.980 Test acc: 97.130 \n",
      "step: 8678 | Train: G_Loss: 1.176, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.520, D_sup_loss: 0.116, D_sup_acc: 97.17 Train acc: 97.100 Test acc: 97.360 \n",
      "step: 8679 | Train: G_Loss: 1.274, D_unsup_loss_real: 0.545, D_unsup_loss_fake: 0.618, D_sup_loss: 0.107, D_sup_acc: 97.39 Train acc: 96.737 Test acc: 97.010 \n",
      "step: 8680 | Train: G_Loss: 1.184, D_unsup_loss_real: 0.537, D_unsup_loss_fake: 0.533, D_sup_loss: 0.127, D_sup_acc: 97.05 Train acc: 96.965 Test acc: 97.190 \n",
      "step: 8681 | Train: G_Loss: 1.288, D_unsup_loss_real: 0.498, D_unsup_loss_fake: 0.506, D_sup_loss: 0.114, D_sup_acc: 97.23 Train acc: 96.687 Test acc: 96.930 \n",
      "step: 8682 | Train: G_Loss: 1.434, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.536, D_sup_loss: 0.127, D_sup_acc: 96.97 Train acc: 97.035 Test acc: 97.250 \n",
      "step: 8683 | Train: G_Loss: 1.364, D_unsup_loss_real: 0.506, D_unsup_loss_fake: 0.568, D_sup_loss: 0.111, D_sup_acc: 97.28 Train acc: 96.917 Test acc: 97.160 \n",
      "step: 8684 | Train: G_Loss: 1.308, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.456, D_sup_loss: 0.117, D_sup_acc: 97.20 Train acc: 96.928 Test acc: 97.130 \n",
      "step: 8685 | Train: G_Loss: 1.156, D_unsup_loss_real: 0.531, D_unsup_loss_fake: 0.563, D_sup_loss: 0.115, D_sup_acc: 97.17 Train acc: 96.750 Test acc: 97.000 \n",
      "step: 8686 | Train: G_Loss: 1.265, D_unsup_loss_real: 0.501, D_unsup_loss_fake: 0.512, D_sup_loss: 0.126, D_sup_acc: 97.04 Train acc: 96.537 Test acc: 96.790 \n",
      "step: 8687 | Train: G_Loss: 1.310, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.573, D_sup_loss: 0.131, D_sup_acc: 96.83 Train acc: 96.958 Test acc: 97.240 \n",
      "step: 8688 | Train: G_Loss: 1.164, D_unsup_loss_real: 0.616, D_unsup_loss_fake: 0.628, D_sup_loss: 0.110, D_sup_acc: 97.27 Train acc: 96.393 Test acc: 96.760 \n",
      "step: 8689 | Train: G_Loss: 1.299, D_unsup_loss_real: 0.666, D_unsup_loss_fake: 0.591, D_sup_loss: 0.140, D_sup_acc: 96.80 Train acc: 96.953 Test acc: 97.250 \n",
      "step: 8690 | Train: G_Loss: 1.222, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.586, D_sup_loss: 0.110, D_sup_acc: 97.28 Train acc: 96.858 Test acc: 97.180 \n",
      "step: 8691 | Train: G_Loss: 1.373, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.518, D_sup_loss: 0.122, D_sup_acc: 97.22 Train acc: 97.073 Test acc: 97.410 \n",
      "step: 8692 | Train: G_Loss: 1.244, D_unsup_loss_real: 0.529, D_unsup_loss_fake: 0.604, D_sup_loss: 0.106, D_sup_acc: 97.44 Train acc: 96.800 Test acc: 97.200 \n",
      "step: 8693 | Train: G_Loss: 1.361, D_unsup_loss_real: 0.517, D_unsup_loss_fake: 0.547, D_sup_loss: 0.124, D_sup_acc: 97.24 Train acc: 96.755 Test acc: 97.170 \n",
      "step: 8694 | Train: G_Loss: 1.248, D_unsup_loss_real: 0.536, D_unsup_loss_fake: 0.554, D_sup_loss: 0.119, D_sup_acc: 97.21 Train acc: 96.957 Test acc: 97.340 \n",
      "step: 8695 | Train: G_Loss: 1.305, D_unsup_loss_real: 0.508, D_unsup_loss_fake: 0.492, D_sup_loss: 0.112, D_sup_acc: 97.37 Train acc: 97.045 Test acc: 97.330 \n",
      "step: 8696 | Train: G_Loss: 1.301, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.554, D_sup_loss: 0.113, D_sup_acc: 97.36 Train acc: 97.033 Test acc: 97.260 \n",
      "step: 8697 | Train: G_Loss: 1.110, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.549, D_sup_loss: 0.111, D_sup_acc: 97.29 Train acc: 96.753 Test acc: 97.040 \n",
      "step: 8698 | Train: G_Loss: 1.268, D_unsup_loss_real: 0.542, D_unsup_loss_fake: 0.566, D_sup_loss: 0.124, D_sup_acc: 97.08 Train acc: 97.010 Test acc: 97.220 \n",
      "step: 8699 | Train: G_Loss: 1.224, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.500, D_sup_loss: 0.114, D_sup_acc: 97.26 Train acc: 96.923 Test acc: 97.340 \n",
      "step: 8700 | Train: G_Loss: 1.226, D_unsup_loss_real: 0.498, D_unsup_loss_fake: 0.540, D_sup_loss: 0.116, D_sup_acc: 97.37 Train acc: 96.883 Test acc: 97.210 \n",
      "Train Classifier Accuracy: 96.883%\n",
      "\n",
      "Test Classifier Accuracy: 97.210%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_8700.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_8700.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 8701 | Train: G_Loss: 1.249, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.571, D_sup_loss: 0.119, D_sup_acc: 97.25 Train acc: 96.880 Test acc: 97.180 \n",
      "step: 8702 | Train: G_Loss: 1.222, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.542, D_sup_loss: 0.117, D_sup_acc: 97.22 Train acc: 96.407 Test acc: 96.670 \n",
      "step: 8703 | Train: G_Loss: 1.130, D_unsup_loss_real: 0.545, D_unsup_loss_fake: 0.606, D_sup_loss: 0.136, D_sup_acc: 96.71 Train acc: 96.610 Test acc: 96.910 \n",
      "step: 8704 | Train: G_Loss: 1.324, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.549, D_sup_loss: 0.130, D_sup_acc: 96.95 Train acc: 96.858 Test acc: 97.160 \n",
      "step: 8705 | Train: G_Loss: 1.323, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.477, D_sup_loss: 0.118, D_sup_acc: 97.20 Train acc: 97.017 Test acc: 97.270 \n",
      "step: 8706 | Train: G_Loss: 1.211, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.503, D_sup_loss: 0.108, D_sup_acc: 97.30 Train acc: 96.795 Test acc: 97.200 \n",
      "step: 8707 | Train: G_Loss: 1.190, D_unsup_loss_real: 0.620, D_unsup_loss_fake: 0.557, D_sup_loss: 0.119, D_sup_acc: 97.24 Train acc: 96.848 Test acc: 97.300 \n",
      "step: 8708 | Train: G_Loss: 1.245, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.532, D_sup_loss: 0.117, D_sup_acc: 97.33 Train acc: 96.893 Test acc: 97.220 \n",
      "step: 8709 | Train: G_Loss: 1.258, D_unsup_loss_real: 0.513, D_unsup_loss_fake: 0.561, D_sup_loss: 0.113, D_sup_acc: 97.26 Train acc: 96.823 Test acc: 97.160 \n",
      "step: 8710 | Train: G_Loss: 1.282, D_unsup_loss_real: 0.526, D_unsup_loss_fake: 0.486, D_sup_loss: 0.117, D_sup_acc: 97.20 Train acc: 96.977 Test acc: 97.340 \n",
      "step: 8711 | Train: G_Loss: 1.192, D_unsup_loss_real: 0.467, D_unsup_loss_fake: 0.555, D_sup_loss: 0.112, D_sup_acc: 97.37 Train acc: 96.897 Test acc: 97.300 \n",
      "step: 8712 | Train: G_Loss: 1.200, D_unsup_loss_real: 0.542, D_unsup_loss_fake: 0.550, D_sup_loss: 0.117, D_sup_acc: 97.33 Train acc: 96.583 Test acc: 96.940 \n",
      "step: 8713 | Train: G_Loss: 1.307, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.553, D_sup_loss: 0.128, D_sup_acc: 96.98 Train acc: 96.848 Test acc: 97.250 \n",
      "step: 8714 | Train: G_Loss: 1.228, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.578, D_sup_loss: 0.114, D_sup_acc: 97.28 Train acc: 96.390 Test acc: 96.750 \n",
      "step: 8715 | Train: G_Loss: 1.240, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.601, D_sup_loss: 0.137, D_sup_acc: 96.79 Train acc: 96.890 Test acc: 97.260 \n",
      "step: 8716 | Train: G_Loss: 1.255, D_unsup_loss_real: 0.515, D_unsup_loss_fake: 0.613, D_sup_loss: 0.112, D_sup_acc: 97.29 Train acc: 96.793 Test acc: 97.110 \n",
      "step: 8717 | Train: G_Loss: 1.175, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.599, D_sup_loss: 0.119, D_sup_acc: 97.15 Train acc: 96.767 Test acc: 97.070 \n",
      "step: 8718 | Train: G_Loss: 1.310, D_unsup_loss_real: 0.544, D_unsup_loss_fake: 0.519, D_sup_loss: 0.121, D_sup_acc: 97.11 Train acc: 96.942 Test acc: 97.310 \n",
      "step: 8719 | Train: G_Loss: 1.414, D_unsup_loss_real: 0.535, D_unsup_loss_fake: 0.577, D_sup_loss: 0.113, D_sup_acc: 97.34 Train acc: 96.767 Test acc: 97.110 \n",
      "step: 8720 | Train: G_Loss: 1.295, D_unsup_loss_real: 0.523, D_unsup_loss_fake: 0.486, D_sup_loss: 0.118, D_sup_acc: 97.15 Train acc: 96.740 Test acc: 97.170 \n",
      "step: 8721 | Train: G_Loss: 1.265, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.530, D_sup_loss: 0.122, D_sup_acc: 97.21 Train acc: 96.683 Test acc: 97.150 \n",
      "step: 8722 | Train: G_Loss: 1.379, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.484, D_sup_loss: 0.123, D_sup_acc: 97.19 Train acc: 96.915 Test acc: 97.320 \n",
      "step: 8723 | Train: G_Loss: 1.202, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.637, D_sup_loss: 0.111, D_sup_acc: 97.35 Train acc: 96.830 Test acc: 97.210 \n",
      "step: 8724 | Train: G_Loss: 1.216, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.557, D_sup_loss: 0.117, D_sup_acc: 97.25 Train acc: 96.890 Test acc: 97.220 \n",
      "step: 8725 | Train: G_Loss: 1.205, D_unsup_loss_real: 0.635, D_unsup_loss_fake: 0.565, D_sup_loss: 0.113, D_sup_acc: 97.26 Train acc: 96.345 Test acc: 96.620 \n",
      "step: 8726 | Train: G_Loss: 1.315, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.584, D_sup_loss: 0.140, D_sup_acc: 96.66 Train acc: 96.830 Test acc: 97.110 \n",
      "step: 8727 | Train: G_Loss: 1.278, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.633, D_sup_loss: 0.117, D_sup_acc: 97.15 Train acc: 95.997 Test acc: 96.350 \n",
      "step: 8728 | Train: G_Loss: 1.395, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.579, D_sup_loss: 0.150, D_sup_acc: 96.40 Train acc: 96.403 Test acc: 96.770 \n",
      "step: 8729 | Train: G_Loss: 1.317, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.560, D_sup_loss: 0.133, D_sup_acc: 96.81 Train acc: 96.550 Test acc: 96.920 \n",
      "step: 8730 | Train: G_Loss: 1.247, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.590, D_sup_loss: 0.129, D_sup_acc: 96.96 Train acc: 96.927 Test acc: 97.190 \n",
      "step: 8731 | Train: G_Loss: 1.379, D_unsup_loss_real: 0.500, D_unsup_loss_fake: 0.578, D_sup_loss: 0.115, D_sup_acc: 97.23 Train acc: 96.845 Test acc: 97.220 \n",
      "step: 8732 | Train: G_Loss: 1.262, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.538, D_sup_loss: 0.114, D_sup_acc: 97.26 Train acc: 96.452 Test acc: 96.700 \n",
      "step: 8733 | Train: G_Loss: 1.159, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.564, D_sup_loss: 0.132, D_sup_acc: 96.74 Train acc: 96.643 Test acc: 97.040 \n",
      "step: 8734 | Train: G_Loss: 1.411, D_unsup_loss_real: 0.542, D_unsup_loss_fake: 0.523, D_sup_loss: 0.128, D_sup_acc: 97.08 Train acc: 96.898 Test acc: 97.190 \n",
      "step: 8735 | Train: G_Loss: 1.222, D_unsup_loss_real: 0.529, D_unsup_loss_fake: 0.585, D_sup_loss: 0.114, D_sup_acc: 97.23 Train acc: 96.933 Test acc: 97.240 \n",
      "step: 8736 | Train: G_Loss: 1.291, D_unsup_loss_real: 0.480, D_unsup_loss_fake: 0.512, D_sup_loss: 0.113, D_sup_acc: 97.27 Train acc: 96.758 Test acc: 97.120 \n",
      "step: 8737 | Train: G_Loss: 1.191, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.522, D_sup_loss: 0.118, D_sup_acc: 97.16 Train acc: 96.760 Test acc: 97.140 \n",
      "step: 8738 | Train: G_Loss: 1.213, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.620, D_sup_loss: 0.124, D_sup_acc: 97.18 Train acc: 96.723 Test acc: 97.100 \n",
      "step: 8739 | Train: G_Loss: 1.253, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.509, D_sup_loss: 0.122, D_sup_acc: 97.14 Train acc: 96.948 Test acc: 97.340 \n",
      "step: 8740 | Train: G_Loss: 1.210, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.565, D_sup_loss: 0.106, D_sup_acc: 97.37 Train acc: 96.750 Test acc: 97.090 \n",
      "step: 8741 | Train: G_Loss: 1.232, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.529, D_sup_loss: 0.121, D_sup_acc: 97.13 Train acc: 97.035 Test acc: 97.400 \n",
      "step: 8742 | Train: G_Loss: 1.216, D_unsup_loss_real: 0.535, D_unsup_loss_fake: 0.560, D_sup_loss: 0.106, D_sup_acc: 97.43 Train acc: 96.923 Test acc: 97.210 \n",
      "step: 8743 | Train: G_Loss: 1.272, D_unsup_loss_real: 0.550, D_unsup_loss_fake: 0.575, D_sup_loss: 0.114, D_sup_acc: 97.25 Train acc: 96.788 Test acc: 97.080 \n",
      "step: 8744 | Train: G_Loss: 1.277, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.609, D_sup_loss: 0.119, D_sup_acc: 97.12 Train acc: 96.785 Test acc: 97.120 \n",
      "step: 8745 | Train: G_Loss: 1.290, D_unsup_loss_real: 0.545, D_unsup_loss_fake: 0.605, D_sup_loss: 0.120, D_sup_acc: 97.16 Train acc: 96.957 Test acc: 97.320 \n",
      "step: 8746 | Train: G_Loss: 1.306, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.635, D_sup_loss: 0.111, D_sup_acc: 97.35 Train acc: 96.823 Test acc: 97.170 \n",
      "step: 8747 | Train: G_Loss: 1.283, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.514, D_sup_loss: 0.122, D_sup_acc: 97.21 Train acc: 96.837 Test acc: 97.140 \n",
      "step: 8748 | Train: G_Loss: 1.243, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.558, D_sup_loss: 0.119, D_sup_acc: 97.18 Train acc: 96.773 Test acc: 97.080 \n",
      "step: 8749 | Train: G_Loss: 1.288, D_unsup_loss_real: 0.545, D_unsup_loss_fake: 0.650, D_sup_loss: 0.122, D_sup_acc: 97.12 Train acc: 96.513 Test acc: 96.860 \n",
      "step: 8750 | Train: G_Loss: 1.540, D_unsup_loss_real: 0.520, D_unsup_loss_fake: 0.466, D_sup_loss: 0.133, D_sup_acc: 96.90 Train acc: 96.713 Test acc: 97.050 \n",
      "step: 8751 | Train: G_Loss: 1.211, D_unsup_loss_real: 0.644, D_unsup_loss_fake: 0.545, D_sup_loss: 0.121, D_sup_acc: 97.09 Train acc: 96.550 Test acc: 96.890 \n",
      "step: 8752 | Train: G_Loss: 1.145, D_unsup_loss_real: 0.634, D_unsup_loss_fake: 0.634, D_sup_loss: 0.136, D_sup_acc: 96.93 Train acc: 96.360 Test acc: 96.650 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 8753 | Train: G_Loss: 1.225, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.569, D_sup_loss: 0.138, D_sup_acc: 96.69 Train acc: 96.925 Test acc: 97.220 \n",
      "step: 8754 | Train: G_Loss: 1.218, D_unsup_loss_real: 0.530, D_unsup_loss_fake: 0.499, D_sup_loss: 0.120, D_sup_acc: 97.26 Train acc: 97.010 Test acc: 97.240 \n",
      "step: 8755 | Train: G_Loss: 1.222, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.463, D_sup_loss: 0.110, D_sup_acc: 97.27 Train acc: 96.772 Test acc: 97.140 \n",
      "step: 8756 | Train: G_Loss: 1.282, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.575, D_sup_loss: 0.123, D_sup_acc: 97.18 Train acc: 97.072 Test acc: 97.360 \n",
      "step: 8757 | Train: G_Loss: 1.172, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.566, D_sup_loss: 0.110, D_sup_acc: 97.39 Train acc: 96.742 Test acc: 97.140 \n",
      "step: 8758 | Train: G_Loss: 1.144, D_unsup_loss_real: 0.524, D_unsup_loss_fake: 0.582, D_sup_loss: 0.122, D_sup_acc: 97.18 Train acc: 96.732 Test acc: 97.030 \n",
      "step: 8759 | Train: G_Loss: 1.276, D_unsup_loss_real: 0.486, D_unsup_loss_fake: 0.543, D_sup_loss: 0.121, D_sup_acc: 97.07 Train acc: 96.747 Test acc: 97.110 \n",
      "step: 8760 | Train: G_Loss: 1.276, D_unsup_loss_real: 0.508, D_unsup_loss_fake: 0.553, D_sup_loss: 0.120, D_sup_acc: 97.15 Train acc: 96.368 Test acc: 96.720 \n",
      "step: 8761 | Train: G_Loss: 1.319, D_unsup_loss_real: 0.542, D_unsup_loss_fake: 0.599, D_sup_loss: 0.140, D_sup_acc: 96.76 Train acc: 96.597 Test acc: 96.850 \n",
      "step: 8762 | Train: G_Loss: 1.179, D_unsup_loss_real: 0.523, D_unsup_loss_fake: 0.524, D_sup_loss: 0.129, D_sup_acc: 96.89 Train acc: 95.960 Test acc: 96.290 \n",
      "step: 8763 | Train: G_Loss: 1.397, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.552, D_sup_loss: 0.153, D_sup_acc: 96.34 Train acc: 96.980 Test acc: 97.220 \n",
      "step: 8764 | Train: G_Loss: 1.399, D_unsup_loss_real: 0.528, D_unsup_loss_fake: 0.558, D_sup_loss: 0.111, D_sup_acc: 97.26 Train acc: 96.953 Test acc: 97.200 \n",
      "step: 8765 | Train: G_Loss: 1.246, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.618, D_sup_loss: 0.114, D_sup_acc: 97.24 Train acc: 96.768 Test acc: 97.140 \n",
      "step: 8766 | Train: G_Loss: 1.274, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.547, D_sup_loss: 0.124, D_sup_acc: 97.18 Train acc: 97.065 Test acc: 97.320 \n",
      "step: 8767 | Train: G_Loss: 1.276, D_unsup_loss_real: 0.442, D_unsup_loss_fake: 0.510, D_sup_loss: 0.106, D_sup_acc: 97.35 Train acc: 96.827 Test acc: 97.160 \n",
      "step: 8768 | Train: G_Loss: 1.403, D_unsup_loss_real: 0.510, D_unsup_loss_fake: 0.512, D_sup_loss: 0.122, D_sup_acc: 97.20 Train acc: 96.918 Test acc: 97.310 \n",
      "step: 8769 | Train: G_Loss: 1.253, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.604, D_sup_loss: 0.112, D_sup_acc: 97.34 Train acc: 97.098 Test acc: 97.400 \n",
      "step: 8770 | Train: G_Loss: 1.270, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.595, D_sup_loss: 0.107, D_sup_acc: 97.43 Train acc: 97.123 Test acc: 97.410 \n",
      "step: 8771 | Train: G_Loss: 1.161, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.476, D_sup_loss: 0.108, D_sup_acc: 97.44 Train acc: 96.982 Test acc: 97.320 \n",
      "step: 8772 | Train: G_Loss: 1.162, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.544, D_sup_loss: 0.112, D_sup_acc: 97.35 Train acc: 96.890 Test acc: 97.290 \n",
      "step: 8773 | Train: G_Loss: 1.328, D_unsup_loss_real: 0.479, D_unsup_loss_fake: 0.535, D_sup_loss: 0.119, D_sup_acc: 97.32 Train acc: 97.103 Test acc: 97.360 \n",
      "step: 8774 | Train: G_Loss: 1.311, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.579, D_sup_loss: 0.106, D_sup_acc: 97.39 Train acc: 96.920 Test acc: 97.240 \n",
      "step: 8775 | Train: G_Loss: 1.226, D_unsup_loss_real: 0.642, D_unsup_loss_fake: 0.529, D_sup_loss: 0.114, D_sup_acc: 97.27 Train acc: 97.050 Test acc: 97.320 \n",
      "step: 8776 | Train: G_Loss: 1.224, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.547, D_sup_loss: 0.110, D_sup_acc: 97.35 Train acc: 96.970 Test acc: 97.260 \n",
      "step: 8777 | Train: G_Loss: 1.169, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.560, D_sup_loss: 0.109, D_sup_acc: 97.29 Train acc: 96.787 Test acc: 97.080 \n",
      "step: 8778 | Train: G_Loss: 1.236, D_unsup_loss_real: 0.461, D_unsup_loss_fake: 0.597, D_sup_loss: 0.118, D_sup_acc: 97.12 Train acc: 96.520 Test acc: 96.770 \n",
      "step: 8779 | Train: G_Loss: 1.215, D_unsup_loss_real: 0.524, D_unsup_loss_fake: 0.477, D_sup_loss: 0.129, D_sup_acc: 96.81 Train acc: 96.998 Test acc: 97.310 \n",
      "step: 8780 | Train: G_Loss: 1.259, D_unsup_loss_real: 0.628, D_unsup_loss_fake: 0.509, D_sup_loss: 0.107, D_sup_acc: 97.34 Train acc: 96.780 Test acc: 97.120 \n",
      "step: 8781 | Train: G_Loss: 1.306, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.536, D_sup_loss: 0.119, D_sup_acc: 97.16 Train acc: 96.818 Test acc: 97.160 \n",
      "step: 8782 | Train: G_Loss: 1.360, D_unsup_loss_real: 0.514, D_unsup_loss_fake: 0.550, D_sup_loss: 0.110, D_sup_acc: 97.20 Train acc: 96.843 Test acc: 97.240 \n",
      "step: 8783 | Train: G_Loss: 1.217, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.570, D_sup_loss: 0.116, D_sup_acc: 97.27 Train acc: 96.932 Test acc: 97.260 \n",
      "step: 8784 | Train: G_Loss: 1.278, D_unsup_loss_real: 0.512, D_unsup_loss_fake: 0.521, D_sup_loss: 0.109, D_sup_acc: 97.29 Train acc: 96.970 Test acc: 97.230 \n",
      "step: 8785 | Train: G_Loss: 1.231, D_unsup_loss_real: 0.533, D_unsup_loss_fake: 0.559, D_sup_loss: 0.106, D_sup_acc: 97.27 Train acc: 96.557 Test acc: 97.010 \n",
      "step: 8786 | Train: G_Loss: 1.278, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.558, D_sup_loss: 0.125, D_sup_acc: 97.05 Train acc: 96.870 Test acc: 97.130 \n",
      "step: 8787 | Train: G_Loss: 1.322, D_unsup_loss_real: 0.520, D_unsup_loss_fake: 0.718, D_sup_loss: 0.111, D_sup_acc: 97.17 Train acc: 96.685 Test acc: 97.060 \n",
      "step: 8788 | Train: G_Loss: 1.339, D_unsup_loss_real: 0.533, D_unsup_loss_fake: 0.573, D_sup_loss: 0.119, D_sup_acc: 97.10 Train acc: 96.490 Test acc: 96.800 \n",
      "step: 8789 | Train: G_Loss: 1.397, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.544, D_sup_loss: 0.129, D_sup_acc: 96.84 Train acc: 96.438 Test acc: 96.790 \n",
      "step: 8790 | Train: G_Loss: 1.319, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.643, D_sup_loss: 0.131, D_sup_acc: 96.83 Train acc: 96.810 Test acc: 97.210 \n",
      "step: 8791 | Train: G_Loss: 1.346, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.526, D_sup_loss: 0.114, D_sup_acc: 97.25 Train acc: 96.880 Test acc: 97.190 \n",
      "step: 8792 | Train: G_Loss: 1.317, D_unsup_loss_real: 0.509, D_unsup_loss_fake: 0.517, D_sup_loss: 0.113, D_sup_acc: 97.23 Train acc: 96.963 Test acc: 97.280 \n",
      "step: 8793 | Train: G_Loss: 1.204, D_unsup_loss_real: 0.510, D_unsup_loss_fake: 0.576, D_sup_loss: 0.108, D_sup_acc: 97.31 Train acc: 96.807 Test acc: 97.080 \n",
      "step: 8794 | Train: G_Loss: 1.218, D_unsup_loss_real: 0.550, D_unsup_loss_fake: 0.614, D_sup_loss: 0.121, D_sup_acc: 97.12 Train acc: 96.978 Test acc: 97.240 \n",
      "step: 8795 | Train: G_Loss: 1.312, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.558, D_sup_loss: 0.109, D_sup_acc: 97.27 Train acc: 96.828 Test acc: 97.050 \n",
      "step: 8796 | Train: G_Loss: 1.404, D_unsup_loss_real: 0.512, D_unsup_loss_fake: 0.517, D_sup_loss: 0.118, D_sup_acc: 97.09 Train acc: 96.937 Test acc: 97.220 \n",
      "step: 8797 | Train: G_Loss: 1.306, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.536, D_sup_loss: 0.106, D_sup_acc: 97.26 Train acc: 96.448 Test acc: 96.700 \n",
      "step: 8798 | Train: G_Loss: 1.296, D_unsup_loss_real: 0.505, D_unsup_loss_fake: 0.547, D_sup_loss: 0.134, D_sup_acc: 96.74 Train acc: 96.855 Test acc: 97.100 \n",
      "step: 8799 | Train: G_Loss: 1.168, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.599, D_sup_loss: 0.111, D_sup_acc: 97.14 Train acc: 96.695 Test acc: 97.030 \n",
      "step: 8800 | Train: G_Loss: 1.442, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.546, D_sup_loss: 0.122, D_sup_acc: 97.07 Train acc: 97.032 Test acc: 97.300 \n",
      "Train Classifier Accuracy: 97.032%\n",
      "\n",
      "Test Classifier Accuracy: 97.300%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_8800.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_8800.h5\n",
      "step: 8801 | Train: G_Loss: 1.481, D_unsup_loss_real: 0.648, D_unsup_loss_fake: 0.505, D_sup_loss: 0.104, D_sup_acc: 97.33 Train acc: 95.585 Test acc: 95.830 \n",
      "step: 8802 | Train: G_Loss: 1.367, D_unsup_loss_real: 0.799, D_unsup_loss_fake: 0.624, D_sup_loss: 0.184, D_sup_acc: 95.88 Train acc: 96.827 Test acc: 97.200 \n",
      "step: 8803 | Train: G_Loss: 1.250, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.627, D_sup_loss: 0.114, D_sup_acc: 97.24 Train acc: 96.322 Test acc: 96.780 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 8804 | Train: G_Loss: 1.351, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.585, D_sup_loss: 0.142, D_sup_acc: 96.82 Train acc: 96.812 Test acc: 97.190 \n",
      "step: 8805 | Train: G_Loss: 1.339, D_unsup_loss_real: 0.546, D_unsup_loss_fake: 0.588, D_sup_loss: 0.115, D_sup_acc: 97.23 Train acc: 96.923 Test acc: 97.220 \n",
      "step: 8806 | Train: G_Loss: 1.276, D_unsup_loss_real: 0.506, D_unsup_loss_fake: 0.606, D_sup_loss: 0.110, D_sup_acc: 97.26 Train acc: 96.997 Test acc: 97.270 \n",
      "step: 8807 | Train: G_Loss: 1.262, D_unsup_loss_real: 0.484, D_unsup_loss_fake: 0.584, D_sup_loss: 0.112, D_sup_acc: 97.30 Train acc: 96.945 Test acc: 97.200 \n",
      "step: 8808 | Train: G_Loss: 1.302, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.586, D_sup_loss: 0.111, D_sup_acc: 97.24 Train acc: 96.203 Test acc: 96.480 \n",
      "step: 8809 | Train: G_Loss: 1.239, D_unsup_loss_real: 0.541, D_unsup_loss_fake: 0.484, D_sup_loss: 0.150, D_sup_acc: 96.52 Train acc: 96.677 Test acc: 97.110 \n",
      "step: 8810 | Train: G_Loss: 1.256, D_unsup_loss_real: 0.500, D_unsup_loss_fake: 0.540, D_sup_loss: 0.122, D_sup_acc: 97.15 Train acc: 96.908 Test acc: 97.230 \n",
      "step: 8811 | Train: G_Loss: 1.235, D_unsup_loss_real: 0.484, D_unsup_loss_fake: 0.567, D_sup_loss: 0.114, D_sup_acc: 97.27 Train acc: 96.937 Test acc: 97.260 \n",
      "step: 8812 | Train: G_Loss: 1.181, D_unsup_loss_real: 0.544, D_unsup_loss_fake: 0.606, D_sup_loss: 0.112, D_sup_acc: 97.29 Train acc: 96.712 Test acc: 97.110 \n",
      "step: 8813 | Train: G_Loss: 1.331, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.602, D_sup_loss: 0.128, D_sup_acc: 97.15 Train acc: 96.877 Test acc: 97.170 \n",
      "step: 8814 | Train: G_Loss: 1.190, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.597, D_sup_loss: 0.114, D_sup_acc: 97.21 Train acc: 96.900 Test acc: 97.140 \n",
      "step: 8815 | Train: G_Loss: 1.282, D_unsup_loss_real: 0.534, D_unsup_loss_fake: 0.541, D_sup_loss: 0.117, D_sup_acc: 97.18 Train acc: 96.827 Test acc: 97.240 \n",
      "step: 8816 | Train: G_Loss: 1.345, D_unsup_loss_real: 0.486, D_unsup_loss_fake: 0.575, D_sup_loss: 0.120, D_sup_acc: 97.27 Train acc: 97.125 Test acc: 97.310 \n",
      "step: 8817 | Train: G_Loss: 1.207, D_unsup_loss_real: 0.658, D_unsup_loss_fake: 0.535, D_sup_loss: 0.107, D_sup_acc: 97.34 Train acc: 96.705 Test acc: 97.060 \n",
      "step: 8818 | Train: G_Loss: 1.286, D_unsup_loss_real: 0.531, D_unsup_loss_fake: 0.642, D_sup_loss: 0.125, D_sup_acc: 97.10 Train acc: 96.742 Test acc: 97.090 \n",
      "step: 8819 | Train: G_Loss: 1.350, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.526, D_sup_loss: 0.121, D_sup_acc: 97.13 Train acc: 96.758 Test acc: 97.140 \n",
      "step: 8820 | Train: G_Loss: 1.282, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.563, D_sup_loss: 0.118, D_sup_acc: 97.18 Train acc: 96.822 Test acc: 97.070 \n",
      "step: 8821 | Train: G_Loss: 1.281, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.550, D_sup_loss: 0.121, D_sup_acc: 97.11 Train acc: 96.927 Test acc: 97.170 \n",
      "step: 8822 | Train: G_Loss: 1.326, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.574, D_sup_loss: 0.115, D_sup_acc: 97.21 Train acc: 97.003 Test acc: 97.190 \n",
      "step: 8823 | Train: G_Loss: 1.331, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.535, D_sup_loss: 0.115, D_sup_acc: 97.23 Train acc: 96.940 Test acc: 97.210 \n",
      "step: 8824 | Train: G_Loss: 1.218, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.594, D_sup_loss: 0.113, D_sup_acc: 97.25 Train acc: 96.815 Test acc: 97.100 \n",
      "step: 8825 | Train: G_Loss: 1.356, D_unsup_loss_real: 0.506, D_unsup_loss_fake: 0.577, D_sup_loss: 0.124, D_sup_acc: 97.14 Train acc: 96.973 Test acc: 97.310 \n",
      "step: 8826 | Train: G_Loss: 1.188, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.564, D_sup_loss: 0.112, D_sup_acc: 97.34 Train acc: 96.845 Test acc: 97.210 \n",
      "step: 8827 | Train: G_Loss: 1.351, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.534, D_sup_loss: 0.119, D_sup_acc: 97.25 Train acc: 97.035 Test acc: 97.310 \n",
      "step: 8828 | Train: G_Loss: 1.274, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.560, D_sup_loss: 0.112, D_sup_acc: 97.34 Train acc: 96.760 Test acc: 97.150 \n",
      "step: 8829 | Train: G_Loss: 1.290, D_unsup_loss_real: 0.499, D_unsup_loss_fake: 0.557, D_sup_loss: 0.121, D_sup_acc: 97.19 Train acc: 96.882 Test acc: 97.240 \n",
      "step: 8830 | Train: G_Loss: 1.325, D_unsup_loss_real: 0.512, D_unsup_loss_fake: 0.531, D_sup_loss: 0.119, D_sup_acc: 97.27 Train acc: 97.027 Test acc: 97.220 \n",
      "step: 8831 | Train: G_Loss: 1.264, D_unsup_loss_real: 0.529, D_unsup_loss_fake: 0.567, D_sup_loss: 0.111, D_sup_acc: 97.26 Train acc: 97.000 Test acc: 97.280 \n",
      "step: 8832 | Train: G_Loss: 1.259, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.552, D_sup_loss: 0.113, D_sup_acc: 97.31 Train acc: 96.913 Test acc: 97.180 \n",
      "step: 8833 | Train: G_Loss: 1.382, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.533, D_sup_loss: 0.117, D_sup_acc: 97.22 Train acc: 97.023 Test acc: 97.210 \n",
      "step: 8834 | Train: G_Loss: 1.282, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.539, D_sup_loss: 0.109, D_sup_acc: 97.25 Train acc: 96.688 Test acc: 97.120 \n",
      "step: 8835 | Train: G_Loss: 1.432, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.480, D_sup_loss: 0.128, D_sup_acc: 97.16 Train acc: 96.880 Test acc: 97.260 \n",
      "step: 8836 | Train: G_Loss: 1.323, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.515, D_sup_loss: 0.116, D_sup_acc: 97.29 Train acc: 96.817 Test acc: 97.160 \n",
      "step: 8837 | Train: G_Loss: 1.252, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.558, D_sup_loss: 0.121, D_sup_acc: 97.20 Train acc: 96.708 Test acc: 97.100 \n",
      "step: 8838 | Train: G_Loss: 1.316, D_unsup_loss_real: 0.490, D_unsup_loss_fake: 0.578, D_sup_loss: 0.123, D_sup_acc: 97.14 Train acc: 96.380 Test acc: 96.860 \n",
      "step: 8839 | Train: G_Loss: 1.298, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.562, D_sup_loss: 0.131, D_sup_acc: 96.90 Train acc: 96.818 Test acc: 97.120 \n",
      "step: 8840 | Train: G_Loss: 1.332, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.516, D_sup_loss: 0.118, D_sup_acc: 97.16 Train acc: 96.938 Test acc: 97.280 \n",
      "step: 8841 | Train: G_Loss: 1.262, D_unsup_loss_real: 0.507, D_unsup_loss_fake: 0.481, D_sup_loss: 0.111, D_sup_acc: 97.31 Train acc: 97.007 Test acc: 97.240 \n",
      "step: 8842 | Train: G_Loss: 1.318, D_unsup_loss_real: 0.504, D_unsup_loss_fake: 0.531, D_sup_loss: 0.110, D_sup_acc: 97.27 Train acc: 96.970 Test acc: 97.190 \n",
      "step: 8843 | Train: G_Loss: 1.230, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.542, D_sup_loss: 0.114, D_sup_acc: 97.23 Train acc: 96.832 Test acc: 97.150 \n",
      "step: 8844 | Train: G_Loss: 1.199, D_unsup_loss_real: 0.541, D_unsup_loss_fake: 0.507, D_sup_loss: 0.116, D_sup_acc: 97.19 Train acc: 96.815 Test acc: 97.030 \n",
      "step: 8845 | Train: G_Loss: 1.318, D_unsup_loss_real: 0.529, D_unsup_loss_fake: 0.619, D_sup_loss: 0.120, D_sup_acc: 97.07 Train acc: 97.037 Test acc: 97.320 \n",
      "step: 8846 | Train: G_Loss: 1.258, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.527, D_sup_loss: 0.107, D_sup_acc: 97.35 Train acc: 96.660 Test acc: 97.030 \n",
      "step: 8847 | Train: G_Loss: 1.181, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.592, D_sup_loss: 0.125, D_sup_acc: 97.07 Train acc: 97.135 Test acc: 97.300 \n",
      "step: 8848 | Train: G_Loss: 1.249, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.590, D_sup_loss: 0.105, D_sup_acc: 97.33 Train acc: 97.040 Test acc: 97.280 \n",
      "step: 8849 | Train: G_Loss: 1.321, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.597, D_sup_loss: 0.109, D_sup_acc: 97.31 Train acc: 96.910 Test acc: 97.260 \n",
      "step: 8850 | Train: G_Loss: 1.363, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.545, D_sup_loss: 0.118, D_sup_acc: 97.29 Train acc: 96.962 Test acc: 97.320 \n",
      "step: 8851 | Train: G_Loss: 1.348, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.538, D_sup_loss: 0.111, D_sup_acc: 97.35 Train acc: 96.813 Test acc: 97.150 \n",
      "step: 8852 | Train: G_Loss: 1.306, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.566, D_sup_loss: 0.121, D_sup_acc: 97.19 Train acc: 96.913 Test acc: 97.140 \n",
      "step: 8853 | Train: G_Loss: 1.263, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.535, D_sup_loss: 0.115, D_sup_acc: 97.18 Train acc: 96.788 Test acc: 97.090 \n",
      "step: 8854 | Train: G_Loss: 1.363, D_unsup_loss_real: 0.509, D_unsup_loss_fake: 0.585, D_sup_loss: 0.125, D_sup_acc: 97.13 Train acc: 96.980 Test acc: 97.190 \n",
      "step: 8855 | Train: G_Loss: 1.261, D_unsup_loss_real: 0.487, D_unsup_loss_fake: 0.566, D_sup_loss: 0.107, D_sup_acc: 97.23 Train acc: 96.858 Test acc: 97.080 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 8856 | Train: G_Loss: 1.290, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.535, D_sup_loss: 0.119, D_sup_acc: 97.12 Train acc: 96.905 Test acc: 97.150 \n",
      "step: 8857 | Train: G_Loss: 1.253, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.583, D_sup_loss: 0.117, D_sup_acc: 97.19 Train acc: 97.060 Test acc: 97.290 \n",
      "step: 8858 | Train: G_Loss: 1.274, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.582, D_sup_loss: 0.108, D_sup_acc: 97.32 Train acc: 96.830 Test acc: 97.070 \n",
      "step: 8859 | Train: G_Loss: 1.251, D_unsup_loss_real: 0.505, D_unsup_loss_fake: 0.519, D_sup_loss: 0.124, D_sup_acc: 97.11 Train acc: 96.950 Test acc: 97.110 \n",
      "step: 8860 | Train: G_Loss: 1.289, D_unsup_loss_real: 0.532, D_unsup_loss_fake: 0.582, D_sup_loss: 0.117, D_sup_acc: 97.15 Train acc: 96.757 Test acc: 97.050 \n",
      "step: 8861 | Train: G_Loss: 1.203, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.550, D_sup_loss: 0.122, D_sup_acc: 97.09 Train acc: 96.873 Test acc: 97.160 \n",
      "step: 8862 | Train: G_Loss: 1.467, D_unsup_loss_real: 0.541, D_unsup_loss_fake: 0.545, D_sup_loss: 0.119, D_sup_acc: 97.20 Train acc: 97.025 Test acc: 97.260 \n",
      "step: 8863 | Train: G_Loss: 1.400, D_unsup_loss_real: 0.658, D_unsup_loss_fake: 0.479, D_sup_loss: 0.108, D_sup_acc: 97.29 Train acc: 96.553 Test acc: 96.660 \n",
      "step: 8864 | Train: G_Loss: 1.232, D_unsup_loss_real: 0.702, D_unsup_loss_fake: 0.627, D_sup_loss: 0.146, D_sup_acc: 96.70 Train acc: 96.888 Test acc: 97.110 \n",
      "step: 8865 | Train: G_Loss: 1.236, D_unsup_loss_real: 0.439, D_unsup_loss_fake: 0.570, D_sup_loss: 0.114, D_sup_acc: 97.15 Train acc: 96.810 Test acc: 97.110 \n",
      "step: 8866 | Train: G_Loss: 1.226, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.628, D_sup_loss: 0.115, D_sup_acc: 97.15 Train acc: 97.002 Test acc: 97.290 \n",
      "step: 8867 | Train: G_Loss: 1.309, D_unsup_loss_real: 0.536, D_unsup_loss_fake: 0.613, D_sup_loss: 0.106, D_sup_acc: 97.32 Train acc: 96.955 Test acc: 97.130 \n",
      "step: 8868 | Train: G_Loss: 1.308, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.608, D_sup_loss: 0.111, D_sup_acc: 97.17 Train acc: 96.742 Test acc: 96.960 \n",
      "step: 8869 | Train: G_Loss: 1.357, D_unsup_loss_real: 0.489, D_unsup_loss_fake: 0.548, D_sup_loss: 0.121, D_sup_acc: 97.00 Train acc: 96.957 Test acc: 97.180 \n",
      "step: 8870 | Train: G_Loss: 1.343, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.521, D_sup_loss: 0.110, D_sup_acc: 97.22 Train acc: 96.860 Test acc: 97.120 \n",
      "step: 8871 | Train: G_Loss: 1.268, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.494, D_sup_loss: 0.117, D_sup_acc: 97.16 Train acc: 96.947 Test acc: 97.200 \n",
      "step: 8872 | Train: G_Loss: 1.245, D_unsup_loss_real: 0.525, D_unsup_loss_fake: 0.533, D_sup_loss: 0.111, D_sup_acc: 97.24 Train acc: 96.855 Test acc: 97.100 \n",
      "step: 8873 | Train: G_Loss: 1.270, D_unsup_loss_real: 0.529, D_unsup_loss_fake: 0.526, D_sup_loss: 0.115, D_sup_acc: 97.14 Train acc: 96.688 Test acc: 96.970 \n",
      "step: 8874 | Train: G_Loss: 1.187, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.569, D_sup_loss: 0.122, D_sup_acc: 97.01 Train acc: 96.797 Test acc: 97.010 \n",
      "step: 8875 | Train: G_Loss: 1.275, D_unsup_loss_real: 0.492, D_unsup_loss_fake: 0.545, D_sup_loss: 0.117, D_sup_acc: 97.05 Train acc: 97.083 Test acc: 97.240 \n",
      "step: 8876 | Train: G_Loss: 1.349, D_unsup_loss_real: 0.525, D_unsup_loss_fake: 0.547, D_sup_loss: 0.106, D_sup_acc: 97.27 Train acc: 97.047 Test acc: 97.160 \n",
      "step: 8877 | Train: G_Loss: 1.236, D_unsup_loss_real: 0.656, D_unsup_loss_fake: 0.550, D_sup_loss: 0.107, D_sup_acc: 97.20 Train acc: 96.732 Test acc: 96.970 \n",
      "step: 8878 | Train: G_Loss: 1.186, D_unsup_loss_real: 0.536, D_unsup_loss_fake: 0.527, D_sup_loss: 0.123, D_sup_acc: 97.01 Train acc: 97.062 Test acc: 97.260 \n",
      "step: 8879 | Train: G_Loss: 1.168, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.543, D_sup_loss: 0.108, D_sup_acc: 97.29 Train acc: 96.975 Test acc: 97.140 \n",
      "step: 8880 | Train: G_Loss: 1.250, D_unsup_loss_real: 0.500, D_unsup_loss_fake: 0.562, D_sup_loss: 0.110, D_sup_acc: 97.18 Train acc: 96.898 Test acc: 97.070 \n",
      "step: 8881 | Train: G_Loss: 1.234, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.665, D_sup_loss: 0.115, D_sup_acc: 97.11 Train acc: 96.888 Test acc: 97.110 \n",
      "step: 8882 | Train: G_Loss: 1.228, D_unsup_loss_real: 0.520, D_unsup_loss_fake: 0.575, D_sup_loss: 0.115, D_sup_acc: 97.15 Train acc: 97.005 Test acc: 97.260 \n",
      "step: 8883 | Train: G_Loss: 1.338, D_unsup_loss_real: 0.615, D_unsup_loss_fake: 0.600, D_sup_loss: 0.110, D_sup_acc: 97.29 Train acc: 97.045 Test acc: 97.310 \n",
      "step: 8884 | Train: G_Loss: 1.212, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.606, D_sup_loss: 0.108, D_sup_acc: 97.34 Train acc: 97.035 Test acc: 97.400 \n",
      "step: 8885 | Train: G_Loss: 1.309, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.561, D_sup_loss: 0.108, D_sup_acc: 97.43 Train acc: 97.037 Test acc: 97.330 \n",
      "step: 8886 | Train: G_Loss: 1.109, D_unsup_loss_real: 0.641, D_unsup_loss_fake: 0.569, D_sup_loss: 0.108, D_sup_acc: 97.36 Train acc: 96.485 Test acc: 96.880 \n",
      "step: 8887 | Train: G_Loss: 1.390, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.539, D_sup_loss: 0.134, D_sup_acc: 96.92 Train acc: 96.952 Test acc: 97.370 \n",
      "step: 8888 | Train: G_Loss: 1.266, D_unsup_loss_real: 0.501, D_unsup_loss_fake: 0.517, D_sup_loss: 0.109, D_sup_acc: 97.40 Train acc: 96.808 Test acc: 97.100 \n",
      "step: 8889 | Train: G_Loss: 1.254, D_unsup_loss_real: 0.500, D_unsup_loss_fake: 0.567, D_sup_loss: 0.119, D_sup_acc: 97.14 Train acc: 97.070 Test acc: 97.280 \n",
      "step: 8890 | Train: G_Loss: 1.166, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.562, D_sup_loss: 0.110, D_sup_acc: 97.31 Train acc: 97.063 Test acc: 97.390 \n",
      "step: 8891 | Train: G_Loss: 1.518, D_unsup_loss_real: 0.542, D_unsup_loss_fake: 0.553, D_sup_loss: 0.109, D_sup_acc: 97.42 Train acc: 97.120 Test acc: 97.420 \n",
      "step: 8892 | Train: G_Loss: 1.412, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.580, D_sup_loss: 0.102, D_sup_acc: 97.45 Train acc: 96.017 Test acc: 96.360 \n",
      "step: 8893 | Train: G_Loss: 1.381, D_unsup_loss_real: 0.743, D_unsup_loss_fake: 0.572, D_sup_loss: 0.170, D_sup_acc: 96.41 Train acc: 96.902 Test acc: 97.220 \n",
      "step: 8894 | Train: G_Loss: 1.359, D_unsup_loss_real: 0.510, D_unsup_loss_fake: 0.544, D_sup_loss: 0.115, D_sup_acc: 97.26 Train acc: 97.013 Test acc: 97.360 \n",
      "step: 8895 | Train: G_Loss: 1.344, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.608, D_sup_loss: 0.106, D_sup_acc: 97.39 Train acc: 96.818 Test acc: 97.180 \n",
      "step: 8896 | Train: G_Loss: 1.312, D_unsup_loss_real: 0.499, D_unsup_loss_fake: 0.513, D_sup_loss: 0.120, D_sup_acc: 97.22 Train acc: 96.835 Test acc: 97.200 \n",
      "step: 8897 | Train: G_Loss: 1.271, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.586, D_sup_loss: 0.119, D_sup_acc: 97.24 Train acc: 97.142 Test acc: 97.460 \n",
      "step: 8898 | Train: G_Loss: 1.269, D_unsup_loss_real: 0.600, D_unsup_loss_fake: 0.546, D_sup_loss: 0.104, D_sup_acc: 97.49 Train acc: 96.918 Test acc: 97.240 \n",
      "step: 8899 | Train: G_Loss: 1.269, D_unsup_loss_real: 0.545, D_unsup_loss_fake: 0.546, D_sup_loss: 0.117, D_sup_acc: 97.27 Train acc: 96.902 Test acc: 97.230 \n",
      "step: 8900 | Train: G_Loss: 1.320, D_unsup_loss_real: 0.535, D_unsup_loss_fake: 0.503, D_sup_loss: 0.119, D_sup_acc: 97.27 Train acc: 97.162 Test acc: 97.390 \n",
      "Train Classifier Accuracy: 97.162%\n",
      "\n",
      "Test Classifier Accuracy: 97.390%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_8900.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_8900.h5\n",
      "step: 8901 | Train: G_Loss: 1.108, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.532, D_sup_loss: 0.102, D_sup_acc: 97.42 Train acc: 96.998 Test acc: 97.230 \n",
      "step: 8902 | Train: G_Loss: 1.260, D_unsup_loss_real: 0.546, D_unsup_loss_fake: 0.610, D_sup_loss: 0.113, D_sup_acc: 97.27 Train acc: 97.042 Test acc: 97.350 \n",
      "step: 8903 | Train: G_Loss: 1.186, D_unsup_loss_real: 0.519, D_unsup_loss_fake: 0.569, D_sup_loss: 0.110, D_sup_acc: 97.38 Train acc: 97.147 Test acc: 97.310 \n",
      "step: 8904 | Train: G_Loss: 1.242, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.515, D_sup_loss: 0.104, D_sup_acc: 97.34 Train acc: 96.655 Test acc: 97.030 \n",
      "step: 8905 | Train: G_Loss: 1.257, D_unsup_loss_real: 0.504, D_unsup_loss_fake: 0.600, D_sup_loss: 0.127, D_sup_acc: 97.07 Train acc: 97.075 Test acc: 97.250 \n",
      "step: 8906 | Train: G_Loss: 1.354, D_unsup_loss_real: 0.541, D_unsup_loss_fake: 0.558, D_sup_loss: 0.105, D_sup_acc: 97.28 Train acc: 97.135 Test acc: 97.340 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 8907 | Train: G_Loss: 1.141, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.540, D_sup_loss: 0.104, D_sup_acc: 97.37 Train acc: 96.818 Test acc: 97.150 \n",
      "step: 8908 | Train: G_Loss: 1.268, D_unsup_loss_real: 0.550, D_unsup_loss_fake: 0.618, D_sup_loss: 0.122, D_sup_acc: 97.19 Train acc: 96.930 Test acc: 97.280 \n",
      "step: 8909 | Train: G_Loss: 1.348, D_unsup_loss_real: 0.492, D_unsup_loss_fake: 0.532, D_sup_loss: 0.116, D_sup_acc: 97.31 Train acc: 96.847 Test acc: 97.190 \n",
      "step: 8910 | Train: G_Loss: 1.356, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.565, D_sup_loss: 0.116, D_sup_acc: 97.23 Train acc: 96.878 Test acc: 97.190 \n",
      "step: 8911 | Train: G_Loss: 1.293, D_unsup_loss_real: 0.646, D_unsup_loss_fake: 0.608, D_sup_loss: 0.116, D_sup_acc: 97.23 Train acc: 96.923 Test acc: 97.160 \n",
      "step: 8912 | Train: G_Loss: 1.341, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.498, D_sup_loss: 0.116, D_sup_acc: 97.20 Train acc: 96.638 Test acc: 96.950 \n",
      "step: 8913 | Train: G_Loss: 1.189, D_unsup_loss_real: 0.529, D_unsup_loss_fake: 0.485, D_sup_loss: 0.127, D_sup_acc: 96.99 Train acc: 96.780 Test acc: 97.120 \n",
      "step: 8914 | Train: G_Loss: 1.400, D_unsup_loss_real: 0.625, D_unsup_loss_fake: 0.562, D_sup_loss: 0.120, D_sup_acc: 97.16 Train acc: 96.740 Test acc: 97.080 \n",
      "step: 8915 | Train: G_Loss: 1.283, D_unsup_loss_real: 0.495, D_unsup_loss_fake: 0.498, D_sup_loss: 0.124, D_sup_acc: 97.12 Train acc: 97.083 Test acc: 97.320 \n",
      "step: 8916 | Train: G_Loss: 1.226, D_unsup_loss_real: 0.659, D_unsup_loss_fake: 0.523, D_sup_loss: 0.108, D_sup_acc: 97.35 Train acc: 96.678 Test acc: 97.050 \n",
      "step: 8917 | Train: G_Loss: 1.287, D_unsup_loss_real: 0.519, D_unsup_loss_fake: 0.534, D_sup_loss: 0.124, D_sup_acc: 97.09 Train acc: 96.972 Test acc: 97.260 \n",
      "step: 8918 | Train: G_Loss: 1.300, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.589, D_sup_loss: 0.111, D_sup_acc: 97.29 Train acc: 96.858 Test acc: 97.070 \n",
      "step: 8919 | Train: G_Loss: 1.276, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.585, D_sup_loss: 0.119, D_sup_acc: 97.11 Train acc: 96.575 Test acc: 96.920 \n",
      "step: 8920 | Train: G_Loss: 1.635, D_unsup_loss_real: 0.451, D_unsup_loss_fake: 0.571, D_sup_loss: 0.127, D_sup_acc: 96.96 Train acc: 96.942 Test acc: 97.200 \n",
      "step: 8921 | Train: G_Loss: 1.376, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.596, D_sup_loss: 0.108, D_sup_acc: 97.24 Train acc: 96.932 Test acc: 97.260 \n",
      "step: 8922 | Train: G_Loss: 1.290, D_unsup_loss_real: 0.596, D_unsup_loss_fake: 0.535, D_sup_loss: 0.116, D_sup_acc: 97.29 Train acc: 96.785 Test acc: 97.120 \n",
      "step: 8923 | Train: G_Loss: 1.321, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.575, D_sup_loss: 0.121, D_sup_acc: 97.16 Train acc: 96.737 Test acc: 97.040 \n",
      "step: 8924 | Train: G_Loss: 1.367, D_unsup_loss_real: 0.534, D_unsup_loss_fake: 0.512, D_sup_loss: 0.124, D_sup_acc: 97.08 Train acc: 96.910 Test acc: 97.260 \n",
      "step: 8925 | Train: G_Loss: 1.273, D_unsup_loss_real: 0.497, D_unsup_loss_fake: 0.566, D_sup_loss: 0.113, D_sup_acc: 97.29 Train acc: 96.973 Test acc: 97.240 \n",
      "step: 8926 | Train: G_Loss: 1.302, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.550, D_sup_loss: 0.112, D_sup_acc: 97.27 Train acc: 96.845 Test acc: 97.210 \n",
      "step: 8927 | Train: G_Loss: 1.325, D_unsup_loss_real: 0.648, D_unsup_loss_fake: 0.496, D_sup_loss: 0.120, D_sup_acc: 97.25 Train acc: 97.070 Test acc: 97.310 \n",
      "step: 8928 | Train: G_Loss: 1.319, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.567, D_sup_loss: 0.103, D_sup_acc: 97.34 Train acc: 96.963 Test acc: 97.290 \n",
      "step: 8929 | Train: G_Loss: 1.339, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.570, D_sup_loss: 0.111, D_sup_acc: 97.32 Train acc: 96.750 Test acc: 97.050 \n",
      "step: 8930 | Train: G_Loss: 1.259, D_unsup_loss_real: 0.509, D_unsup_loss_fake: 0.662, D_sup_loss: 0.124, D_sup_acc: 97.09 Train acc: 96.940 Test acc: 97.220 \n",
      "step: 8931 | Train: G_Loss: 1.286, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.548, D_sup_loss: 0.114, D_sup_acc: 97.26 Train acc: 96.917 Test acc: 97.220 \n",
      "step: 8932 | Train: G_Loss: 1.229, D_unsup_loss_real: 0.526, D_unsup_loss_fake: 0.532, D_sup_loss: 0.114, D_sup_acc: 97.26 Train acc: 96.230 Test acc: 96.540 \n",
      "step: 8933 | Train: G_Loss: 1.379, D_unsup_loss_real: 0.627, D_unsup_loss_fake: 0.464, D_sup_loss: 0.149, D_sup_acc: 96.58 Train acc: 96.928 Test acc: 97.260 \n",
      "step: 8934 | Train: G_Loss: 1.355, D_unsup_loss_real: 0.499, D_unsup_loss_fake: 0.545, D_sup_loss: 0.118, D_sup_acc: 97.29 Train acc: 96.903 Test acc: 97.150 \n",
      "step: 8935 | Train: G_Loss: 1.254, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.644, D_sup_loss: 0.112, D_sup_acc: 97.19 Train acc: 96.815 Test acc: 97.180 \n",
      "step: 8936 | Train: G_Loss: 1.383, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.582, D_sup_loss: 0.122, D_sup_acc: 97.22 Train acc: 96.368 Test acc: 96.710 \n",
      "step: 8937 | Train: G_Loss: 1.452, D_unsup_loss_real: 0.523, D_unsup_loss_fake: 0.470, D_sup_loss: 0.137, D_sup_acc: 96.75 Train acc: 96.798 Test acc: 97.120 \n",
      "step: 8938 | Train: G_Loss: 1.323, D_unsup_loss_real: 0.584, D_unsup_loss_fake: 0.498, D_sup_loss: 0.116, D_sup_acc: 97.16 Train acc: 96.897 Test acc: 97.220 \n",
      "step: 8939 | Train: G_Loss: 1.250, D_unsup_loss_real: 0.626, D_unsup_loss_fake: 0.547, D_sup_loss: 0.111, D_sup_acc: 97.26 Train acc: 96.733 Test acc: 97.040 \n",
      "step: 8940 | Train: G_Loss: 1.363, D_unsup_loss_real: 0.504, D_unsup_loss_fake: 0.489, D_sup_loss: 0.124, D_sup_acc: 97.08 Train acc: 96.982 Test acc: 97.280 \n",
      "step: 8941 | Train: G_Loss: 1.180, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.539, D_sup_loss: 0.112, D_sup_acc: 97.31 Train acc: 96.832 Test acc: 97.150 \n",
      "step: 8942 | Train: G_Loss: 1.369, D_unsup_loss_real: 0.489, D_unsup_loss_fake: 0.593, D_sup_loss: 0.119, D_sup_acc: 97.19 Train acc: 96.772 Test acc: 97.150 \n",
      "step: 8943 | Train: G_Loss: 1.246, D_unsup_loss_real: 0.515, D_unsup_loss_fake: 0.541, D_sup_loss: 0.118, D_sup_acc: 97.19 Train acc: 97.000 Test acc: 97.320 \n",
      "step: 8944 | Train: G_Loss: 1.201, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.510, D_sup_loss: 0.111, D_sup_acc: 97.35 Train acc: 96.673 Test acc: 97.010 \n",
      "step: 8945 | Train: G_Loss: 1.265, D_unsup_loss_real: 0.510, D_unsup_loss_fake: 0.534, D_sup_loss: 0.129, D_sup_acc: 97.05 Train acc: 96.952 Test acc: 97.230 \n",
      "step: 8946 | Train: G_Loss: 1.385, D_unsup_loss_real: 0.499, D_unsup_loss_fake: 0.575, D_sup_loss: 0.110, D_sup_acc: 97.27 Train acc: 96.728 Test acc: 97.070 \n",
      "step: 8947 | Train: G_Loss: 1.219, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.563, D_sup_loss: 0.127, D_sup_acc: 97.11 Train acc: 96.900 Test acc: 97.190 \n",
      "step: 8948 | Train: G_Loss: 1.290, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.552, D_sup_loss: 0.116, D_sup_acc: 97.23 Train acc: 96.652 Test acc: 96.980 \n",
      "step: 8949 | Train: G_Loss: 1.289, D_unsup_loss_real: 0.609, D_unsup_loss_fake: 0.636, D_sup_loss: 0.127, D_sup_acc: 97.02 Train acc: 96.832 Test acc: 97.230 \n",
      "step: 8950 | Train: G_Loss: 1.336, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.587, D_sup_loss: 0.119, D_sup_acc: 97.27 Train acc: 97.080 Test acc: 97.410 \n",
      "step: 8951 | Train: G_Loss: 1.229, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.526, D_sup_loss: 0.105, D_sup_acc: 97.44 Train acc: 97.055 Test acc: 97.320 \n",
      "step: 8952 | Train: G_Loss: 1.206, D_unsup_loss_real: 0.501, D_unsup_loss_fake: 0.566, D_sup_loss: 0.109, D_sup_acc: 97.35 Train acc: 97.035 Test acc: 97.380 \n",
      "step: 8953 | Train: G_Loss: 1.224, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.520, D_sup_loss: 0.110, D_sup_acc: 97.41 Train acc: 96.797 Test acc: 97.170 \n",
      "step: 8954 | Train: G_Loss: 1.354, D_unsup_loss_real: 0.510, D_unsup_loss_fake: 0.546, D_sup_loss: 0.119, D_sup_acc: 97.21 Train acc: 96.987 Test acc: 97.300 \n",
      "step: 8955 | Train: G_Loss: 1.233, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.528, D_sup_loss: 0.109, D_sup_acc: 97.33 Train acc: 97.017 Test acc: 97.310 \n",
      "step: 8956 | Train: G_Loss: 1.203, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.558, D_sup_loss: 0.111, D_sup_acc: 97.34 Train acc: 96.958 Test acc: 97.270 \n",
      "step: 8957 | Train: G_Loss: 1.297, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.547, D_sup_loss: 0.114, D_sup_acc: 97.30 Train acc: 97.025 Test acc: 97.280 \n",
      "step: 8958 | Train: G_Loss: 1.204, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.509, D_sup_loss: 0.110, D_sup_acc: 97.31 Train acc: 96.892 Test acc: 97.140 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 8959 | Train: G_Loss: 1.210, D_unsup_loss_real: 0.487, D_unsup_loss_fake: 0.532, D_sup_loss: 0.119, D_sup_acc: 97.18 Train acc: 97.090 Test acc: 97.380 \n",
      "step: 8960 | Train: G_Loss: 1.292, D_unsup_loss_real: 0.492, D_unsup_loss_fake: 0.587, D_sup_loss: 0.109, D_sup_acc: 97.41 Train acc: 96.878 Test acc: 97.170 \n",
      "step: 8961 | Train: G_Loss: 1.317, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.554, D_sup_loss: 0.120, D_sup_acc: 97.21 Train acc: 97.160 Test acc: 97.390 \n",
      "step: 8962 | Train: G_Loss: 1.237, D_unsup_loss_real: 0.524, D_unsup_loss_fake: 0.535, D_sup_loss: 0.104, D_sup_acc: 97.42 Train acc: 96.627 Test acc: 96.930 \n",
      "step: 8963 | Train: G_Loss: 1.403, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.547, D_sup_loss: 0.129, D_sup_acc: 96.97 Train acc: 97.105 Test acc: 97.320 \n",
      "step: 8964 | Train: G_Loss: 1.206, D_unsup_loss_real: 0.523, D_unsup_loss_fake: 0.520, D_sup_loss: 0.106, D_sup_acc: 97.35 Train acc: 96.992 Test acc: 97.210 \n",
      "step: 8965 | Train: G_Loss: 1.332, D_unsup_loss_real: 0.513, D_unsup_loss_fake: 0.561, D_sup_loss: 0.112, D_sup_acc: 97.25 Train acc: 97.095 Test acc: 97.320 \n",
      "step: 8966 | Train: G_Loss: 1.360, D_unsup_loss_real: 0.518, D_unsup_loss_fake: 0.508, D_sup_loss: 0.109, D_sup_acc: 97.35 Train acc: 97.142 Test acc: 97.380 \n",
      "step: 8967 | Train: G_Loss: 1.229, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.516, D_sup_loss: 0.106, D_sup_acc: 97.41 Train acc: 96.960 Test acc: 97.270 \n",
      "step: 8968 | Train: G_Loss: 1.259, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.511, D_sup_loss: 0.113, D_sup_acc: 97.30 Train acc: 96.803 Test acc: 97.150 \n",
      "step: 8969 | Train: G_Loss: 1.361, D_unsup_loss_real: 0.475, D_unsup_loss_fake: 0.603, D_sup_loss: 0.122, D_sup_acc: 97.19 Train acc: 96.920 Test acc: 97.150 \n",
      "step: 8970 | Train: G_Loss: 1.153, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.531, D_sup_loss: 0.118, D_sup_acc: 97.19 Train acc: 97.088 Test acc: 97.290 \n",
      "step: 8971 | Train: G_Loss: 1.260, D_unsup_loss_real: 0.475, D_unsup_loss_fake: 0.534, D_sup_loss: 0.107, D_sup_acc: 97.32 Train acc: 96.652 Test acc: 96.960 \n",
      "step: 8972 | Train: G_Loss: 1.333, D_unsup_loss_real: 0.516, D_unsup_loss_fake: 0.609, D_sup_loss: 0.130, D_sup_acc: 97.00 Train acc: 96.722 Test acc: 97.000 \n",
      "step: 8973 | Train: G_Loss: 1.223, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.569, D_sup_loss: 0.123, D_sup_acc: 97.04 Train acc: 95.787 Test acc: 96.130 \n",
      "step: 8974 | Train: G_Loss: 1.226, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.552, D_sup_loss: 0.162, D_sup_acc: 96.18 Train acc: 96.268 Test acc: 96.620 \n",
      "step: 8975 | Train: G_Loss: 1.312, D_unsup_loss_real: 0.493, D_unsup_loss_fake: 0.545, D_sup_loss: 0.144, D_sup_acc: 96.66 Train acc: 96.695 Test acc: 96.990 \n",
      "step: 8976 | Train: G_Loss: 1.301, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.583, D_sup_loss: 0.123, D_sup_acc: 97.03 Train acc: 96.927 Test acc: 97.130 \n",
      "step: 8977 | Train: G_Loss: 1.293, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.604, D_sup_loss: 0.116, D_sup_acc: 97.17 Train acc: 96.722 Test acc: 96.950 \n",
      "step: 8978 | Train: G_Loss: 1.321, D_unsup_loss_real: 0.535, D_unsup_loss_fake: 0.546, D_sup_loss: 0.124, D_sup_acc: 96.99 Train acc: 96.930 Test acc: 97.160 \n",
      "step: 8979 | Train: G_Loss: 1.368, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.557, D_sup_loss: 0.114, D_sup_acc: 97.20 Train acc: 97.075 Test acc: 97.320 \n",
      "step: 8980 | Train: G_Loss: 1.270, D_unsup_loss_real: 0.519, D_unsup_loss_fake: 0.529, D_sup_loss: 0.109, D_sup_acc: 97.35 Train acc: 96.812 Test acc: 97.000 \n",
      "step: 8981 | Train: G_Loss: 1.408, D_unsup_loss_real: 0.462, D_unsup_loss_fake: 0.554, D_sup_loss: 0.119, D_sup_acc: 97.04 Train acc: 97.067 Test acc: 97.260 \n",
      "step: 8982 | Train: G_Loss: 1.371, D_unsup_loss_real: 0.646, D_unsup_loss_fake: 0.574, D_sup_loss: 0.106, D_sup_acc: 97.29 Train acc: 96.917 Test acc: 97.160 \n",
      "step: 8983 | Train: G_Loss: 1.257, D_unsup_loss_real: 0.604, D_unsup_loss_fake: 0.576, D_sup_loss: 0.115, D_sup_acc: 97.20 Train acc: 96.840 Test acc: 97.170 \n",
      "step: 8984 | Train: G_Loss: 1.282, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.604, D_sup_loss: 0.119, D_sup_acc: 97.21 Train acc: 96.922 Test acc: 97.180 \n",
      "step: 8985 | Train: G_Loss: 1.321, D_unsup_loss_real: 0.606, D_unsup_loss_fake: 0.509, D_sup_loss: 0.115, D_sup_acc: 97.22 Train acc: 96.855 Test acc: 97.130 \n",
      "step: 8986 | Train: G_Loss: 1.270, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.575, D_sup_loss: 0.120, D_sup_acc: 97.17 Train acc: 96.752 Test acc: 97.040 \n",
      "step: 8987 | Train: G_Loss: 1.417, D_unsup_loss_real: 0.524, D_unsup_loss_fake: 0.553, D_sup_loss: 0.124, D_sup_acc: 97.08 Train acc: 97.057 Test acc: 97.360 \n",
      "step: 8988 | Train: G_Loss: 1.357, D_unsup_loss_real: 0.536, D_unsup_loss_fake: 0.596, D_sup_loss: 0.109, D_sup_acc: 97.39 Train acc: 96.858 Test acc: 97.070 \n",
      "step: 8989 | Train: G_Loss: 1.276, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.522, D_sup_loss: 0.120, D_sup_acc: 97.11 Train acc: 96.628 Test acc: 96.870 \n",
      "step: 8990 | Train: G_Loss: 1.225, D_unsup_loss_real: 0.586, D_unsup_loss_fake: 0.571, D_sup_loss: 0.129, D_sup_acc: 96.91 Train acc: 96.672 Test acc: 96.930 \n",
      "step: 8991 | Train: G_Loss: 1.374, D_unsup_loss_real: 0.541, D_unsup_loss_fake: 0.565, D_sup_loss: 0.134, D_sup_acc: 96.97 Train acc: 97.077 Test acc: 97.340 \n",
      "step: 8992 | Train: G_Loss: 1.252, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.534, D_sup_loss: 0.112, D_sup_acc: 97.37 Train acc: 96.808 Test acc: 97.110 \n",
      "step: 8993 | Train: G_Loss: 1.286, D_unsup_loss_real: 0.495, D_unsup_loss_fake: 0.574, D_sup_loss: 0.126, D_sup_acc: 97.15 Train acc: 96.838 Test acc: 97.230 \n",
      "step: 8994 | Train: G_Loss: 1.243, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.570, D_sup_loss: 0.119, D_sup_acc: 97.27 Train acc: 96.820 Test acc: 97.120 \n",
      "step: 8995 | Train: G_Loss: 1.319, D_unsup_loss_real: 0.644, D_unsup_loss_fake: 0.512, D_sup_loss: 0.122, D_sup_acc: 97.16 Train acc: 96.987 Test acc: 97.250 \n",
      "step: 8996 | Train: G_Loss: 1.247, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.524, D_sup_loss: 0.112, D_sup_acc: 97.28 Train acc: 96.820 Test acc: 97.150 \n",
      "step: 8997 | Train: G_Loss: 1.404, D_unsup_loss_real: 0.528, D_unsup_loss_fake: 0.535, D_sup_loss: 0.120, D_sup_acc: 97.19 Train acc: 96.957 Test acc: 97.260 \n",
      "step: 8998 | Train: G_Loss: 1.226, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.694, D_sup_loss: 0.112, D_sup_acc: 97.29 Train acc: 96.802 Test acc: 97.120 \n",
      "step: 8999 | Train: G_Loss: 1.279, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.518, D_sup_loss: 0.121, D_sup_acc: 97.16 Train acc: 97.105 Test acc: 97.390 \n",
      "step: 9000 | Train: G_Loss: 1.239, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.613, D_sup_loss: 0.109, D_sup_acc: 97.42 Train acc: 96.548 Test acc: 96.820 \n",
      "Train Classifier Accuracy: 96.548%\n",
      "\n",
      "Test Classifier Accuracy: 96.820%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_9000.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_9000.h5\n",
      "step: 9001 | Train: G_Loss: 1.259, D_unsup_loss_real: 0.624, D_unsup_loss_fake: 0.573, D_sup_loss: 0.137, D_sup_acc: 96.86 Train acc: 96.887 Test acc: 97.110 \n",
      "step: 9002 | Train: G_Loss: 1.357, D_unsup_loss_real: 0.527, D_unsup_loss_fake: 0.623, D_sup_loss: 0.118, D_sup_acc: 97.15 Train acc: 96.882 Test acc: 97.090 \n",
      "step: 9003 | Train: G_Loss: 1.346, D_unsup_loss_real: 0.541, D_unsup_loss_fake: 0.587, D_sup_loss: 0.116, D_sup_acc: 97.13 Train acc: 97.057 Test acc: 97.300 \n",
      "step: 9004 | Train: G_Loss: 1.222, D_unsup_loss_real: 0.533, D_unsup_loss_fake: 0.569, D_sup_loss: 0.111, D_sup_acc: 97.33 Train acc: 96.947 Test acc: 97.290 \n",
      "step: 9005 | Train: G_Loss: 1.370, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.495, D_sup_loss: 0.116, D_sup_acc: 97.32 Train acc: 97.063 Test acc: 97.370 \n",
      "step: 9006 | Train: G_Loss: 1.492, D_unsup_loss_real: 0.476, D_unsup_loss_fake: 0.534, D_sup_loss: 0.110, D_sup_acc: 97.40 Train acc: 97.030 Test acc: 97.290 \n",
      "step: 9007 | Train: G_Loss: 1.238, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.539, D_sup_loss: 0.109, D_sup_acc: 97.32 Train acc: 96.972 Test acc: 97.290 \n",
      "step: 9008 | Train: G_Loss: 1.184, D_unsup_loss_real: 0.507, D_unsup_loss_fake: 0.484, D_sup_loss: 0.115, D_sup_acc: 97.32 Train acc: 96.807 Test acc: 97.120 \n",
      "step: 9009 | Train: G_Loss: 1.497, D_unsup_loss_real: 0.512, D_unsup_loss_fake: 0.541, D_sup_loss: 0.120, D_sup_acc: 97.16 Train acc: 96.998 Test acc: 97.310 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 9010 | Train: G_Loss: 1.325, D_unsup_loss_real: 0.530, D_unsup_loss_fake: 0.549, D_sup_loss: 0.108, D_sup_acc: 97.34 Train acc: 96.922 Test acc: 97.160 \n",
      "step: 9011 | Train: G_Loss: 1.281, D_unsup_loss_real: 0.550, D_unsup_loss_fake: 0.547, D_sup_loss: 0.120, D_sup_acc: 97.20 Train acc: 97.042 Test acc: 97.280 \n",
      "step: 9012 | Train: G_Loss: 1.241, D_unsup_loss_real: 0.544, D_unsup_loss_fake: 0.563, D_sup_loss: 0.111, D_sup_acc: 97.31 Train acc: 97.115 Test acc: 97.450 \n",
      "step: 9013 | Train: G_Loss: 1.281, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.494, D_sup_loss: 0.107, D_sup_acc: 97.48 Train acc: 97.132 Test acc: 97.450 \n",
      "step: 9014 | Train: G_Loss: 1.223, D_unsup_loss_real: 0.524, D_unsup_loss_fake: 0.598, D_sup_loss: 0.104, D_sup_acc: 97.48 Train acc: 96.770 Test acc: 97.150 \n",
      "step: 9015 | Train: G_Loss: 1.283, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.554, D_sup_loss: 0.121, D_sup_acc: 97.19 Train acc: 96.813 Test acc: 97.120 \n",
      "step: 9016 | Train: G_Loss: 1.339, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.570, D_sup_loss: 0.123, D_sup_acc: 97.16 Train acc: 97.030 Test acc: 97.300 \n",
      "step: 9017 | Train: G_Loss: 1.195, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.524, D_sup_loss: 0.110, D_sup_acc: 97.33 Train acc: 96.822 Test acc: 97.160 \n",
      "step: 9018 | Train: G_Loss: 1.343, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.579, D_sup_loss: 0.121, D_sup_acc: 97.20 Train acc: 97.227 Test acc: 97.420 \n",
      "step: 9019 | Train: G_Loss: 1.255, D_unsup_loss_real: 0.505, D_unsup_loss_fake: 0.513, D_sup_loss: 0.105, D_sup_acc: 97.45 Train acc: 97.155 Test acc: 97.370 \n",
      "step: 9020 | Train: G_Loss: 1.232, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.519, D_sup_loss: 0.107, D_sup_acc: 97.40 Train acc: 97.078 Test acc: 97.310 \n",
      "step: 9021 | Train: G_Loss: 1.240, D_unsup_loss_real: 0.527, D_unsup_loss_fake: 0.583, D_sup_loss: 0.112, D_sup_acc: 97.34 Train acc: 96.953 Test acc: 97.250 \n",
      "step: 9022 | Train: G_Loss: 1.259, D_unsup_loss_real: 0.495, D_unsup_loss_fake: 0.612, D_sup_loss: 0.119, D_sup_acc: 97.28 Train acc: 96.682 Test acc: 96.980 \n",
      "step: 9023 | Train: G_Loss: 1.374, D_unsup_loss_real: 0.524, D_unsup_loss_fake: 0.545, D_sup_loss: 0.129, D_sup_acc: 97.02 Train acc: 97.198 Test acc: 97.450 \n",
      "step: 9024 | Train: G_Loss: 1.315, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.560, D_sup_loss: 0.102, D_sup_acc: 97.48 Train acc: 96.885 Test acc: 97.290 \n",
      "step: 9025 | Train: G_Loss: 1.189, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.546, D_sup_loss: 0.121, D_sup_acc: 97.32 Train acc: 96.697 Test acc: 97.060 \n",
      "step: 9026 | Train: G_Loss: 1.416, D_unsup_loss_real: 0.514, D_unsup_loss_fake: 0.625, D_sup_loss: 0.126, D_sup_acc: 97.10 Train acc: 96.950 Test acc: 97.300 \n",
      "step: 9027 | Train: G_Loss: 1.331, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.518, D_sup_loss: 0.114, D_sup_acc: 97.33 Train acc: 97.118 Test acc: 97.350 \n",
      "step: 9028 | Train: G_Loss: 1.249, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.474, D_sup_loss: 0.106, D_sup_acc: 97.38 Train acc: 97.065 Test acc: 97.330 \n",
      "step: 9029 | Train: G_Loss: 1.151, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.524, D_sup_loss: 0.108, D_sup_acc: 97.36 Train acc: 96.428 Test acc: 96.780 \n",
      "step: 9030 | Train: G_Loss: 1.412, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.556, D_sup_loss: 0.141, D_sup_acc: 96.82 Train acc: 97.097 Test acc: 97.280 \n",
      "step: 9031 | Train: G_Loss: 1.326, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.507, D_sup_loss: 0.106, D_sup_acc: 97.31 Train acc: 97.142 Test acc: 97.370 \n",
      "step: 9032 | Train: G_Loss: 1.259, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.551, D_sup_loss: 0.103, D_sup_acc: 97.40 Train acc: 96.500 Test acc: 96.670 \n",
      "step: 9033 | Train: G_Loss: 1.218, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.662, D_sup_loss: 0.136, D_sup_acc: 96.71 Train acc: 96.917 Test acc: 97.310 \n",
      "step: 9034 | Train: G_Loss: 1.393, D_unsup_loss_real: 0.628, D_unsup_loss_fake: 0.587, D_sup_loss: 0.117, D_sup_acc: 97.34 Train acc: 97.157 Test acc: 97.370 \n",
      "step: 9035 | Train: G_Loss: 1.364, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.581, D_sup_loss: 0.106, D_sup_acc: 97.40 Train acc: 97.087 Test acc: 97.310 \n",
      "step: 9036 | Train: G_Loss: 1.175, D_unsup_loss_real: 0.511, D_unsup_loss_fake: 0.568, D_sup_loss: 0.109, D_sup_acc: 97.34 Train acc: 96.373 Test acc: 96.640 \n",
      "step: 9037 | Train: G_Loss: 1.260, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.539, D_sup_loss: 0.142, D_sup_acc: 96.68 Train acc: 96.915 Test acc: 97.290 \n",
      "step: 9038 | Train: G_Loss: 1.347, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.520, D_sup_loss: 0.115, D_sup_acc: 97.32 Train acc: 96.973 Test acc: 97.190 \n",
      "step: 9039 | Train: G_Loss: 1.276, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.591, D_sup_loss: 0.111, D_sup_acc: 97.23 Train acc: 96.252 Test acc: 96.580 \n",
      "step: 9040 | Train: G_Loss: 1.170, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.499, D_sup_loss: 0.146, D_sup_acc: 96.62 Train acc: 96.260 Test acc: 96.650 \n",
      "step: 9041 | Train: G_Loss: 1.431, D_unsup_loss_real: 0.534, D_unsup_loss_fake: 0.497, D_sup_loss: 0.145, D_sup_acc: 96.69 Train acc: 96.808 Test acc: 97.000 \n",
      "step: 9042 | Train: G_Loss: 1.478, D_unsup_loss_real: 0.644, D_unsup_loss_fake: 0.557, D_sup_loss: 0.109, D_sup_acc: 97.04 Train acc: 96.118 Test acc: 96.370 \n",
      "step: 9043 | Train: G_Loss: 1.358, D_unsup_loss_real: 0.797, D_unsup_loss_fake: 0.638, D_sup_loss: 0.163, D_sup_acc: 96.42 Train acc: 96.938 Test acc: 97.190 \n",
      "step: 9044 | Train: G_Loss: 1.261, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.556, D_sup_loss: 0.111, D_sup_acc: 97.23 Train acc: 96.862 Test acc: 97.160 \n",
      "step: 9045 | Train: G_Loss: 1.178, D_unsup_loss_real: 0.475, D_unsup_loss_fake: 0.586, D_sup_loss: 0.118, D_sup_acc: 97.20 Train acc: 96.918 Test acc: 97.240 \n",
      "step: 9046 | Train: G_Loss: 1.223, D_unsup_loss_real: 0.518, D_unsup_loss_fake: 0.560, D_sup_loss: 0.117, D_sup_acc: 97.27 Train acc: 97.065 Test acc: 97.260 \n",
      "step: 9047 | Train: G_Loss: 1.317, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.621, D_sup_loss: 0.110, D_sup_acc: 97.29 Train acc: 96.762 Test acc: 97.120 \n",
      "step: 9048 | Train: G_Loss: 1.294, D_unsup_loss_real: 0.529, D_unsup_loss_fake: 0.549, D_sup_loss: 0.125, D_sup_acc: 97.16 Train acc: 97.118 Test acc: 97.310 \n",
      "step: 9049 | Train: G_Loss: 1.269, D_unsup_loss_real: 0.503, D_unsup_loss_fake: 0.551, D_sup_loss: 0.106, D_sup_acc: 97.34 Train acc: 96.883 Test acc: 97.090 \n",
      "step: 9050 | Train: G_Loss: 1.304, D_unsup_loss_real: 0.492, D_unsup_loss_fake: 0.519, D_sup_loss: 0.119, D_sup_acc: 97.13 Train acc: 96.878 Test acc: 97.160 \n",
      "step: 9051 | Train: G_Loss: 1.260, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.560, D_sup_loss: 0.118, D_sup_acc: 97.20 Train acc: 96.783 Test acc: 97.050 \n",
      "step: 9052 | Train: G_Loss: 1.275, D_unsup_loss_real: 0.529, D_unsup_loss_fake: 0.628, D_sup_loss: 0.123, D_sup_acc: 97.09 Train acc: 96.735 Test acc: 96.950 \n",
      "step: 9053 | Train: G_Loss: 1.467, D_unsup_loss_real: 0.602, D_unsup_loss_fake: 0.545, D_sup_loss: 0.125, D_sup_acc: 96.99 Train acc: 97.058 Test acc: 97.230 \n",
      "step: 9054 | Train: G_Loss: 1.238, D_unsup_loss_real: 0.607, D_unsup_loss_fake: 0.552, D_sup_loss: 0.110, D_sup_acc: 97.27 Train acc: 96.822 Test acc: 97.050 \n",
      "step: 9055 | Train: G_Loss: 1.289, D_unsup_loss_real: 0.483, D_unsup_loss_fake: 0.498, D_sup_loss: 0.125, D_sup_acc: 97.09 Train acc: 96.818 Test acc: 97.000 \n",
      "step: 9056 | Train: G_Loss: 1.240, D_unsup_loss_real: 0.536, D_unsup_loss_fake: 0.542, D_sup_loss: 0.123, D_sup_acc: 97.04 Train acc: 97.032 Test acc: 97.230 \n",
      "step: 9057 | Train: G_Loss: 1.264, D_unsup_loss_real: 0.528, D_unsup_loss_fake: 0.546, D_sup_loss: 0.114, D_sup_acc: 97.27 Train acc: 96.990 Test acc: 97.210 \n",
      "step: 9058 | Train: G_Loss: 1.345, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.476, D_sup_loss: 0.114, D_sup_acc: 97.25 Train acc: 97.155 Test acc: 97.250 \n",
      "step: 9059 | Train: G_Loss: 1.258, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.615, D_sup_loss: 0.107, D_sup_acc: 97.28 Train acc: 96.923 Test acc: 97.180 \n",
      "step: 9060 | Train: G_Loss: 1.136, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.539, D_sup_loss: 0.115, D_sup_acc: 97.22 Train acc: 96.802 Test acc: 97.110 \n",
      "step: 9061 | Train: G_Loss: 1.292, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.617, D_sup_loss: 0.123, D_sup_acc: 97.15 Train acc: 96.753 Test acc: 97.120 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 9062 | Train: G_Loss: 1.362, D_unsup_loss_real: 0.529, D_unsup_loss_fake: 0.526, D_sup_loss: 0.123, D_sup_acc: 97.16 Train acc: 96.760 Test acc: 97.180 \n",
      "step: 9063 | Train: G_Loss: 1.321, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.565, D_sup_loss: 0.119, D_sup_acc: 97.22 Train acc: 96.763 Test acc: 97.070 \n",
      "step: 9064 | Train: G_Loss: 1.408, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.502, D_sup_loss: 0.122, D_sup_acc: 97.11 Train acc: 97.082 Test acc: 97.330 \n",
      "step: 9065 | Train: G_Loss: 1.378, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.488, D_sup_loss: 0.108, D_sup_acc: 97.36 Train acc: 97.022 Test acc: 97.280 \n",
      "step: 9066 | Train: G_Loss: 1.191, D_unsup_loss_real: 0.543, D_unsup_loss_fake: 0.589, D_sup_loss: 0.109, D_sup_acc: 97.31 Train acc: 97.000 Test acc: 97.260 \n",
      "step: 9067 | Train: G_Loss: 1.300, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.594, D_sup_loss: 0.112, D_sup_acc: 97.29 Train acc: 96.815 Test acc: 97.140 \n",
      "step: 9068 | Train: G_Loss: 1.350, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.530, D_sup_loss: 0.118, D_sup_acc: 97.18 Train acc: 97.153 Test acc: 97.280 \n",
      "step: 9069 | Train: G_Loss: 1.209, D_unsup_loss_real: 0.514, D_unsup_loss_fake: 0.506, D_sup_loss: 0.103, D_sup_acc: 97.31 Train acc: 96.728 Test acc: 97.020 \n",
      "step: 9070 | Train: G_Loss: 1.402, D_unsup_loss_real: 0.529, D_unsup_loss_fake: 0.527, D_sup_loss: 0.127, D_sup_acc: 97.06 Train acc: 97.025 Test acc: 97.370 \n",
      "step: 9071 | Train: G_Loss: 1.277, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.504, D_sup_loss: 0.109, D_sup_acc: 97.40 Train acc: 97.125 Test acc: 97.370 \n",
      "step: 9072 | Train: G_Loss: 1.177, D_unsup_loss_real: 0.508, D_unsup_loss_fake: 0.539, D_sup_loss: 0.103, D_sup_acc: 97.40 Train acc: 96.925 Test acc: 97.170 \n",
      "step: 9073 | Train: G_Loss: 1.294, D_unsup_loss_real: 0.527, D_unsup_loss_fake: 0.572, D_sup_loss: 0.118, D_sup_acc: 97.21 Train acc: 95.965 Test acc: 96.120 \n",
      "step: 9074 | Train: G_Loss: 1.385, D_unsup_loss_real: 0.533, D_unsup_loss_fake: 0.658, D_sup_loss: 0.156, D_sup_acc: 96.17 Train acc: 97.072 Test acc: 97.390 \n",
      "step: 9075 | Train: G_Loss: 1.343, D_unsup_loss_real: 0.542, D_unsup_loss_fake: 0.480, D_sup_loss: 0.104, D_sup_acc: 97.42 Train acc: 96.893 Test acc: 97.250 \n",
      "step: 9076 | Train: G_Loss: 1.421, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.587, D_sup_loss: 0.113, D_sup_acc: 97.28 Train acc: 96.807 Test acc: 97.250 \n",
      "step: 9077 | Train: G_Loss: 1.316, D_unsup_loss_real: 0.483, D_unsup_loss_fake: 0.549, D_sup_loss: 0.119, D_sup_acc: 97.28 Train acc: 96.797 Test acc: 97.230 \n",
      "step: 9078 | Train: G_Loss: 1.315, D_unsup_loss_real: 0.640, D_unsup_loss_fake: 0.506, D_sup_loss: 0.120, D_sup_acc: 97.27 Train acc: 96.995 Test acc: 97.220 \n",
      "step: 9079 | Train: G_Loss: 1.360, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.559, D_sup_loss: 0.113, D_sup_acc: 97.26 Train acc: 97.158 Test acc: 97.390 \n",
      "step: 9080 | Train: G_Loss: 1.256, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.608, D_sup_loss: 0.101, D_sup_acc: 97.42 Train acc: 96.738 Test acc: 97.110 \n",
      "step: 9081 | Train: G_Loss: 1.314, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.620, D_sup_loss: 0.128, D_sup_acc: 97.15 Train acc: 97.100 Test acc: 97.350 \n",
      "step: 9082 | Train: G_Loss: 1.285, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.569, D_sup_loss: 0.107, D_sup_acc: 97.38 Train acc: 96.777 Test acc: 97.090 \n",
      "step: 9083 | Train: G_Loss: 1.299, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.674, D_sup_loss: 0.119, D_sup_acc: 97.13 Train acc: 97.107 Test acc: 97.420 \n",
      "step: 9084 | Train: G_Loss: 1.326, D_unsup_loss_real: 0.631, D_unsup_loss_fake: 0.558, D_sup_loss: 0.107, D_sup_acc: 97.45 Train acc: 96.828 Test acc: 97.230 \n",
      "step: 9085 | Train: G_Loss: 1.341, D_unsup_loss_real: 0.515, D_unsup_loss_fake: 0.498, D_sup_loss: 0.120, D_sup_acc: 97.27 Train acc: 96.855 Test acc: 97.190 \n",
      "step: 9086 | Train: G_Loss: 1.264, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.549, D_sup_loss: 0.119, D_sup_acc: 97.23 Train acc: 96.938 Test acc: 97.350 \n",
      "step: 9087 | Train: G_Loss: 1.176, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.558, D_sup_loss: 0.118, D_sup_acc: 97.38 Train acc: 96.977 Test acc: 97.340 \n",
      "step: 9088 | Train: G_Loss: 1.335, D_unsup_loss_real: 0.567, D_unsup_loss_fake: 0.583, D_sup_loss: 0.117, D_sup_acc: 97.37 Train acc: 97.068 Test acc: 97.410 \n",
      "step: 9089 | Train: G_Loss: 1.263, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.560, D_sup_loss: 0.108, D_sup_acc: 97.44 Train acc: 96.770 Test acc: 97.160 \n",
      "step: 9090 | Train: G_Loss: 1.282, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.589, D_sup_loss: 0.120, D_sup_acc: 97.20 Train acc: 96.843 Test acc: 97.160 \n",
      "step: 9091 | Train: G_Loss: 1.264, D_unsup_loss_real: 0.581, D_unsup_loss_fake: 0.578, D_sup_loss: 0.121, D_sup_acc: 97.20 Train acc: 96.500 Test acc: 96.730 \n",
      "step: 9092 | Train: G_Loss: 1.362, D_unsup_loss_real: 0.504, D_unsup_loss_fake: 0.595, D_sup_loss: 0.135, D_sup_acc: 96.77 Train acc: 96.975 Test acc: 97.260 \n",
      "step: 9093 | Train: G_Loss: 1.362, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.485, D_sup_loss: 0.115, D_sup_acc: 97.29 Train acc: 97.050 Test acc: 97.330 \n",
      "step: 9094 | Train: G_Loss: 1.303, D_unsup_loss_real: 0.531, D_unsup_loss_fake: 0.555, D_sup_loss: 0.108, D_sup_acc: 97.36 Train acc: 96.842 Test acc: 97.120 \n",
      "step: 9095 | Train: G_Loss: 1.334, D_unsup_loss_real: 0.534, D_unsup_loss_fake: 0.470, D_sup_loss: 0.125, D_sup_acc: 97.16 Train acc: 97.093 Test acc: 97.440 \n",
      "step: 9096 | Train: G_Loss: 1.303, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.552, D_sup_loss: 0.108, D_sup_acc: 97.47 Train acc: 96.852 Test acc: 97.110 \n",
      "step: 9097 | Train: G_Loss: 1.435, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.511, D_sup_loss: 0.120, D_sup_acc: 97.15 Train acc: 97.108 Test acc: 97.330 \n",
      "step: 9098 | Train: G_Loss: 1.285, D_unsup_loss_real: 0.545, D_unsup_loss_fake: 0.552, D_sup_loss: 0.103, D_sup_acc: 97.36 Train acc: 96.908 Test acc: 97.260 \n",
      "step: 9099 | Train: G_Loss: 1.277, D_unsup_loss_real: 0.433, D_unsup_loss_fake: 0.525, D_sup_loss: 0.115, D_sup_acc: 97.29 Train acc: 96.922 Test acc: 97.220 \n",
      "step: 9100 | Train: G_Loss: 1.257, D_unsup_loss_real: 0.595, D_unsup_loss_fake: 0.536, D_sup_loss: 0.114, D_sup_acc: 97.26 Train acc: 96.957 Test acc: 97.270 \n",
      "Train Classifier Accuracy: 96.957%\n",
      "\n",
      "Test Classifier Accuracy: 97.270%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_9100.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_9100.h5\n",
      "step: 9101 | Train: G_Loss: 1.327, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.539, D_sup_loss: 0.111, D_sup_acc: 97.30 Train acc: 96.945 Test acc: 97.300 \n",
      "step: 9102 | Train: G_Loss: 1.215, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.611, D_sup_loss: 0.114, D_sup_acc: 97.33 Train acc: 96.970 Test acc: 97.270 \n",
      "step: 9103 | Train: G_Loss: 1.350, D_unsup_loss_real: 0.535, D_unsup_loss_fake: 0.518, D_sup_loss: 0.110, D_sup_acc: 97.30 Train acc: 96.852 Test acc: 97.190 \n",
      "step: 9104 | Train: G_Loss: 1.285, D_unsup_loss_real: 0.505, D_unsup_loss_fake: 0.522, D_sup_loss: 0.120, D_sup_acc: 97.23 Train acc: 96.678 Test acc: 97.000 \n",
      "step: 9105 | Train: G_Loss: 1.332, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.564, D_sup_loss: 0.124, D_sup_acc: 97.04 Train acc: 96.942 Test acc: 97.200 \n",
      "step: 9106 | Train: G_Loss: 1.193, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.554, D_sup_loss: 0.110, D_sup_acc: 97.24 Train acc: 96.852 Test acc: 97.210 \n",
      "step: 9107 | Train: G_Loss: 1.180, D_unsup_loss_real: 0.520, D_unsup_loss_fake: 0.579, D_sup_loss: 0.119, D_sup_acc: 97.25 Train acc: 96.732 Test acc: 97.020 \n",
      "step: 9108 | Train: G_Loss: 1.176, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.543, D_sup_loss: 0.123, D_sup_acc: 97.06 Train acc: 96.940 Test acc: 97.370 \n",
      "step: 9109 | Train: G_Loss: 1.300, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.551, D_sup_loss: 0.111, D_sup_acc: 97.40 Train acc: 96.888 Test acc: 97.260 \n",
      "step: 9110 | Train: G_Loss: 1.254, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.537, D_sup_loss: 0.115, D_sup_acc: 97.29 Train acc: 97.003 Test acc: 97.340 \n",
      "step: 9111 | Train: G_Loss: 1.339, D_unsup_loss_real: 0.599, D_unsup_loss_fake: 0.528, D_sup_loss: 0.110, D_sup_acc: 97.37 Train acc: 96.797 Test acc: 97.110 \n",
      "step: 9112 | Train: G_Loss: 1.321, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.555, D_sup_loss: 0.123, D_sup_acc: 97.15 Train acc: 96.877 Test acc: 97.180 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 9113 | Train: G_Loss: 1.394, D_unsup_loss_real: 0.541, D_unsup_loss_fake: 0.489, D_sup_loss: 0.115, D_sup_acc: 97.22 Train acc: 96.968 Test acc: 97.240 \n",
      "step: 9114 | Train: G_Loss: 1.285, D_unsup_loss_real: 0.648, D_unsup_loss_fake: 0.504, D_sup_loss: 0.109, D_sup_acc: 97.27 Train acc: 96.678 Test acc: 97.040 \n",
      "step: 9115 | Train: G_Loss: 1.273, D_unsup_loss_real: 0.541, D_unsup_loss_fake: 0.550, D_sup_loss: 0.124, D_sup_acc: 97.08 Train acc: 96.878 Test acc: 97.220 \n",
      "step: 9116 | Train: G_Loss: 1.218, D_unsup_loss_real: 0.635, D_unsup_loss_fake: 0.635, D_sup_loss: 0.112, D_sup_acc: 97.26 Train acc: 96.573 Test acc: 96.850 \n",
      "step: 9117 | Train: G_Loss: 1.262, D_unsup_loss_real: 0.518, D_unsup_loss_fake: 0.567, D_sup_loss: 0.130, D_sup_acc: 96.89 Train acc: 96.505 Test acc: 96.800 \n",
      "step: 9118 | Train: G_Loss: 1.277, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.590, D_sup_loss: 0.131, D_sup_acc: 96.84 Train acc: 96.885 Test acc: 97.190 \n",
      "step: 9119 | Train: G_Loss: 1.396, D_unsup_loss_real: 0.564, D_unsup_loss_fake: 0.574, D_sup_loss: 0.115, D_sup_acc: 97.23 Train acc: 97.033 Test acc: 97.340 \n",
      "step: 9120 | Train: G_Loss: 1.319, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.517, D_sup_loss: 0.106, D_sup_acc: 97.37 Train acc: 96.902 Test acc: 97.150 \n",
      "step: 9121 | Train: G_Loss: 1.366, D_unsup_loss_real: 0.488, D_unsup_loss_fake: 0.566, D_sup_loss: 0.115, D_sup_acc: 97.19 Train acc: 96.693 Test acc: 96.970 \n",
      "step: 9122 | Train: G_Loss: 1.393, D_unsup_loss_real: 0.533, D_unsup_loss_fake: 0.575, D_sup_loss: 0.123, D_sup_acc: 97.01 Train acc: 96.962 Test acc: 97.240 \n",
      "step: 9123 | Train: G_Loss: 1.342, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.626, D_sup_loss: 0.113, D_sup_acc: 97.27 Train acc: 96.752 Test acc: 97.080 \n",
      "step: 9124 | Train: G_Loss: 1.246, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.552, D_sup_loss: 0.119, D_sup_acc: 97.12 Train acc: 96.628 Test acc: 97.050 \n",
      "step: 9125 | Train: G_Loss: 1.284, D_unsup_loss_real: 0.538, D_unsup_loss_fake: 0.498, D_sup_loss: 0.125, D_sup_acc: 97.09 Train acc: 96.970 Test acc: 97.340 \n",
      "step: 9126 | Train: G_Loss: 1.193, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.573, D_sup_loss: 0.108, D_sup_acc: 97.37 Train acc: 96.930 Test acc: 97.310 \n",
      "step: 9127 | Train: G_Loss: 1.360, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.547, D_sup_loss: 0.111, D_sup_acc: 97.34 Train acc: 97.113 Test acc: 97.330 \n",
      "step: 9128 | Train: G_Loss: 1.264, D_unsup_loss_real: 0.520, D_unsup_loss_fake: 0.546, D_sup_loss: 0.104, D_sup_acc: 97.36 Train acc: 96.718 Test acc: 96.970 \n",
      "step: 9129 | Train: G_Loss: 1.381, D_unsup_loss_real: 0.550, D_unsup_loss_fake: 0.647, D_sup_loss: 0.121, D_sup_acc: 97.01 Train acc: 96.710 Test acc: 97.080 \n",
      "step: 9130 | Train: G_Loss: 1.247, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.585, D_sup_loss: 0.120, D_sup_acc: 97.12 Train acc: 97.032 Test acc: 97.370 \n",
      "step: 9131 | Train: G_Loss: 1.274, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.518, D_sup_loss: 0.107, D_sup_acc: 97.40 Train acc: 97.128 Test acc: 97.410 \n",
      "step: 9132 | Train: G_Loss: 1.227, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.543, D_sup_loss: 0.101, D_sup_acc: 97.44 Train acc: 96.937 Test acc: 97.270 \n",
      "step: 9133 | Train: G_Loss: 1.228, D_unsup_loss_real: 0.545, D_unsup_loss_fake: 0.585, D_sup_loss: 0.113, D_sup_acc: 97.30 Train acc: 96.970 Test acc: 97.280 \n",
      "step: 9134 | Train: G_Loss: 1.284, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.547, D_sup_loss: 0.111, D_sup_acc: 97.31 Train acc: 96.717 Test acc: 97.080 \n",
      "step: 9135 | Train: G_Loss: 1.187, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.571, D_sup_loss: 0.117, D_sup_acc: 97.12 Train acc: 96.845 Test acc: 97.190 \n",
      "step: 9136 | Train: G_Loss: 1.327, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.515, D_sup_loss: 0.115, D_sup_acc: 97.23 Train acc: 96.627 Test acc: 96.980 \n",
      "step: 9137 | Train: G_Loss: 1.287, D_unsup_loss_real: 0.528, D_unsup_loss_fake: 0.544, D_sup_loss: 0.122, D_sup_acc: 97.02 Train acc: 96.353 Test acc: 96.750 \n",
      "step: 9138 | Train: G_Loss: 1.284, D_unsup_loss_real: 0.566, D_unsup_loss_fake: 0.602, D_sup_loss: 0.133, D_sup_acc: 96.79 Train acc: 96.975 Test acc: 97.290 \n",
      "step: 9139 | Train: G_Loss: 1.269, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.593, D_sup_loss: 0.108, D_sup_acc: 97.32 Train acc: 96.643 Test acc: 97.010 \n",
      "step: 9140 | Train: G_Loss: 1.432, D_unsup_loss_real: 0.486, D_unsup_loss_fake: 0.539, D_sup_loss: 0.125, D_sup_acc: 97.05 Train acc: 96.993 Test acc: 97.250 \n",
      "step: 9141 | Train: G_Loss: 1.217, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.549, D_sup_loss: 0.108, D_sup_acc: 97.28 Train acc: 96.605 Test acc: 97.000 \n",
      "step: 9142 | Train: G_Loss: 1.289, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.572, D_sup_loss: 0.128, D_sup_acc: 97.04 Train acc: 96.833 Test acc: 97.130 \n",
      "step: 9143 | Train: G_Loss: 1.321, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.553, D_sup_loss: 0.117, D_sup_acc: 97.17 Train acc: 96.968 Test acc: 97.270 \n",
      "step: 9144 | Train: G_Loss: 1.242, D_unsup_loss_real: 0.535, D_unsup_loss_fake: 0.497, D_sup_loss: 0.109, D_sup_acc: 97.30 Train acc: 96.733 Test acc: 97.030 \n",
      "step: 9145 | Train: G_Loss: 1.360, D_unsup_loss_real: 0.493, D_unsup_loss_fake: 0.496, D_sup_loss: 0.122, D_sup_acc: 97.07 Train acc: 96.718 Test acc: 96.970 \n",
      "step: 9146 | Train: G_Loss: 1.289, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.551, D_sup_loss: 0.122, D_sup_acc: 97.01 Train acc: 96.818 Test acc: 97.090 \n",
      "step: 9147 | Train: G_Loss: 1.439, D_unsup_loss_real: 0.517, D_unsup_loss_fake: 0.586, D_sup_loss: 0.116, D_sup_acc: 97.13 Train acc: 97.020 Test acc: 97.280 \n",
      "step: 9148 | Train: G_Loss: 1.464, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.518, D_sup_loss: 0.104, D_sup_acc: 97.31 Train acc: 96.743 Test acc: 97.140 \n",
      "step: 9149 | Train: G_Loss: 1.375, D_unsup_loss_real: 0.503, D_unsup_loss_fake: 0.488, D_sup_loss: 0.120, D_sup_acc: 97.18 Train acc: 96.555 Test acc: 96.950 \n",
      "step: 9150 | Train: G_Loss: 1.337, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.456, D_sup_loss: 0.129, D_sup_acc: 96.99 Train acc: 96.785 Test acc: 97.220 \n",
      "step: 9151 | Train: G_Loss: 1.216, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.508, D_sup_loss: 0.119, D_sup_acc: 97.26 Train acc: 96.968 Test acc: 97.290 \n",
      "step: 9152 | Train: G_Loss: 1.345, D_unsup_loss_real: 0.532, D_unsup_loss_fake: 0.528, D_sup_loss: 0.111, D_sup_acc: 97.32 Train acc: 96.843 Test acc: 97.170 \n",
      "step: 9153 | Train: G_Loss: 1.287, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.534, D_sup_loss: 0.115, D_sup_acc: 97.21 Train acc: 96.672 Test acc: 97.050 \n",
      "step: 9154 | Train: G_Loss: 1.260, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.602, D_sup_loss: 0.126, D_sup_acc: 97.09 Train acc: 97.053 Test acc: 97.400 \n",
      "step: 9155 | Train: G_Loss: 1.397, D_unsup_loss_real: 0.434, D_unsup_loss_fake: 0.511, D_sup_loss: 0.105, D_sup_acc: 97.43 Train acc: 97.040 Test acc: 97.360 \n",
      "step: 9156 | Train: G_Loss: 1.316, D_unsup_loss_real: 0.545, D_unsup_loss_fake: 0.475, D_sup_loss: 0.106, D_sup_acc: 97.39 Train acc: 96.728 Test acc: 97.060 \n",
      "step: 9157 | Train: G_Loss: 1.383, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.524, D_sup_loss: 0.122, D_sup_acc: 97.10 Train acc: 97.037 Test acc: 97.320 \n",
      "step: 9158 | Train: G_Loss: 1.274, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.543, D_sup_loss: 0.103, D_sup_acc: 97.35 Train acc: 96.740 Test acc: 97.000 \n",
      "step: 9159 | Train: G_Loss: 1.444, D_unsup_loss_real: 0.486, D_unsup_loss_fake: 0.504, D_sup_loss: 0.120, D_sup_acc: 97.04 Train acc: 96.918 Test acc: 97.300 \n",
      "step: 9160 | Train: G_Loss: 1.161, D_unsup_loss_real: 0.508, D_unsup_loss_fake: 0.509, D_sup_loss: 0.108, D_sup_acc: 97.33 Train acc: 96.422 Test acc: 96.810 \n",
      "step: 9161 | Train: G_Loss: 1.171, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.606, D_sup_loss: 0.130, D_sup_acc: 96.85 Train acc: 96.547 Test acc: 96.910 \n",
      "step: 9162 | Train: G_Loss: 1.504, D_unsup_loss_real: 0.529, D_unsup_loss_fake: 0.573, D_sup_loss: 0.127, D_sup_acc: 96.95 Train acc: 96.877 Test acc: 97.150 \n",
      "step: 9163 | Train: G_Loss: 1.284, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.529, D_sup_loss: 0.112, D_sup_acc: 97.19 Train acc: 96.792 Test acc: 97.110 \n",
      "step: 9164 | Train: G_Loss: 1.327, D_unsup_loss_real: 0.466, D_unsup_loss_fake: 0.545, D_sup_loss: 0.118, D_sup_acc: 97.15 Train acc: 96.932 Test acc: 97.210 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 9165 | Train: G_Loss: 1.372, D_unsup_loss_real: 0.517, D_unsup_loss_fake: 0.601, D_sup_loss: 0.112, D_sup_acc: 97.25 Train acc: 96.845 Test acc: 97.170 \n",
      "step: 9166 | Train: G_Loss: 1.307, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.523, D_sup_loss: 0.112, D_sup_acc: 97.21 Train acc: 96.515 Test acc: 96.940 \n",
      "step: 9167 | Train: G_Loss: 1.240, D_unsup_loss_real: 0.593, D_unsup_loss_fake: 0.556, D_sup_loss: 0.127, D_sup_acc: 96.98 Train acc: 96.890 Test acc: 97.240 \n",
      "step: 9168 | Train: G_Loss: 1.357, D_unsup_loss_real: 0.488, D_unsup_loss_fake: 0.560, D_sup_loss: 0.113, D_sup_acc: 97.27 Train acc: 96.920 Test acc: 97.220 \n",
      "step: 9169 | Train: G_Loss: 1.232, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.621, D_sup_loss: 0.112, D_sup_acc: 97.26 Train acc: 96.558 Test acc: 96.940 \n",
      "step: 9170 | Train: G_Loss: 1.352, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.515, D_sup_loss: 0.128, D_sup_acc: 96.98 Train acc: 96.690 Test acc: 96.970 \n",
      "step: 9171 | Train: G_Loss: 1.255, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.573, D_sup_loss: 0.120, D_sup_acc: 97.01 Train acc: 96.738 Test acc: 97.060 \n",
      "step: 9172 | Train: G_Loss: 1.311, D_unsup_loss_real: 0.590, D_unsup_loss_fake: 0.645, D_sup_loss: 0.126, D_sup_acc: 97.10 Train acc: 96.908 Test acc: 97.220 \n",
      "step: 9173 | Train: G_Loss: 1.383, D_unsup_loss_real: 0.486, D_unsup_loss_fake: 0.608, D_sup_loss: 0.115, D_sup_acc: 97.26 Train acc: 96.855 Test acc: 97.080 \n",
      "step: 9174 | Train: G_Loss: 1.393, D_unsup_loss_real: 0.519, D_unsup_loss_fake: 0.585, D_sup_loss: 0.116, D_sup_acc: 97.12 Train acc: 96.667 Test acc: 96.960 \n",
      "step: 9175 | Train: G_Loss: 1.279, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.534, D_sup_loss: 0.123, D_sup_acc: 97.00 Train acc: 96.392 Test acc: 96.840 \n",
      "step: 9176 | Train: G_Loss: 1.344, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.580, D_sup_loss: 0.135, D_sup_acc: 96.88 Train acc: 96.722 Test acc: 96.970 \n",
      "step: 9177 | Train: G_Loss: 1.291, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.579, D_sup_loss: 0.122, D_sup_acc: 97.01 Train acc: 96.490 Test acc: 96.800 \n",
      "step: 9178 | Train: G_Loss: 1.324, D_unsup_loss_real: 0.610, D_unsup_loss_fake: 0.559, D_sup_loss: 0.132, D_sup_acc: 96.84 Train acc: 96.542 Test acc: 96.850 \n",
      "step: 9179 | Train: G_Loss: 1.298, D_unsup_loss_real: 0.452, D_unsup_loss_fake: 0.543, D_sup_loss: 0.130, D_sup_acc: 96.89 Train acc: 96.622 Test acc: 96.960 \n",
      "step: 9180 | Train: G_Loss: 1.303, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.461, D_sup_loss: 0.127, D_sup_acc: 97.00 Train acc: 96.692 Test acc: 97.000 \n",
      "step: 9181 | Train: G_Loss: 1.206, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.455, D_sup_loss: 0.124, D_sup_acc: 97.04 Train acc: 96.895 Test acc: 97.210 \n",
      "step: 9182 | Train: G_Loss: 1.161, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.518, D_sup_loss: 0.114, D_sup_acc: 97.25 Train acc: 96.817 Test acc: 97.110 \n",
      "step: 9183 | Train: G_Loss: 1.316, D_unsup_loss_real: 0.509, D_unsup_loss_fake: 0.648, D_sup_loss: 0.117, D_sup_acc: 97.15 Train acc: 96.580 Test acc: 96.990 \n",
      "step: 9184 | Train: G_Loss: 1.325, D_unsup_loss_real: 0.532, D_unsup_loss_fake: 0.521, D_sup_loss: 0.126, D_sup_acc: 97.03 Train acc: 96.933 Test acc: 97.200 \n",
      "step: 9185 | Train: G_Loss: 1.259, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.517, D_sup_loss: 0.109, D_sup_acc: 97.24 Train acc: 96.735 Test acc: 97.000 \n",
      "step: 9186 | Train: G_Loss: 1.231, D_unsup_loss_real: 0.495, D_unsup_loss_fake: 0.549, D_sup_loss: 0.126, D_sup_acc: 97.04 Train acc: 96.830 Test acc: 97.110 \n",
      "step: 9187 | Train: G_Loss: 1.307, D_unsup_loss_real: 0.603, D_unsup_loss_fake: 0.582, D_sup_loss: 0.118, D_sup_acc: 97.15 Train acc: 96.922 Test acc: 97.150 \n",
      "step: 9188 | Train: G_Loss: 1.415, D_unsup_loss_real: 0.617, D_unsup_loss_fake: 0.618, D_sup_loss: 0.115, D_sup_acc: 97.19 Train acc: 97.065 Test acc: 97.250 \n",
      "step: 9189 | Train: G_Loss: 1.197, D_unsup_loss_real: 0.500, D_unsup_loss_fake: 0.587, D_sup_loss: 0.109, D_sup_acc: 97.28 Train acc: 96.722 Test acc: 97.000 \n",
      "step: 9190 | Train: G_Loss: 1.480, D_unsup_loss_real: 0.473, D_unsup_loss_fake: 0.520, D_sup_loss: 0.124, D_sup_acc: 97.04 Train acc: 96.892 Test acc: 97.090 \n",
      "step: 9191 | Train: G_Loss: 1.272, D_unsup_loss_real: 0.527, D_unsup_loss_fake: 0.540, D_sup_loss: 0.113, D_sup_acc: 97.13 Train acc: 96.792 Test acc: 97.080 \n",
      "step: 9192 | Train: G_Loss: 1.244, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.507, D_sup_loss: 0.118, D_sup_acc: 97.12 Train acc: 96.818 Test acc: 97.090 \n",
      "step: 9193 | Train: G_Loss: 1.318, D_unsup_loss_real: 0.512, D_unsup_loss_fake: 0.557, D_sup_loss: 0.116, D_sup_acc: 97.13 Train acc: 96.805 Test acc: 97.080 \n",
      "step: 9194 | Train: G_Loss: 1.324, D_unsup_loss_real: 0.520, D_unsup_loss_fake: 0.533, D_sup_loss: 0.116, D_sup_acc: 97.12 Train acc: 96.398 Test acc: 96.820 \n",
      "step: 9195 | Train: G_Loss: 1.321, D_unsup_loss_real: 0.561, D_unsup_loss_fake: 0.523, D_sup_loss: 0.132, D_sup_acc: 96.86 Train acc: 96.982 Test acc: 97.350 \n",
      "step: 9196 | Train: G_Loss: 1.252, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.521, D_sup_loss: 0.107, D_sup_acc: 97.38 Train acc: 96.878 Test acc: 97.200 \n",
      "step: 9197 | Train: G_Loss: 1.278, D_unsup_loss_real: 0.575, D_unsup_loss_fake: 0.591, D_sup_loss: 0.115, D_sup_acc: 97.24 Train acc: 96.823 Test acc: 97.190 \n",
      "step: 9198 | Train: G_Loss: 1.342, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.607, D_sup_loss: 0.116, D_sup_acc: 97.23 Train acc: 96.447 Test acc: 96.860 \n",
      "step: 9199 | Train: G_Loss: 1.283, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.487, D_sup_loss: 0.134, D_sup_acc: 96.90 Train acc: 96.602 Test acc: 96.930 \n",
      "step: 9200 | Train: G_Loss: 1.272, D_unsup_loss_real: 0.507, D_unsup_loss_fake: 0.493, D_sup_loss: 0.126, D_sup_acc: 96.97 Train acc: 97.052 Test acc: 97.270 \n",
      "Train Classifier Accuracy: 97.052%\n",
      "\n",
      "Test Classifier Accuracy: 97.270%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_9200.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_9200.h5\n",
      "step: 9201 | Train: G_Loss: 1.252, D_unsup_loss_real: 0.536, D_unsup_loss_fake: 0.505, D_sup_loss: 0.103, D_sup_acc: 97.30 Train acc: 96.520 Test acc: 96.920 \n",
      "step: 9202 | Train: G_Loss: 1.376, D_unsup_loss_real: 0.661, D_unsup_loss_fake: 0.607, D_sup_loss: 0.130, D_sup_acc: 96.96 Train acc: 96.807 Test acc: 97.160 \n",
      "step: 9203 | Train: G_Loss: 1.302, D_unsup_loss_real: 0.550, D_unsup_loss_fake: 0.570, D_sup_loss: 0.116, D_sup_acc: 97.20 Train acc: 96.922 Test acc: 97.260 \n",
      "step: 9204 | Train: G_Loss: 1.351, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.578, D_sup_loss: 0.114, D_sup_acc: 97.29 Train acc: 96.545 Test acc: 96.930 \n",
      "step: 9205 | Train: G_Loss: 1.339, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.509, D_sup_loss: 0.128, D_sup_acc: 96.97 Train acc: 96.558 Test acc: 96.950 \n",
      "step: 9206 | Train: G_Loss: 1.406, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.510, D_sup_loss: 0.126, D_sup_acc: 96.99 Train acc: 96.862 Test acc: 97.150 \n",
      "step: 9207 | Train: G_Loss: 1.118, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.574, D_sup_loss: 0.112, D_sup_acc: 97.19 Train acc: 96.935 Test acc: 97.190 \n",
      "step: 9208 | Train: G_Loss: 1.352, D_unsup_loss_real: 0.485, D_unsup_loss_fake: 0.554, D_sup_loss: 0.114, D_sup_acc: 97.23 Train acc: 96.732 Test acc: 96.990 \n",
      "step: 9209 | Train: G_Loss: 1.361, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.574, D_sup_loss: 0.121, D_sup_acc: 97.03 Train acc: 97.072 Test acc: 97.320 \n",
      "step: 9210 | Train: G_Loss: 1.167, D_unsup_loss_real: 0.669, D_unsup_loss_fake: 0.530, D_sup_loss: 0.104, D_sup_acc: 97.35 Train acc: 96.570 Test acc: 96.970 \n",
      "step: 9211 | Train: G_Loss: 1.364, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.615, D_sup_loss: 0.132, D_sup_acc: 97.01 Train acc: 96.707 Test acc: 97.010 \n",
      "step: 9212 | Train: G_Loss: 1.365, D_unsup_loss_real: 0.511, D_unsup_loss_fake: 0.602, D_sup_loss: 0.124, D_sup_acc: 97.05 Train acc: 96.885 Test acc: 97.090 \n",
      "step: 9213 | Train: G_Loss: 1.220, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.558, D_sup_loss: 0.115, D_sup_acc: 97.13 Train acc: 96.828 Test acc: 97.140 \n",
      "step: 9214 | Train: G_Loss: 1.418, D_unsup_loss_real: 0.511, D_unsup_loss_fake: 0.570, D_sup_loss: 0.120, D_sup_acc: 97.18 Train acc: 96.973 Test acc: 97.150 \n",
      "step: 9215 | Train: G_Loss: 1.334, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.549, D_sup_loss: 0.111, D_sup_acc: 97.19 Train acc: 96.712 Test acc: 96.980 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 9216 | Train: G_Loss: 1.243, D_unsup_loss_real: 0.618, D_unsup_loss_fake: 0.545, D_sup_loss: 0.127, D_sup_acc: 97.02 Train acc: 96.672 Test acc: 96.900 \n",
      "step: 9217 | Train: G_Loss: 1.372, D_unsup_loss_real: 0.573, D_unsup_loss_fake: 0.519, D_sup_loss: 0.125, D_sup_acc: 96.94 Train acc: 96.913 Test acc: 97.140 \n",
      "step: 9218 | Train: G_Loss: 1.300, D_unsup_loss_real: 0.548, D_unsup_loss_fake: 0.575, D_sup_loss: 0.112, D_sup_acc: 97.18 Train acc: 96.638 Test acc: 96.940 \n",
      "step: 9219 | Train: G_Loss: 1.283, D_unsup_loss_real: 0.519, D_unsup_loss_fake: 0.543, D_sup_loss: 0.128, D_sup_acc: 96.98 Train acc: 96.650 Test acc: 96.930 \n",
      "step: 9220 | Train: G_Loss: 1.276, D_unsup_loss_real: 0.644, D_unsup_loss_fake: 0.548, D_sup_loss: 0.128, D_sup_acc: 96.97 Train acc: 96.627 Test acc: 96.930 \n",
      "step: 9221 | Train: G_Loss: 1.462, D_unsup_loss_real: 0.488, D_unsup_loss_fake: 0.578, D_sup_loss: 0.123, D_sup_acc: 96.97 Train acc: 96.917 Test acc: 97.170 \n",
      "step: 9222 | Train: G_Loss: 1.194, D_unsup_loss_real: 0.613, D_unsup_loss_fake: 0.522, D_sup_loss: 0.113, D_sup_acc: 97.21 Train acc: 96.873 Test acc: 97.170 \n",
      "step: 9223 | Train: G_Loss: 1.214, D_unsup_loss_real: 0.434, D_unsup_loss_fake: 0.595, D_sup_loss: 0.117, D_sup_acc: 97.21 Train acc: 96.748 Test acc: 97.010 \n",
      "step: 9224 | Train: G_Loss: 1.373, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.543, D_sup_loss: 0.123, D_sup_acc: 97.05 Train acc: 96.913 Test acc: 97.210 \n",
      "step: 9225 | Train: G_Loss: 1.206, D_unsup_loss_real: 0.637, D_unsup_loss_fake: 0.566, D_sup_loss: 0.114, D_sup_acc: 97.25 Train acc: 96.847 Test acc: 97.090 \n",
      "step: 9226 | Train: G_Loss: 1.307, D_unsup_loss_real: 0.504, D_unsup_loss_fake: 0.562, D_sup_loss: 0.119, D_sup_acc: 97.13 Train acc: 96.790 Test acc: 97.090 \n",
      "step: 9227 | Train: G_Loss: 1.180, D_unsup_loss_real: 0.569, D_unsup_loss_fake: 0.483, D_sup_loss: 0.117, D_sup_acc: 97.13 Train acc: 96.538 Test acc: 96.850 \n",
      "step: 9228 | Train: G_Loss: 1.258, D_unsup_loss_real: 0.555, D_unsup_loss_fake: 0.578, D_sup_loss: 0.125, D_sup_acc: 96.89 Train acc: 96.573 Test acc: 96.840 \n",
      "step: 9229 | Train: G_Loss: 1.270, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.527, D_sup_loss: 0.128, D_sup_acc: 96.88 Train acc: 96.787 Test acc: 97.070 \n",
      "step: 9230 | Train: G_Loss: 1.197, D_unsup_loss_real: 0.493, D_unsup_loss_fake: 0.537, D_sup_loss: 0.118, D_sup_acc: 97.11 Train acc: 96.763 Test acc: 97.060 \n",
      "step: 9231 | Train: G_Loss: 1.329, D_unsup_loss_real: 0.494, D_unsup_loss_fake: 0.637, D_sup_loss: 0.119, D_sup_acc: 97.10 Train acc: 96.765 Test acc: 96.990 \n",
      "step: 9232 | Train: G_Loss: 1.291, D_unsup_loss_real: 0.554, D_unsup_loss_fake: 0.553, D_sup_loss: 0.124, D_sup_acc: 97.03 Train acc: 96.773 Test acc: 97.060 \n",
      "step: 9233 | Train: G_Loss: 1.402, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.527, D_sup_loss: 0.118, D_sup_acc: 97.10 Train acc: 97.123 Test acc: 97.310 \n",
      "step: 9234 | Train: G_Loss: 1.207, D_unsup_loss_real: 0.576, D_unsup_loss_fake: 0.553, D_sup_loss: 0.105, D_sup_acc: 97.34 Train acc: 97.047 Test acc: 97.230 \n",
      "step: 9235 | Train: G_Loss: 1.319, D_unsup_loss_real: 0.520, D_unsup_loss_fake: 0.532, D_sup_loss: 0.109, D_sup_acc: 97.27 Train acc: 96.958 Test acc: 97.180 \n",
      "step: 9236 | Train: G_Loss: 1.179, D_unsup_loss_real: 0.619, D_unsup_loss_fake: 0.582, D_sup_loss: 0.114, D_sup_acc: 97.22 Train acc: 96.883 Test acc: 97.170 \n",
      "step: 9237 | Train: G_Loss: 1.348, D_unsup_loss_real: 0.531, D_unsup_loss_fake: 0.593, D_sup_loss: 0.118, D_sup_acc: 97.21 Train acc: 96.750 Test acc: 97.070 \n",
      "step: 9238 | Train: G_Loss: 1.292, D_unsup_loss_real: 0.539, D_unsup_loss_fake: 0.525, D_sup_loss: 0.118, D_sup_acc: 97.11 Train acc: 96.753 Test acc: 97.050 \n",
      "step: 9239 | Train: G_Loss: 1.259, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.501, D_sup_loss: 0.124, D_sup_acc: 97.09 Train acc: 96.652 Test acc: 96.990 \n",
      "step: 9240 | Train: G_Loss: 1.288, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.583, D_sup_loss: 0.128, D_sup_acc: 97.03 Train acc: 96.918 Test acc: 97.280 \n",
      "step: 9241 | Train: G_Loss: 1.474, D_unsup_loss_real: 0.528, D_unsup_loss_fake: 0.508, D_sup_loss: 0.117, D_sup_acc: 97.31 Train acc: 96.910 Test acc: 97.240 \n",
      "step: 9242 | Train: G_Loss: 1.409, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.597, D_sup_loss: 0.113, D_sup_acc: 97.27 Train acc: 96.775 Test acc: 97.130 \n",
      "step: 9243 | Train: G_Loss: 1.264, D_unsup_loss_real: 0.542, D_unsup_loss_fake: 0.534, D_sup_loss: 0.117, D_sup_acc: 97.17 Train acc: 97.033 Test acc: 97.270 \n",
      "step: 9244 | Train: G_Loss: 1.216, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.580, D_sup_loss: 0.109, D_sup_acc: 97.30 Train acc: 96.698 Test acc: 97.040 \n",
      "step: 9245 | Train: G_Loss: 1.338, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.585, D_sup_loss: 0.125, D_sup_acc: 97.08 Train acc: 97.055 Test acc: 97.300 \n",
      "step: 9246 | Train: G_Loss: 1.224, D_unsup_loss_real: 0.673, D_unsup_loss_fake: 0.552, D_sup_loss: 0.106, D_sup_acc: 97.33 Train acc: 96.308 Test acc: 96.590 \n",
      "step: 9247 | Train: G_Loss: 1.269, D_unsup_loss_real: 0.535, D_unsup_loss_fake: 0.572, D_sup_loss: 0.143, D_sup_acc: 96.63 Train acc: 96.883 Test acc: 97.150 \n",
      "step: 9248 | Train: G_Loss: 1.322, D_unsup_loss_real: 0.506, D_unsup_loss_fake: 0.506, D_sup_loss: 0.115, D_sup_acc: 97.19 Train acc: 97.032 Test acc: 97.280 \n",
      "step: 9249 | Train: G_Loss: 1.276, D_unsup_loss_real: 0.598, D_unsup_loss_fake: 0.625, D_sup_loss: 0.110, D_sup_acc: 97.31 Train acc: 96.720 Test acc: 97.100 \n",
      "step: 9250 | Train: G_Loss: 1.331, D_unsup_loss_real: 0.445, D_unsup_loss_fake: 0.566, D_sup_loss: 0.123, D_sup_acc: 97.14 Train acc: 97.037 Test acc: 97.300 \n",
      "step: 9251 | Train: G_Loss: 1.318, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.592, D_sup_loss: 0.109, D_sup_acc: 97.33 Train acc: 97.010 Test acc: 97.290 \n",
      "step: 9252 | Train: G_Loss: 1.326, D_unsup_loss_real: 0.655, D_unsup_loss_fake: 0.537, D_sup_loss: 0.112, D_sup_acc: 97.32 Train acc: 96.920 Test acc: 97.270 \n",
      "step: 9253 | Train: G_Loss: 1.166, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.617, D_sup_loss: 0.113, D_sup_acc: 97.30 Train acc: 96.750 Test acc: 97.080 \n",
      "step: 9254 | Train: G_Loss: 1.176, D_unsup_loss_real: 0.508, D_unsup_loss_fake: 0.683, D_sup_loss: 0.121, D_sup_acc: 97.12 Train acc: 96.585 Test acc: 96.970 \n",
      "step: 9255 | Train: G_Loss: 1.246, D_unsup_loss_real: 0.550, D_unsup_loss_fake: 0.577, D_sup_loss: 0.128, D_sup_acc: 97.01 Train acc: 96.612 Test acc: 97.090 \n",
      "step: 9256 | Train: G_Loss: 1.532, D_unsup_loss_real: 0.570, D_unsup_loss_fake: 0.585, D_sup_loss: 0.130, D_sup_acc: 97.13 Train acc: 96.923 Test acc: 97.200 \n",
      "step: 9257 | Train: G_Loss: 1.341, D_unsup_loss_real: 0.497, D_unsup_loss_fake: 0.503, D_sup_loss: 0.113, D_sup_acc: 97.24 Train acc: 96.363 Test acc: 96.780 \n",
      "step: 9258 | Train: G_Loss: 1.230, D_unsup_loss_real: 0.536, D_unsup_loss_fake: 0.543, D_sup_loss: 0.138, D_sup_acc: 96.82 Train acc: 96.740 Test acc: 97.040 \n",
      "step: 9259 | Train: G_Loss: 1.393, D_unsup_loss_real: 0.589, D_unsup_loss_fake: 0.510, D_sup_loss: 0.122, D_sup_acc: 97.08 Train acc: 97.098 Test acc: 97.350 \n",
      "step: 9260 | Train: G_Loss: 1.223, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.565, D_sup_loss: 0.104, D_sup_acc: 97.38 Train acc: 96.998 Test acc: 97.380 \n",
      "step: 9261 | Train: G_Loss: 1.263, D_unsup_loss_real: 0.597, D_unsup_loss_fake: 0.555, D_sup_loss: 0.114, D_sup_acc: 97.41 Train acc: 96.853 Test acc: 97.200 \n",
      "step: 9262 | Train: G_Loss: 1.348, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.569, D_sup_loss: 0.115, D_sup_acc: 97.24 Train acc: 96.967 Test acc: 97.330 \n",
      "step: 9263 | Train: G_Loss: 1.302, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.543, D_sup_loss: 0.113, D_sup_acc: 97.36 Train acc: 96.718 Test acc: 97.140 \n",
      "step: 9264 | Train: G_Loss: 1.284, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.553, D_sup_loss: 0.126, D_sup_acc: 97.18 Train acc: 96.927 Test acc: 97.320 \n",
      "step: 9265 | Train: G_Loss: 1.324, D_unsup_loss_real: 0.493, D_unsup_loss_fake: 0.569, D_sup_loss: 0.116, D_sup_acc: 97.35 Train acc: 96.407 Test acc: 96.760 \n",
      "step: 9266 | Train: G_Loss: 1.363, D_unsup_loss_real: 0.608, D_unsup_loss_fake: 0.455, D_sup_loss: 0.140, D_sup_acc: 96.80 Train acc: 97.072 Test acc: 97.390 \n",
      "step: 9267 | Train: G_Loss: 1.149, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.524, D_sup_loss: 0.103, D_sup_acc: 97.42 Train acc: 96.772 Test acc: 97.200 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 9268 | Train: G_Loss: 1.269, D_unsup_loss_real: 0.547, D_unsup_loss_fake: 0.629, D_sup_loss: 0.120, D_sup_acc: 97.24 Train acc: 97.133 Test acc: 97.450 \n",
      "step: 9269 | Train: G_Loss: 1.198, D_unsup_loss_real: 0.523, D_unsup_loss_fake: 0.507, D_sup_loss: 0.103, D_sup_acc: 97.48 Train acc: 97.078 Test acc: 97.400 \n",
      "step: 9270 | Train: G_Loss: 1.300, D_unsup_loss_real: 0.446, D_unsup_loss_fake: 0.632, D_sup_loss: 0.108, D_sup_acc: 97.43 Train acc: 97.115 Test acc: 97.550 \n",
      "step: 9271 | Train: G_Loss: 1.344, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.595, D_sup_loss: 0.101, D_sup_acc: 97.58 Train acc: 96.373 Test acc: 96.680 \n",
      "step: 9272 | Train: G_Loss: 1.327, D_unsup_loss_real: 0.546, D_unsup_loss_fake: 0.537, D_sup_loss: 0.147, D_sup_acc: 96.72 Train acc: 97.097 Test acc: 97.410 \n",
      "step: 9273 | Train: G_Loss: 1.375, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.564, D_sup_loss: 0.108, D_sup_acc: 97.44 Train acc: 96.630 Test acc: 96.900 \n",
      "step: 9274 | Train: G_Loss: 1.344, D_unsup_loss_real: 0.614, D_unsup_loss_fake: 0.476, D_sup_loss: 0.136, D_sup_acc: 96.94 Train acc: 96.853 Test acc: 97.270 \n",
      "step: 9275 | Train: G_Loss: 1.284, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.614, D_sup_loss: 0.117, D_sup_acc: 97.30 Train acc: 96.953 Test acc: 97.280 \n",
      "step: 9276 | Train: G_Loss: 1.321, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.583, D_sup_loss: 0.113, D_sup_acc: 97.31 Train acc: 97.075 Test acc: 97.330 \n",
      "step: 9277 | Train: G_Loss: 1.307, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.559, D_sup_loss: 0.108, D_sup_acc: 97.36 Train acc: 96.952 Test acc: 97.280 \n",
      "step: 9278 | Train: G_Loss: 1.334, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.558, D_sup_loss: 0.117, D_sup_acc: 97.31 Train acc: 97.010 Test acc: 97.360 \n",
      "step: 9279 | Train: G_Loss: 1.191, D_unsup_loss_real: 0.612, D_unsup_loss_fake: 0.532, D_sup_loss: 0.111, D_sup_acc: 97.39 Train acc: 96.747 Test acc: 97.060 \n",
      "step: 9280 | Train: G_Loss: 1.291, D_unsup_loss_real: 0.552, D_unsup_loss_fake: 0.612, D_sup_loss: 0.129, D_sup_acc: 97.10 Train acc: 96.870 Test acc: 97.120 \n",
      "step: 9281 | Train: G_Loss: 1.390, D_unsup_loss_real: 0.507, D_unsup_loss_fake: 0.588, D_sup_loss: 0.122, D_sup_acc: 97.16 Train acc: 97.050 Test acc: 97.370 \n",
      "step: 9282 | Train: G_Loss: 1.294, D_unsup_loss_real: 0.571, D_unsup_loss_fake: 0.524, D_sup_loss: 0.112, D_sup_acc: 97.40 Train acc: 97.135 Test acc: 97.480 \n",
      "step: 9283 | Train: G_Loss: 1.289, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.494, D_sup_loss: 0.106, D_sup_acc: 97.51 Train acc: 97.033 Test acc: 97.300 \n",
      "step: 9284 | Train: G_Loss: 1.262, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.592, D_sup_loss: 0.114, D_sup_acc: 97.33 Train acc: 97.037 Test acc: 97.290 \n",
      "step: 9285 | Train: G_Loss: 1.170, D_unsup_loss_real: 0.611, D_unsup_loss_fake: 0.507, D_sup_loss: 0.113, D_sup_acc: 97.32 Train acc: 96.945 Test acc: 97.280 \n",
      "step: 9286 | Train: G_Loss: 1.240, D_unsup_loss_real: 0.601, D_unsup_loss_fake: 0.576, D_sup_loss: 0.113, D_sup_acc: 97.31 Train acc: 96.665 Test acc: 97.120 \n",
      "step: 9287 | Train: G_Loss: 1.361, D_unsup_loss_real: 0.466, D_unsup_loss_fake: 0.524, D_sup_loss: 0.130, D_sup_acc: 97.16 Train acc: 97.027 Test acc: 97.330 \n",
      "step: 9288 | Train: G_Loss: 1.258, D_unsup_loss_real: 0.633, D_unsup_loss_fake: 0.538, D_sup_loss: 0.109, D_sup_acc: 97.36 Train acc: 96.913 Test acc: 97.230 \n",
      "step: 9289 | Train: G_Loss: 1.334, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.598, D_sup_loss: 0.119, D_sup_acc: 97.27 Train acc: 97.107 Test acc: 97.400 \n",
      "step: 9290 | Train: G_Loss: 1.262, D_unsup_loss_real: 0.535, D_unsup_loss_fake: 0.532, D_sup_loss: 0.104, D_sup_acc: 97.43 Train acc: 96.923 Test acc: 97.180 \n",
      "step: 9291 | Train: G_Loss: 1.427, D_unsup_loss_real: 0.485, D_unsup_loss_fake: 0.506, D_sup_loss: 0.115, D_sup_acc: 97.22 Train acc: 96.827 Test acc: 97.110 \n",
      "step: 9292 | Train: G_Loss: 1.351, D_unsup_loss_real: 0.591, D_unsup_loss_fake: 0.562, D_sup_loss: 0.121, D_sup_acc: 97.15 Train acc: 96.968 Test acc: 97.260 \n",
      "step: 9293 | Train: G_Loss: 1.362, D_unsup_loss_real: 0.513, D_unsup_loss_fake: 0.494, D_sup_loss: 0.113, D_sup_acc: 97.29 Train acc: 97.043 Test acc: 97.350 \n",
      "step: 9294 | Train: G_Loss: 1.345, D_unsup_loss_real: 0.565, D_unsup_loss_fake: 0.562, D_sup_loss: 0.109, D_sup_acc: 97.38 Train acc: 96.983 Test acc: 97.350 \n",
      "step: 9295 | Train: G_Loss: 1.270, D_unsup_loss_real: 0.654, D_unsup_loss_fake: 0.609, D_sup_loss: 0.107, D_sup_acc: 97.38 Train acc: 96.513 Test acc: 96.960 \n",
      "step: 9296 | Train: G_Loss: 1.384, D_unsup_loss_real: 0.515, D_unsup_loss_fake: 0.607, D_sup_loss: 0.135, D_sup_acc: 97.00 Train acc: 97.110 Test acc: 97.270 \n",
      "step: 9297 | Train: G_Loss: 1.295, D_unsup_loss_real: 0.643, D_unsup_loss_fake: 0.621, D_sup_loss: 0.107, D_sup_acc: 97.30 Train acc: 96.698 Test acc: 97.020 \n",
      "step: 9298 | Train: G_Loss: 1.277, D_unsup_loss_real: 0.583, D_unsup_loss_fake: 0.558, D_sup_loss: 0.131, D_sup_acc: 97.06 Train acc: 96.863 Test acc: 97.150 \n",
      "step: 9299 | Train: G_Loss: 1.374, D_unsup_loss_real: 0.542, D_unsup_loss_fake: 0.560, D_sup_loss: 0.116, D_sup_acc: 97.19 Train acc: 96.667 Test acc: 96.980 \n",
      "step: 9300 | Train: G_Loss: 1.287, D_unsup_loss_real: 0.585, D_unsup_loss_fake: 0.543, D_sup_loss: 0.125, D_sup_acc: 97.02 Train acc: 96.747 Test acc: 97.050 \n",
      "Train Classifier Accuracy: 96.747%\n",
      "\n",
      "Test Classifier Accuracy: 97.050%\n",
      "\n",
      ">Saving models Generator: Logs/SSGAN_MNIST/Classifier_100/generator_model_9300.h5 and Supervised: Logs/SSGAN_MNIST/Classifier_100/supervised_model_9300.h5\n",
      "step: 9301 | Train: G_Loss: 1.208, D_unsup_loss_real: 0.642, D_unsup_loss_fake: 0.506, D_sup_loss: 0.124, D_sup_acc: 97.09 Train acc: 96.925 Test acc: 97.240 \n",
      "step: 9302 | Train: G_Loss: 1.178, D_unsup_loss_real: 0.578, D_unsup_loss_fake: 0.519, D_sup_loss: 0.112, D_sup_acc: 97.27 Train acc: 96.620 Test acc: 97.020 \n",
      "step: 9303 | Train: G_Loss: 1.198, D_unsup_loss_real: 0.514, D_unsup_loss_fake: 0.550, D_sup_loss: 0.131, D_sup_acc: 97.06 Train acc: 96.913 Test acc: 97.230 \n",
      "step: 9304 | Train: G_Loss: 1.341, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.623, D_sup_loss: 0.114, D_sup_acc: 97.27 Train acc: 96.998 Test acc: 97.290 \n",
      "step: 9305 | Train: G_Loss: 1.199, D_unsup_loss_real: 0.577, D_unsup_loss_fake: 0.547, D_sup_loss: 0.107, D_sup_acc: 97.32 Train acc: 96.560 Test acc: 96.870 \n",
      "step: 9306 | Train: G_Loss: 1.264, D_unsup_loss_real: 0.551, D_unsup_loss_fake: 0.539, D_sup_loss: 0.132, D_sup_acc: 96.91 Train acc: 96.923 Test acc: 97.220 \n",
      "step: 9307 | Train: G_Loss: 1.201, D_unsup_loss_real: 0.650, D_unsup_loss_fake: 0.562, D_sup_loss: 0.109, D_sup_acc: 97.26 Train acc: 96.628 Test acc: 96.920 \n",
      "step: 9308 | Train: G_Loss: 1.365, D_unsup_loss_real: 0.542, D_unsup_loss_fake: 0.618, D_sup_loss: 0.128, D_sup_acc: 96.96 Train acc: 96.652 Test acc: 96.980 \n",
      "step: 9309 | Train: G_Loss: 1.401, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.575, D_sup_loss: 0.120, D_sup_acc: 97.02 Train acc: 96.887 Test acc: 97.140 \n",
      "step: 9310 | Train: G_Loss: 1.238, D_unsup_loss_real: 0.501, D_unsup_loss_fake: 0.508, D_sup_loss: 0.110, D_sup_acc: 97.18 Train acc: 96.570 Test acc: 96.900 \n",
      "step: 9311 | Train: G_Loss: 1.282, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.567, D_sup_loss: 0.131, D_sup_acc: 96.94 Train acc: 96.892 Test acc: 97.140 \n",
      "step: 9312 | Train: G_Loss: 1.367, D_unsup_loss_real: 0.473, D_unsup_loss_fake: 0.542, D_sup_loss: 0.115, D_sup_acc: 97.18 Train acc: 96.873 Test acc: 97.140 \n",
      "step: 9313 | Train: G_Loss: 1.207, D_unsup_loss_real: 0.605, D_unsup_loss_fake: 0.499, D_sup_loss: 0.115, D_sup_acc: 97.18 Train acc: 96.723 Test acc: 97.020 \n",
      "step: 9314 | Train: G_Loss: 1.280, D_unsup_loss_real: 0.514, D_unsup_loss_fake: 0.592, D_sup_loss: 0.126, D_sup_acc: 97.06 Train acc: 96.790 Test acc: 97.090 \n",
      "step: 9315 | Train: G_Loss: 1.375, D_unsup_loss_real: 0.594, D_unsup_loss_fake: 0.620, D_sup_loss: 0.118, D_sup_acc: 97.13 Train acc: 96.907 Test acc: 97.280 \n",
      "step: 9316 | Train: G_Loss: 1.188, D_unsup_loss_real: 0.592, D_unsup_loss_fake: 0.605, D_sup_loss: 0.114, D_sup_acc: 97.31 Train acc: 96.973 Test acc: 97.200 \n",
      "step: 9317 | Train: G_Loss: 1.172, D_unsup_loss_real: 0.639, D_unsup_loss_fake: 0.606, D_sup_loss: 0.108, D_sup_acc: 97.24 Train acc: 96.660 Test acc: 96.920 \n",
      "step: 9318 | Train: G_Loss: 1.273, D_unsup_loss_real: 0.559, D_unsup_loss_fake: 0.628, D_sup_loss: 0.134, D_sup_acc: 96.96 Train acc: 96.788 Test acc: 97.090 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 9319 | Train: G_Loss: 1.261, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.615, D_sup_loss: 0.124, D_sup_acc: 97.13 Train acc: 96.685 Test acc: 96.820 \n",
      "step: 9320 | Train: G_Loss: 1.284, D_unsup_loss_real: 0.505, D_unsup_loss_fake: 0.508, D_sup_loss: 0.132, D_sup_acc: 96.86 Train acc: 97.055 Test acc: 97.310 \n",
      "step: 9321 | Train: G_Loss: 1.263, D_unsup_loss_real: 0.563, D_unsup_loss_fake: 0.554, D_sup_loss: 0.109, D_sup_acc: 97.34 Train acc: 97.003 Test acc: 97.240 \n",
      "step: 9322 | Train: G_Loss: 1.274, D_unsup_loss_real: 0.540, D_unsup_loss_fake: 0.577, D_sup_loss: 0.115, D_sup_acc: 97.27 Train acc: 96.980 Test acc: 97.230 \n",
      "step: 9323 | Train: G_Loss: 1.240, D_unsup_loss_real: 0.515, D_unsup_loss_fake: 0.521, D_sup_loss: 0.115, D_sup_acc: 97.27 Train acc: 96.993 Test acc: 97.280 \n",
      "step: 9324 | Train: G_Loss: 1.227, D_unsup_loss_real: 0.568, D_unsup_loss_fake: 0.504, D_sup_loss: 0.113, D_sup_acc: 97.31 Train acc: 97.040 Test acc: 97.260 \n",
      "step: 9325 | Train: G_Loss: 1.236, D_unsup_loss_real: 0.558, D_unsup_loss_fake: 0.575, D_sup_loss: 0.111, D_sup_acc: 97.29 Train acc: 96.922 Test acc: 97.220 \n",
      "step: 9326 | Train: G_Loss: 1.274, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.503, D_sup_loss: 0.115, D_sup_acc: 97.26 Train acc: 96.885 Test acc: 97.160 \n",
      "step: 9327 | Train: G_Loss: 1.434, D_unsup_loss_real: 0.461, D_unsup_loss_fake: 0.643, D_sup_loss: 0.119, D_sup_acc: 97.20 Train acc: 96.942 Test acc: 97.230 \n",
      "step: 9328 | Train: G_Loss: 1.288, D_unsup_loss_real: 0.623, D_unsup_loss_fake: 0.554, D_sup_loss: 0.113, D_sup_acc: 97.27 Train acc: 96.888 Test acc: 97.130 \n",
      "step: 9329 | Train: G_Loss: 1.402, D_unsup_loss_real: 0.484, D_unsup_loss_fake: 0.588, D_sup_loss: 0.118, D_sup_acc: 97.17 Train acc: 96.853 Test acc: 97.110 \n",
      "step: 9330 | Train: G_Loss: 1.426, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.546, D_sup_loss: 0.119, D_sup_acc: 97.15 Train acc: 97.003 Test acc: 97.180 \n",
      "step: 9331 | Train: G_Loss: 1.290, D_unsup_loss_real: 0.588, D_unsup_loss_fake: 0.560, D_sup_loss: 0.111, D_sup_acc: 97.22 Train acc: 97.043 Test acc: 97.300 \n",
      "step: 9332 | Train: G_Loss: 1.378, D_unsup_loss_real: 0.557, D_unsup_loss_fake: 0.519, D_sup_loss: 0.109, D_sup_acc: 97.33 Train acc: 97.048 Test acc: 97.310 \n",
      "step: 9333 | Train: G_Loss: 1.272, D_unsup_loss_real: 0.530, D_unsup_loss_fake: 0.497, D_sup_loss: 0.107, D_sup_acc: 97.34 Train acc: 96.907 Test acc: 97.190 \n",
      "step: 9334 | Train: G_Loss: 1.378, D_unsup_loss_real: 0.508, D_unsup_loss_fake: 0.539, D_sup_loss: 0.114, D_sup_acc: 97.23 Train acc: 96.852 Test acc: 97.210 \n",
      "step: 9335 | Train: G_Loss: 1.294, D_unsup_loss_real: 0.572, D_unsup_loss_fake: 0.513, D_sup_loss: 0.112, D_sup_acc: 97.25 Train acc: 96.550 Test acc: 96.840 \n",
      "step: 9336 | Train: G_Loss: 1.301, D_unsup_loss_real: 0.504, D_unsup_loss_fake: 0.636, D_sup_loss: 0.126, D_sup_acc: 96.88 Train acc: 96.948 Test acc: 97.110 \n",
      "step: 9337 | Train: G_Loss: 1.284, D_unsup_loss_real: 0.622, D_unsup_loss_fake: 0.547, D_sup_loss: 0.115, D_sup_acc: 97.15 Train acc: 96.745 Test acc: 97.040 \n",
      "step: 9338 | Train: G_Loss: 1.395, D_unsup_loss_real: 0.496, D_unsup_loss_fake: 0.487, D_sup_loss: 0.119, D_sup_acc: 97.08 Train acc: 96.850 Test acc: 97.020 \n",
      "step: 9339 | Train: G_Loss: 1.279, D_unsup_loss_real: 0.536, D_unsup_loss_fake: 0.577, D_sup_loss: 0.115, D_sup_acc: 97.06 Train acc: 96.687 Test acc: 96.990 \n",
      "step: 9340 | Train: G_Loss: 1.330, D_unsup_loss_real: 0.697, D_unsup_loss_fake: 0.587, D_sup_loss: 0.122, D_sup_acc: 97.03 Train acc: 97.017 Test acc: 97.330 \n",
      "step: 9341 | Train: G_Loss: 1.211, D_unsup_loss_real: 0.562, D_unsup_loss_fake: 0.541, D_sup_loss: 0.109, D_sup_acc: 97.36 Train acc: 96.915 Test acc: 97.210 \n",
      "step: 9342 | Train: G_Loss: 1.252, D_unsup_loss_real: 0.512, D_unsup_loss_fake: 0.549, D_sup_loss: 0.121, D_sup_acc: 97.25 Train acc: 96.982 Test acc: 97.280 \n",
      "step: 9343 | Train: G_Loss: 1.276, D_unsup_loss_real: 0.553, D_unsup_loss_fake: 0.596, D_sup_loss: 0.115, D_sup_acc: 97.31 Train acc: 96.617 Test acc: 96.940 \n",
      "step: 9344 | Train: G_Loss: 1.234, D_unsup_loss_real: 0.560, D_unsup_loss_fake: 0.586, D_sup_loss: 0.128, D_sup_acc: 96.98 Train acc: 96.567 Test acc: 96.770 \n",
      "step: 9345 | Train: G_Loss: 1.194, D_unsup_loss_real: 0.544, D_unsup_loss_fake: 0.600, D_sup_loss: 0.132, D_sup_acc: 96.81 Train acc: 96.745 Test acc: 96.990 \n",
      "step: 9346 | Train: G_Loss: 1.397, D_unsup_loss_real: 0.527, D_unsup_loss_fake: 0.505, D_sup_loss: 0.123, D_sup_acc: 97.03 Train acc: 96.565 Test acc: 96.670 \n",
      "step: 9347 | Train: G_Loss: 1.218, D_unsup_loss_real: 0.574, D_unsup_loss_fake: 0.565, D_sup_loss: 0.128, D_sup_acc: 96.71 Train acc: 96.663 Test acc: 96.990 \n",
      "step: 9348 | Train: G_Loss: 1.343, D_unsup_loss_real: 0.508, D_unsup_loss_fake: 0.534, D_sup_loss: 0.127, D_sup_acc: 97.03 Train acc: 96.830 Test acc: 97.050 \n",
      "step: 9349 | Train: G_Loss: 1.236, D_unsup_loss_real: 0.503, D_unsup_loss_fake: 0.570, D_sup_loss: 0.117, D_sup_acc: 97.09 Train acc: 96.952 Test acc: 97.330 \n",
      "step: 9350 | Train: G_Loss: 1.506, D_unsup_loss_real: 0.530, D_unsup_loss_fake: 0.600, D_sup_loss: 0.112, D_sup_acc: 97.36 Train acc: 97.005 Test acc: 97.270 \n",
      "step: 9351 | Train: G_Loss: 1.283, D_unsup_loss_real: 0.549, D_unsup_loss_fake: 0.533, D_sup_loss: 0.111, D_sup_acc: 97.30 Train acc: 97.020 Test acc: 97.260 \n",
      "step: 9352 | Train: G_Loss: 1.256, D_unsup_loss_real: 0.587, D_unsup_loss_fake: 0.522, D_sup_loss: 0.113, D_sup_acc: 97.29 Train acc: 97.172 Test acc: 97.500 \n",
      "step: 9353 | Train: G_Loss: 1.239, D_unsup_loss_real: 0.579, D_unsup_loss_fake: 0.531, D_sup_loss: 0.106, D_sup_acc: 97.53 Train acc: 96.938 Test acc: 97.140 \n",
      "step: 9354 | Train: G_Loss: 1.366, D_unsup_loss_real: 0.501, D_unsup_loss_fake: 0.507, D_sup_loss: 0.117, D_sup_acc: 97.18 Train acc: 96.905 Test acc: 97.140 \n",
      "step: 9355 | Train: G_Loss: 1.296, D_unsup_loss_real: 0.523, D_unsup_loss_fake: 0.539, D_sup_loss: 0.115, D_sup_acc: 97.18 Train acc: 96.587 Test acc: 96.840 \n",
      "step: 9356 | Train: G_Loss: 1.229, D_unsup_loss_real: 0.550, D_unsup_loss_fake: 0.529, D_sup_loss: 0.128, D_sup_acc: 96.88 Train acc: 96.895 Test acc: 97.150 \n",
      "step: 9357 | Train: G_Loss: 1.223, D_unsup_loss_real: 0.580, D_unsup_loss_fake: 0.599, D_sup_loss: 0.116, D_sup_acc: 97.19 Train acc: 96.973 Test acc: 97.310 \n",
      "step: 9358 | Train: G_Loss: 1.294, D_unsup_loss_real: 0.556, D_unsup_loss_fake: 0.609, D_sup_loss: 0.114, D_sup_acc: 97.34 Train acc: 97.057 Test acc: 97.340 \n",
      "step: 9359 | Train: G_Loss: 1.283, D_unsup_loss_real: 0.582, D_unsup_loss_fake: 0.604, D_sup_loss: 0.109, D_sup_acc: 97.37 Train acc: 96.715 Test acc: 97.010 \n",
      "step: 9360 | Train: G_Loss: 1.347, D_unsup_loss_real: 0.525, D_unsup_loss_fake: 0.598, D_sup_loss: 0.127, D_sup_acc: 97.05 Train acc: 96.837 Test acc: 97.190 \n"
     ]
    }
   ],
   "source": [
    "train(generator_model, unsupervised_model, supervised_model, gan_model, \n",
    "      dataset, dataset_test, latent_dim=100, \n",
    "      n_epochs=20, n_batch=128, percent_samples=labeled_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaaf366",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "209c440b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1) (60000,) (10000, 28, 28, 1) (10000,)\n"
     ]
    }
   ],
   "source": [
    "dataset, dataset_test = load_real_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "540b0d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08fceaa9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# supervised_model = load_model(f'{LOG_PATH}supervised_model_14000.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1db02aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Classifier Accuracy: 96.837%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = dataset\n",
    "_, acc = supervised_model.evaluate(X_train, y_train, verbose=0)\n",
    "print('Train Classifier Accuracy: %.3f%%\\n' % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6cbae68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Classifier Accuracy: 97.190%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = dataset_test\n",
    "_, acc = supervised_model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test Classifier Accuracy: %.3f%%\\n' % (acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384510a7",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f60143bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99c84f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_file = pd.read_csv(f\"{LOG_PATH}SSL_GAN.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ddc965f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>generator_loss</th>\n",
       "      <th>unsupervised_real_loss</th>\n",
       "      <th>unsupervised_fake_loss</th>\n",
       "      <th>supervised_loss</th>\n",
       "      <th>supervised_acc</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.095382</td>\n",
       "      <td>0.095556</td>\n",
       "      <td>2.398630</td>\n",
       "      <td>2.299938</td>\n",
       "      <td>8.593750</td>\n",
       "      <td>2.293584</td>\n",
       "      <td>2.293459</td>\n",
       "      <td>8.728334</td>\n",
       "      <td>8.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.095508</td>\n",
       "      <td>0.093782</td>\n",
       "      <td>2.397270</td>\n",
       "      <td>2.293458</td>\n",
       "      <td>8.283965</td>\n",
       "      <td>2.285372</td>\n",
       "      <td>2.285137</td>\n",
       "      <td>10.946666</td>\n",
       "      <td>10.590000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.095615</td>\n",
       "      <td>0.091901</td>\n",
       "      <td>2.396449</td>\n",
       "      <td>2.285036</td>\n",
       "      <td>10.643760</td>\n",
       "      <td>2.271984</td>\n",
       "      <td>2.271023</td>\n",
       "      <td>14.051667</td>\n",
       "      <td>13.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.095703</td>\n",
       "      <td>0.090860</td>\n",
       "      <td>2.395925</td>\n",
       "      <td>2.270950</td>\n",
       "      <td>13.823065</td>\n",
       "      <td>2.260263</td>\n",
       "      <td>2.258880</td>\n",
       "      <td>19.248334</td>\n",
       "      <td>19.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.095767</td>\n",
       "      <td>0.089000</td>\n",
       "      <td>2.396179</td>\n",
       "      <td>2.258721</td>\n",
       "      <td>19.944708</td>\n",
       "      <td>2.245254</td>\n",
       "      <td>2.243669</td>\n",
       "      <td>11.240000</td>\n",
       "      <td>11.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9355</th>\n",
       "      <td>1.228951</td>\n",
       "      <td>0.550325</td>\n",
       "      <td>0.529393</td>\n",
       "      <td>0.127977</td>\n",
       "      <td>96.879935</td>\n",
       "      <td>0.125027</td>\n",
       "      <td>0.117541</td>\n",
       "      <td>96.894997</td>\n",
       "      <td>97.149998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9356</th>\n",
       "      <td>1.222783</td>\n",
       "      <td>0.580093</td>\n",
       "      <td>0.598951</td>\n",
       "      <td>0.116113</td>\n",
       "      <td>97.186017</td>\n",
       "      <td>0.123704</td>\n",
       "      <td>0.115099</td>\n",
       "      <td>96.973336</td>\n",
       "      <td>97.310001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9357</th>\n",
       "      <td>1.293612</td>\n",
       "      <td>0.556307</td>\n",
       "      <td>0.608583</td>\n",
       "      <td>0.113708</td>\n",
       "      <td>97.343999</td>\n",
       "      <td>0.119290</td>\n",
       "      <td>0.110706</td>\n",
       "      <td>97.056669</td>\n",
       "      <td>97.340000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9358</th>\n",
       "      <td>1.283313</td>\n",
       "      <td>0.582315</td>\n",
       "      <td>0.604121</td>\n",
       "      <td>0.109379</td>\n",
       "      <td>97.373617</td>\n",
       "      <td>0.136665</td>\n",
       "      <td>0.128676</td>\n",
       "      <td>96.714997</td>\n",
       "      <td>97.009999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9359</th>\n",
       "      <td>1.347432</td>\n",
       "      <td>0.524664</td>\n",
       "      <td>0.597738</td>\n",
       "      <td>0.127146</td>\n",
       "      <td>97.047788</td>\n",
       "      <td>0.128234</td>\n",
       "      <td>0.119210</td>\n",
       "      <td>96.836668</td>\n",
       "      <td>97.189999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9360 rows  9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      generator_loss  unsupervised_real_loss  unsupervised_fake_loss  \\\n",
       "0           0.095382                0.095556                2.398630   \n",
       "1           0.095508                0.093782                2.397270   \n",
       "2           0.095615                0.091901                2.396449   \n",
       "3           0.095703                0.090860                2.395925   \n",
       "4           0.095767                0.089000                2.396179   \n",
       "...              ...                     ...                     ...   \n",
       "9355        1.228951                0.550325                0.529393   \n",
       "9356        1.222783                0.580093                0.598951   \n",
       "9357        1.293612                0.556307                0.608583   \n",
       "9358        1.283313                0.582315                0.604121   \n",
       "9359        1.347432                0.524664                0.597738   \n",
       "\n",
       "      supervised_loss  supervised_acc  train_loss  test_loss  train_acc  \\\n",
       "0            2.299938        8.593750    2.293584   2.293459   8.728334   \n",
       "1            2.293458        8.283965    2.285372   2.285137  10.946666   \n",
       "2            2.285036       10.643760    2.271984   2.271023  14.051667   \n",
       "3            2.270950       13.823065    2.260263   2.258880  19.248334   \n",
       "4            2.258721       19.944708    2.245254   2.243669  11.240000   \n",
       "...               ...             ...         ...        ...        ...   \n",
       "9355         0.127977       96.879935    0.125027   0.117541  96.894997   \n",
       "9356         0.116113       97.186017    0.123704   0.115099  96.973336   \n",
       "9357         0.113708       97.343999    0.119290   0.110706  97.056669   \n",
       "9358         0.109379       97.373617    0.136665   0.128676  96.714997   \n",
       "9359         0.127146       97.047788    0.128234   0.119210  96.836668   \n",
       "\n",
       "       test_acc  \n",
       "0      8.250000  \n",
       "1     10.590000  \n",
       "2     13.840000  \n",
       "3     19.900000  \n",
       "4     11.480000  \n",
       "...         ...  \n",
       "9355  97.149998  \n",
       "9356  97.310001  \n",
       "9357  97.340000  \n",
       "9358  97.009999  \n",
       "9359  97.189999  \n",
       "\n",
       "[9360 rows x 9 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_file = results_file.iloc[:,1:]\n",
    "log_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce28f6b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "log_file.iloc[:, [0,1,2,3]].plot(figsize=(12,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8116fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "log_file.iloc[:, [5,6]].plot(figsize=(12,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f162f703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "log_file.iloc[:, [7,8]].plot(figsize=(12,8), ylim=(0,100), yticks=range(0,110,10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
